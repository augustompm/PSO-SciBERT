paper_id;title;abstract;author_keywords;index_keywords;combined_text;ml_paradigm_id;ml_paradigm_name;ml_paradigm_subcategories;disaster_type_id;disaster_type_name;management_phase_id;management_phase_name
1;Prediction of reservoir water levels via an improved attention mechanism based on CNN − LSTM;Water level prediction is crucial for flood control scheduling and water resource management. The application of various deep learning methods to water level prediction in reservoirs is limited. Accurate water level prediction aids in optimizing reservoir operation strategies, ensuring flood safety downstream and meeting water supply demands. To achieve accurate predictions, a new structure based on a convolutional neural network − long short-term memory (CNN − LSTM) model is proposed, which incorporates a self-attention mechanism and a local attention mechanism in an SL − CNN − LSTM coupled model. Using the Three Gorges Reservoir head area in China as a case study, hydrometeorological data from three points in the reservoir's head area and upstream water level characteristics are used as input variables. Data collected every six hours from 2008 to 2021 were used, with the model trained and tested at an 8:2 ratio. The study revealed that a two-layer CNN configuration performed best in most models. The SL − CNN − LSTM-2 model achieved the best performance across all the metrics, with an R2 of 0.9988, an MAE of 0.2767, an RMSE of 0.3404, and a MAPE of 0.1717, particularly for extreme water level predictions with minimal residuals, validating its strong ability to balance long- and short-term dependencies. Additionally, the model effectively extracts features and captures critical information in time series data, balancing learning capacity and computational efficiency. The research results are highly important for water resource management in large reservoirs, providing reliable technical support for flood control scheduling and water resource optimization. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.;"Attention mechanism; CNN − LSTM; Extreme water level; Three Gorges Reservoir; Water level prediction";"Convolutional neural networks; Deep neural networks; Enterprise resource management; Enterprise resource planning; Information management; Long short-term memory; Multilayer neural networks; Petroleum reservoir evaluation; Prediction models; Reservoirs (water); Resource allocation; Water management; Attention mechanisms; Convolutional neural network; Convolutional neural network − long short-term memory; Extreme water level; Prediction of reservoir; Scheduling resources; Short term memory; Three Gorge reservoir; Water level prediction; Water resources management; Rivers";"Prediction of reservoir water levels via an improved attention mechanism based on CNN − LSTM Water level prediction is crucial for flood control scheduling and water resource management. The application of various deep learning methods to water level prediction in reservoirs is limited. Accurate water level prediction aids in optimizing reservoir operation strategies, ensuring flood safety downstream and meeting water supply demands. To achieve accurate predictions, a new structure based on a convolutional neural network − long short-term memory (CNN − LSTM) model is proposed, which incorporates a self-attention mechanism and a local attention mechanism in an SL − CNN − LSTM coupled model. Using the Three Gorges Reservoir head area in China as a case study, hydrometeorological data from three points in the reservoir's head area and upstream water level characteristics are used as input variables. Data collected every six hours from 2008 to 2021 were used, with the model trained and tested at an 8:2 ratio. The study revealed that a two-layer CNN configuration performed best in most models. The SL − CNN − LSTM-2 model achieved the best performance across all the metrics, with an R2 of 0.9988, an MAE of 0.2767, an RMSE of 0.3404, and a MAPE of 0.1717, particularly for extreme water level predictions with minimal residuals, validating its strong ability to balance long- and short-term dependencies. Additionally, the model effectively extracts features and captures critical information in time series data, balancing learning capacity and computational efficiency. The research results are highly important for water resource management in large reservoirs, providing reliable technical support for flood control scheduling and water resource optimization. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025. Attention mechanism; CNN − LSTM; Extreme water level; Three Gorges Reservoir; Water level prediction Convolutional neural networks; Deep neural networks; Enterprise resource management; Enterprise resource planning; Information management; Long short-term memory; Multilayer neural networks; Petroleum reservoir evaluation; Prediction models; Reservoirs (water); Resource allocation; Water management; Attention mechanisms; Convolutional neural network; Convolutional neural network − long short-term memory; Extreme water level; Prediction of reservoir; Scheduling resources; Short term memory; Three Gorge reservoir; Water level prediction; Water resources management; Rivers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
2;Developing ground motion prediction models for West Java: A machine learning approach to support Indonesia's earthquake early warning system;Indonesia, one of the most earthquake-prone countries in the world, is currently developing an Earthquake Early Warning (EEW) system. A key component of this system, the Regional EEW, relies on Ground Motion Prediction models (GMPMs) to issue end-user alerts. However, in West Java, one of the pilot regions for this project, there is a lack of region-specific GMPMs essential for accurate early warnings. Traditionally, GMPMs are developed using linear regression based on complex, predefined mathematical equations and coefficients. However, Machine learning offers the advantages of bypassing the need for predefined equations and effectively capturing the nonlinear behavior present in ground motion data. To address this gap, we evaluated three machine learning algorithms (i.e. Artificial Neural Network [ANN], Gradient Boosting [GB], and Random Forest [RF]) to develop GMPMs for three tectonic categories: shallow-crustal, interface, and intraslab. These models were used to predict Peak Ground Acceleration (PGA) in West Java, utilizing 3116 strong ground motion records from 365 earthquakes with moment magnitude ranging from 2.4 to 7 and epicentral distance between 5.5 and 867 km, recorded since 2010. Our results show that The Gradient Boosting model outperformed the others across all three tectonic categories, with the lowest Mean Squared Error values (0.94, 0.60, 0.65), and Standard Deviation of Residuals (0.97, 0.77, 0.80), as well as the highest Pearson correlation coefficient-value (0.83, 0.88, 0.90) for shallow-crustal, interface, and intraslab events, respectively, demonstrating strong accuracy in predicting PGA. The model was further validated with recent earthquake data and from 2024 showing good agreement and confirming its robustness. Epicentral Distance and Moment Magnitude were the most influential in predicting PGA among the six explanatory variables used in this study. These findings highlight the potential of machine learning models to improve the accuracy of ground-shaking predictions, contributing to the success of Indonesia's Earthquake Early Warning System (EEWS). © 2024 The Authors;"Earthquake; Ground motion prediction; Indonesia'S earthquake early warning system; Machine learning";"Adaptive boosting; Adversarial machine learning; Earthquake effects; Earthquake engineering; Java programming language; Linear regression; Machine learning; Prediction models; Crustals; Earthquake early warning systems; Gradient boosting; Ground motion prediction; Ground-motion prediction models; Indonesia; Indonesia'S earthquake early warning system; Machine-learning; Peak ground acceleration; West javas; Mean square error";"Developing ground motion prediction models for West Java: A machine learning approach to support Indonesia's earthquake early warning system Indonesia, one of the most earthquake-prone countries in the world, is currently developing an Earthquake Early Warning (EEW) system. A key component of this system, the Regional EEW, relies on Ground Motion Prediction models (GMPMs) to issue end-user alerts. However, in West Java, one of the pilot regions for this project, there is a lack of region-specific GMPMs essential for accurate early warnings. Traditionally, GMPMs are developed using linear regression based on complex, predefined mathematical equations and coefficients. However, Machine learning offers the advantages of bypassing the need for predefined equations and effectively capturing the nonlinear behavior present in ground motion data. To address this gap, we evaluated three machine learning algorithms (i.e. Artificial Neural Network [ANN], Gradient Boosting [GB], and Random Forest [RF]) to develop GMPMs for three tectonic categories: shallow-crustal, interface, and intraslab. These models were used to predict Peak Ground Acceleration (PGA) in West Java, utilizing 3116 strong ground motion records from 365 earthquakes with moment magnitude ranging from 2.4 to 7 and epicentral distance between 5.5 and 867 km, recorded since 2010. Our results show that The Gradient Boosting model outperformed the others across all three tectonic categories, with the lowest Mean Squared Error values (0.94, 0.60, 0.65), and Standard Deviation of Residuals (0.97, 0.77, 0.80), as well as the highest Pearson correlation coefficient-value (0.83, 0.88, 0.90) for shallow-crustal, interface, and intraslab events, respectively, demonstrating strong accuracy in predicting PGA. The model was further validated with recent earthquake data and from 2024 showing good agreement and confirming its robustness. Epicentral Distance and Moment Magnitude were the most influential in predicting PGA among the six explanatory variables used in this study. These findings highlight the potential of machine learning models to improve the accuracy of ground-shaking predictions, contributing to the success of Indonesia's Earthquake Early Warning System (EEWS). © 2024 The Authors Earthquake; Ground motion prediction; Indonesia'S earthquake early warning system; Machine learning Adaptive boosting; Adversarial machine learning; Earthquake effects; Earthquake engineering; Java programming language; Linear regression; Machine learning; Prediction models; Crustals; Earthquake early warning systems; Gradient boosting; Ground motion prediction; Ground-motion prediction models; Indonesia; Indonesia'S earthquake early warning system; Machine-learning; Peak ground acceleration; West javas; Mean square error";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
3;Post-earthquake functionality and resilience prediction of bridge networks based on data-driven machine learning method;Earthquake-induced bridge damage can disrupt transportation networks, potentially hindering emergency response and post-disaster recovery efforts, and posing public safety risks in affected areas. Rapid and accurate assessment of post-earthquake resilience of bridge networks is crucial for evaluating urban seismic performance. Traditional resilience assessment methods, constrained by complex traffic distribution processes, struggle to quickly evaluate the traffic performance of bridge networks during the post-earthquake recovery period. This paper presents a two-layer stacking ensemble model for predicting the functionality and resilience of bridge networks. The first layer integrates advantages of four base learners, including random forest (RF), artificial neural network (ANN), convolutional neural network (CNN), and extreme gradient boosting (XGBoost). The second layer completes regression of functionality based on a support vector machine (SVM). Bayesian optimization and 5-fold cross-validation are employed for hyperparameter tuning of the ensemble model. Finally, the proposed model is validated using the Sioux-Falls bridge network. Results demonstrate that the developed model provides rapid predictions of post-earthquake network functionality and resilience. Additionally, this model can guide post-earthquake repair decisions and assist in optimizing the allocation of regional repair resources. © 2024;"Bridge networks; Data-driven prediction; Seismic resilience; Stacking ensemble";"Adaptive boosting; Disaster prevention; Earthquake effects; Earthquake engineering; Emergency services; Markov processes; Multilayer neural networks; Risk assessment; Support vector regression; Urban transportation; Bridge damage; Bridge networks; Data driven; Data-driven prediction; Ensemble models; Machine learning methods; Network-based; Seismic resilience; Stacking ensemble; Stackings; accuracy assessment; artificial neural network; bridge; dynamic analysis; dynamic response; earthquake engineering; earthquake mechanism; machine learning; optimization; prediction; repair; structural analysis; structural response; Convolutional neural networks";"Post-earthquake functionality and resilience prediction of bridge networks based on data-driven machine learning method Earthquake-induced bridge damage can disrupt transportation networks, potentially hindering emergency response and post-disaster recovery efforts, and posing public safety risks in affected areas. Rapid and accurate assessment of post-earthquake resilience of bridge networks is crucial for evaluating urban seismic performance. Traditional resilience assessment methods, constrained by complex traffic distribution processes, struggle to quickly evaluate the traffic performance of bridge networks during the post-earthquake recovery period. This paper presents a two-layer stacking ensemble model for predicting the functionality and resilience of bridge networks. The first layer integrates advantages of four base learners, including random forest (RF), artificial neural network (ANN), convolutional neural network (CNN), and extreme gradient boosting (XGBoost). The second layer completes regression of functionality based on a support vector machine (SVM). Bayesian optimization and 5-fold cross-validation are employed for hyperparameter tuning of the ensemble model. Finally, the proposed model is validated using the Sioux-Falls bridge network. Results demonstrate that the developed model provides rapid predictions of post-earthquake network functionality and resilience. Additionally, this model can guide post-earthquake repair decisions and assist in optimizing the allocation of regional repair resources. © 2024 Bridge networks; Data-driven prediction; Seismic resilience; Stacking ensemble Adaptive boosting; Disaster prevention; Earthquake effects; Earthquake engineering; Emergency services; Markov processes; Multilayer neural networks; Risk assessment; Support vector regression; Urban transportation; Bridge damage; Bridge networks; Data driven; Data-driven prediction; Ensemble models; Machine learning methods; Network-based; Seismic resilience; Stacking ensemble; Stackings; accuracy assessment; artificial neural network; bridge; dynamic analysis; dynamic response; earthquake engineering; earthquake mechanism; machine learning; optimization; prediction; repair; structural analysis; structural response; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
4;Potential for seasonal flood forecasting in West Africa using climate indexes;Floods are among the most devastating natural disasters and are expected to become more severe with changing climate and population growth. Flood forecasting is one of the key components of flood risk reduction. The potential for seasonal flood forecasting through climate indexes has not been studied for West Africa so far. This work investigates how climate indicators can be used to predict in advance, one to several months ahead of the flood season, above or below normal flood discharge in West Africa. Six global and regional climate indexes were screened for their potential to predict flood discharge of 56 river gauging stations across West Africa. Forecasting models are developed, based on simple and multiple linear regressions between climate indexes and annual maximum discharge, and evaluated using the relative operating characteristics and the relative operating levels scores. The western dipole mode index is the most skillful individual climate index for above normal flood prediction. Combining climate indexes via multiple linear regressions outperforms individual climate indexes for both above and below normal flood prediction. The models show forecasting skills for up to 4 months prior to the flood season. Hence, this study opens promising possibilities for seasonal flood forecasting in West Africa. This may help alert disaster reduction agencies of entering a period of an increased chance of flooding and may trigger adequate mitigation measures. © 2022 The Authors. Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd.;"climate indexes; flood occurrence; seasonal flood forecasting; teleconnection; West Africa";"West Africa; flood forecasting; flooding; mitigation; regional climate; risk assessment; teleconnection";"Potential for seasonal flood forecasting in West Africa using climate indexes Floods are among the most devastating natural disasters and are expected to become more severe with changing climate and population growth. Flood forecasting is one of the key components of flood risk reduction. The potential for seasonal flood forecasting through climate indexes has not been studied for West Africa so far. This work investigates how climate indicators can be used to predict in advance, one to several months ahead of the flood season, above or below normal flood discharge in West Africa. Six global and regional climate indexes were screened for their potential to predict flood discharge of 56 river gauging stations across West Africa. Forecasting models are developed, based on simple and multiple linear regressions between climate indexes and annual maximum discharge, and evaluated using the relative operating characteristics and the relative operating levels scores. The western dipole mode index is the most skillful individual climate index for above normal flood prediction. Combining climate indexes via multiple linear regressions outperforms individual climate indexes for both above and below normal flood prediction. The models show forecasting skills for up to 4 months prior to the flood season. Hence, this study opens promising possibilities for seasonal flood forecasting in West Africa. This may help alert disaster reduction agencies of entering a period of an increased chance of flooding and may trigger adequate mitigation measures. © 2022 The Authors. Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd. climate indexes; flood occurrence; seasonal flood forecasting; teleconnection; West Africa West Africa; flood forecasting; flooding; mitigation; regional climate; risk assessment; teleconnection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
5;Interpretable machine learning models and decision-making mechanisms for landslide hazard assessment under different rainfall conditions;Standard machine learning methods often fail to accurately and timely predict rainfall-induced landslides due to their limited consideration of rainfall and other triggering factors. This study introduces an innovative landslide susceptibility assessment method, extending analysis from the regional to individual slope unit level. By examining 1,164 landslides in Fengjie County since 2010 and incorporating precipitation data from 1 to 13 days prior to each event, we identified two key rainfall-induced factors: daily rainfall (DR) and early effective rainfall (ER). Machine learning samples were generated by simulating rainfall data from non-landslide units. During model training, CatBoost emerged as the superior model due to its exceptional handling of categorical features and demonstrated efficiency and robustness across 100 simulated datasets. SHAP analysis reveals that DR and ER are the primary factors influencing the model, with their contributions to landslide occurrence transitioning from inhibitory to promotive within a specific value range as they increase. Visualization using ArcGIS Pro highlighted significant differences in slope unit susceptibility under various rainfall conditions, clearly showing risk levels. These findings provide a valuable foundation for early warning systems and predictions of rainfall-induced landslides, potentially improving disaster preparedness and mitigation strategies. © 2025 Elsevier Ltd;"CatBoost regression model; Different rainfall conditions; Landslide hazard; SHAP";"Catboost regression model; Daily rainfall; Different rainfall condition; Effective rainfall; Landslide hazard; Rainfall condition; Rainfall induced landslides; Regression modelling; SHAP; Slope unit; Adversarial machine learning";"Interpretable machine learning models and decision-making mechanisms for landslide hazard assessment under different rainfall conditions Standard machine learning methods often fail to accurately and timely predict rainfall-induced landslides due to their limited consideration of rainfall and other triggering factors. This study introduces an innovative landslide susceptibility assessment method, extending analysis from the regional to individual slope unit level. By examining 1,164 landslides in Fengjie County since 2010 and incorporating precipitation data from 1 to 13 days prior to each event, we identified two key rainfall-induced factors: daily rainfall (DR) and early effective rainfall (ER). Machine learning samples were generated by simulating rainfall data from non-landslide units. During model training, CatBoost emerged as the superior model due to its exceptional handling of categorical features and demonstrated efficiency and robustness across 100 simulated datasets. SHAP analysis reveals that DR and ER are the primary factors influencing the model, with their contributions to landslide occurrence transitioning from inhibitory to promotive within a specific value range as they increase. Visualization using ArcGIS Pro highlighted significant differences in slope unit susceptibility under various rainfall conditions, clearly showing risk levels. These findings provide a valuable foundation for early warning systems and predictions of rainfall-induced landslides, potentially improving disaster preparedness and mitigation strategies. © 2025 Elsevier Ltd CatBoost regression model; Different rainfall conditions; Landslide hazard; SHAP Catboost regression model; Daily rainfall; Different rainfall condition; Effective rainfall; Landslide hazard; Rainfall condition; Rainfall induced landslides; Regression modelling; SHAP; Slope unit; Adversarial machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
6;Quantitative analysis framework for the benefit-risk feedback system of watershed reservoir groups;The dynamic nature of environmental changes poses significant challenges to watershed management, particularly when there is a lack of objective methodologies for evaluating multiple scenarios within simulations. This deficiency often leads to an inadequate understanding of the benefits and risks associated with reservoir operations, thereby hindering the formulation of scientific decisions. To address the above issues, we have improved the conditional value at risk (ICVaR) and proposed a novel quantitative framework for assessing the complex interplay between benefits and risks. This framework is further enhanced by integrating with the panel vector auto-regression (PVAR), providing a more comprehensive approach to decision-making. Taking the Wudongde, Baihetan, Xiluodu, and Xiangjiaba reservoirs in the lower Jinsha River—collectively known as the Jinxia Reservoir Group—as case studies, a multi-objective optimization operational model is designed to effectively integrate flood control with power generation objectives. The analysis reveals that the flood control and economic operation of the Jinxia Reservoir Group exhibit consistency when encountering floods of design frequency P ≥ 1%. Their competition for the flood-carrying capacity exceeds that for water resources. It is recommended that the focus of joint operation should shift from optimizing water resource allocation to reservoir storage capacity. In terms of universally applicable methodologies, the ICVaR is capable of retaining data fluctuations, effectively leveraging both tail risk and front benefit data. This approach significantly diminishes the evaluation error of operational schemes from 50.16% to below 5%. The quantitative analysis framework adeptly addresses the issue of spurious regression in evaluation indicators, clarifies the feedback response relationship between reservoirs and operational demands, empowers managers to identify key operational nodes and vulnerable links, and facilitates the development of adaptive regulation mechanisms. The findings of this study contribute to evaluating and managing the operational benefits and risks of reservoir groups. © 2025;"Calculation of evaluation indicators for multi-scenario simulation; Improved conditional value at risk (ICVaR); Joint operation of reservoir group; Panel vector auto-regression (PVAR); Watershed management";"China; Jinsha River; Multiple linear regression; Petroleum reservoir evaluation; Reservoirs (water); Resource allocation; Risk analysis; Risk assessment; Risk management; Auto regression; Calculation of evaluation indicator for multi-scenario simulation; Conditional Value-at-Risk; Evaluation indicators; Improved conditional value at risk; Joint operation of reservoir group; Joint operations; Multi scenarios; Panel vector auto-regression; Scenario simulations; Watersheds management; basin management; flood control; power generation; quantitative analysis; regression analysis; resource allocation; scenario analysis; water resource; watershed; Article; climate change; controlled study; environmental change; feedback system; flooding; genetic algorithm; multiobjective optimization; quantitative analysis; resource allocation; river; shipping; water supply; watershed; watershed management; Decision making";"Quantitative analysis framework for the benefit-risk feedback system of watershed reservoir groups The dynamic nature of environmental changes poses significant challenges to watershed management, particularly when there is a lack of objective methodologies for evaluating multiple scenarios within simulations. This deficiency often leads to an inadequate understanding of the benefits and risks associated with reservoir operations, thereby hindering the formulation of scientific decisions. To address the above issues, we have improved the conditional value at risk (ICVaR) and proposed a novel quantitative framework for assessing the complex interplay between benefits and risks. This framework is further enhanced by integrating with the panel vector auto-regression (PVAR), providing a more comprehensive approach to decision-making. Taking the Wudongde, Baihetan, Xiluodu, and Xiangjiaba reservoirs in the lower Jinsha River—collectively known as the Jinxia Reservoir Group—as case studies, a multi-objective optimization operational model is designed to effectively integrate flood control with power generation objectives. The analysis reveals that the flood control and economic operation of the Jinxia Reservoir Group exhibit consistency when encountering floods of design frequency P ≥ 1%. Their competition for the flood-carrying capacity exceeds that for water resources. It is recommended that the focus of joint operation should shift from optimizing water resource allocation to reservoir storage capacity. In terms of universally applicable methodologies, the ICVaR is capable of retaining data fluctuations, effectively leveraging both tail risk and front benefit data. This approach significantly diminishes the evaluation error of operational schemes from 50.16% to below 5%. The quantitative analysis framework adeptly addresses the issue of spurious regression in evaluation indicators, clarifies the feedback response relationship between reservoirs and operational demands, empowers managers to identify key operational nodes and vulnerable links, and facilitates the development of adaptive regulation mechanisms. The findings of this study contribute to evaluating and managing the operational benefits and risks of reservoir groups. © 2025 Calculation of evaluation indicators for multi-scenario simulation; Improved conditional value at risk (ICVaR); Joint operation of reservoir group; Panel vector auto-regression (PVAR); Watershed management China; Jinsha River; Multiple linear regression; Petroleum reservoir evaluation; Reservoirs (water); Resource allocation; Risk analysis; Risk assessment; Risk management; Auto regression; Calculation of evaluation indicator for multi-scenario simulation; Conditional Value-at-Risk; Evaluation indicators; Improved conditional value at risk; Joint operation of reservoir group; Joint operations; Multi scenarios; Panel vector auto-regression; Scenario simulations; Watersheds management; basin management; flood control; power generation; quantitative analysis; regression analysis; resource allocation; scenario analysis; water resource; watershed; Article; climate change; controlled study; environmental change; feedback system; flooding; genetic algorithm; multiobjective optimization; quantitative analysis; resource allocation; river; shipping; water supply; watershed; watershed management; Decision making";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
7;Time reversal imaging and transfer learning for spatial and temporal seismic source location;This article presents the application of Time Reversal Imaging (TRI) and transfer learning methods for spatial and temporal seismic wave location. The study applies the ResNet-50 model, pre-trained on the basis of ImageNet images, and later retrained using seismic wave field component images. The objective of the study was to provide an accurate classification of seismic source areas and determine the temporal localization of seismic events. The research involved training the ResNet-50 model based on datasets of wave field component images obtained through the backpropagation of reversed waveforms in simplified geological models. The classification was evaluated using performance metrics. Additionally, to assess its effectiveness in realistic scenarios the methodology was applied to the complex Marmousi velocity model. The results show that the combined TRI and transfer learning approach is highly effective in classifying seismic source areas. The trained model successfully identifies patterns unique to seismic wave components, enabling precise spatial localization. Additionally, the method accurately determines the focusing time, which is essential for the temporal localization of seismic events. The article includes research on the influence of receiver network geometry on localization outcomes. By examining various receiver configurations, valuable insights have been gained, further improving the practical applicability of the method. The study highlights the potential for further advances by extending the methodology to three-dimensional models, although there remains a need to address various computational challenges. Three-dimensional modeling would enhance the accuracy of source localization, especially in the case of seismic sources characterized by dominant isotropic components. In conclusion, the combination of TRI and transfer learning presents a promising approach for ensuring precise spatial and temporal seismic wave location. This methodology has the potential to enhance seismic monitoring, early warning systems, and make a significant contribution to earthquake engineering and hazard assessment. © 2025 Elsevier Ltd;"Complex geological models; Seismic source location; Time reversal imaging; Transfer learning";"Engineering geology; Geologic models; Seismic response; Complex geological model; Field components; Geological models; Seismic source; Seismic source location; Source area; Sources location; Time reversal imaging; Transfer learning; Wavefields; accuracy assessment; back propagation; complexity; early warning system; earthquake engineering; hazard assessment; image analysis; machine learning; mathematical analysis; numerical model; seismic source; spatiotemporal analysis; three-dimensional modeling; wave field; Seismic waves";"Time reversal imaging and transfer learning for spatial and temporal seismic source location This article presents the application of Time Reversal Imaging (TRI) and transfer learning methods for spatial and temporal seismic wave location. The study applies the ResNet-50 model, pre-trained on the basis of ImageNet images, and later retrained using seismic wave field component images. The objective of the study was to provide an accurate classification of seismic source areas and determine the temporal localization of seismic events. The research involved training the ResNet-50 model based on datasets of wave field component images obtained through the backpropagation of reversed waveforms in simplified geological models. The classification was evaluated using performance metrics. Additionally, to assess its effectiveness in realistic scenarios the methodology was applied to the complex Marmousi velocity model. The results show that the combined TRI and transfer learning approach is highly effective in classifying seismic source areas. The trained model successfully identifies patterns unique to seismic wave components, enabling precise spatial localization. Additionally, the method accurately determines the focusing time, which is essential for the temporal localization of seismic events. The article includes research on the influence of receiver network geometry on localization outcomes. By examining various receiver configurations, valuable insights have been gained, further improving the practical applicability of the method. The study highlights the potential for further advances by extending the methodology to three-dimensional models, although there remains a need to address various computational challenges. Three-dimensional modeling would enhance the accuracy of source localization, especially in the case of seismic sources characterized by dominant isotropic components. In conclusion, the combination of TRI and transfer learning presents a promising approach for ensuring precise spatial and temporal seismic wave location. This methodology has the potential to enhance seismic monitoring, early warning systems, and make a significant contribution to earthquake engineering and hazard assessment. © 2025 Elsevier Ltd Complex geological models; Seismic source location; Time reversal imaging; Transfer learning Engineering geology; Geologic models; Seismic response; Complex geological model; Field components; Geological models; Seismic source; Seismic source location; Source area; Sources location; Time reversal imaging; Transfer learning; Wavefields; accuracy assessment; back propagation; complexity; early warning system; earthquake engineering; hazard assessment; image analysis; machine learning; mathematical analysis; numerical model; seismic source; spatiotemporal analysis; three-dimensional modeling; wave field; Seismic waves";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;2;Preparation
8;Interactions and Driving Force of Land Cover and Ecosystem Service Before and After the Earthquake in Wenchuan County;"The Wenchuan earthquake, an unexpected magnitude 8.0 mega-earthquake that struck on 12 May 2008, significantly changed land cover (LC), particularly affecting vegetation and rock cover. However, the long-term effects of LC changes on ecosystem services (ESs) remain unclear in earthquake-affected regions, especially across different spatial scales. This study, focusing on Wenchuan County, employs a multi-model framework that integrates fractional vegetation coverage (FVC), rock exposure rate (FR), and ecosystem services (ESs), combining correlation analysis, geographically weighted regression (GWR), Self-organizing map (SOM) clustering, and XGBoost-SHAP model, to analyze the spatiotemporal dynamics, interrelationships, and driving mechanisms of land cover (LC) and ESs before and after the earthquake. Results show that: (1) From 2000 to 2020, FVC and FR fluctuated markedly under earthquake influence, with slight declines in habitat quality (HQ) and carbon storage (CS) and notable improvements in soil conservation (SC) and water yield (WY). (2) With increasing elevation, the FVC–CS–SC group exhibited a downward trend and synergy, while the FR–HQ–WY group increased and also showed synergy; trade-offs and synergies became more pronounced at larger scales, displaying strong spatiotemporal heterogeneity. (3) Elevation (explaining 10–60% of variance) was the main driver for LC and ESs, with land use, slope, human activities, climate, and geological conditions significantly impacting individual indicators. At the same time, the existing geological hazard points are mainly concentrated along both sides of the river valleys, which may be associated with intensified human–land conflicts. These findings offer valuable insights into ecological restoration and sustainable development in earthquake-affected regions. © 2025 by the authors.";"driving forces; earthquake; ecosystem services; interactions; land cover";"China; Sichuan; Wenchuan; carbon storage; ecosystem service; habitat quality; land cover; regression analysis; restoration ecology; self organizing map; Sichuan earthquake 2008; soil conservation; vegetation cover; water yield";"Interactions and Driving Force of Land Cover and Ecosystem Service Before and After the Earthquake in Wenchuan County The Wenchuan earthquake, an unexpected magnitude 8.0 mega-earthquake that struck on 12 May 2008, significantly changed land cover (LC), particularly affecting vegetation and rock cover. However, the long-term effects of LC changes on ecosystem services (ESs) remain unclear in earthquake-affected regions, especially across different spatial scales. This study, focusing on Wenchuan County, employs a multi-model framework that integrates fractional vegetation coverage (FVC), rock exposure rate (FR), and ecosystem services (ESs), combining correlation analysis, geographically weighted regression (GWR), Self-organizing map (SOM) clustering, and XGBoost-SHAP model, to analyze the spatiotemporal dynamics, interrelationships, and driving mechanisms of land cover (LC) and ESs before and after the earthquake. Results show that: (1) From 2000 to 2020, FVC and FR fluctuated markedly under earthquake influence, with slight declines in habitat quality (HQ) and carbon storage (CS) and notable improvements in soil conservation (SC) and water yield (WY). (2) With increasing elevation, the FVC–CS–SC group exhibited a downward trend and synergy, while the FR–HQ–WY group increased and also showed synergy; trade-offs and synergies became more pronounced at larger scales, displaying strong spatiotemporal heterogeneity. (3) Elevation (explaining 10–60% of variance) was the main driver for LC and ESs, with land use, slope, human activities, climate, and geological conditions significantly impacting individual indicators. At the same time, the existing geological hazard points are mainly concentrated along both sides of the river valleys, which may be associated with intensified human–land conflicts. These findings offer valuable insights into ecological restoration and sustainable development in earthquake-affected regions. © 2025 by the authors. driving forces; earthquake; ecosystem services; interactions; land cover China; Sichuan; Wenchuan; carbon storage; ecosystem service; habitat quality; land cover; regression analysis; restoration ecology; self organizing map; Sichuan earthquake 2008; soil conservation; vegetation cover; water yield";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
9;Multi-strategy improved snow ablation optimizer: a case study of optimization of kernel extreme learning machine for flood prediction;The Kernel Extreme Learning Machine (KELM) has the advantage of automatically extracting data features, learning and processing nonlinear problems from historical data, which can help achieve better prediction results for flood prediction problems with complex and sudden causes. Traditional flood disaster prediction usually only considers one influencing factor without considering the complex factors that affect flood occurrence. This article develops a new method for predicting the probability of flood occurrence based on 20 influencing factors. Firstly, in order to better utilize KELM performance, an improved snow ablation optimization algorithm (MESAO) was proposed for subsequent experiments by introducing a level based selection pressure mechanism, covariance matrix learning strategy, historical position based boundary adjustment strategy, and random centroid reverse learning strategy into snow ablation optimization (SAO). Secondly, MESAO is used to perform hyperparameter optimization on the regularization coefficient C and kernel function parameter S of the KELM model. Finally, the construction of a multi feature input–output model for the application of MESAO-KELM in flood prediction problems was completed. In terms of hyperparameter optimization, the numerical experimental results of this method were superior to the prediction results of 10 other intelligent algorithms and 5 regression prediction models. According to the evaluation index results, the best adaptability of MESAO optimized KELM and higher prediction accuracy and stability compared to other prediction models were demonstrated. This method overcomes the limitations of traditional prediction models based on a single influencing factor and can predict the probability of flood occurrence based on complex and variable factors. It can be said that MESAO-KELM has strong generalization ability. Accurate flood prediction can provide early warning and take measures in advance to protect and reduce the impact of floods on human and social development. © The Author(s) 2025.;"Boundary adjustment; Flood forecasting; Kernel extreme learning machine; Prediction accuracy; Snow ablation optimizer";"Adversarial machine learning; Contrastive Learning; Regression analysis; Snow; Weather forecasting; Boundary adjustment; Flood forecasting; Flood prediction; Kernel extreme learning machine; Learning machines; Optimizers; Prediction accuracy; Prediction modelling; Snow ablation; Snow ablation optimizer; Prediction models";"Multi-strategy improved snow ablation optimizer: a case study of optimization of kernel extreme learning machine for flood prediction The Kernel Extreme Learning Machine (KELM) has the advantage of automatically extracting data features, learning and processing nonlinear problems from historical data, which can help achieve better prediction results for flood prediction problems with complex and sudden causes. Traditional flood disaster prediction usually only considers one influencing factor without considering the complex factors that affect flood occurrence. This article develops a new method for predicting the probability of flood occurrence based on 20 influencing factors. Firstly, in order to better utilize KELM performance, an improved snow ablation optimization algorithm (MESAO) was proposed for subsequent experiments by introducing a level based selection pressure mechanism, covariance matrix learning strategy, historical position based boundary adjustment strategy, and random centroid reverse learning strategy into snow ablation optimization (SAO). Secondly, MESAO is used to perform hyperparameter optimization on the regularization coefficient C and kernel function parameter S of the KELM model. Finally, the construction of a multi feature input–output model for the application of MESAO-KELM in flood prediction problems was completed. In terms of hyperparameter optimization, the numerical experimental results of this method were superior to the prediction results of 10 other intelligent algorithms and 5 regression prediction models. According to the evaluation index results, the best adaptability of MESAO optimized KELM and higher prediction accuracy and stability compared to other prediction models were demonstrated. This method overcomes the limitations of traditional prediction models based on a single influencing factor and can predict the probability of flood occurrence based on complex and variable factors. It can be said that MESAO-KELM has strong generalization ability. Accurate flood prediction can provide early warning and take measures in advance to protect and reduce the impact of floods on human and social development. © The Author(s) 2025. Boundary adjustment; Flood forecasting; Kernel extreme learning machine; Prediction accuracy; Snow ablation optimizer Adversarial machine learning; Contrastive Learning; Regression analysis; Snow; Weather forecasting; Boundary adjustment; Flood forecasting; Flood prediction; Kernel extreme learning machine; Learning machines; Optimizers; Prediction accuracy; Prediction modelling; Snow ablation; Snow ablation optimizer; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
10;Accelerated Artificial Bee Colony Optimization for Cost-Sensitive Neural Networks in Multi-Class Problems;Metaheuristics are advanced problem-solving techniques that develop efficient algorithms to address complex challenges, while neural networks are algorithms inspired by the structure and function of the human brain. Combining these approaches enables the resolution of complex optimization problems that traditional methods struggle to solve. This study presents a novel approach integrating the ABC algorithm with ANNs for weight optimization. The method is further enhanced by vectorization and parallelization techniques on both CPU and GPU to improve computational efficiency. Additionally, this study introduces a cost-sensitive fitness function tailored for multi-class classification to optimize results by considering relationships between target class levels. It validates these advancements in two critical applications: network intrusion detection and earthquake damage estimation. Notably, this study makes a significant contribution to earthquake damage assessment by leveraging machine learning algorithms and metaheuristics to enhance predictive models and decision-making in disaster response. By addressing the dynamic nature of earthquake damage, this research fills a critical gap in existing models and broadens the understanding of how machine learning and metaheuristics can improve disaster response strategies. In both domains, the ABC-ANN implementation yields promising results, particularly in earthquake damage estimation, where the cost-sensitive approach demonstrates satisfactory outcomes in macro-F1 and accuracy. The best results for macro-F1, weighted-F1, and overall accuracy provides best results with the UNSW-NB15 and earthquake datasets, showing values of 64%, 72%, 68%, and 60%, 80%, and 79%, respectively. Comparative performance evaluations reveal that the proposed parallel ABC-ANN model, incorporating the novel cost-sensitive fitness function and enhanced by vectorization and parallelization techniques, significantly reduces training time and outperforms state-of-the-art methods in terms of macro-F1 and accuracy in both network intrusion detection and earthquake damage estimation. © 2025 The Author(s). Expert Systems published by John Wiley & Sons Ltd.;"anomaly detection; artificial bee colony; artificial neural network; cost-sensitive learning; earthquake damage estimation; GPU parallelization; network intrusion detection; swarm intelligence; UNSW-NB15";"Cost estimating; Earthquake effects; Earthquake engineering; Intrusion detection; Problem solving; Swarm intelligence; Anomaly detection; Artificial bee colony; Artificial bees; Cost-sensitive learning; Damage estimation; Earthquake damage estimation; Earthquake damages; GPU parallelization; Network intrusion detection; Neural-networks; Parallelizations; UNSW-nb15; Damage detection";"Accelerated Artificial Bee Colony Optimization for Cost-Sensitive Neural Networks in Multi-Class Problems Metaheuristics are advanced problem-solving techniques that develop efficient algorithms to address complex challenges, while neural networks are algorithms inspired by the structure and function of the human brain. Combining these approaches enables the resolution of complex optimization problems that traditional methods struggle to solve. This study presents a novel approach integrating the ABC algorithm with ANNs for weight optimization. The method is further enhanced by vectorization and parallelization techniques on both CPU and GPU to improve computational efficiency. Additionally, this study introduces a cost-sensitive fitness function tailored for multi-class classification to optimize results by considering relationships between target class levels. It validates these advancements in two critical applications: network intrusion detection and earthquake damage estimation. Notably, this study makes a significant contribution to earthquake damage assessment by leveraging machine learning algorithms and metaheuristics to enhance predictive models and decision-making in disaster response. By addressing the dynamic nature of earthquake damage, this research fills a critical gap in existing models and broadens the understanding of how machine learning and metaheuristics can improve disaster response strategies. In both domains, the ABC-ANN implementation yields promising results, particularly in earthquake damage estimation, where the cost-sensitive approach demonstrates satisfactory outcomes in macro-F1 and accuracy. The best results for macro-F1, weighted-F1, and overall accuracy provides best results with the UNSW-NB15 and earthquake datasets, showing values of 64%, 72%, 68%, and 60%, 80%, and 79%, respectively. Comparative performance evaluations reveal that the proposed parallel ABC-ANN model, incorporating the novel cost-sensitive fitness function and enhanced by vectorization and parallelization techniques, significantly reduces training time and outperforms state-of-the-art methods in terms of macro-F1 and accuracy in both network intrusion detection and earthquake damage estimation. © 2025 The Author(s). Expert Systems published by John Wiley & Sons Ltd. anomaly detection; artificial bee colony; artificial neural network; cost-sensitive learning; earthquake damage estimation; GPU parallelization; network intrusion detection; swarm intelligence; UNSW-NB15 Cost estimating; Earthquake effects; Earthquake engineering; Intrusion detection; Problem solving; Swarm intelligence; Anomaly detection; Artificial bee colony; Artificial bees; Cost-sensitive learning; Damage estimation; Earthquake damage estimation; Earthquake damages; GPU parallelization; Network intrusion detection; Neural-networks; Parallelizations; UNSW-nb15; Damage detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
11;Non-parametric ground motion model for displacement response spectra and Fling for Himalayan region using machine learning;Displacement response spectra (DRS) are crucial for seismic design as earthquake damage correlates more with displacements than forces. Previous efforts to develop attenuation relations for DRS have been largely approximate. Permanent displacement or Fling poses significant design, repair and rehabilitation challenges. Consideration of DRS and Fling in seismic design and performance assessment necessitates its accurate estimation. This paper presents the two Artificial Neural Network (ANN)-based non-parametric Ground Motion Models (GMMs). The first model predicts DRS for horizontal and vertical spectral ordinates. The second model focuses on predicting the Fling step in fault-parallel, fault-normal and vertical components. Both the models are developed for the Himalayan region. Given the limited availability of recorded data, ground motion recorded in tectonically similar regions is also utilized to develop DRS GMM. The sparsely recorded Fling data in the Himalayan region is supplemented by additional Fling values simulated using a physics-based approach, alongside data recorded from tectonically similar regions. The simulated Fling values are validated against recorded Fling data. The performance of developed GMMs is compared with existing GMMs and seismic codes which demonstrated its satisfactory performance. The correlation coefficient for ordinates of DRS and Fling are reported to be greater than 0.86 and 0.80, respectively. © 2024 Elsevier Ltd;"Artificial neural network; Displacement response spectra; Fling; Mixed-effect regression; Physics-based ground motion simulation";"Earthquake effects; Seismic response; Displacement response spectra; Fling; Ground motion modeling; Ground motions simulations; Mixed effects; Mixed-effect regression; Neural-networks; Nonparametrics; Physic-based ground motion simulation; Physics-based; Seismic design";"Non-parametric ground motion model for displacement response spectra and Fling for Himalayan region using machine learning Displacement response spectra (DRS) are crucial for seismic design as earthquake damage correlates more with displacements than forces. Previous efforts to develop attenuation relations for DRS have been largely approximate. Permanent displacement or Fling poses significant design, repair and rehabilitation challenges. Consideration of DRS and Fling in seismic design and performance assessment necessitates its accurate estimation. This paper presents the two Artificial Neural Network (ANN)-based non-parametric Ground Motion Models (GMMs). The first model predicts DRS for horizontal and vertical spectral ordinates. The second model focuses on predicting the Fling step in fault-parallel, fault-normal and vertical components. Both the models are developed for the Himalayan region. Given the limited availability of recorded data, ground motion recorded in tectonically similar regions is also utilized to develop DRS GMM. The sparsely recorded Fling data in the Himalayan region is supplemented by additional Fling values simulated using a physics-based approach, alongside data recorded from tectonically similar regions. The simulated Fling values are validated against recorded Fling data. The performance of developed GMMs is compared with existing GMMs and seismic codes which demonstrated its satisfactory performance. The correlation coefficient for ordinates of DRS and Fling are reported to be greater than 0.86 and 0.80, respectively. © 2024 Elsevier Ltd Artificial neural network; Displacement response spectra; Fling; Mixed-effect regression; Physics-based ground motion simulation Earthquake effects; Seismic response; Displacement response spectra; Fling; Ground motion modeling; Ground motions simulations; Mixed effects; Mixed-effect regression; Neural-networks; Nonparametrics; Physic-based ground motion simulation; Physics-based; Seismic design";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
12;IsoMapGen: Framework for early prediction of peak ground acceleration using tripartite feature extraction and gated attention model;Time series data associated with seismic activities pose significant challenges in disaster preparedness. These challenges underscore the need for reliable and timely damage assessments, critical for developing effective response strategies. The computation of Peak Ground Acceleration (PGA) is central to these assessments, serving as a crucial element in generating dynamic damage maps essential for managing rescue operations. Traditional approaches usually derive PGA from full-length accelerograms after an event, a process that is often complicated and prone to delays. In this work, Isoseismal Map Generator (IsoMapGen) is an end-to-end deep-learning framework engineered to predict early PGA using the initial few seconds of the primary waveform. This model integrates a novel spatio-temporal learning approach with gated component-wise attention mechanisms to enhance PGA and magnitude predictions for real-time damage mapping. It employs a chained prediction methodology that dynamically updates damage maps in response to incoming seismic data. The waveform, as well as tabular features extracted from the waveform, are passed in the model. The data imbalance in high-magnitude earthquake records of the tabular datasets has been addressed through synthetic data using a Conditional Tabular Generative Adversarial Network (CTGAN). CTGAN's application in generating synthetic earthquake indicator data is largely unexplored. A detailed comparative analysis of IsoMapGen has been designed against established baseline models, highlighting its strong performance in real-time applications. The models’ efficacy was demonstrated by successfully predicting site-specific PGA from early three seconds of ground motion related to three recent earthquakes of magnitude 7.6, 6.1, and 5.8 MJMA, that occurred on January 01, 2024. This represents notable progress in earthquake damage mitigation using early PGA prediction. Furthermore, this work could be utilized for other short-length time series characterization problems. © 2024;"Attention mechanism; Deep learning; Earthquake; Magnitude; Peak ground acceleration";"Adversarial machine learning; Deep learning; Disaster prevention; Disasters; Earthquake effects; Earthquake engineering; Generative adversarial networks; Mapping; Prediction models; Seismic response; Attention mechanisms; Attention model; Damage maps; Deep learning; Early prediction; Features extraction; Magnitude; Peak ground acceleration; Time-series data; Waveforms; earthquake magnitude; earthquake prediction; machine learning; peak acceleration; seismic data; seismicity; Time series";"IsoMapGen: Framework for early prediction of peak ground acceleration using tripartite feature extraction and gated attention model Time series data associated with seismic activities pose significant challenges in disaster preparedness. These challenges underscore the need for reliable and timely damage assessments, critical for developing effective response strategies. The computation of Peak Ground Acceleration (PGA) is central to these assessments, serving as a crucial element in generating dynamic damage maps essential for managing rescue operations. Traditional approaches usually derive PGA from full-length accelerograms after an event, a process that is often complicated and prone to delays. In this work, Isoseismal Map Generator (IsoMapGen) is an end-to-end deep-learning framework engineered to predict early PGA using the initial few seconds of the primary waveform. This model integrates a novel spatio-temporal learning approach with gated component-wise attention mechanisms to enhance PGA and magnitude predictions for real-time damage mapping. It employs a chained prediction methodology that dynamically updates damage maps in response to incoming seismic data. The waveform, as well as tabular features extracted from the waveform, are passed in the model. The data imbalance in high-magnitude earthquake records of the tabular datasets has been addressed through synthetic data using a Conditional Tabular Generative Adversarial Network (CTGAN). CTGAN's application in generating synthetic earthquake indicator data is largely unexplored. A detailed comparative analysis of IsoMapGen has been designed against established baseline models, highlighting its strong performance in real-time applications. The models’ efficacy was demonstrated by successfully predicting site-specific PGA from early three seconds of ground motion related to three recent earthquakes of magnitude 7.6, 6.1, and 5.8 MJMA, that occurred on January 01, 2024. This represents notable progress in earthquake damage mitigation using early PGA prediction. Furthermore, this work could be utilized for other short-length time series characterization problems. © 2024 Attention mechanism; Deep learning; Earthquake; Magnitude; Peak ground acceleration Adversarial machine learning; Deep learning; Disaster prevention; Disasters; Earthquake effects; Earthquake engineering; Generative adversarial networks; Mapping; Prediction models; Seismic response; Attention mechanisms; Attention model; Damage maps; Deep learning; Early prediction; Features extraction; Magnitude; Peak ground acceleration; Time-series data; Waveforms; earthquake magnitude; earthquake prediction; machine learning; peak acceleration; seismic data; seismicity; Time series";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
13;Surrogate-based decision-making for post-earthquake recovery scheduling and resilience assessment of subway systems considering the effect of infrastructure interdependency;This study proposes a method for post-earthquake recovery and seismic resilience assessment of subway systems considering the effect of infrastructure interdependency. A two-stage recovery decision-making model is established. In the pre-earthquake stage, reserved restoration resources are determined by balancing recovery speed and recovery efficiency. In the post-earthquake stage, the optimal restoration schedules are obtained by solving the established functional recovery optimization model using the NSGA-II. A deep learning-based surrogate model for functionality computation is developed to significantly reduce the computational costs in the recovery decision-making and optimization process. A seismic resilience assessment and recovery decision-making process is conducted for a real, large-scale subway system. The results indicate that the subway system exhibits high recovery efficiency in its early-stage recovery and a high level of seismic resilience. The study highlights the significant impact of the municipal power supply system (MPSS) as a critical infrastructure system. The recovery time affected by the MPSS accounts for more than 33% of the total recovery time of the subway system. Ignoring the impact of the MPSS led to an average 12.13% underestimation of the center of resilience. These results underscore the substantial influence of the MPSS on the recovery and seismic resilience of subway systems. © 2024 Elsevier Ltd;"Infrastructure interdependency; Resilience; Subway system; Surrogate model; Urban rail transit";"Earthquake effects; Light rail transit; Mass transportation; Railroad transportation; Subways; Urban transportation; Decisions makings; Infrastructure interdependencies; Power supply; Recovery efficiency; Resilience; Seismic resilience; Subway systems; Supply system; Surrogate modeling; Urban rail transit; Restoration";"Surrogate-based decision-making for post-earthquake recovery scheduling and resilience assessment of subway systems considering the effect of infrastructure interdependency This study proposes a method for post-earthquake recovery and seismic resilience assessment of subway systems considering the effect of infrastructure interdependency. A two-stage recovery decision-making model is established. In the pre-earthquake stage, reserved restoration resources are determined by balancing recovery speed and recovery efficiency. In the post-earthquake stage, the optimal restoration schedules are obtained by solving the established functional recovery optimization model using the NSGA-II. A deep learning-based surrogate model for functionality computation is developed to significantly reduce the computational costs in the recovery decision-making and optimization process. A seismic resilience assessment and recovery decision-making process is conducted for a real, large-scale subway system. The results indicate that the subway system exhibits high recovery efficiency in its early-stage recovery and a high level of seismic resilience. The study highlights the significant impact of the municipal power supply system (MPSS) as a critical infrastructure system. The recovery time affected by the MPSS accounts for more than 33% of the total recovery time of the subway system. Ignoring the impact of the MPSS led to an average 12.13% underestimation of the center of resilience. These results underscore the substantial influence of the MPSS on the recovery and seismic resilience of subway systems. © 2024 Elsevier Ltd Infrastructure interdependency; Resilience; Subway system; Surrogate model; Urban rail transit Earthquake effects; Light rail transit; Mass transportation; Railroad transportation; Subways; Urban transportation; Decisions makings; Infrastructure interdependencies; Power supply; Recovery efficiency; Resilience; Seismic resilience; Subway systems; Supply system; Surrogate modeling; Urban rail transit; Restoration";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
14;Enhancing prediction of wildfire occurrence and behavior in Alaska using spatio-temporal clustering and ensemble machine learning;Wildfires are an integral part of Alaska's ecological landscape, shaping its boreal forests and tundra. However, recent shifts in wildfire frequency, intensity, and seasonality pose unprecedented challenges for fire management in Alaska's remote and ecologically vulnerable regions. This study addresses the challenge of wildfire occurrence and behavior prediction in Alaska by developing a comprehensive framework that leverages satellite-based data, geospatial features, advanced optimization, and machine learning (ML). First, NASA's Fire Information for Resource Management System (FIRMS) dataset spanning +20 years is processed using a spatio-temporal clustering algorithm to create refined wildfire datasets. A sequential Genetic Algorithm (GA) is employed for cost-effective feature selection from 49 geospatial features, including remote sensing and reanalysis data. Histogram Gradient Boosting (HistGB) is then used for predictive modeling of wildfire occurrence, burnt area, and wildfire duration. This ensemble model's performance is benchmarked across four prediction horizons (same-day, +7 days, +30 days, +90 days) and against various conventional ML and deep learning techniques. Results highlight key factors influencing wildfire dynamics in Alaska and demonstrate substantial improvements in prediction accuracy (e.g., an average improvement of 72.62% in wildfire occurrence accuracy regardless of prediction horizon), offering valuable insights for risk assessment and resource allocation in wildfire management in Alaska. © 2024 The Authors;"Alaska; Deep learning; Feature selection; Genetic algorithms; Machine learning; Permafrost lanscape; Wildfires";"Alaska; United States; cluster analysis; genetic algorithm; machine learning; model validation; permafrost; remote sensing; resource allocation; risk assessment; spatiotemporal analysis; wildfire";"Enhancing prediction of wildfire occurrence and behavior in Alaska using spatio-temporal clustering and ensemble machine learning Wildfires are an integral part of Alaska's ecological landscape, shaping its boreal forests and tundra. However, recent shifts in wildfire frequency, intensity, and seasonality pose unprecedented challenges for fire management in Alaska's remote and ecologically vulnerable regions. This study addresses the challenge of wildfire occurrence and behavior prediction in Alaska by developing a comprehensive framework that leverages satellite-based data, geospatial features, advanced optimization, and machine learning (ML). First, NASA's Fire Information for Resource Management System (FIRMS) dataset spanning +20 years is processed using a spatio-temporal clustering algorithm to create refined wildfire datasets. A sequential Genetic Algorithm (GA) is employed for cost-effective feature selection from 49 geospatial features, including remote sensing and reanalysis data. Histogram Gradient Boosting (HistGB) is then used for predictive modeling of wildfire occurrence, burnt area, and wildfire duration. This ensemble model's performance is benchmarked across four prediction horizons (same-day, +7 days, +30 days, +90 days) and against various conventional ML and deep learning techniques. Results highlight key factors influencing wildfire dynamics in Alaska and demonstrate substantial improvements in prediction accuracy (e.g., an average improvement of 72.62% in wildfire occurrence accuracy regardless of prediction horizon), offering valuable insights for risk assessment and resource allocation in wildfire management in Alaska. © 2024 The Authors Alaska; Deep learning; Feature selection; Genetic algorithms; Machine learning; Permafrost lanscape; Wildfires Alaska; United States; cluster analysis; genetic algorithm; machine learning; model validation; permafrost; remote sensing; resource allocation; risk assessment; spatiotemporal analysis; wildfire";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
15;Broadcast Storm Mitigation Routing Method in LEO Satellite Networks Evaluated with SDN;Low Earth Orbit satellite networks are becoming a crucial component of modern communication infrastructure, offering global coverage and lower latency compared to geostationary satellites. However, their dynamic nature and constant movement pose significant challenges to network traffic management, making them vulnerable to broadcast storms, which can lead to severe congestion and even denial of service events. Current solutions for broadcast storm mitigation in terrestrial networks, such as loop prevention mechanisms, are not directly applicable to LEO networks due to their highly dynamic topology. In this paper, we propose a novel approach to prevent broadcast storms in LEO satellite networks, which cluster using Density-Based Spatial Clustering to prevent broadcast storms in LEO satellite networks and then construct Minimum Spanning Tree based on cluster headers. Additionally, we leverage Software-Defined Networking to provide centralized control over the network, enabling real-time routing adjustments and efficient traffic management. © 2024 Copyright held by the owner/author(s).;"Low Earth Orbit satellite networks; Network Routing; Network Security; Software-Defined Networking; Traffic Management";"Air traffic control; Emergency traffic control; Highway administration; Resource allocation; Satellite communication systems; Traffic congestion; Trees (mathematics); Tropics; Broadcast storm; LEO satellite networks; Low earth orbit satellite network; Low earth orbit satellites; Networks security; Routing methods; Routings; Satellite network; Software-defined networkings; Traffic management; Geostationary satellites";"Broadcast Storm Mitigation Routing Method in LEO Satellite Networks Evaluated with SDN Low Earth Orbit satellite networks are becoming a crucial component of modern communication infrastructure, offering global coverage and lower latency compared to geostationary satellites. However, their dynamic nature and constant movement pose significant challenges to network traffic management, making them vulnerable to broadcast storms, which can lead to severe congestion and even denial of service events. Current solutions for broadcast storm mitigation in terrestrial networks, such as loop prevention mechanisms, are not directly applicable to LEO networks due to their highly dynamic topology. In this paper, we propose a novel approach to prevent broadcast storms in LEO satellite networks, which cluster using Density-Based Spatial Clustering to prevent broadcast storms in LEO satellite networks and then construct Minimum Spanning Tree based on cluster headers. Additionally, we leverage Software-Defined Networking to provide centralized control over the network, enabling real-time routing adjustments and efficient traffic management. © 2024 Copyright held by the owner/author(s). Low Earth Orbit satellite networks; Network Routing; Network Security; Software-Defined Networking; Traffic Management Air traffic control; Emergency traffic control; Highway administration; Resource allocation; Satellite communication systems; Traffic congestion; Trees (mathematics); Tropics; Broadcast storm; LEO satellite networks; Low earth orbit satellite network; Low earth orbit satellites; Networks security; Routing methods; Routings; Satellite network; Software-defined networkings; Traffic management; Geostationary satellites";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;-1;NULL
16;A Novel Decision Support System for Early Flood Warning Using Cellular Automata and Support Vector Machine;"Urban flood warning systems are designed to monitor flood propagation on floodplains. These early warning systems are based on accurate runoff and flood spreading prediction, and use (a) history rainfall-runoff information and (b) digital elevation data to predict water depth and flood extent in urban areas. However, these traditional decision support systems (DSSs) cannot be applied to real time processes due to the computational cost required for running two dimensional (2D) hydrodynamic models. In this paper, we propose a novel Support Vector Machine (SVM) ĝ""€ Cellular Automata (CA) based DSS, to deliver (near) real time flood warnings for flood defence managers and public authorities. The proposed DSS was tested and validated on Thamesmead, London. The results in this study indicated that the SVM model accurately predicted discharge compared to measured data, and the CA-based model using irregular big cells showed that the predicted water depth and flood extent were comparable to a commercial 2D hydrodynamic model's simulated results and the predictions only took less than 3 seconds to run for the whole floodplain. The proposed real time DSS could save thousands of lives for high risk flood zones. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.";"Machine Learning; Model Calibration; Model Validation; Rainfall-Runoff Modelling";"Automata theory; Digital storage; Support vector machines; Cellular automatons; Decision supports; Flood plains; Flood warning; Machine-learning; Model calibration; Model validation; Rainfall - Runoff modelling; Support systems; Support vectors machine; Rain";"A Novel Decision Support System for Early Flood Warning Using Cellular Automata and Support Vector Machine Urban flood warning systems are designed to monitor flood propagation on floodplains. These early warning systems are based on accurate runoff and flood spreading prediction, and use (a) history rainfall-runoff information and (b) digital elevation data to predict water depth and flood extent in urban areas. However, these traditional decision support systems (DSSs) cannot be applied to real time processes due to the computational cost required for running two dimensional (2D) hydrodynamic models. In this paper, we propose a novel Support Vector Machine (SVM) ĝ""€ Cellular Automata (CA) based DSS, to deliver (near) real time flood warnings for flood defence managers and public authorities. The proposed DSS was tested and validated on Thamesmead, London. The results in this study indicated that the SVM model accurately predicted discharge compared to measured data, and the CA-based model using irregular big cells showed that the predicted water depth and flood extent were comparable to a commercial 2D hydrodynamic model's simulated results and the predictions only took less than 3 seconds to run for the whole floodplain. The proposed real time DSS could save thousands of lives for high risk flood zones. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. Machine Learning; Model Calibration; Model Validation; Rainfall-Runoff Modelling Automata theory; Digital storage; Support vector machines; Cellular automatons; Decision supports; Flood plains; Flood warning; Machine-learning; Model calibration; Model validation; Rainfall - Runoff modelling; Support systems; Support vectors machine; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
17;Chopped Basalt Fibers Reinforced Mortar for Strengthening the Architectural Heritage;"The high seismic vulnerability of unreinforced masonry buildings urgently calls for researchers to develop sustainable reinforcing methods and materials. This paper presents an innovative lime-based mortar reinforced with randomly oriented basalt fibers for the reinforcement of masonry heritage. The main aim of this study is to understand the effect of the content and the length of basalt fibers on the mortar’s mechanical behavior. As a cementitious material made mostly out of lime, the mortar is chemically compatible with the historical substrate and therefore suitable in cases of restoration works on architectural heritage. Moreover, the chopped basalt fibers are randomly oriented, and this characteristic makes the overall layer effective in all directions, as the state of stress induced by seismic action is directionally undetermined. The newly proposed reinforcement system is characterized by a twofold aspect related to sustainability: 30% of the aggregates composing the mortar mix design is a recycled result of the ruins of the 2009 L’Aquila earthquake, and the chopped fibers are made out of basalt, widely known for its environmentally supportable peculiarity. The study consists of testing samples characterized by two fiber lengths and six fiber contents, along with one set of plain mortar samples. Specimens measuring 160 mm × 40 mm × 40 mm are first tested in a three-point bending (TPB) configuration, aiming to determine the flexural strength and the post-peak capacity through the calculation of the fracture energy. Then, the two broken pieces resulting from the TPB tests, each measuring 80 mm × 40 mm × 40 mm, are tested in splitting and compression, respectively, aiming to compute the tensile and compressive strengths. Finally, to provide a trend for the mortar’s mechanical properties, a regression analysis is performed by fitting the experimental data with simple linear, polynomial, and exponential regression models. Results show that: (i) both fiber content and fiber length are responsible for a linear increase of the flexural strength and the fracture energy; (ii) for both short- and long-fiber mortar samples, the tensile strength and the compressive strength parabolically increase with the fiber content; (iii) the increase in fiber content and fiber length always generates a reduction in the conglomerate workability. The fiber content (FC) optimization with respect to the mechanical properties leads to a basalt FC equal to 1.2% for long-fiber samples and an FC equal to 1.9% for short-fiber ones. © 2025 by the authors.";"analytical formulation; basalt chopped fibers; experimental campaign; lime-based mortar; strengthening systems; vulnerability unreinforced masonry";NULL;"Chopped Basalt Fibers Reinforced Mortar for Strengthening the Architectural Heritage The high seismic vulnerability of unreinforced masonry buildings urgently calls for researchers to develop sustainable reinforcing methods and materials. This paper presents an innovative lime-based mortar reinforced with randomly oriented basalt fibers for the reinforcement of masonry heritage. The main aim of this study is to understand the effect of the content and the length of basalt fibers on the mortar’s mechanical behavior. As a cementitious material made mostly out of lime, the mortar is chemically compatible with the historical substrate and therefore suitable in cases of restoration works on architectural heritage. Moreover, the chopped basalt fibers are randomly oriented, and this characteristic makes the overall layer effective in all directions, as the state of stress induced by seismic action is directionally undetermined. The newly proposed reinforcement system is characterized by a twofold aspect related to sustainability: 30% of the aggregates composing the mortar mix design is a recycled result of the ruins of the 2009 L’Aquila earthquake, and the chopped fibers are made out of basalt, widely known for its environmentally supportable peculiarity. The study consists of testing samples characterized by two fiber lengths and six fiber contents, along with one set of plain mortar samples. Specimens measuring 160 mm × 40 mm × 40 mm are first tested in a three-point bending (TPB) configuration, aiming to determine the flexural strength and the post-peak capacity through the calculation of the fracture energy. Then, the two broken pieces resulting from the TPB tests, each measuring 80 mm × 40 mm × 40 mm, are tested in splitting and compression, respectively, aiming to compute the tensile and compressive strengths. Finally, to provide a trend for the mortar’s mechanical properties, a regression analysis is performed by fitting the experimental data with simple linear, polynomial, and exponential regression models. Results show that: (i) both fiber content and fiber length are responsible for a linear increase of the flexural strength and the fracture energy; (ii) for both short- and long-fiber mortar samples, the tensile strength and the compressive strength parabolically increase with the fiber content; (iii) the increase in fiber content and fiber length always generates a reduction in the conglomerate workability. The fiber content (FC) optimization with respect to the mechanical properties leads to a basalt FC equal to 1.2% for long-fiber samples and an FC equal to 1.9% for short-fiber ones. © 2025 by the authors. analytical formulation; basalt chopped fibers; experimental campaign; lime-based mortar; strengthening systems; vulnerability unreinforced masonry NULL";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
18;Machine Learning Approaches for Early Warning of Tsunami Induced by Volcano Flank Collapse and Implication for Future Risk Management: Case of Anak Krakatau;A tsunami triggered by volcanic collapse is a low-probability but high-impact event. Unlike tsunamis triggered by earthquakes which the mechanism is well understood, volcanic tsunami events have complex trigger mechanisms and occur with little to no warning such as the event in December 2018 of the Anak Krakatau volcano tsunami, making it more difficult to detect and issue a warning. We adopted collapse tsunami machine learning (ML) approach model which does not require source information, to predict maximum tsunami amplitude on four coastal stations. Observations from six synthetic observation stations around Anak Krakatau volcano were used as input for collapse tsunami ML. The 320 collapse scenarios triggering tsunamis with various parameters and directions were generated to train model. To evaluate the accuracy and reliability of the tsunami simulations, we conducted a comparison between the simulated waveforms and those recorded at four coastal stations during the December 2018 event. The RMSE values between predicted and actual (via forward tsunami) of Random Forest model consistently provide the most accurate predictions ranging from 0.0586 to 0.1945 across three out of the four stations. We also applied deep learning algorithms, LSTM, and Complex LSTM to predict tsunami full waveform by using short-duration observation as input. Furthermore, we also pointed out the potential of risk management that can be explored and integrated from results of the maximum tsunami amplitude and arrival time predictions for support decision-making. We suggest that the ML approach could be a good alternative for volcanic tsunamis early warning purposes. © 2025 Elsevier Ltd;"Anak Krakatau; Early warning; Machine learning; Prediction; Volcanic tsunamis";"Anak Krakatau; Krakatau; Lampung; Contrastive Learning; Decision making; Risk management; Anak krakatau; Coastal station; Early warning; Flank collapse; Machine learning approaches; Machine-learning; Risks management; Volcanic tsunami; Volcanics; Volcano flanks; algorithm; arrival time; early warning system; machine learning; prediction; trigger mechanism; tsunami; volcano; Volcanoes";"Machine Learning Approaches for Early Warning of Tsunami Induced by Volcano Flank Collapse and Implication for Future Risk Management: Case of Anak Krakatau A tsunami triggered by volcanic collapse is a low-probability but high-impact event. Unlike tsunamis triggered by earthquakes which the mechanism is well understood, volcanic tsunami events have complex trigger mechanisms and occur with little to no warning such as the event in December 2018 of the Anak Krakatau volcano tsunami, making it more difficult to detect and issue a warning. We adopted collapse tsunami machine learning (ML) approach model which does not require source information, to predict maximum tsunami amplitude on four coastal stations. Observations from six synthetic observation stations around Anak Krakatau volcano were used as input for collapse tsunami ML. The 320 collapse scenarios triggering tsunamis with various parameters and directions were generated to train model. To evaluate the accuracy and reliability of the tsunami simulations, we conducted a comparison between the simulated waveforms and those recorded at four coastal stations during the December 2018 event. The RMSE values between predicted and actual (via forward tsunami) of Random Forest model consistently provide the most accurate predictions ranging from 0.0586 to 0.1945 across three out of the four stations. We also applied deep learning algorithms, LSTM, and Complex LSTM to predict tsunami full waveform by using short-duration observation as input. Furthermore, we also pointed out the potential of risk management that can be explored and integrated from results of the maximum tsunami amplitude and arrival time predictions for support decision-making. We suggest that the ML approach could be a good alternative for volcanic tsunamis early warning purposes. © 2025 Elsevier Ltd Anak Krakatau; Early warning; Machine learning; Prediction; Volcanic tsunamis Anak Krakatau; Krakatau; Lampung; Contrastive Learning; Decision making; Risk management; Anak krakatau; Coastal station; Early warning; Flank collapse; Machine learning approaches; Machine-learning; Risks management; Volcanic tsunami; Volcanics; Volcano flanks; algorithm; arrival time; early warning system; machine learning; prediction; trigger mechanism; tsunami; volcano; Volcanoes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
19;Estimation of high-resolution tsunami inundation depth using deep learning models: Case study of pangandaran, indonesia;Tsunami inundation maps are essential for early warning systems, guiding community evacuations to safe zones. However, producing high-resolution tsunami inundation maps is challenging due to the computational demands of numerical simulation. Thus, we propose a deep learning (DL) approach for real-time high-resolution tsunami inundation depth prediction. Our study focused on Pangandaran, Indonesia, which suffered a devastating tsunami in 2006 and remains at risk from future events in the Sunda Subduction zone. Before real-time application, the DL model requires training with low-resolution tsunami height as inputs and high-resolution inundation maps as outputs. The pre-computed tsunami training models were generated from highly diverse earthquake sources with 1044 scenarios via numerical simulations. We conducted 15 training experiments using a U-Net model with different sample sizes (1044, 800, 600, 400, and 200) and input resolutions (81, 27, and 9 arc-second). The trained U-Net using 1044 samples and 27 arc-second input resolution achieved reasonable accuracy compared to the numerical forward model as a reference, indicated by 0.357 of mean-square-error (MSE), 0.973 of intersection over union (IoU), and 0.983 of F1 score, taking less than 3 min of computational time. A retrospective forecast test for the 2006 Java tsunami demonstrated that the DL model reasonably reconstructs the reference inundation map (MSE of 0.45, IoU score of 0.97, and F1 score of 0.99) and predicts inundation heights comparable to observed values (K number of 1.13). Our proposed U-Net model offers reliable accuracy and computational efficiency, establishing it as a critical tool for timely early warnings. © 2025 The Author(s);"Machine learning; Numerical simulation; Sunda subduction zone; Tsunami early warning; Tsunami inundation";"Adversarial machine learning; Contrastive Learning; Deep learning; Digital elevation model; Risk assessment; Early warning; High resolution; Indonesia; Inundation maps; Learning models; Machine-learning; Subduction zones; Sunda subduction zone; Tsunami early warning; Tsunami inundation; Mean square error";"Estimation of high-resolution tsunami inundation depth using deep learning models: Case study of pangandaran, indonesia Tsunami inundation maps are essential for early warning systems, guiding community evacuations to safe zones. However, producing high-resolution tsunami inundation maps is challenging due to the computational demands of numerical simulation. Thus, we propose a deep learning (DL) approach for real-time high-resolution tsunami inundation depth prediction. Our study focused on Pangandaran, Indonesia, which suffered a devastating tsunami in 2006 and remains at risk from future events in the Sunda Subduction zone. Before real-time application, the DL model requires training with low-resolution tsunami height as inputs and high-resolution inundation maps as outputs. The pre-computed tsunami training models were generated from highly diverse earthquake sources with 1044 scenarios via numerical simulations. We conducted 15 training experiments using a U-Net model with different sample sizes (1044, 800, 600, 400, and 200) and input resolutions (81, 27, and 9 arc-second). The trained U-Net using 1044 samples and 27 arc-second input resolution achieved reasonable accuracy compared to the numerical forward model as a reference, indicated by 0.357 of mean-square-error (MSE), 0.973 of intersection over union (IoU), and 0.983 of F1 score, taking less than 3 min of computational time. A retrospective forecast test for the 2006 Java tsunami demonstrated that the DL model reasonably reconstructs the reference inundation map (MSE of 0.45, IoU score of 0.97, and F1 score of 0.99) and predicts inundation heights comparable to observed values (K number of 1.13). Our proposed U-Net model offers reliable accuracy and computational efficiency, establishing it as a critical tool for timely early warnings. © 2025 The Author(s) Machine learning; Numerical simulation; Sunda subduction zone; Tsunami early warning; Tsunami inundation Adversarial machine learning; Contrastive Learning; Deep learning; Digital elevation model; Risk assessment; Early warning; High resolution; Indonesia; Inundation maps; Learning models; Machine-learning; Subduction zones; Sunda subduction zone; Tsunami early warning; Tsunami inundation; Mean square error";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
20;Permafrost destabilization induced hazard mapping in Himalayas using machine learning methods;Permafrost destabilization induced hazard-susceptibility modeling was performed using the machine-learning models (Random Forest and Logistic Regression) in the Alaknanda basin in Garhwal Himalaya. The machine-learning model was trained using debris flow initiation points, rock glaciers, and major landslides. Twelve independent factors were considered for hazard potential assessment that comprises elevation, slope, aspect, topographic position index, topographic roughness index, topographic wetness index, profile curvature, distance from rivers, distance from roads, lineament density, mean temperature of the warmest quarter, and precipitation. The classification accuracy of the Random Forest and Logistic Regression on the testing dataset was 0.791 and 0.594, respectively. The key findings revealed that Random Forest outperformed Logistic Regression, with Receiver Operating Characteristic curve values of 0.89 and 0.609, respectively. The hazard susceptibility map was presented using a lower (0) to higher risk (1) scale. The very high susceptibility zones of permafrost destabilization induced mass wasting for each model were 7.17 % and 4.02 % of the study area. The results in the form of hazard susceptibility maps have the potential for making mitigation strategies, infrastructure planning, and disaster response in the Alaknanda basin. © 2025 COSPAR;"Debris flow initiation; Landslide; Logistic Regression; Mass wasting; Random Forest; Rock glacier";"Decision trees; Deforestation; Emergency services; Glacial geology; Glaciers; Landslides; Logging (forestry); Logistic regression; Mapping; Random forests; Risk assessment; Debris flow initiation; Debris flows; Hazards mappings; Himalayas; Logistics regressions; Machine learning models; Mass wasting; Random forests; Rock glaciers; Susceptibility maps; Permafrost";"Permafrost destabilization induced hazard mapping in Himalayas using machine learning methods Permafrost destabilization induced hazard-susceptibility modeling was performed using the machine-learning models (Random Forest and Logistic Regression) in the Alaknanda basin in Garhwal Himalaya. The machine-learning model was trained using debris flow initiation points, rock glaciers, and major landslides. Twelve independent factors were considered for hazard potential assessment that comprises elevation, slope, aspect, topographic position index, topographic roughness index, topographic wetness index, profile curvature, distance from rivers, distance from roads, lineament density, mean temperature of the warmest quarter, and precipitation. The classification accuracy of the Random Forest and Logistic Regression on the testing dataset was 0.791 and 0.594, respectively. The key findings revealed that Random Forest outperformed Logistic Regression, with Receiver Operating Characteristic curve values of 0.89 and 0.609, respectively. The hazard susceptibility map was presented using a lower (0) to higher risk (1) scale. The very high susceptibility zones of permafrost destabilization induced mass wasting for each model were 7.17 % and 4.02 % of the study area. The results in the form of hazard susceptibility maps have the potential for making mitigation strategies, infrastructure planning, and disaster response in the Alaknanda basin. © 2025 COSPAR Debris flow initiation; Landslide; Logistic Regression; Mass wasting; Random Forest; Rock glacier Decision trees; Deforestation; Emergency services; Glacial geology; Glaciers; Landslides; Logging (forestry); Logistic regression; Mapping; Random forests; Risk assessment; Debris flow initiation; Debris flows; Hazards mappings; Himalayas; Logistics regressions; Machine learning models; Mass wasting; Random forests; Rock glaciers; Susceptibility maps; Permafrost";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
21;An Automatic Calibration Framework of Storm Water Management Model Based on KPCA-SSA-BPNN and its Application in Urban Stormwater Flood Simulation;The Storm Water Management Model (SWMM) is widely applied to simulate urban flood disasters and water resources management. Accurate parameter calibration is crucial for model performance. To enhance calibration efficiency and accuracy, this study combines the advantages of kernel principal component analysis (KPCA), sparrow search algorithm (SSA), and back propagation neural network (BPNN) to propose a new framework for SWMM model parameter calibration. KPCA extracts key features from high-dimensional calibration data, creating a reduced-dimension training dataset for BPNN. SSA optimizes the initial weights and thresholds of BPNN, leading to a KPCA-SSA-BPNN surrogate model that accurately captures the relationship between SWMM parameters and simulation results. This approach facilitates rapid and precise parameter calibration. A sub-catchment in Zhuzhou City, Hunan Province, China, was selected as the study area, using seven rainfall events for calibration and validation. Compared with manual calibration, standalone BPNN, and other hybrid methods (PSO-BPNN, SSA-BPNN, PCA-PSO-BPNN, PCA-SSA-BPNN, and KPCA-PSO-BPNN), the KPCA-SSA-BPNN method demonstrated superior performance, achieving an effective calibration probability of 93.8%, significantly higher than BPNN’s 40.3%. Furthermore, KPCA-SSA-BPNN consistently yielded higher Nash–Sutcliffe efficiency (NSE) and relative error of node peak water depth (REp) in most scenarios. With an average NSE of 0.913, REp of 2.847%, and a calibration time of 22.7 s, this method demonstrates considerable feasibility and practicality for real-world engineering applications. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2025.;"BPNN; KPCA; Parameter calibration; SSA; SWMM";"China; Hunan; Zhuzhou [Hunan]; Flood damage; Neural networks; Resource allocation; Runoff; Storm sewers; Automatic calibration; Back-propagation neural networks; Kernel principal component analyses (KPCA); Model-based OPC; Modeling parameters; Neural network application; Parameters calibrations; Search Algorithms; Sparrow search algorithm; Stormwater management model(SWMM); algorithm; artificial neural network; automation; back propagation; calibration; computer simulation; numerical model; pattern recognition; principal component analysis; stormwater; water management; Efficiency";"An Automatic Calibration Framework of Storm Water Management Model Based on KPCA-SSA-BPNN and its Application in Urban Stormwater Flood Simulation The Storm Water Management Model (SWMM) is widely applied to simulate urban flood disasters and water resources management. Accurate parameter calibration is crucial for model performance. To enhance calibration efficiency and accuracy, this study combines the advantages of kernel principal component analysis (KPCA), sparrow search algorithm (SSA), and back propagation neural network (BPNN) to propose a new framework for SWMM model parameter calibration. KPCA extracts key features from high-dimensional calibration data, creating a reduced-dimension training dataset for BPNN. SSA optimizes the initial weights and thresholds of BPNN, leading to a KPCA-SSA-BPNN surrogate model that accurately captures the relationship between SWMM parameters and simulation results. This approach facilitates rapid and precise parameter calibration. A sub-catchment in Zhuzhou City, Hunan Province, China, was selected as the study area, using seven rainfall events for calibration and validation. Compared with manual calibration, standalone BPNN, and other hybrid methods (PSO-BPNN, SSA-BPNN, PCA-PSO-BPNN, PCA-SSA-BPNN, and KPCA-PSO-BPNN), the KPCA-SSA-BPNN method demonstrated superior performance, achieving an effective calibration probability of 93.8%, significantly higher than BPNN’s 40.3%. Furthermore, KPCA-SSA-BPNN consistently yielded higher Nash–Sutcliffe efficiency (NSE) and relative error of node peak water depth (REp) in most scenarios. With an average NSE of 0.913, REp of 2.847%, and a calibration time of 22.7 s, this method demonstrates considerable feasibility and practicality for real-world engineering applications. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2025. BPNN; KPCA; Parameter calibration; SSA; SWMM China; Hunan; Zhuzhou [Hunan]; Flood damage; Neural networks; Resource allocation; Runoff; Storm sewers; Automatic calibration; Back-propagation neural networks; Kernel principal component analyses (KPCA); Model-based OPC; Modeling parameters; Neural network application; Parameters calibrations; Search Algorithms; Sparrow search algorithm; Stormwater management model(SWMM); algorithm; artificial neural network; automation; back propagation; calibration; computer simulation; numerical model; pattern recognition; principal component analysis; stormwater; water management; Efficiency";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
22;Flood of techniques and drought of theories: emotion mining in disasters;Emotion mining has become a crucial tool for understanding human emotions during disasters, leveraging the extensive data generated on social media platforms. This paper aims to summarize existing research on emotion mining within disaster contexts, highlighting both significant discoveries and persistent issues. On the one hand, emotion mining techniques have achieved acceptable accuracy enabling applications such as rapid damage assessment and mental health surveillance. On the other hand, with many studies adopting data-driven approaches, several methodological issues remain. These include arbitrary emotion classification, ignoring biases inherent in data collection from social media, such as the overrepresentation of individuals from higher socioeconomic status on Twitter, and the lack of application of theoretical frameworks like cross-cultural comparisons. These problems can be summarized as a notable lack of theory-driven research and ignoring insights from social and behavioral sciences. This paper underscores the need for interdisciplinary collaboration between computer scientists and social scientists to develop more robust and theoretically grounded approaches in emotion mining. By addressing these gaps, we aim to enhance the effectiveness and reliability of emotion mining methodologies, ultimately contributing to improved disaster preparedness, response, and recovery. © The Author(s) 2024.;"Emotion; Emotion mining; Natural disasters; Sentiment analysis; Technological disasters";NULL;"Flood of techniques and drought of theories: emotion mining in disasters Emotion mining has become a crucial tool for understanding human emotions during disasters, leveraging the extensive data generated on social media platforms. This paper aims to summarize existing research on emotion mining within disaster contexts, highlighting both significant discoveries and persistent issues. On the one hand, emotion mining techniques have achieved acceptable accuracy enabling applications such as rapid damage assessment and mental health surveillance. On the other hand, with many studies adopting data-driven approaches, several methodological issues remain. These include arbitrary emotion classification, ignoring biases inherent in data collection from social media, such as the overrepresentation of individuals from higher socioeconomic status on Twitter, and the lack of application of theoretical frameworks like cross-cultural comparisons. These problems can be summarized as a notable lack of theory-driven research and ignoring insights from social and behavioral sciences. This paper underscores the need for interdisciplinary collaboration between computer scientists and social scientists to develop more robust and theoretically grounded approaches in emotion mining. By addressing these gaps, we aim to enhance the effectiveness and reliability of emotion mining methodologies, ultimately contributing to improved disaster preparedness, response, and recovery. © The Author(s) 2024. Emotion; Emotion mining; Natural disasters; Sentiment analysis; Technological disasters NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
23;AI-Driven Global Disaster Intelligence from News Media;Open-source disaster intelligence (OSDI) is crucial for improving situational awareness, disaster preparedness, and real-time decision-making. Traditional OSDI frameworks often rely on social media data, which are susceptible to misinformation and credibility issues. This study proposes a novel AI-driven framework utilizing automated data collection from 444 large-scale online news portals, including CNN, BBC, CBS News, and The Guardian, to enhance data reliability. Over a 514-day period (27 September 2023 to 26 February 2025), 1.25 million news articles were collected, of which 17,884 were autonomously classified as disaster-related using Generative Pre-Trained Transformer (GPT) models. The analysis identified 185 distinct countries and 6068 unique locations, offering unprecedented geospatial and temporal intelligence. Advanced clustering and predictive analytics techniques, including K-means, DBSCAN, seasonal decomposition (STL), Fourier transform, and ARIMA, were employed to detect geographical hotspots, cyclical patterns, and temporal dependencies. The ARIMA (2, 1, 2) model achieved a mean squared error (MSE) of 823,761, demonstrating high predictive accuracy. Key findings highlight that the USA (6548 disasters), India (1393 disasters), and Australia (1260 disasters) are the most disaster-prone countries, while hurricanes/typhoons/cyclones (5227 occurrences), floods (3360 occurrences), and wildfires (2724 occurrences) are the most frequent disaster types. The framework establishes a comprehensive methodology for integrating geospatial clustering, temporal analysis, and multimodal data processing in OSDI. By leveraging AI automation and diverse news sources, this study provides a scalable, adaptable, and ethically robust solution for proactive disaster management, improving global resilience and preparedness. © 2025 by the authors.;"AI; disaster intelligence; geospatial and temporal intelligence; news media data mining; open-source disaster intelligence; predictive disaster modeling";NULL;"AI-Driven Global Disaster Intelligence from News Media Open-source disaster intelligence (OSDI) is crucial for improving situational awareness, disaster preparedness, and real-time decision-making. Traditional OSDI frameworks often rely on social media data, which are susceptible to misinformation and credibility issues. This study proposes a novel AI-driven framework utilizing automated data collection from 444 large-scale online news portals, including CNN, BBC, CBS News, and The Guardian, to enhance data reliability. Over a 514-day period (27 September 2023 to 26 February 2025), 1.25 million news articles were collected, of which 17,884 were autonomously classified as disaster-related using Generative Pre-Trained Transformer (GPT) models. The analysis identified 185 distinct countries and 6068 unique locations, offering unprecedented geospatial and temporal intelligence. Advanced clustering and predictive analytics techniques, including K-means, DBSCAN, seasonal decomposition (STL), Fourier transform, and ARIMA, were employed to detect geographical hotspots, cyclical patterns, and temporal dependencies. The ARIMA (2, 1, 2) model achieved a mean squared error (MSE) of 823,761, demonstrating high predictive accuracy. Key findings highlight that the USA (6548 disasters), India (1393 disasters), and Australia (1260 disasters) are the most disaster-prone countries, while hurricanes/typhoons/cyclones (5227 occurrences), floods (3360 occurrences), and wildfires (2724 occurrences) are the most frequent disaster types. The framework establishes a comprehensive methodology for integrating geospatial clustering, temporal analysis, and multimodal data processing in OSDI. By leveraging AI automation and diverse news sources, this study provides a scalable, adaptable, and ethically robust solution for proactive disaster management, improving global resilience and preparedness. © 2025 by the authors. AI; disaster intelligence; geospatial and temporal intelligence; news media data mining; open-source disaster intelligence; predictive disaster modeling NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;2;Preparation
24;Landslide susceptibility assessment for Uttarakhand, a Himalayan state of India, using multi-criteria decision making, bivariate, and machine learning models;Background: Landslides, among the most catastrophic natural hazards, result from natural and anthropogenic factors, causing substantial financial losses, infrastructural damage, fatalities, and environmental degradation. Uttarakhand, with its unique topographical and hydrological conditions, unplanned human settlements, and changing precipitation patterns, is highly susceptible to landslides. Methods: This study evaluates landslide susceptibility for Uttarakhand, a Himalayan state in India, by employing bivariate analysis, multi-criteria decision-making, and advanced machine learning models, such as Random Forest and Extreme Gradient Boosting (XGBoost). A total of sixteen landslide influencing factors were used for performing landslide hazard susceptibility zonation, including the innovative use of geomorphons for detailed terrain analysis. Results: Approximately 18.47% of the study area was classified as high to very high landslide susceptibility zones, and 21% was classified into the moderate susceptibility category. High to very high susceptibility zones were concentrated in the Uttarkashi, Chamoli, and Pithoragarh districts of the Lesser and Higher Himalayas, areas characterized by rangelands and high annual rainfall. Conversely, very low to low susceptibility zones were predominantly located in the Tarai-Bhabar and Sub-Himalayan districts, including Haridwar and Udham Singh Nagar. The Random Forest and XGBoost models demonstrated superior predictive performance. Conclusions: The spatially explicit landslide susceptibility maps provide critical insights for urban planners, disaster management agencies, and environmentalists, aiding in developing effective strategies for landslide risk reduction and promoting sustainable development in Uttarakhand. This study exemplifies applying advanced analytical techniques to address landslide susceptibility and related soil erosion and water resource management challenges in Uttarakhand. © The Author(s) 2025.;"Bivariate analysis; Landslide susceptibility; Machine learning; MCDA; Uttarakhand";NULL;"Landslide susceptibility assessment for Uttarakhand, a Himalayan state of India, using multi-criteria decision making, bivariate, and machine learning models Background: Landslides, among the most catastrophic natural hazards, result from natural and anthropogenic factors, causing substantial financial losses, infrastructural damage, fatalities, and environmental degradation. Uttarakhand, with its unique topographical and hydrological conditions, unplanned human settlements, and changing precipitation patterns, is highly susceptible to landslides. Methods: This study evaluates landslide susceptibility for Uttarakhand, a Himalayan state in India, by employing bivariate analysis, multi-criteria decision-making, and advanced machine learning models, such as Random Forest and Extreme Gradient Boosting (XGBoost). A total of sixteen landslide influencing factors were used for performing landslide hazard susceptibility zonation, including the innovative use of geomorphons for detailed terrain analysis. Results: Approximately 18.47% of the study area was classified as high to very high landslide susceptibility zones, and 21% was classified into the moderate susceptibility category. High to very high susceptibility zones were concentrated in the Uttarkashi, Chamoli, and Pithoragarh districts of the Lesser and Higher Himalayas, areas characterized by rangelands and high annual rainfall. Conversely, very low to low susceptibility zones were predominantly located in the Tarai-Bhabar and Sub-Himalayan districts, including Haridwar and Udham Singh Nagar. The Random Forest and XGBoost models demonstrated superior predictive performance. Conclusions: The spatially explicit landslide susceptibility maps provide critical insights for urban planners, disaster management agencies, and environmentalists, aiding in developing effective strategies for landslide risk reduction and promoting sustainable development in Uttarakhand. This study exemplifies applying advanced analytical techniques to address landslide susceptibility and related soil erosion and water resource management challenges in Uttarakhand. © The Author(s) 2025. Bivariate analysis; Landslide susceptibility; Machine learning; MCDA; Uttarakhand NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
25;Improving Indonesia's tsunami early warning: Part I: Developing synthetic tsunami scenarios and initial deployment;Indonesia's Java subduction zone has triggered devastating tsunamis, emphasizing the need for a robust Tsunami Early Warning System, specifically for Southern Java, Bali, and Nusa Tenggara. With only six Ocean Bottom Pressure Gauges (OBPGs) currently monitoring tsunami propagation in the deep sea, optimized future sensor deployment is crucial. This paper, the first in a two-part series, proposes new observation networks to enhance tsunami early warning system. Our methodology involves developing synthetic stochastic-slip earthquake-induced tsunami simulations, delineating tsunami lead times, and applying empirical orthogonal functions (EOF) to determine spatial modal energy. We also assess the reliability of spacing and bathymetry for potential sensor locations. Our analysis reveals potential locations for additional OBPGs across the area. The proposed network consists of 42 additional sensors, demonstrating the potential for earlier warnings. These findings lay the groundwork for the second part of our series, where we will develop advanced forecasting models incorporating deep learning techniques based on the proposed location and further optimize sensor locations with the novel approach of hybrid optimizer and deep learning model. By establishing an improved observation network, this study contributes to more effective tsunami early warning systems in Indonesia, potentially mitigating the impact of future events on coastal communities. © 2024 The Authors;"OBPGs; Optimization; Tsunami early warning system";"Bali; East Nusa Tenggara; Greater Sunda Islands; Indonesia; Java; Lesser Sunda Islands; Sunda Isles; Bathymetry; Deep learning; Earthquakes; Java programming language; Pressure gages; Tsunamis; Bottom pressure gages; Early warning; Indonesia; Initial deployments; Observation networks; Ocean bottom pressure gauge; Optimisations; Sensor location; Subduction zones; Tsunami early-warning systems; early warning system; earthquake event; future prospect; optimization; scenario analysis; stochasticity; strike-slip fault; tsunami event; Orthogonal functions";"Improving Indonesia's tsunami early warning: Part I: Developing synthetic tsunami scenarios and initial deployment Indonesia's Java subduction zone has triggered devastating tsunamis, emphasizing the need for a robust Tsunami Early Warning System, specifically for Southern Java, Bali, and Nusa Tenggara. With only six Ocean Bottom Pressure Gauges (OBPGs) currently monitoring tsunami propagation in the deep sea, optimized future sensor deployment is crucial. This paper, the first in a two-part series, proposes new observation networks to enhance tsunami early warning system. Our methodology involves developing synthetic stochastic-slip earthquake-induced tsunami simulations, delineating tsunami lead times, and applying empirical orthogonal functions (EOF) to determine spatial modal energy. We also assess the reliability of spacing and bathymetry for potential sensor locations. Our analysis reveals potential locations for additional OBPGs across the area. The proposed network consists of 42 additional sensors, demonstrating the potential for earlier warnings. These findings lay the groundwork for the second part of our series, where we will develop advanced forecasting models incorporating deep learning techniques based on the proposed location and further optimize sensor locations with the novel approach of hybrid optimizer and deep learning model. By establishing an improved observation network, this study contributes to more effective tsunami early warning systems in Indonesia, potentially mitigating the impact of future events on coastal communities. © 2024 The Authors OBPGs; Optimization; Tsunami early warning system Bali; East Nusa Tenggara; Greater Sunda Islands; Indonesia; Java; Lesser Sunda Islands; Sunda Isles; Bathymetry; Deep learning; Earthquakes; Java programming language; Pressure gages; Tsunamis; Bottom pressure gages; Early warning; Indonesia; Initial deployments; Observation networks; Ocean bottom pressure gauge; Optimisations; Sensor location; Subduction zones; Tsunami early-warning systems; early warning system; earthquake event; future prospect; optimization; scenario analysis; stochasticity; strike-slip fault; tsunami event; Orthogonal functions";-1;Não Classificado;NULL;1.1;Geological;2;Preparation
26;Reinforcement learning-based model for road network maintenance and rehabilitation programming considering flood occurrences;This study proposed an optimisation model for determining road maintenance and rehabilitation (M&R) actions under flood occurrence and budget constraints. Variants of reinforcement learning (RL) were then proposed to optimise decisions to maximise the benefits experienced by traffic users owing to the maintenance programme. The RLs were trained to solve the M&R problem using conventional and performance-based contracting (PBC) schemes. Numerical experiments on a road network with various flood levels revealed that the proposed RL could decrease the network roughness level by up to 25%. In addition, the PBC scheme for maintaining roads with flood risk required approximately 19.35% additional funding compared with the scenario where no flood occurred. From a contractor’s standpoint, this increase can be viewed as a potential risk associated with participating in the PBC programme in the event of a flood. © 2025 Informa UK Limited, trading as Taylor & Francis Group.;"flood risk; greedy heuristics; performance-based contracting; Reinforcement learning; road maintenance and rehabilitation";"Condition based maintenance; Motor transportation; Reinforcement learning; Flood risks; Greedy heuristics; Learning Based Models; Maintenance and rehabilitations; Network maintenances; Performance-based contracting; Reinforcement learnings; Road maintenance; Road network; Road rehabilitation; Budget control";"Reinforcement learning-based model for road network maintenance and rehabilitation programming considering flood occurrences This study proposed an optimisation model for determining road maintenance and rehabilitation (M&R) actions under flood occurrence and budget constraints. Variants of reinforcement learning (RL) were then proposed to optimise decisions to maximise the benefits experienced by traffic users owing to the maintenance programme. The RLs were trained to solve the M&R problem using conventional and performance-based contracting (PBC) schemes. Numerical experiments on a road network with various flood levels revealed that the proposed RL could decrease the network roughness level by up to 25%. In addition, the PBC scheme for maintaining roads with flood risk required approximately 19.35% additional funding compared with the scenario where no flood occurred. From a contractor’s standpoint, this increase can be viewed as a potential risk associated with participating in the PBC programme in the event of a flood. © 2025 Informa UK Limited, trading as Taylor & Francis Group. flood risk; greedy heuristics; performance-based contracting; Reinforcement learning; road maintenance and rehabilitation Condition based maintenance; Motor transportation; Reinforcement learning; Flood risks; Greedy heuristics; Learning Based Models; Maintenance and rehabilitations; Network maintenances; Performance-based contracting; Reinforcement learnings; Road maintenance; Road network; Road rehabilitation; Budget control";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.2;Hydrological;1;Prevention
27;Comparative Analysis of Deep Learning Methods for Real-Time Estimation of Earthquake Magnitude;In recent years, although a variety of deep learning models have been developed for magnitude estimation, the complex and variable nature of earthquakes limits the generalizability and accuracy of these models. In this study, we selected the waveform data of the Japan earthquake. We applied four deep learning techniques (MagNet combined with bidirectional long- and short-term memory network Bi-LSTM, DCRNN with deepened CNN layers, DCRNNAmp with the introduction of a global scale factor, and Exams with a multilayered CNN architecture) for real-time magnitude estimation. By comparing the estimation errors of each model in the first 3 s after the earthquake, it is found that the DCRNNAmp performs the best, with an MAE of 0.287, an RMSE of 0.397, and an R2 of 0.737 in the first 3 s after the arrival of the P-wave, and the inclusion of S-wave seismic-phase information is found to significantly improve the accuracy of the magnitude estimation, which suggests that S-wave seismic-phase waveform features can enrich the model’s understanding of the relationship between the seismic phases. It shows that S-wave phase waveform features can enrich the model’s knowledge of the relationship between seismic fluctuations and magnitude. The epicentral distance positively correlates with the magnitude estimation, and the model can converge faster with the improved signal-to-noise ratio. Despite the shortcomings of model design and opaque internal mechanisms, this study provides important evidence for deep learning in earthquake estimation, demonstrating its potential to improve the accuracy of on-site earthquake early warning (EEW) systems. The estimation capability can be further improved by optimizing the model and exploring new features. © 2025 by the authors.;"deep learning; earthquake early warning (EEW); magnitude estimation; real-time estimation";"Earthquakes; Gravity waves; Long short-term memory; Seismic design; Seismic response; Seismic waves; Waveform analysis; Comparative analyzes; Deep learning; Earthquake early warning; Magnitude estimation; Phase waveforms; Real-time estimation; S-waves; Seismic phase; Waveform features; Shear waves";"Comparative Analysis of Deep Learning Methods for Real-Time Estimation of Earthquake Magnitude In recent years, although a variety of deep learning models have been developed for magnitude estimation, the complex and variable nature of earthquakes limits the generalizability and accuracy of these models. In this study, we selected the waveform data of the Japan earthquake. We applied four deep learning techniques (MagNet combined with bidirectional long- and short-term memory network Bi-LSTM, DCRNN with deepened CNN layers, DCRNNAmp with the introduction of a global scale factor, and Exams with a multilayered CNN architecture) for real-time magnitude estimation. By comparing the estimation errors of each model in the first 3 s after the earthquake, it is found that the DCRNNAmp performs the best, with an MAE of 0.287, an RMSE of 0.397, and an R2 of 0.737 in the first 3 s after the arrival of the P-wave, and the inclusion of S-wave seismic-phase information is found to significantly improve the accuracy of the magnitude estimation, which suggests that S-wave seismic-phase waveform features can enrich the model’s understanding of the relationship between the seismic phases. It shows that S-wave phase waveform features can enrich the model’s knowledge of the relationship between seismic fluctuations and magnitude. The epicentral distance positively correlates with the magnitude estimation, and the model can converge faster with the improved signal-to-noise ratio. Despite the shortcomings of model design and opaque internal mechanisms, this study provides important evidence for deep learning in earthquake estimation, demonstrating its potential to improve the accuracy of on-site earthquake early warning (EEW) systems. The estimation capability can be further improved by optimizing the model and exploring new features. © 2025 by the authors. deep learning; earthquake early warning (EEW); magnitude estimation; real-time estimation Earthquakes; Gravity waves; Long short-term memory; Seismic design; Seismic response; Seismic waves; Waveform analysis; Comparative analyzes; Deep learning; Earthquake early warning; Magnitude estimation; Phase waveforms; Real-time estimation; S-waves; Seismic phase; Waveform features; Shear waves";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
28;A fuzzy-based approach for clustering the meteorological drought over Iran;The climatic and hydrological variables play a significant role in water resources planning and management, due to their high spatial and temporal variability. While the classification of climatic regions based on various variables is challenging, effective zoning can improve our response to climate-induced hazards like floods and droughts. This study addresses these challenges by employing the C-means clustering algorithm to classify Iran based on monthly and annual precipitation, temperature, and meteorological drought, using observational data from 71 synoptic stations and satellite products, including ERA-5 and NASA-Power, spanning the years 1987 to 2017. The findings of this study indicate that the monthly performance of the ERA-5 satellite surpasses that of NASA-Power. Specifically, the ERA-5 satellite exhibited optimal performance in September and October, achieving R² values of 0.76 and 0.70, respectively, while NASA-Power recorded its highest performance in May and June, with R² values of 0.59. Conversely, the annual precipitation performance demonstrated superior results. The classification analysis revealed that the Central Plateau of Iran constitutes the largest region, consistently classified within the 1 to 2 class range across all scenarios, encompassing 50% of the country’s total area. The findings indicated that the northwest area, characterized by a cold and frigid climate, is consistently grouped in the same category across most terrestrial and satellite scenarios, encompassing approximately 13% of Iran’s territory within this climate. In the majority of scenarios, the northern part of Iran, which experiences the highest levels of rainfall, is divided into two distinct regions, both of which occupy the smallest land area, each being less than 1% of Iran’s total expanse. All scenarios categorize Iran into 5 to 9 classes, with the 5-class scenario corresponding to monthly precipitation and SPI 3 yielding 9 classes. Based on the homogeneity results, the classification derived from monthly and annual precipitation data is identified as the most effective. Furthermore, the results suggest that the classification based on NASA-Power satellite data aligns more closely with observational classifications. Effective climate zoning can contribute to environmental sustainability by optimizing resource management and minimizing the adverse impacts of climate change. The findings of this study can be beneficial for agriculture, selecting appropriate dam sites, and effective water resource management. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.;"C-means clustering; Climate zoning; ERA-5; NASA-power; Regional classification";"Iran; Drought; Rain; Resource allocation; Tropics; Water management; Annual precipitation; C-Means clustering; Climate zoning; Clusterings; ERA-5; Meteorological drought; NASA-power; Performance; Power; Regional classifications; algorithm; classification; cluster analysis; drought; fuzzy mathematics; precipitation (climatology); water management; zoning system; NASA";"A fuzzy-based approach for clustering the meteorological drought over Iran The climatic and hydrological variables play a significant role in water resources planning and management, due to their high spatial and temporal variability. While the classification of climatic regions based on various variables is challenging, effective zoning can improve our response to climate-induced hazards like floods and droughts. This study addresses these challenges by employing the C-means clustering algorithm to classify Iran based on monthly and annual precipitation, temperature, and meteorological drought, using observational data from 71 synoptic stations and satellite products, including ERA-5 and NASA-Power, spanning the years 1987 to 2017. The findings of this study indicate that the monthly performance of the ERA-5 satellite surpasses that of NASA-Power. Specifically, the ERA-5 satellite exhibited optimal performance in September and October, achieving R² values of 0.76 and 0.70, respectively, while NASA-Power recorded its highest performance in May and June, with R² values of 0.59. Conversely, the annual precipitation performance demonstrated superior results. The classification analysis revealed that the Central Plateau of Iran constitutes the largest region, consistently classified within the 1 to 2 class range across all scenarios, encompassing 50% of the country’s total area. The findings indicated that the northwest area, characterized by a cold and frigid climate, is consistently grouped in the same category across most terrestrial and satellite scenarios, encompassing approximately 13% of Iran’s territory within this climate. In the majority of scenarios, the northern part of Iran, which experiences the highest levels of rainfall, is divided into two distinct regions, both of which occupy the smallest land area, each being less than 1% of Iran’s total expanse. All scenarios categorize Iran into 5 to 9 classes, with the 5-class scenario corresponding to monthly precipitation and SPI 3 yielding 9 classes. Based on the homogeneity results, the classification derived from monthly and annual precipitation data is identified as the most effective. Furthermore, the results suggest that the classification based on NASA-Power satellite data aligns more closely with observational classifications. Effective climate zoning can contribute to environmental sustainability by optimizing resource management and minimizing the adverse impacts of climate change. The findings of this study can be beneficial for agriculture, selecting appropriate dam sites, and effective water resource management. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024. C-means clustering; Climate zoning; ERA-5; NASA-power; Regional classification Iran; Drought; Rain; Resource allocation; Tropics; Water management; Annual precipitation; C-Means clustering; Climate zoning; Clusterings; ERA-5; Meteorological drought; NASA-power; Performance; Power; Regional classifications; algorithm; classification; cluster analysis; drought; fuzzy mathematics; precipitation (climatology); water management; zoning system; NASA";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.4;Climatological;1;Prevention
29;Investigating the Condition and Flood Effects of Undocumented Levees, A Case Study Within the Waimea Floodplain;Levees play a crucial role in flood protection, but globally, there is a need for more knowledge about levee networks and their flood routing effects. Without complete knowledge, the question arises: ‘What is the flood risk associated with an unknown or partially known levee portfolio?’ Unknown or undocumented levees can be maladaptive and undermine system resilience. However, current literature often does not acknowledge undocumented levees, assuming all assets are known. A greater understanding would provide insight into present vulnerabilities and enable more complete management of our flood protection systems, reducing communities' risk. Our research assessed the physical condition of two undocumented levees in a case study. Computational flood modelling then simulated (1) their present condition, (2) their removal and (3) their reconstruction to a good physical condition. This determined their effect on inundation area and building damages, allowing their classification. The undocumented levees in the case study were significantly degraded, leading to an insignificant impact on flood routing and flood damages in their present state. However, if reconstructed, the levees could be valuable if the surrounding land were developed. More broadly, this study illustrates the importance of identifying and integrating undocumented levees into network modelling and maintenance. © 2025 The Author(s). Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd.;"embankments and levees; flood defence measures; hydraulic modelling; risk mapping";"New Zealand; South Island; Tasman; Waimea Plains; embankment; flood routing; hydraulic conductivity; levee; research method; risk assessment";"Investigating the Condition and Flood Effects of Undocumented Levees, A Case Study Within the Waimea Floodplain Levees play a crucial role in flood protection, but globally, there is a need for more knowledge about levee networks and their flood routing effects. Without complete knowledge, the question arises: ‘What is the flood risk associated with an unknown or partially known levee portfolio?’ Unknown or undocumented levees can be maladaptive and undermine system resilience. However, current literature often does not acknowledge undocumented levees, assuming all assets are known. A greater understanding would provide insight into present vulnerabilities and enable more complete management of our flood protection systems, reducing communities' risk. Our research assessed the physical condition of two undocumented levees in a case study. Computational flood modelling then simulated (1) their present condition, (2) their removal and (3) their reconstruction to a good physical condition. This determined their effect on inundation area and building damages, allowing their classification. The undocumented levees in the case study were significantly degraded, leading to an insignificant impact on flood routing and flood damages in their present state. However, if reconstructed, the levees could be valuable if the surrounding land were developed. More broadly, this study illustrates the importance of identifying and integrating undocumented levees into network modelling and maintenance. © 2025 The Author(s). Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd. embankments and levees; flood defence measures; hydraulic modelling; risk mapping New Zealand; South Island; Tasman; Waimea Plains; embankment; flood routing; hydraulic conductivity; levee; research method; risk assessment";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
30;Socioeconomic disparities in hurricane-induced power outages: Insights from multi-hurricane data in Florida using XGBoost;This study explores the importance of socioeconomic factors in hurricane-induced power outages in Florida. An XGBoost regression framework that incorporates a comprehensive feature set, including diverse socioeconomic factors, hurricane hazards, and physical exposure, is introduced. To reduce random deviations in importance observed in prior single hurricane studies, data for 11 Florida hurricanes is processed and analyzed, sourced from various state and federal agencies. To further enhance the robustness of model findings, analysis was conducted on 66 independent repetition runs filtered from 250 model iterations to control for overfitting. An extended formulation of SHAP values across iterations is introduced to enable a nuanced assessment of feature importance. Results show that socioeconomic variables account for 19% of the model prediction. This finding underscores the presence and significance of social inequities in hurricane outages. The unemployment rate, percentage of disabled, and racial/ethnic minorities are found as the most important predictors. Two new variables – flooding and substations per county – are assessed in this study, but they are found to have no notable contribution to power outages. The findings of this study provide new insights into the interplay between socioeconomic conditions and power system performance, aiding outage prevention efforts by identifying socioeconomic inequalities in pre-existing conditions and system operations. The findings of this study highlight systemic socioeconomic vulnerabilities in power grid resilience, offering critical insights for policymakers to allocate resources and improve disaster response strategies. While the model is tailored for Florida, its structure could be adapted to assess power outage disparities in other hurricane-prone regions. © 2025 The Authors;"Energy inequality; Multiple hurricane-induced outages; Power system disparities; SHapley Additive exPlanations (SHAP); XGBoost regression model";"Energy; Energy inequality; Multiple hurricane-induced outage; Power; Power outage; Power system disparity; Regression modelling; Shapley; Shapley additive explanation; Xgboost regression model; disaster management; energy dissipation; extreme event; hazard assessment; hurricane; regression analysis; socioeconomic conditions";"Socioeconomic disparities in hurricane-induced power outages: Insights from multi-hurricane data in Florida using XGBoost This study explores the importance of socioeconomic factors in hurricane-induced power outages in Florida. An XGBoost regression framework that incorporates a comprehensive feature set, including diverse socioeconomic factors, hurricane hazards, and physical exposure, is introduced. To reduce random deviations in importance observed in prior single hurricane studies, data for 11 Florida hurricanes is processed and analyzed, sourced from various state and federal agencies. To further enhance the robustness of model findings, analysis was conducted on 66 independent repetition runs filtered from 250 model iterations to control for overfitting. An extended formulation of SHAP values across iterations is introduced to enable a nuanced assessment of feature importance. Results show that socioeconomic variables account for 19% of the model prediction. This finding underscores the presence and significance of social inequities in hurricane outages. The unemployment rate, percentage of disabled, and racial/ethnic minorities are found as the most important predictors. Two new variables – flooding and substations per county – are assessed in this study, but they are found to have no notable contribution to power outages. The findings of this study provide new insights into the interplay between socioeconomic conditions and power system performance, aiding outage prevention efforts by identifying socioeconomic inequalities in pre-existing conditions and system operations. The findings of this study highlight systemic socioeconomic vulnerabilities in power grid resilience, offering critical insights for policymakers to allocate resources and improve disaster response strategies. While the model is tailored for Florida, its structure could be adapted to assess power outage disparities in other hurricane-prone regions. © 2025 The Authors Energy inequality; Multiple hurricane-induced outages; Power system disparities; SHapley Additive exPlanations (SHAP); XGBoost regression model Energy; Energy inequality; Multiple hurricane-induced outage; Power; Power outage; Power system disparity; Regression modelling; Shapley; Shapley additive explanation; Xgboost regression model; disaster management; energy dissipation; extreme event; hazard assessment; hurricane; regression analysis; socioeconomic conditions";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
31;Strategic Management and Seismic Resilience Enhancement of Water Distribution Network Using Artificial Neural Network Model;The seismic resilience of water distribution networks (WDNs) as the most critical urban infrastructure is a major aspect of crisis management. Developing accurate computational models to evaluate the seismic behavior of WDNs and enhance seismic resilience remains a major research challenge. This paper introduces a novel model to evaluate the seismic vulnerability of WDNs through an artificial neural network (ANN) approach. To generate initial data to develop the seismic damage prediction model, extensive numerical modeling was performed using commercially available software to extract the strain behavior of buried pipes under seismic loading. A total of 720 numerical simulations were performed. The numerical model was validated using an experimental model. The results showed that the numerical model had an error smaller than 10% in evaluating the strain behavior of buried pipes, suggesting satisfactory model performance. This paper used a multilayer perceptron (MLP) and the Levenberg-Marquardt algorithm (LMA) because it has shown good performance in regression applications. The sensitivity of the proposed model to the number of hidden layers was analyzed, and the MLP with 15 hidden layers was found to be optimal in predicting the strain of the buried pipe under seismic loading, with a mean squared error (MSE) of 0.301 and a correlation coefficient R=0.969. The proposed seismic damage prediction model was executed on the WDN of Tehran, Iran, based on the initial data set under five seismic loading scenarios, calculating the numbers of breaks and leaks. The seismic resilience of the WDN was evaluated using damage, minimum water demand, and restoration time indices. Several strategies were proposed to enhance the seismic resilience of WDNs. The developed seismic resilience assessment model in this study has the capability to be applied and implemented across other WDNs. © 2024 American Society of Civil Engineers.;"Artificial neural network (ANN); Seismic resilience; Seismic vulnerability; Strategic management; Water distribution system (WDN)";"Earthquake effects; Fracture mechanics; Multilayer neural networks; Risk management; Seismic response; Artificial neural network; Buried pipes; Distribution systems; Neural-networks; Seismic resilience; Seismic vulnerability; Strategic management; Water distribution networks; Water distribution system; Water distributions; Strategic planning";"Strategic Management and Seismic Resilience Enhancement of Water Distribution Network Using Artificial Neural Network Model The seismic resilience of water distribution networks (WDNs) as the most critical urban infrastructure is a major aspect of crisis management. Developing accurate computational models to evaluate the seismic behavior of WDNs and enhance seismic resilience remains a major research challenge. This paper introduces a novel model to evaluate the seismic vulnerability of WDNs through an artificial neural network (ANN) approach. To generate initial data to develop the seismic damage prediction model, extensive numerical modeling was performed using commercially available software to extract the strain behavior of buried pipes under seismic loading. A total of 720 numerical simulations were performed. The numerical model was validated using an experimental model. The results showed that the numerical model had an error smaller than 10% in evaluating the strain behavior of buried pipes, suggesting satisfactory model performance. This paper used a multilayer perceptron (MLP) and the Levenberg-Marquardt algorithm (LMA) because it has shown good performance in regression applications. The sensitivity of the proposed model to the number of hidden layers was analyzed, and the MLP with 15 hidden layers was found to be optimal in predicting the strain of the buried pipe under seismic loading, with a mean squared error (MSE) of 0.301 and a correlation coefficient R=0.969. The proposed seismic damage prediction model was executed on the WDN of Tehran, Iran, based on the initial data set under five seismic loading scenarios, calculating the numbers of breaks and leaks. The seismic resilience of the WDN was evaluated using damage, minimum water demand, and restoration time indices. Several strategies were proposed to enhance the seismic resilience of WDNs. The developed seismic resilience assessment model in this study has the capability to be applied and implemented across other WDNs. © 2024 American Society of Civil Engineers. Artificial neural network (ANN); Seismic resilience; Seismic vulnerability; Strategic management; Water distribution system (WDN) Earthquake effects; Fracture mechanics; Multilayer neural networks; Risk management; Seismic response; Artificial neural network; Buried pipes; Distribution systems; Neural-networks; Seismic resilience; Seismic vulnerability; Strategic management; Water distribution networks; Water distribution system; Water distributions; Strategic planning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
32;Unraveling the response of forests to drought with explainable artificial intelligence (XAI);Increases in the frequency and intensity of droughts and heat waves are threatening forests around the world. Climate-driven tree dieback and mortality is associated with devastating ecological and societal consequences, including the loss of carbon sequestration, habitat provisioning, and water filtration services. A spatially fine-grained understanding of the site characteristics making forests more susceptible to drought is still lacking. Furthermore, the complexity of drought effects on forests, which can be cumulative and delayed, demands investigation of the most appropriate meteorological indicators. To address this research gap, we investigated the drivers of drought-induced forest damage in a particularly drought-affected region of Central Europe using SHapley Additive exPlanations (SHAP) values, an explainable artificial intelligence (XAI) method which allows for the relevance of predictors to be quantified spatially. To develop a reproducible approach that facilitates transferability to other regions, open-source data was used to characterize the meteorological, vegetation, topographical, and soil drivers of tree vulnerability, representing 41 predictors in total. The forest drought response was characterized as a binary variable (“damaged” or “unchanged”) at a 30-m resolution based on the Normalized Difference Moisture Index (NDMI) anomaly (%) between a baseline period (2013–2017) and recent years (2018–2022). We revealed critical tipping points beyond which the forest ecosystem shifted towards a damaged state: <81 % tree cover density, <4 % of broadleaf trees, and < 24 m canopy height. Our study provides an enhanced understanding of trees’ response to drought, which can support forest managers aiming to make forests more climate-resilient, and serves as a prototype for interpretable early-warning systems. © 2025 The Author(s);"Environmental drivers; Machine learning; Normalized Difference Moisture Index (NDMI); Remote sensing; Resistance; SHapley Additive exPlanation";"Central Europe; Drought; Carbon sequestration; Environmental driver; Heatwaves; Machine-learning; Normalized difference moisture index; Normalized difference moisture indices; Remote-sensing; Resistance; Shapley; Shapley additive explanation; artificial intelligence; carbon sequestration; drought resistance; early warning system; forest ecosystem; machine learning; mortality; remote sensing; Forest ecology";"Unraveling the response of forests to drought with explainable artificial intelligence (XAI) Increases in the frequency and intensity of droughts and heat waves are threatening forests around the world. Climate-driven tree dieback and mortality is associated with devastating ecological and societal consequences, including the loss of carbon sequestration, habitat provisioning, and water filtration services. A spatially fine-grained understanding of the site characteristics making forests more susceptible to drought is still lacking. Furthermore, the complexity of drought effects on forests, which can be cumulative and delayed, demands investigation of the most appropriate meteorological indicators. To address this research gap, we investigated the drivers of drought-induced forest damage in a particularly drought-affected region of Central Europe using SHapley Additive exPlanations (SHAP) values, an explainable artificial intelligence (XAI) method which allows for the relevance of predictors to be quantified spatially. To develop a reproducible approach that facilitates transferability to other regions, open-source data was used to characterize the meteorological, vegetation, topographical, and soil drivers of tree vulnerability, representing 41 predictors in total. The forest drought response was characterized as a binary variable (“damaged” or “unchanged”) at a 30-m resolution based on the Normalized Difference Moisture Index (NDMI) anomaly (%) between a baseline period (2013–2017) and recent years (2018–2022). We revealed critical tipping points beyond which the forest ecosystem shifted towards a damaged state: <81 % tree cover density, <4 % of broadleaf trees, and < 24 m canopy height. Our study provides an enhanced understanding of trees’ response to drought, which can support forest managers aiming to make forests more climate-resilient, and serves as a prototype for interpretable early-warning systems. © 2025 The Author(s) Environmental drivers; Machine learning; Normalized Difference Moisture Index (NDMI); Remote sensing; Resistance; SHapley Additive exPlanation Central Europe; Drought; Carbon sequestration; Environmental driver; Heatwaves; Machine-learning; Normalized difference moisture index; Normalized difference moisture indices; Remote-sensing; Resistance; Shapley; Shapley additive explanation; artificial intelligence; carbon sequestration; drought resistance; early warning system; forest ecosystem; machine learning; mortality; remote sensing; Forest ecology";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
33;Geospatial and NDBI approaches for the Musi River basin morphometric studies in the metropolitan urban Cities of India;This study aims to advance the understanding of hydrological dynamics and geomorphological characteristics of the Musi River Basin through the novel application of Remote Sensing (RS) and Geographical Information System (GIS)-based morphometric analysis across three delineated sub-basins (SB-1, SB-2, SB-3). By examining parameters such as stream order, bifurcation ratio, drainage density, and land cover changes using the Normalized Difference Built-up Index (NDBI), the study provides a comprehensive overview of the basin's surface hydrology and urbanization impact. Key findings reveal low surface runoff and high permeability, supported by low drainage density (0.64 km/km2) and stream frequency (0.48 streams/km2). SB-3′s elevated bifurcation ratio (5.0) highlights substantial structural control, contrasting with moderate bifurcation ratios in SB-1 and SB-2. Hypsometric analysis indicates a transition from youthful to mature phases across the basin, with SB-3′s ruggedness values reflecting moderate erosion. A 21-year land cover analysis (2000–2021) based on NDBI highlights notable changes: water bodies (2.67 %), moisture soils (5.04 %), built-up lands (3.10 %), and vegetation (2.74 %) have all increased, while fallow lands/wastelands decreased by 13.54 %. The urban expansion, driven by rapid population growth, underscores pressing ecosystem challenges. Validation of the land cover analysis yielded a robust AUC score of 0.840, affirming the reliability of the NDBI-based classification. This research showcases the efficacy of DEM-based morphometric methods for precise catchment delineation and stream order classification, providing critical insights for effective water resource management and watershed planning in the Musi River Basin. © 2024 COSPAR;"Land use pattern; Morphometric analysis; NDBI; RS and GIS";"Catchments; Population statistics; Resource allocation; River basin projects; Runoff; Soil moisture; Bifurcation ratio; Geographical information; Land use pattern; Morphometric analysis; Normalized difference build-up index; Normalized differences; Remote sensing and geographical information system; Remote sensing information; River basins; Stream order; Rivers";"Geospatial and NDBI approaches for the Musi River basin morphometric studies in the metropolitan urban Cities of India This study aims to advance the understanding of hydrological dynamics and geomorphological characteristics of the Musi River Basin through the novel application of Remote Sensing (RS) and Geographical Information System (GIS)-based morphometric analysis across three delineated sub-basins (SB-1, SB-2, SB-3). By examining parameters such as stream order, bifurcation ratio, drainage density, and land cover changes using the Normalized Difference Built-up Index (NDBI), the study provides a comprehensive overview of the basin's surface hydrology and urbanization impact. Key findings reveal low surface runoff and high permeability, supported by low drainage density (0.64 km/km2) and stream frequency (0.48 streams/km2). SB-3′s elevated bifurcation ratio (5.0) highlights substantial structural control, contrasting with moderate bifurcation ratios in SB-1 and SB-2. Hypsometric analysis indicates a transition from youthful to mature phases across the basin, with SB-3′s ruggedness values reflecting moderate erosion. A 21-year land cover analysis (2000–2021) based on NDBI highlights notable changes: water bodies (2.67 %), moisture soils (5.04 %), built-up lands (3.10 %), and vegetation (2.74 %) have all increased, while fallow lands/wastelands decreased by 13.54 %. The urban expansion, driven by rapid population growth, underscores pressing ecosystem challenges. Validation of the land cover analysis yielded a robust AUC score of 0.840, affirming the reliability of the NDBI-based classification. This research showcases the efficacy of DEM-based morphometric methods for precise catchment delineation and stream order classification, providing critical insights for effective water resource management and watershed planning in the Musi River Basin. © 2024 COSPAR Land use pattern; Morphometric analysis; NDBI; RS and GIS Catchments; Population statistics; Resource allocation; River basin projects; Runoff; Soil moisture; Bifurcation ratio; Geographical information; Land use pattern; Morphometric analysis; Normalized difference build-up index; Normalized differences; Remote sensing and geographical information system; Remote sensing information; River basins; Stream order; Rivers";-1;Não Classificado;NULL;-1;NULL;1;Prevention
34;Analyzing wildfire evacuation dynamics with agent-based modeling in damaged road networks;Wildfires increasingly threaten residents in the Western United States. Despite numerous state and local initiatives aimed at mitigating these risks, completely eliminating the wildfire dangers remains unfeasible due to substantial inherent uncertainties. In this case, evacuation is the most important and effective strategy for reducing human casualties during wildfire events. While the primary goal of evacuation–moving people at risk to safer places–appears straightforward to achieve, the reality is complicated by unpredictable human behaviors and the surge in travel demand, which often results in severe traffic congestion and, consequently, a heightened risk to human lives. In addition, the reduced traffic-carrying capacities of road segments due to wildfires further exacerbate these challenges. In this context, wildfire evacuation simulation can serve as an effective experimental means for emergency management and evacuation planning, offering a cost-effective method to identify bottlenecks and critical congestion points during an evacuation. This paper proposes an agent-based modeling (ABM) framework specifically designed to simulate wildfire evacuations in damaged transportation settings. The proposed framework uniquely integrates wildfire simulation and road network vulnerability assessment with ABM, allowing for a detailed representation of human behaviors during evacuations and the dynamic network functionality in microscopic traffic simulation. A notable contribution of this study is its fully probabilistic approach, which evaluates evacuation performance and identifies critical components of the road network not under a single scenario but under a range of representative scenarios. This probabilistic perspective provides a more comprehensive understanding of potentially vulnerable and congested points, thereby enabling emergency managers and transportation planners to better allocate resources and enhance mobility during wildfire evacuations. The effectiveness of the ABM framework is demonstrated through its application in simulating wildfire evacuations in the City of Santa Clarita, California. The simulation results aid in both pre-fire planning and emergency decision-making, ultimately contributing to improved evacuation strategies and public safety during wildfire events. © 2025 Elsevier Ltd;"Agent-based modeling; Evacuation; Machine learning; Traffic simulation; Vulnerability assessment; Wildfire";"Decision making; Emergency traffic control; Highway administration; Public risks; Risk assessment; Risk management; Traffic congestion; Agent based modeling frameworks; Agent-based model; Evacuation; Evacuation dynamics; Human behaviors; Machine-learning; Road network; Traffic simulations; Vulnerability assessments; Wildfire; accident; adult; aged; Article; behavior; California; controlled study; cost effectiveness analysis; damaged road network; decision making; emergency evacuation; emergency management; environmental risk; environmental vulnerability; female; human; male; middle aged; risk; risk management; safety; traffic accident; traffic and transport; traffic congestion; travel; vulnerability assessment; weather; wildfire; Premixed flames";"Analyzing wildfire evacuation dynamics with agent-based modeling in damaged road networks Wildfires increasingly threaten residents in the Western United States. Despite numerous state and local initiatives aimed at mitigating these risks, completely eliminating the wildfire dangers remains unfeasible due to substantial inherent uncertainties. In this case, evacuation is the most important and effective strategy for reducing human casualties during wildfire events. While the primary goal of evacuation–moving people at risk to safer places–appears straightforward to achieve, the reality is complicated by unpredictable human behaviors and the surge in travel demand, which often results in severe traffic congestion and, consequently, a heightened risk to human lives. In addition, the reduced traffic-carrying capacities of road segments due to wildfires further exacerbate these challenges. In this context, wildfire evacuation simulation can serve as an effective experimental means for emergency management and evacuation planning, offering a cost-effective method to identify bottlenecks and critical congestion points during an evacuation. This paper proposes an agent-based modeling (ABM) framework specifically designed to simulate wildfire evacuations in damaged transportation settings. The proposed framework uniquely integrates wildfire simulation and road network vulnerability assessment with ABM, allowing for a detailed representation of human behaviors during evacuations and the dynamic network functionality in microscopic traffic simulation. A notable contribution of this study is its fully probabilistic approach, which evaluates evacuation performance and identifies critical components of the road network not under a single scenario but under a range of representative scenarios. This probabilistic perspective provides a more comprehensive understanding of potentially vulnerable and congested points, thereby enabling emergency managers and transportation planners to better allocate resources and enhance mobility during wildfire evacuations. The effectiveness of the ABM framework is demonstrated through its application in simulating wildfire evacuations in the City of Santa Clarita, California. The simulation results aid in both pre-fire planning and emergency decision-making, ultimately contributing to improved evacuation strategies and public safety during wildfire events. © 2025 Elsevier Ltd Agent-based modeling; Evacuation; Machine learning; Traffic simulation; Vulnerability assessment; Wildfire Decision making; Emergency traffic control; Highway administration; Public risks; Risk assessment; Risk management; Traffic congestion; Agent based modeling frameworks; Agent-based model; Evacuation; Evacuation dynamics; Human behaviors; Machine-learning; Road network; Traffic simulations; Vulnerability assessments; Wildfire; accident; adult; aged; Article; behavior; California; controlled study; cost effectiveness analysis; damaged road network; decision making; emergency evacuation; emergency management; environmental risk; environmental vulnerability; female; human; male; middle aged; risk; risk management; safety; traffic accident; traffic and transport; traffic congestion; travel; vulnerability assessment; weather; wildfire; Premixed flames";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
35;Adaptive transformer-based multi-task learning framework for synchronous prediction of substation flooding and outage risks;Flooding disasters significantly threaten substation security, and forecasting risks of flooding and resulting outages within the substation is crucial for taking preventive measures and enhancing the substation's resilience. Existing models may suffer from low accuracy of risk prediction due to the difficulty of handling nonlinear multi-factors, dynamic temporal dependencies, and unbalanced data. Additionally, they rarely forecast flooding and outages simultaneously, leading to incomplete risk assessments. Therefore, a novel Transformer-based multi-task learning model (MTformer) is proposed to simultaneously predict flooding and outage risk within substations. MTformer is an attention-based shared encoder-decoder architecture that can achieve shared feature extraction and collaborative prediction. This model adopts three improved strategies: adaptive temporal encoding to enhance temporal dependency extraction, feature perception strategy to fuse heterogeneous data inputs, and training balancing strategy to balance multi-task training and reduce the impact of data imbalance. The experiment results show that the MTformer effectively predicts substation flooding and outage risks and outperforms the mainstream predictive model, with a decrease of 47.96 % in RMSE for flooding prediction and an increase of 39.82 % in F1 for outage prediction. Case studies demonstrate the potential of MTformer as a decision-making tool for proactive disaster mitigation and recovery planning. © 2025 Elsevier B.V.;"Multi-task learning; Risk prediction and assessment; Substation flooding; Substation outage; Transformer algorithm";"Multi-task learning; Prediction models; Risk assessment; Flooding risks; Floodings; Multitask learning; Outage risk; Prediction and assessments; Risk predictions; Risks assessments; Substation flooding; Substation outage; Transformer algorithm; Transformer substations";"Adaptive transformer-based multi-task learning framework for synchronous prediction of substation flooding and outage risks Flooding disasters significantly threaten substation security, and forecasting risks of flooding and resulting outages within the substation is crucial for taking preventive measures and enhancing the substation's resilience. Existing models may suffer from low accuracy of risk prediction due to the difficulty of handling nonlinear multi-factors, dynamic temporal dependencies, and unbalanced data. Additionally, they rarely forecast flooding and outages simultaneously, leading to incomplete risk assessments. Therefore, a novel Transformer-based multi-task learning model (MTformer) is proposed to simultaneously predict flooding and outage risk within substations. MTformer is an attention-based shared encoder-decoder architecture that can achieve shared feature extraction and collaborative prediction. This model adopts three improved strategies: adaptive temporal encoding to enhance temporal dependency extraction, feature perception strategy to fuse heterogeneous data inputs, and training balancing strategy to balance multi-task training and reduce the impact of data imbalance. The experiment results show that the MTformer effectively predicts substation flooding and outage risks and outperforms the mainstream predictive model, with a decrease of 47.96 % in RMSE for flooding prediction and an increase of 39.82 % in F1 for outage prediction. Case studies demonstrate the potential of MTformer as a decision-making tool for proactive disaster mitigation and recovery planning. © 2025 Elsevier B.V. Multi-task learning; Risk prediction and assessment; Substation flooding; Substation outage; Transformer algorithm Multi-task learning; Prediction models; Risk assessment; Flooding risks; Floodings; Multitask learning; Outage risk; Prediction and assessments; Risk predictions; Risks assessments; Substation flooding; Substation outage; Transformer algorithm; Transformer substations";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;1;Prevention
36;A novel intelligent model for high-precision reconstruction of dam-break free surface profiles from sparse monitoring data;Accurate reconstruction of the dam-break free surface profiles from sparse monitoring water levels is a challenging task in flood disaster warning. To address this issue, this article introduces a novel intelligent model that leverages fully connected autoencoder (FCAE) to acquire low-order latent space representations and random forest (RF) to establish the relationship between sparse monitoring water levels and latent space representations, which is named as FCAE-RF. The proposed model is tested on two benchmark cases, including the advection of a shock profile and the classical dam-break flow. The results indicate that FCAE-RF provides satisfactory prediction performances for both uniform and nonuniform distributions of the monitoring locations. Besides, the adopted FCAE displays better performance than the widely used proper orthogonal decomposition and convolutional autoencoder in the dimension reduction, while the employed RF presents larger superiority than the commonly used ridge regression and multi-layer perceptron in the establishment of the nonlinear relationship. These promising results indicate that FCAE-RF can be a robust model in the application of flood management and control. © 2025 Author(s).;NULL;"Benchmarking; Flood damage; Surface reconstruction; Auto encoders; Dam-breaks; Disaster warnings; Flood disaster; Free surfaces; High-precision; Intelligent models; Low order; Random forests; Surface profiles; Flood control";"A novel intelligent model for high-precision reconstruction of dam-break free surface profiles from sparse monitoring data Accurate reconstruction of the dam-break free surface profiles from sparse monitoring water levels is a challenging task in flood disaster warning. To address this issue, this article introduces a novel intelligent model that leverages fully connected autoencoder (FCAE) to acquire low-order latent space representations and random forest (RF) to establish the relationship between sparse monitoring water levels and latent space representations, which is named as FCAE-RF. The proposed model is tested on two benchmark cases, including the advection of a shock profile and the classical dam-break flow. The results indicate that FCAE-RF provides satisfactory prediction performances for both uniform and nonuniform distributions of the monitoring locations. Besides, the adopted FCAE displays better performance than the widely used proper orthogonal decomposition and convolutional autoencoder in the dimension reduction, while the employed RF presents larger superiority than the commonly used ridge regression and multi-layer perceptron in the establishment of the nonlinear relationship. These promising results indicate that FCAE-RF can be a robust model in the application of flood management and control. © 2025 Author(s). NULL Benchmarking; Flood damage; Surface reconstruction; Auto encoders; Dam-breaks; Disaster warnings; Flood disaster; Free surfaces; High-precision; Intelligent models; Low order; Random forests; Surface profiles; Flood control";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
37;Different Time-Increment Rainfall Prediction Models: a Machine Learning Approach Using Various Input Scenarios;This study investigates the utilization of machine learning techniques, including Linear Regression, Gradient Boost, and LSTM algorithms, for rainfall prediction across different timeframes (hourly, daily, and monthly). Data spanning from 2015 to 2022 from meteorological stations in the Langat basin river region (Pejabat, Kajang, and Petaling) is employed for model development and evaluation. The primary objectives encompass crafting predictive models, assessing their ability to capture rainfall patterns, and analyzing the impact of various input parameters on model performance. Emphasizing the critical significance of accurate rainfall forecasting in domains like agriculture, water resource management, and flood prediction, particularly amidst evolving climate dynamics, this research sheds light on the intricate nuances of rainfall prediction through scrutiny of distinct machine learning techniques. The results were revealed that for hourly rainfall data analysis at Pejabat, the LSTM model had the best accuracy, while for Kajang and Petaling, the Linear Regression model was best depending on the geographic and temporal conditions of the catching area. The Gradient Boost Regressor was excellent at predicting Kajang’s daily rainfall, and the ensemble technique was sometimes better. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"Gradient Boost Regressor; LSTM; Machine Learning; Malaysia; Rainfall Forecasting";"Malaysia; Adversarial machine learning; Contrastive Learning; Rain; Resource allocation; Weather forecasting; Gradient boost regressor; LSTM; Machine learning techniques; Machine-learning; Malaysia; Petaling; Prediction modelling; Rainfall forecasting; Rainfall prediction; Time increments; data set; gradient analysis; machine learning; parameterization; precipitation intensity; prediction; regression analysis; scenario analysis; Prediction models";"Different Time-Increment Rainfall Prediction Models: a Machine Learning Approach Using Various Input Scenarios This study investigates the utilization of machine learning techniques, including Linear Regression, Gradient Boost, and LSTM algorithms, for rainfall prediction across different timeframes (hourly, daily, and monthly). Data spanning from 2015 to 2022 from meteorological stations in the Langat basin river region (Pejabat, Kajang, and Petaling) is employed for model development and evaluation. The primary objectives encompass crafting predictive models, assessing their ability to capture rainfall patterns, and analyzing the impact of various input parameters on model performance. Emphasizing the critical significance of accurate rainfall forecasting in domains like agriculture, water resource management, and flood prediction, particularly amidst evolving climate dynamics, this research sheds light on the intricate nuances of rainfall prediction through scrutiny of distinct machine learning techniques. The results were revealed that for hourly rainfall data analysis at Pejabat, the LSTM model had the best accuracy, while for Kajang and Petaling, the Linear Regression model was best depending on the geographic and temporal conditions of the catching area. The Gradient Boost Regressor was excellent at predicting Kajang’s daily rainfall, and the ensemble technique was sometimes better. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. Gradient Boost Regressor; LSTM; Machine Learning; Malaysia; Rainfall Forecasting Malaysia; Adversarial machine learning; Contrastive Learning; Rain; Resource allocation; Weather forecasting; Gradient boost regressor; LSTM; Machine learning techniques; Machine-learning; Malaysia; Petaling; Prediction modelling; Rainfall forecasting; Rainfall prediction; Time increments; data set; gradient analysis; machine learning; parameterization; precipitation intensity; prediction; regression analysis; scenario analysis; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
38;Earthquake epicenter prediction from the Java-Bali radon gas telemonitoring station using machine learning;Predicting the location of earthquake epicenters is a critical aspect of earthquake forecasting, as it complements efforts to determine the time and magnitude of seismic events. This research addresses the challenge posed by the uncertainty in epicenter locations, particularly along the extensive plate faults of Indo-Australia and Eurasia. In these regions, effective earthquake prediction is compromised without accurate epicenter information, impeding mitigation strategies and complicating disaster impact estimation. The primary objective of this study is to devise an algorithm for forecasting earthquake epicenter locations by harnessing variations in radon gas concentrations on southern Java Island, Indonesia, as a predictive precursor. Using a supervised machine learning approach, this study integrates radon gas concentration data to predict the distance between a radon gas telemonitoring station and the impending earthquake epicenter. Three distinct machine learning algorithms were evaluated using data from six Java-Bali radon gas telemonitoring stations within an early warning system. The random forest algorithm emerged as the most effective, yielding an average root mean square error of 453.10 kilometers. The findings of this research significantly contribute to earthquake risk mitigation efforts. This work enhances our capability to anticipate seismic events, and more effective disaster preparedness and response strategies in earthquake-prone regions. © 2025, Intelektual Pustaka Media Utama. All rights reserved.;"Early warning system; Earthquake prediction; Location; Machine learning; Radon";NULL;"Earthquake epicenter prediction from the Java-Bali radon gas telemonitoring station using machine learning Predicting the location of earthquake epicenters is a critical aspect of earthquake forecasting, as it complements efforts to determine the time and magnitude of seismic events. This research addresses the challenge posed by the uncertainty in epicenter locations, particularly along the extensive plate faults of Indo-Australia and Eurasia. In these regions, effective earthquake prediction is compromised without accurate epicenter information, impeding mitigation strategies and complicating disaster impact estimation. The primary objective of this study is to devise an algorithm for forecasting earthquake epicenter locations by harnessing variations in radon gas concentrations on southern Java Island, Indonesia, as a predictive precursor. Using a supervised machine learning approach, this study integrates radon gas concentration data to predict the distance between a radon gas telemonitoring station and the impending earthquake epicenter. Three distinct machine learning algorithms were evaluated using data from six Java-Bali radon gas telemonitoring stations within an early warning system. The random forest algorithm emerged as the most effective, yielding an average root mean square error of 453.10 kilometers. The findings of this research significantly contribute to earthquake risk mitigation efforts. This work enhances our capability to anticipate seismic events, and more effective disaster preparedness and response strategies in earthquake-prone regions. © 2025, Intelektual Pustaka Media Utama. All rights reserved. Early warning system; Earthquake prediction; Location; Machine learning; Radon NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
39;Unmanned Aerial Vehicle (UAV) Based Disaster Detection and Crowd Sensing Using Deep Learning Models;Natural disasters, such as earthquakes, floods, fires, and hurricanes, result in extensive damage and challenge effective disaster management and rescue efforts. Timely detection of disaster zones and accurate crowd density estimation are essential for optimizing response strategies. This study presents a UAV-based system utilizing deep learning models for disaster detection and crowd analysis to enhance situational awareness in disaster-affected areas. UAVs capture high-resolution aerial imagery, which is analyzed using the Aerial Image Dataset for Emergency Response (AIDER) for disaster zone classification, achieving over 90% accuracy in identifying critical events like building collapses, fires, floods, and traffic incidents. In addition, crowd density estimation is performed using the Shanghaitech dataset and CSRNet model, specifically designed for high-density crowd counting, providing reliable data on population clusters in disaster zones. Our approach demonstrates notable improvements over existing methods by integrating UAV capabilities with deep learning for real-time analysis, enabling better-informed decision-making during emergency response. This system offers a scalable, efficient solution for rapid disaster detection and crowd management, significantly advancing the capabilities of automated disaster response. © 2025 Copyright held by the owner/author(s).;"Aerial Imagery; Crowd Density Estimation; Crowd Sensing; CSRNet; Deep Learning; Disaster Detection; Emergency Response; Image Classification; Unmanned Aerial Vehicles (UAVs)";"Aerial photography; Aircraft accidents; Crowdsourcing; Deep learning; Disasters; Earthquakes; Risk management; Aerial imagery; Aerial vehicle; Crowd density; Crowd density estimation; Crowd sensing; CSRNet; Deep learning; Density estimation; Disaster detection; Emergency response; Images classification; Unmanned aerial vehicle; Unmanned aerial vehicles (UAV)";"Unmanned Aerial Vehicle (UAV) Based Disaster Detection and Crowd Sensing Using Deep Learning Models Natural disasters, such as earthquakes, floods, fires, and hurricanes, result in extensive damage and challenge effective disaster management and rescue efforts. Timely detection of disaster zones and accurate crowd density estimation are essential for optimizing response strategies. This study presents a UAV-based system utilizing deep learning models for disaster detection and crowd analysis to enhance situational awareness in disaster-affected areas. UAVs capture high-resolution aerial imagery, which is analyzed using the Aerial Image Dataset for Emergency Response (AIDER) for disaster zone classification, achieving over 90% accuracy in identifying critical events like building collapses, fires, floods, and traffic incidents. In addition, crowd density estimation is performed using the Shanghaitech dataset and CSRNet model, specifically designed for high-density crowd counting, providing reliable data on population clusters in disaster zones. Our approach demonstrates notable improvements over existing methods by integrating UAV capabilities with deep learning for real-time analysis, enabling better-informed decision-making during emergency response. This system offers a scalable, efficient solution for rapid disaster detection and crowd management, significantly advancing the capabilities of automated disaster response. © 2025 Copyright held by the owner/author(s). Aerial Imagery; Crowd Density Estimation; Crowd Sensing; CSRNet; Deep Learning; Disaster Detection; Emergency Response; Image Classification; Unmanned Aerial Vehicles (UAVs) Aerial photography; Aircraft accidents; Crowdsourcing; Deep learning; Disasters; Earthquakes; Risk management; Aerial imagery; Aerial vehicle; Crowd density; Crowd density estimation; Crowd sensing; CSRNet; Deep learning; Density estimation; Disaster detection; Emergency response; Images classification; Unmanned aerial vehicle; Unmanned aerial vehicles (UAV)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
40;The value of a statistical life in assessing seismic resilience enhancement policies: A stated preference binary choice experiment;To decide about investments in appropriate seismic vulnerability reduction and resilience enhancement measures, policymakers need to acquire adequate insight into the value of a statistical life (VSL). The VSL measures the monetary value the public assigns to reducing mortality due to a specific risk (e.g., dying from an earthquake) over a given period. In the absence of appropriate models to estimate the context-specific VSL for a given community, past studies have resorted to approaches such as using the VSL estimated for other purposes (e.g., transportation and environmental safety) or other countries (e.g., the US). This can lead to under- or over-estimation of VSL, misappropriation of the funds for seismic vulnerability reduction programs, and adverse economic, political, and social consequences. This study proposes a methodology that follows the stated preference binary choice approach to estimate the willingness to pay (WTP) to reduce the risk of earthquake-induced mortalities. The proposed method, which is applied to quantify VSL in Iran, involves using a questionnaire to collect the needed information about people's choices when investing in various risk reduction measures. It uses a test containing positivity and proportionality components to check the responses’ consistency. Regression modeling is used to estimate the respondents’ WTP and VSL. © 2024;"Benefit-cost analysis; Community and infrastructure resilience; Contingent valuation (CV); Seismic vulnerability reduction; Stated preferences (SP); Value of a statistical life (VSL)";"Earthquakes; Risk assessment; Risk perception; Benefit/cost analysis; Community resiliences; Contingent valuation; Contingent valuations; Infrastructure resiliences; Seismic vulnerability; Seismic vulnerability reduction; Stated preference; Stated preferences; Value of a statistical life; Value of a statistical lives; Vulnerability reductions; Cost benefit analysis";"The value of a statistical life in assessing seismic resilience enhancement policies: A stated preference binary choice experiment To decide about investments in appropriate seismic vulnerability reduction and resilience enhancement measures, policymakers need to acquire adequate insight into the value of a statistical life (VSL). The VSL measures the monetary value the public assigns to reducing mortality due to a specific risk (e.g., dying from an earthquake) over a given period. In the absence of appropriate models to estimate the context-specific VSL for a given community, past studies have resorted to approaches such as using the VSL estimated for other purposes (e.g., transportation and environmental safety) or other countries (e.g., the US). This can lead to under- or over-estimation of VSL, misappropriation of the funds for seismic vulnerability reduction programs, and adverse economic, political, and social consequences. This study proposes a methodology that follows the stated preference binary choice approach to estimate the willingness to pay (WTP) to reduce the risk of earthquake-induced mortalities. The proposed method, which is applied to quantify VSL in Iran, involves using a questionnaire to collect the needed information about people's choices when investing in various risk reduction measures. It uses a test containing positivity and proportionality components to check the responses’ consistency. Regression modeling is used to estimate the respondents’ WTP and VSL. © 2024 Benefit-cost analysis; Community and infrastructure resilience; Contingent valuation (CV); Seismic vulnerability reduction; Stated preferences (SP); Value of a statistical life (VSL) Earthquakes; Risk assessment; Risk perception; Benefit/cost analysis; Community resiliences; Contingent valuation; Contingent valuations; Infrastructure resiliences; Seismic vulnerability; Seismic vulnerability reduction; Stated preference; Stated preferences; Value of a statistical life; Value of a statistical lives; Vulnerability reductions; Cost benefit analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
41;A surrogate machine learning modeling approach for enhancing the efficiency of urban flood modeling at metropolitan scales;Urban drainage systems in metropolitan areas are highly complex, posing significant challenges for effective stormwater management. Traditional models like Storm Water Management Model (SWMM) are widely used but become inefficient at large scales with intricate drainage networks. This limitation is particularly critical for early warning systems, which require fast and simplified flood modeling methods. This study investigates surrogate machine learning (ML) models for efficient urban flood modeling at a metropolitan scale. Using SWMM as benchmark, the proposed model demonstrates its ability to replicate SWMM results, offering a more efficient alternative. We partition the system into hydrologically connected clusters, reducing 66,482 manholes to 363 manageable units. The approach combines this clustering strategy with ML modeling to predict key surcharge variables (flood duration, peak, and volume) for individual manholes across complex drainage system. Model validation demonstrates robust performance (R² > 0.8 for extreme events) while reducing computational time by 92.6%. Feature importance analysis reveals depth and duration as primary drivers of flood prediction, with model accuracy correlating to infrastructure density. The surrogate models excel particularly at predicting extreme events, with varying performance across different rainfall conditions. This computational efficiency enables real-time prediction updates crucial for emergency response planning and flood management strategies. © 2025;"Cluster analysis; Computational efficiency; Machine learning; Stormwater management; Surrogate modeling; Urban drainage systems";"Benchmarking; Cluster analysis; Efficiency; Risk assessment; Extreme events; Machine learning models; Machine-learning; Metropolitan area; Modeling approach; Stormwater management model(SWMM); Stormwater managements; Surrogate modeling; Urban drainage systems; Urban flood modelling; cluster analysis; drainage network; flooding; machine learning; metropolitan area; stormwater; water management; Storm sewers";"A surrogate machine learning modeling approach for enhancing the efficiency of urban flood modeling at metropolitan scales Urban drainage systems in metropolitan areas are highly complex, posing significant challenges for effective stormwater management. Traditional models like Storm Water Management Model (SWMM) are widely used but become inefficient at large scales with intricate drainage networks. This limitation is particularly critical for early warning systems, which require fast and simplified flood modeling methods. This study investigates surrogate machine learning (ML) models for efficient urban flood modeling at a metropolitan scale. Using SWMM as benchmark, the proposed model demonstrates its ability to replicate SWMM results, offering a more efficient alternative. We partition the system into hydrologically connected clusters, reducing 66,482 manholes to 363 manageable units. The approach combines this clustering strategy with ML modeling to predict key surcharge variables (flood duration, peak, and volume) for individual manholes across complex drainage system. Model validation demonstrates robust performance (R² > 0.8 for extreme events) while reducing computational time by 92.6%. Feature importance analysis reveals depth and duration as primary drivers of flood prediction, with model accuracy correlating to infrastructure density. The surrogate models excel particularly at predicting extreme events, with varying performance across different rainfall conditions. This computational efficiency enables real-time prediction updates crucial for emergency response planning and flood management strategies. © 2025 Cluster analysis; Computational efficiency; Machine learning; Stormwater management; Surrogate modeling; Urban drainage systems Benchmarking; Cluster analysis; Efficiency; Risk assessment; Extreme events; Machine learning models; Machine-learning; Metropolitan area; Modeling approach; Stormwater management model(SWMM); Stormwater managements; Surrogate modeling; Urban drainage systems; Urban flood modelling; cluster analysis; drainage network; flooding; machine learning; metropolitan area; stormwater; water management; Storm sewers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
42;An efficient seismic damage prediction method for high-speed railway track-bridge systems based on multi-task learning;Accurate and swift damage prediction at multiple critical locations in high-speed railway track-bridge systems (HSRTBSs) is crucial for achieving an effective post-earthquake structural restoration. Currently, seismic damage prediction methods mostly concentrate on singular prediction tasks at fixed structural points and cannot meet the multifaceted requirements of complex infrastructures that require damage assessments across various susceptible components and pivotal locations. To address this shortcoming, this study introduces a multi-task deep learning seismic damage prediction method (TAL) applicable to both classification and regression tasks. The proposed TAL adopts a multi-task neural network that includes three modules: a shared network module based on temporal convolutional networks (TCNs), a branched network module that integrates the long short-term memory (LSTM) models through self-attention mechanisms, and a damage classification module. In addition, a joint loss function of the damage classification module is implemented using an enhanced GradNorm gradient optimization method. This study performs a classification task to categorize bearing damage in the bridge substructure and a regression task to assess sliding layer damage in the track substructure using a high-speed railway track-bridge system. This facilitates multi-point damage prediction across diverse locations and components. The results indicate that the proposed TAL multi-task neural network can accurately predict multi-point damage in a high-speed railway track-bridge system under stochastic seismic activities, achieving an average precision of 97 % for bearing assessments and 95 % for sliding layers. In addition, it is shown that the damage classification module can effectively balance varying task losses using a gradient-refined joint loss function, achieving a robust performance in diverse neural network architectures. The analysis results show that each element of the proposed model can significantly improve the overall prediction accuracy of the model. Moreover, the multi-head self-attention mechanism synthesizes local features with global contextual insights. The results presented in this study could provide technical support for regional seismic risk evaluations and advancing performance-based seismic design techniques. © 2025 Elsevier Ltd;"Composite neural network; High-speed railway track-bridge system; Multi-task learning; Stochastic earthquake";"Bridge components; Earthquake effects; Prediction models; Railroad bridges; Railroad tracks; Railroad transportation; Railroad yards and terminals; Railroads; Risk analysis; Seismic design; Bridge systems; Composite neural networks; High-speed railway track-bridge system; High-speed railways; Multi tasks; Multitask learning; Prediction methods; Railway track; Seismic damage prediction; Stochastic earthquakes; artificial neural network; bridge; earthquake damage; high-speed train; machine learning; prediction; seismic design; Risk assessment";"An efficient seismic damage prediction method for high-speed railway track-bridge systems based on multi-task learning Accurate and swift damage prediction at multiple critical locations in high-speed railway track-bridge systems (HSRTBSs) is crucial for achieving an effective post-earthquake structural restoration. Currently, seismic damage prediction methods mostly concentrate on singular prediction tasks at fixed structural points and cannot meet the multifaceted requirements of complex infrastructures that require damage assessments across various susceptible components and pivotal locations. To address this shortcoming, this study introduces a multi-task deep learning seismic damage prediction method (TAL) applicable to both classification and regression tasks. The proposed TAL adopts a multi-task neural network that includes three modules: a shared network module based on temporal convolutional networks (TCNs), a branched network module that integrates the long short-term memory (LSTM) models through self-attention mechanisms, and a damage classification module. In addition, a joint loss function of the damage classification module is implemented using an enhanced GradNorm gradient optimization method. This study performs a classification task to categorize bearing damage in the bridge substructure and a regression task to assess sliding layer damage in the track substructure using a high-speed railway track-bridge system. This facilitates multi-point damage prediction across diverse locations and components. The results indicate that the proposed TAL multi-task neural network can accurately predict multi-point damage in a high-speed railway track-bridge system under stochastic seismic activities, achieving an average precision of 97 % for bearing assessments and 95 % for sliding layers. In addition, it is shown that the damage classification module can effectively balance varying task losses using a gradient-refined joint loss function, achieving a robust performance in diverse neural network architectures. The analysis results show that each element of the proposed model can significantly improve the overall prediction accuracy of the model. Moreover, the multi-head self-attention mechanism synthesizes local features with global contextual insights. The results presented in this study could provide technical support for regional seismic risk evaluations and advancing performance-based seismic design techniques. © 2025 Elsevier Ltd Composite neural network; High-speed railway track-bridge system; Multi-task learning; Stochastic earthquake Bridge components; Earthquake effects; Prediction models; Railroad bridges; Railroad tracks; Railroad transportation; Railroad yards and terminals; Railroads; Risk analysis; Seismic design; Bridge systems; Composite neural networks; High-speed railway track-bridge system; High-speed railways; Multi tasks; Multitask learning; Prediction methods; Railway track; Seismic damage prediction; Stochastic earthquakes; artificial neural network; bridge; earthquake damage; high-speed train; machine learning; prediction; seismic design; Risk assessment";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;4;Recovery
43;Fire-Image-DenseNet (FIDN) for predicting wildfire burnt area using remote sensing data;Predicting the extent of massive wildfires once ignited is essential to reduce the subsequent socioeconomic losses and environmental damage, but challenging because of the complexity of fire behavior. Existing physics-based models are limited in predicting large or long-duration wildfire events. Here, we develop a deep-learning-based predictive model, Fire-Image-DenseNet (FIDN), that uses spatial features derived from both near real-time and reanalysis data on the environmental and meteorological drivers of wildfire. We trained and tested this model using more than 300 individual wildfires that occurred between 2012 and 2019 in the western US. In contrast to existing models, the performance of FIDN does not degrade with fire size or duration. Furthermore, it predicts final burnt area accurately even in very heterogeneous landscapes in terms of fuel density and flammability. The FIDN model showed higher accuracy, with a mean squared error (MSE) about 82% and 67% lower than those of the predictive models based on cellular automata (CA) and the minimum travel time (MTT) approaches, respectively. Its structural similarity index measure (SSIM) averages 97%, outperforming the CA and FlamMap MTT models by 6% and 2%, respectively. Additionally, FIDN is approximately three orders of magnitude faster than both CA and MTT models. The enhanced computational efficiency and accuracy advancements offer vital insights for strategic planning and resource allocation for firefighting operations. © 2024 The Authors;"Cellular automata; Deep learning; Densenet; FlamMap; Wildfire prediction";"Fire extinguishers; Mean square error; Network security; Prediction models; Resource allocation; Strategic planning; Travel time; Burnt areas; Cellular automatons; Deep learning; Densenet; Flammap; Minimum travel time; Predictive models; Remote sensing data; Travel time model; Wildfire prediction; cellular automaton; complexity; data set; environmental change; machine learning; prediction; remote sensing; wildfire; Premixed flames";"Fire-Image-DenseNet (FIDN) for predicting wildfire burnt area using remote sensing data Predicting the extent of massive wildfires once ignited is essential to reduce the subsequent socioeconomic losses and environmental damage, but challenging because of the complexity of fire behavior. Existing physics-based models are limited in predicting large or long-duration wildfire events. Here, we develop a deep-learning-based predictive model, Fire-Image-DenseNet (FIDN), that uses spatial features derived from both near real-time and reanalysis data on the environmental and meteorological drivers of wildfire. We trained and tested this model using more than 300 individual wildfires that occurred between 2012 and 2019 in the western US. In contrast to existing models, the performance of FIDN does not degrade with fire size or duration. Furthermore, it predicts final burnt area accurately even in very heterogeneous landscapes in terms of fuel density and flammability. The FIDN model showed higher accuracy, with a mean squared error (MSE) about 82% and 67% lower than those of the predictive models based on cellular automata (CA) and the minimum travel time (MTT) approaches, respectively. Its structural similarity index measure (SSIM) averages 97%, outperforming the CA and FlamMap MTT models by 6% and 2%, respectively. Additionally, FIDN is approximately three orders of magnitude faster than both CA and MTT models. The enhanced computational efficiency and accuracy advancements offer vital insights for strategic planning and resource allocation for firefighting operations. © 2024 The Authors Cellular automata; Deep learning; Densenet; FlamMap; Wildfire prediction Fire extinguishers; Mean square error; Network security; Prediction models; Resource allocation; Strategic planning; Travel time; Burnt areas; Cellular automatons; Deep learning; Densenet; Flammap; Minimum travel time; Predictive models; Remote sensing data; Travel time model; Wildfire prediction; cellular automaton; complexity; data set; environmental change; machine learning; prediction; remote sensing; wildfire; Premixed flames";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;3;Response
44;Disaster Management Systems: Utilizing YOLOv9 for Precise Monitoring of River Flood Flow Levels Using Video Surveillance;Flood disasters pose significant threats to human lives and infrastructure, necessitating advanced methods for the timely and accurate monitoring of water levels in rivers. This study introduces an innovative approach utilizing the recent YOLOv9 (You Only Look Once, version 9) deep learning model to detect and monitor river water flow levels effectively. By leveraging YOLOv9’s robust object detection capabilities, the proposed system achieves precise segmentation and real-time analysis of river water regions. The method not only identifies river water areas with high accuracy but also quantifies flow levels, providing critical data for flood prediction and management. Comprehensive testing was conducted using diverse datasets representing various river conditions and flow scenarios. The results demonstrate that YOLOv9 significantly outperforms traditional methods in terms of detection speed and accuracy, making it a valuable tool for enhancing flood disaster response and management strategies. This work contributes to the development of more resilient and proactive flood disaster management systems through the integration of state-of-the-art machine learning techniques, offering significant improvements in predictive capabilities and response effectiveness. The proposed system was trained and evaluated using real-time river flood datasets, achieving a notable accuracy rate of 98%, a mean average precision (mAP) of 97.5%, precision of 98%, and recall of 98% in predicting flood level characteristics. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2025.;"Deep learning; Flood disaster management; Machine learning; Object detection; River water flow detection; Water level segmentation; YOLOv9";NULL;"Disaster Management Systems: Utilizing YOLOv9 for Precise Monitoring of River Flood Flow Levels Using Video Surveillance Flood disasters pose significant threats to human lives and infrastructure, necessitating advanced methods for the timely and accurate monitoring of water levels in rivers. This study introduces an innovative approach utilizing the recent YOLOv9 (You Only Look Once, version 9) deep learning model to detect and monitor river water flow levels effectively. By leveraging YOLOv9’s robust object detection capabilities, the proposed system achieves precise segmentation and real-time analysis of river water regions. The method not only identifies river water areas with high accuracy but also quantifies flow levels, providing critical data for flood prediction and management. Comprehensive testing was conducted using diverse datasets representing various river conditions and flow scenarios. The results demonstrate that YOLOv9 significantly outperforms traditional methods in terms of detection speed and accuracy, making it a valuable tool for enhancing flood disaster response and management strategies. This work contributes to the development of more resilient and proactive flood disaster management systems through the integration of state-of-the-art machine learning techniques, offering significant improvements in predictive capabilities and response effectiveness. The proposed system was trained and evaluated using real-time river flood datasets, achieving a notable accuracy rate of 98%, a mean average precision (mAP) of 97.5%, precision of 98%, and recall of 98% in predicting flood level characteristics. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2025. Deep learning; Flood disaster management; Machine learning; Object detection; River water flow detection; Water level segmentation; YOLOv9 NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
45;Onsite intensity prediction for earthquake early warning with multimodal deep learning;Rapid and accurate prediction of onsite intensity is essential for evaluating earthquake damage to the ground and buildings in earthquake early warning (EEW). Some scholars have used a single feature (such as peak displacement) extracted from the P-wave to establish onsite intensity prediction equations or employ single-mode data (such as seismic waves) to establish machine learning prediction models. However, accurately predicting onsite intensity using one type of data and limited information is difficult. To effectively utilize the information within data of different modalities and improve the reliability of onsite intensity prediction, using earthquake events collected from the Japanese K-NET network, we propose the Multimodal Onsite Intensity Predictor (MOIPor) based on deep learning for EEW and explore the feasibility of MOIPor for onsite intensity prediction in the region of Japan. MOIPor extracts features from three different multimodal data, namely, time-domain data, spectrum data, and text data, obtained from a single station and then uses feature fusion and a multilayer perceptron to output the predicted onsite intensity. The results show that when data from 3 s after the P-wave arrival are used in the test dataset, the reliability of the onsite intensity prediction of MOIPor is greater than that of baseline models. Moreover, we applied MOIPor to the 2021 M7.3 earthquake sequence along the coast of Fukushima in Japan for offline testing, and within 5 s after P-wave arrival, the accuracy of successful alarms based on the intensity predicted by MOIPor exceeded 90 %, with no false alarms. We infer from the results that MOIPor can predict onsite intensity for EEW quickly and reliably in the Japanese region. © 2025 Elsevier Ltd;"Deep learning; Earthquake early warning; Intensity; Multimodal; Onsite";"Japan; Accurate prediction; Deep learning; Earthquake damages; Earthquake early warning; Intensity; Intensity prediction; Multi-modal; Onsite; P-wave arrival; Peak displacement; early warning system; earthquake damage; earthquake intensity; earthquake magnitude; earthquake prediction; machine learning; P-wave; Prediction models";"Onsite intensity prediction for earthquake early warning with multimodal deep learning Rapid and accurate prediction of onsite intensity is essential for evaluating earthquake damage to the ground and buildings in earthquake early warning (EEW). Some scholars have used a single feature (such as peak displacement) extracted from the P-wave to establish onsite intensity prediction equations or employ single-mode data (such as seismic waves) to establish machine learning prediction models. However, accurately predicting onsite intensity using one type of data and limited information is difficult. To effectively utilize the information within data of different modalities and improve the reliability of onsite intensity prediction, using earthquake events collected from the Japanese K-NET network, we propose the Multimodal Onsite Intensity Predictor (MOIPor) based on deep learning for EEW and explore the feasibility of MOIPor for onsite intensity prediction in the region of Japan. MOIPor extracts features from three different multimodal data, namely, time-domain data, spectrum data, and text data, obtained from a single station and then uses feature fusion and a multilayer perceptron to output the predicted onsite intensity. The results show that when data from 3 s after the P-wave arrival are used in the test dataset, the reliability of the onsite intensity prediction of MOIPor is greater than that of baseline models. Moreover, we applied MOIPor to the 2021 M7.3 earthquake sequence along the coast of Fukushima in Japan for offline testing, and within 5 s after P-wave arrival, the accuracy of successful alarms based on the intensity predicted by MOIPor exceeded 90 %, with no false alarms. We infer from the results that MOIPor can predict onsite intensity for EEW quickly and reliably in the Japanese region. © 2025 Elsevier Ltd Deep learning; Earthquake early warning; Intensity; Multimodal; Onsite Japan; Accurate prediction; Deep learning; Earthquake damages; Earthquake early warning; Intensity; Intensity prediction; Multi-modal; Onsite; P-wave arrival; Peak displacement; early warning system; earthquake damage; earthquake intensity; earthquake magnitude; earthquake prediction; machine learning; P-wave; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
46;Reinforced NEAT Algorithms for Autonomous Rover Navigation in Multi-Room Dynamic Scenario;This paper demonstrates the performance of autonomous rovers utilizing NeuroEvolution of Augmenting Topologies (NEAT) in multi-room scenarios and explores their potential applications in wildfire management and search and rescue missions. Simulations in three- and four-room scenarios were conducted over 100 to 10,000 generations, comparing standard learning with transfer learning from a pre-trained single-room model. The task required rovers to visit all rooms before returning to the starting point. Performance metrics included fitness score, successful room visits, and return rates. The results revealed significant improvements in rover performance across generations for both scenarios, with transfer learning providing substantial advantages, particularly in early generations. Transfer learning achieved 32 successful returns after 10,000 generations for the three-room scenario compared to 34 with standard learning. In the four-room scenario, transfer learning achieved 32 successful returns. Heatmap analyses highlighted efficient navigation strategies, particularly around starting points and target zones. This study highlights NEAT’s adaptability to complex navigation problems, showcasing the utility of transfer learning. Additionally, it proposes the integration of NEAT with UAV systems and collaborative robotic frameworks for fire suppression, fuel characterization, and dynamic fire boundary detection, further strengthening its role in real-world emergency management. © 2025 by the authors.;"artificial neural networks; autonomous navigation; evolutionary algorithms; fire boundary detection; firefighting robotics; fuel characterization; multi-room environments; NEAT (NeuroEvolution of Augmenting Topologies); path planning; reinforcement learning; rover simulation; transfer learning; UAV collaboration; wildfire management";NULL;"Reinforced NEAT Algorithms for Autonomous Rover Navigation in Multi-Room Dynamic Scenario This paper demonstrates the performance of autonomous rovers utilizing NeuroEvolution of Augmenting Topologies (NEAT) in multi-room scenarios and explores their potential applications in wildfire management and search and rescue missions. Simulations in three- and four-room scenarios were conducted over 100 to 10,000 generations, comparing standard learning with transfer learning from a pre-trained single-room model. The task required rovers to visit all rooms before returning to the starting point. Performance metrics included fitness score, successful room visits, and return rates. The results revealed significant improvements in rover performance across generations for both scenarios, with transfer learning providing substantial advantages, particularly in early generations. Transfer learning achieved 32 successful returns after 10,000 generations for the three-room scenario compared to 34 with standard learning. In the four-room scenario, transfer learning achieved 32 successful returns. Heatmap analyses highlighted efficient navigation strategies, particularly around starting points and target zones. This study highlights NEAT’s adaptability to complex navigation problems, showcasing the utility of transfer learning. Additionally, it proposes the integration of NEAT with UAV systems and collaborative robotic frameworks for fire suppression, fuel characterization, and dynamic fire boundary detection, further strengthening its role in real-world emergency management. © 2025 by the authors. artificial neural networks; autonomous navigation; evolutionary algorithms; fire boundary detection; firefighting robotics; fuel characterization; multi-room environments; NEAT (NeuroEvolution of Augmenting Topologies); path planning; reinforcement learning; rover simulation; transfer learning; UAV collaboration; wildfire management NULL";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.4;Climatological;3;Response
47;A storm-time global electron density reconstruction model in three-dimensions based on artificial neural networks;We present results of a dedicated global storm-time model for the reconstruction of ionospheric electron density in three-dimensions. Using the storm criterion of |Dst|⩾50 nT and Kp⩾4, the model is constructed using a combination of radio occultation and ionosonde data during the periods of 2006–2021 and 2000–2020, respectively. From the ionosonde data, only the bottomside electron density profiles up to the maximum height of the F2 layer (hmF2) are considered. In addition to the selection of storm-time data only for the model development, we have investigated the inclusion of time history for the geomagnetic storm indicator Kp at 9 and 33 h in an attempt to take into account the delay of physical processes related to atmospheric gravity waves or traveling ionospheric disturbances and thermospheric composition changes which drive varying ionospheric storm effects during storm conditions. Based on incoherent scatter radar data and in comparison with the IRI 2020 model, the developed storm-time model provides foF2 modelling improvement of above 50% during the storm main phase over Millstone Hill (42.6°N, 71.5°W) and Tromsø (69.6°N, 19.2°E) for the storm periods of 3–6 November 2021 and 23–25 March 2023, respectively. Modelled results for Jicamarca (11.8°S, 77.2°W) show that the storm-time model estimates foF2 by an improvement of over 20% during the main phase of the 07–10 September 2017 storm period. As the ionospheric conditions return to quiet time levels, the IRI 2020 model perform better than the constructed storm -time model © 2024 COSPAR;"Electron density modelling; Ionosonde and radio occultation data; IRI model";"Atmospheric electricity; Carrier concentration; Digital storage; Electron density measurement; Electrons; Geomagnetism; Radiosondes; Density reconstruction; Electron density models; Electron-density profile; Ionosonde and radio occultation data; Ionosonde data; Ionospheric electron densities; IRI model; Radio occultations; Three dimensions; Time modeling; Storms";"A storm-time global electron density reconstruction model in three-dimensions based on artificial neural networks We present results of a dedicated global storm-time model for the reconstruction of ionospheric electron density in three-dimensions. Using the storm criterion of |Dst|⩾50 nT and Kp⩾4, the model is constructed using a combination of radio occultation and ionosonde data during the periods of 2006–2021 and 2000–2020, respectively. From the ionosonde data, only the bottomside electron density profiles up to the maximum height of the F2 layer (hmF2) are considered. In addition to the selection of storm-time data only for the model development, we have investigated the inclusion of time history for the geomagnetic storm indicator Kp at 9 and 33 h in an attempt to take into account the delay of physical processes related to atmospheric gravity waves or traveling ionospheric disturbances and thermospheric composition changes which drive varying ionospheric storm effects during storm conditions. Based on incoherent scatter radar data and in comparison with the IRI 2020 model, the developed storm-time model provides foF2 modelling improvement of above 50% during the storm main phase over Millstone Hill (42.6°N, 71.5°W) and Tromsø (69.6°N, 19.2°E) for the storm periods of 3–6 November 2021 and 23–25 March 2023, respectively. Modelled results for Jicamarca (11.8°S, 77.2°W) show that the storm-time model estimates foF2 by an improvement of over 20% during the main phase of the 07–10 September 2017 storm period. As the ionospheric conditions return to quiet time levels, the IRI 2020 model perform better than the constructed storm -time model © 2024 COSPAR Electron density modelling; Ionosonde and radio occultation data; IRI model Atmospheric electricity; Carrier concentration; Digital storage; Electron density measurement; Electrons; Geomagnetism; Radiosondes; Density reconstruction; Electron density models; Electron-density profile; Ionosonde and radio occultation data; Ionosonde data; Ionospheric electron densities; IRI model; Radio occultations; Three dimensions; Time modeling; Storms";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
48;Improvement of failure time prediction of rainfall-induced landslides while reducing the fluctuation by the relationship between velocity and acceleration;Background: Measurement of surface displacement (SD) plays a major role in predicting the failure time of landslides triggered by rainfall and is the basis for predicting the failure time of a slope Fukuzono (In proc. IVth International Conference and Field Workshops on Landslides, Tokyo, 1985). To study this concept, small-scale indoor slope models of sandy materials were tested under simulated rainfall with constant discharge to observe the behaviour of SD. Five cases of experiments were conducted by changing the conditions of the model, such as rainfall intensity, slope inclination and void ratio. Then, the relationship between the velocity and acceleration of the SD was examined for each case. Accordingly, failure time was predicted using two different methods and considering the behaviour of the slope just before failure. The first method was the original Fukuzon’s inverse-velocity (INV) method, which uses the inverse number of velocities and the increment of velocities at two different times. The second method was a new approach based on the velocity and acceleration (VAA) method and was carried out using the logarithmic acceleration and logarithmic velocity values for linear regression analysis. Results: The linear relationship between velocity and acceleration on a logarithmic scale could be established under different stress conditions. The relationship between velocity and acceleration was identical under different stress conditions within the small range of experimental conditions and variations in the small-scale indoor model slope. However, in the relationship obtained from the natural slope experiment, Futtsu revealed that the relationship varied slightly from the indoor small-scale model slope’s results. The analysis results showed that the VAA method could acquire a more accurate failure time than the INV method. Conclusions: The linear relationship between the velocity and acceleration of the SD shows that an early warning system for rainfall-induced landslides can be well established based on the relationship between the velocity and the acceleration of SD. The most accurate failure time prediction by the VAA method was due to reducing the scattering of the velocity values generated by the noise of the measured SD data using the least-squares method. In contrast, the individual velocity values greatly affected the INV method. © The Author(s) 2025.;"Acceleration; Displacement; Model slope; Rainfall-induced landslide; Velocity";NULL;"Improvement of failure time prediction of rainfall-induced landslides while reducing the fluctuation by the relationship between velocity and acceleration Background: Measurement of surface displacement (SD) plays a major role in predicting the failure time of landslides triggered by rainfall and is the basis for predicting the failure time of a slope Fukuzono (In proc. IVth International Conference and Field Workshops on Landslides, Tokyo, 1985). To study this concept, small-scale indoor slope models of sandy materials were tested under simulated rainfall with constant discharge to observe the behaviour of SD. Five cases of experiments were conducted by changing the conditions of the model, such as rainfall intensity, slope inclination and void ratio. Then, the relationship between the velocity and acceleration of the SD was examined for each case. Accordingly, failure time was predicted using two different methods and considering the behaviour of the slope just before failure. The first method was the original Fukuzon’s inverse-velocity (INV) method, which uses the inverse number of velocities and the increment of velocities at two different times. The second method was a new approach based on the velocity and acceleration (VAA) method and was carried out using the logarithmic acceleration and logarithmic velocity values for linear regression analysis. Results: The linear relationship between velocity and acceleration on a logarithmic scale could be established under different stress conditions. The relationship between velocity and acceleration was identical under different stress conditions within the small range of experimental conditions and variations in the small-scale indoor model slope. However, in the relationship obtained from the natural slope experiment, Futtsu revealed that the relationship varied slightly from the indoor small-scale model slope’s results. The analysis results showed that the VAA method could acquire a more accurate failure time than the INV method. Conclusions: The linear relationship between the velocity and acceleration of the SD shows that an early warning system for rainfall-induced landslides can be well established based on the relationship between the velocity and the acceleration of SD. The most accurate failure time prediction by the VAA method was due to reducing the scattering of the velocity values generated by the noise of the measured SD data using the least-squares method. In contrast, the individual velocity values greatly affected the INV method. © The Author(s) 2025. Acceleration; Displacement; Model slope; Rainfall-induced landslide; Velocity NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
49;Development of a novel hybrid hydrodynamic particle simulation methodology to estimate discharge coefficient for broad-crested weirs;Weirs are crucial for flow measurement and water level regulation, contributing to water resources management by controlling discharge, reducing flood risks, and supporting irrigation and ecosystems. This study introduces a novel methodology that combines hydrodynamic particle simulation with physics-enhanced machine learning models to estimate the discharge coefficient (Cd) for broad-crested weirs. Thrrough conducting 432 simulations, impacts of geometric parameters, including crest length, weir height, slope angles on Cd were investigated. The primary outcome is the development of multi-variable regression equations to predict Cd, along with detailed water level and velocity profile analyses. Three advanced models: Physics-Enhanced Machine Learning (PEML), Physics-Regularized Regression Trees (PRRT), and Hybrid Hydrodynamic Particle Simulation (HHPS) are evaluated. The HHPS model outperforms others with DC of 0.998 and 0.996, RMSE of 0.013 and 0.017, WI of 0.999 and 0.998, and NSE of 0.998 and 0.997 for training and testing dataset, respectively, showing exceptional predictive accuracy. A sensitivity analysis using SHapley Additive exPlanations (SHAP) was used in this study. Upstream head-to-weir height ratio (H1/P) and flow rate (Q) with SHAP values of +0.15 and + 0.11, respectively, have the greatest impact on Cd modeling. Also, this study enhances the understanding of weir flow dynamics and provides practical tools for engineers and hydrologists. By integrating physics-based simulations with machine learning, it sets a new precision benchmark for hydraulic structure design and analysis, impacting water resource management and environmental engineering. © 2024 Elsevier Ltd;"Broad-crested weir; Discharge coefficient; Hybrid hydrodynamic particle simulation; Numerical simulation; Physics based machine learning; Physics-enhanced machine learning; Physics-regularized regression trees";"Benchmarking; Flood control; Hydraulic tools; Hydrodynamics; Regression analysis; Resource allocation; Sensitivity analysis; Water management; Broad-crested weir; Discharge coefficients; Hybrid hydrodynamic particle simulation; Hydrodynamic particles; Machine-learning; Particle simulations; Physic based machine learning; Physic-enhanced machine learning; Physic-regularized regression tree; Physics-based; Regression trees; Weirs";"Development of a novel hybrid hydrodynamic particle simulation methodology to estimate discharge coefficient for broad-crested weirs Weirs are crucial for flow measurement and water level regulation, contributing to water resources management by controlling discharge, reducing flood risks, and supporting irrigation and ecosystems. This study introduces a novel methodology that combines hydrodynamic particle simulation with physics-enhanced machine learning models to estimate the discharge coefficient (Cd) for broad-crested weirs. Thrrough conducting 432 simulations, impacts of geometric parameters, including crest length, weir height, slope angles on Cd were investigated. The primary outcome is the development of multi-variable regression equations to predict Cd, along with detailed water level and velocity profile analyses. Three advanced models: Physics-Enhanced Machine Learning (PEML), Physics-Regularized Regression Trees (PRRT), and Hybrid Hydrodynamic Particle Simulation (HHPS) are evaluated. The HHPS model outperforms others with DC of 0.998 and 0.996, RMSE of 0.013 and 0.017, WI of 0.999 and 0.998, and NSE of 0.998 and 0.997 for training and testing dataset, respectively, showing exceptional predictive accuracy. A sensitivity analysis using SHapley Additive exPlanations (SHAP) was used in this study. Upstream head-to-weir height ratio (H1/P) and flow rate (Q) with SHAP values of +0.15 and + 0.11, respectively, have the greatest impact on Cd modeling. Also, this study enhances the understanding of weir flow dynamics and provides practical tools for engineers and hydrologists. By integrating physics-based simulations with machine learning, it sets a new precision benchmark for hydraulic structure design and analysis, impacting water resource management and environmental engineering. © 2024 Elsevier Ltd Broad-crested weir; Discharge coefficient; Hybrid hydrodynamic particle simulation; Numerical simulation; Physics based machine learning; Physics-enhanced machine learning; Physics-regularized regression trees Benchmarking; Flood control; Hydraulic tools; Hydrodynamics; Regression analysis; Resource allocation; Sensitivity analysis; Water management; Broad-crested weir; Discharge coefficients; Hybrid hydrodynamic particle simulation; Hydrodynamic particles; Machine-learning; Particle simulations; Physic based machine learning; Physic-enhanced machine learning; Physic-regularized regression tree; Physics-based; Regression trees; Weirs";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
50;Discrimination of earthquakes, explosions, and collapses based on the deep learning: Applications to DiTing 2.0 dataset;The discrimination of natural and unnatural seismic events is an important part of earthquake monitoring and early warning. Deep learning algorithms, with their powerful feature extraction and classification capabilities, are extensively applied in seismic event identification. In this study, we utilized the DiTing 2.0 dataset to develop binary-class networks for distinguishing low-magnitude earthquakes from explosions, as well as three-class networks for identifying low-magnitude earthquakes, explosions, and collapses. The accuracies achieved for discriminating earthquakes from explosions using waveform and spectrogram datasets are 94% and 87%, respectively. The accuracies for discriminating earthquakes, explosions, and collapses using waveform and spectrogram datasets are 85% and 83%, respectively. We then apply the trained three-class model to discriminate explosions and collapses in four different regions in China. The prediction results indicate that the trained model can accurately identify event types and exhibits a good performance in low-magnitude seismic event (ML <5) discrimination, demonstrating the effectiveness and generality of the models developed in this study. © 2024 Elsevier Ltd;"Collapse; Convolutional neural networks; Explosion; Natural earthquake";"China; Collapse; Convolutional neural network; Early warning; Earthquake monitoring; Event identification; Feature extraction and classification; Natural earthquake; Seismic event; Spectrograms; Waveforms; artificial neural network; collapse; data set; earthquake event; earthquake magnitude; explosion; identification method; seismic discrimination; waveform analysis; Convolutional neural networks";"Discrimination of earthquakes, explosions, and collapses based on the deep learning: Applications to DiTing 2.0 dataset The discrimination of natural and unnatural seismic events is an important part of earthquake monitoring and early warning. Deep learning algorithms, with their powerful feature extraction and classification capabilities, are extensively applied in seismic event identification. In this study, we utilized the DiTing 2.0 dataset to develop binary-class networks for distinguishing low-magnitude earthquakes from explosions, as well as three-class networks for identifying low-magnitude earthquakes, explosions, and collapses. The accuracies achieved for discriminating earthquakes from explosions using waveform and spectrogram datasets are 94% and 87%, respectively. The accuracies for discriminating earthquakes, explosions, and collapses using waveform and spectrogram datasets are 85% and 83%, respectively. We then apply the trained three-class model to discriminate explosions and collapses in four different regions in China. The prediction results indicate that the trained model can accurately identify event types and exhibits a good performance in low-magnitude seismic event (ML <5) discrimination, demonstrating the effectiveness and generality of the models developed in this study. © 2024 Elsevier Ltd Collapse; Convolutional neural networks; Explosion; Natural earthquake China; Collapse; Convolutional neural network; Early warning; Earthquake monitoring; Event identification; Feature extraction and classification; Natural earthquake; Seismic event; Spectrograms; Waveforms; artificial neural network; collapse; data set; earthquake event; earthquake magnitude; explosion; identification method; seismic discrimination; waveform analysis; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
51;Effectiveness of Random Forest Model for Flash Flood Susceptibility in the Himalayan Region;Machine learning (ML) approach is being increasingly recognized as vital tool for disaster risk reduction owing to its ability to address both the scale and the impact of a disaster. Risk of flash floods is currently a major problem across the Himalayan region. In present study we evaluated effectiveness of Random Forest (RF) model for flash flood susceptibility, based on real-world data in the Indian Central Himalaya Region (ICHR), where recurrent flash floods are being experienced every year. A geospatial dataset including 200 flash flood locations and eight conditioning factors namely elevation, slope, aspect, profile curvature, distance from river, annual rainfall, land use land cover (LULC) and lithology was used for performance evaluation of Random Forest model for flash flood susceptibility assessment. The effectiveness of the model is evaluated using area under the ROC curve (0.922), accuracy (0.925), precision (0.903), recall (0.921) and F-score (0.911) metrics. The results show that random forest could be an effective tool for flash flood susceptibility assessment in the Indian Central Himalayan Region. Furthermore, by considering optimum conditioning factors based on topographical, geological and hydrological conditions, the model can be used by managers and planners for flash flood management and sustainable conservation of the human society in the other Himalayan regions. Copyright © 2024 by Author/s and Licensed by JISEM.;"Conditioning Factors; Ensemble machine learning; Geospatial dataset; Mountainous Region";NULL;"Effectiveness of Random Forest Model for Flash Flood Susceptibility in the Himalayan Region Machine learning (ML) approach is being increasingly recognized as vital tool for disaster risk reduction owing to its ability to address both the scale and the impact of a disaster. Risk of flash floods is currently a major problem across the Himalayan region. In present study we evaluated effectiveness of Random Forest (RF) model for flash flood susceptibility, based on real-world data in the Indian Central Himalaya Region (ICHR), where recurrent flash floods are being experienced every year. A geospatial dataset including 200 flash flood locations and eight conditioning factors namely elevation, slope, aspect, profile curvature, distance from river, annual rainfall, land use land cover (LULC) and lithology was used for performance evaluation of Random Forest model for flash flood susceptibility assessment. The effectiveness of the model is evaluated using area under the ROC curve (0.922), accuracy (0.925), precision (0.903), recall (0.921) and F-score (0.911) metrics. The results show that random forest could be an effective tool for flash flood susceptibility assessment in the Indian Central Himalayan Region. Furthermore, by considering optimum conditioning factors based on topographical, geological and hydrological conditions, the model can be used by managers and planners for flash flood management and sustainable conservation of the human society in the other Himalayan regions. Copyright © 2024 by Author/s and Licensed by JISEM. Conditioning Factors; Ensemble machine learning; Geospatial dataset; Mountainous Region NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
52;Reinforcement learning-dijkstra-genetic algorithm for debris removal problem under different scenarios: An earthquake case study;Road reconstruction during post-earthquake crises is vital for search operations, relief efforts, the movement of individuals from affected areas, and the sending of food and relief items. Blockage of the roads may prevent the delivery of critical goods or cause a lack of cars and ambulances access to the affected areas to evacuate injured people. In many studies, road debris removal is a sophisticated issue, so only a few roads were considered or aimed at finding a route through which all affected areas can be visited. Since roads have limited capacity for vehicle traffic during severe earthquakes, a lack of full debris removal may lead to delayed movement of injured people from affected areas, and debris left after the crisis may result in environmental and psychological harm to affected people. To solve this problem, a two-step model-free approach based on reinforcement learning methods is developed for full debris removal from damaged roads. In the first step, the damaged area is initially assessed using an unmanned aerial vehicle (UAV) and a state-action-reward-state-action (SARSA) algorithm to estimate the damage. In the second step, debris removal groups remove debris from all blocked roads based on the information reported by the UAV. For this purpose, a heuristic algorithm is developed based on debris removal groups, the divergence of the genetic algorithm is ensured and the agent training matrix is regulated using the neural network. The proposed solution methods are examined under three scenarios of severe, moderate, and weak earthquakes in Rudbar city as an artificial intelligence application. The results indicate that the debris removal operation is completed in less than 80 h in the severe case if the proposed method is used, whereas the algorithm achieves the proper solution in less than 45 min. © 2025 Elsevier Ltd;"Artificial intelligence application; Debris removal; Neural network; Post-earthquake; Reinforced learning; Rudbar";"Unmanned aerial vehicles (UAV); Aerial vehicle; Affected area; Artificial intelligence application; Debris removal; Dijkstra; Neural-networks; Post-earthquake; Reinforced learning; Reinforcement learnings; Rudbar; Reinforcement learning";"Reinforcement learning-dijkstra-genetic algorithm for debris removal problem under different scenarios: An earthquake case study Road reconstruction during post-earthquake crises is vital for search operations, relief efforts, the movement of individuals from affected areas, and the sending of food and relief items. Blockage of the roads may prevent the delivery of critical goods or cause a lack of cars and ambulances access to the affected areas to evacuate injured people. In many studies, road debris removal is a sophisticated issue, so only a few roads were considered or aimed at finding a route through which all affected areas can be visited. Since roads have limited capacity for vehicle traffic during severe earthquakes, a lack of full debris removal may lead to delayed movement of injured people from affected areas, and debris left after the crisis may result in environmental and psychological harm to affected people. To solve this problem, a two-step model-free approach based on reinforcement learning methods is developed for full debris removal from damaged roads. In the first step, the damaged area is initially assessed using an unmanned aerial vehicle (UAV) and a state-action-reward-state-action (SARSA) algorithm to estimate the damage. In the second step, debris removal groups remove debris from all blocked roads based on the information reported by the UAV. For this purpose, a heuristic algorithm is developed based on debris removal groups, the divergence of the genetic algorithm is ensured and the agent training matrix is regulated using the neural network. The proposed solution methods are examined under three scenarios of severe, moderate, and weak earthquakes in Rudbar city as an artificial intelligence application. The results indicate that the debris removal operation is completed in less than 80 h in the severe case if the proposed method is used, whereas the algorithm achieves the proper solution in less than 45 min. © 2025 Elsevier Ltd Artificial intelligence application; Debris removal; Neural network; Post-earthquake; Reinforced learning; Rudbar Unmanned aerial vehicles (UAV); Aerial vehicle; Affected area; Artificial intelligence application; Debris removal; Dijkstra; Neural-networks; Post-earthquake; Reinforced learning; Reinforcement learnings; Rudbar; Reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.1;Geological;3;Response
53;Evaluation of seismic vulnerability using clustering based on fuzzy hypergraphs;Seismic vulnerability assessment is essential for reducing risks associated with earthquakes, particularly in urban regions with varied building attributes. This study introduces a novel methodology utilizing fuzzy hypergraph-based clustering to classify buildings into seismic risk categories. A dataset of 1000 buildings from the specified zone was analyzed, incorporating attributes such as building age, height, density, material type, and proximity to seismic hazards. The model achieved an overall accuracy of 90.10%, with exceptional performance in identifying high-risk buildings (97.66 %) and moderate success in classifying low-risk buildings (87.10 %). Error analysis revealed challenges in the low-risk category due to overlapping attribute characteristics and limited granularity near category boundaries. Clustering results underscored significant patterns: older, taller buildings were primarily classified as high-risk, while newer, shorter structures were generally assigned to the low-risk category. The proposed approach integrates fuzzy membership, non-membership, and hesitation degrees, offering a comprehensive framework for nuanced seismic vulnerability classification. Visualization tools, such as heatmaps and geographic maps, provided actionable insights into risk distributions, enabling targeted seismic risk mitigation strategies. Despite some challenges in refining category thresholds, the methodology demonstrates its utility for urban resilience planning and highlights areas for future enhancement, such as incorporating advanced geotechnical features and retrofitting data. This study establishes a robust framework for analyzing seismic vulnerabilities, equipping policymakers with the insights needed to prioritize retrofitting efforts and disaster preparedness measures effectively. © 2025 Institution of Structural Engineers;"Clustering; Earthquake; Fuzzy hypergraph; Fuzzy membership degree; Seismic vulnerability";"Disaster prevention; Earthquake effects; Earthquake engineering; Fuzzy clustering; Risk assessment; Seismic response; Urban planning; Clusterings; Fuzzy hypergraph; Fuzzy membership; Fuzzy membership degree; Hyper graph; Membership degrees; Risk categories; Seismic vulnerability; Urban regions; Vulnerability assessments; Retrofitting";"Evaluation of seismic vulnerability using clustering based on fuzzy hypergraphs Seismic vulnerability assessment is essential for reducing risks associated with earthquakes, particularly in urban regions with varied building attributes. This study introduces a novel methodology utilizing fuzzy hypergraph-based clustering to classify buildings into seismic risk categories. A dataset of 1000 buildings from the specified zone was analyzed, incorporating attributes such as building age, height, density, material type, and proximity to seismic hazards. The model achieved an overall accuracy of 90.10%, with exceptional performance in identifying high-risk buildings (97.66 %) and moderate success in classifying low-risk buildings (87.10 %). Error analysis revealed challenges in the low-risk category due to overlapping attribute characteristics and limited granularity near category boundaries. Clustering results underscored significant patterns: older, taller buildings were primarily classified as high-risk, while newer, shorter structures were generally assigned to the low-risk category. The proposed approach integrates fuzzy membership, non-membership, and hesitation degrees, offering a comprehensive framework for nuanced seismic vulnerability classification. Visualization tools, such as heatmaps and geographic maps, provided actionable insights into risk distributions, enabling targeted seismic risk mitigation strategies. Despite some challenges in refining category thresholds, the methodology demonstrates its utility for urban resilience planning and highlights areas for future enhancement, such as incorporating advanced geotechnical features and retrofitting data. This study establishes a robust framework for analyzing seismic vulnerabilities, equipping policymakers with the insights needed to prioritize retrofitting efforts and disaster preparedness measures effectively. © 2025 Institution of Structural Engineers Clustering; Earthquake; Fuzzy hypergraph; Fuzzy membership degree; Seismic vulnerability Disaster prevention; Earthquake effects; Earthquake engineering; Fuzzy clustering; Risk assessment; Seismic response; Urban planning; Clusterings; Fuzzy hypergraph; Fuzzy membership; Fuzzy membership degree; Hyper graph; Membership degrees; Risk categories; Seismic vulnerability; Urban regions; Vulnerability assessments; Retrofitting";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
54;Optimizing Deep Learning Models for Fire Detection, Classification, and Segmentation Using Satellite Images;Earth observation (EO) satellites offer significant potential in wildfire detection and assessment due to their ability to provide fine spatial, temporal, and spectral resolutions. Over the past decade, satellite data have been systematically utilized to monitor wildfire dynamics and evaluate their impacts, leading to substantial advancements in wildfire management strategies. The present study contributes to this field by enhancing the frequency and accuracy of wildfire detection through advanced techniques for detecting, classifying, and segmenting wildfires using satellite imagery. Publicly available multi-sensor satellite data, such as Landsat, Sentinel-1, and Sentinel-2, from 2018 to 2020 were employed, providing temporal observation frequencies of up to five days, which represents a 25% increase compared to traditional monitoring approaches. Sophisticated algorithms were developed and implemented to improve the accuracy of fire detection while minimizing false alarms. The study evaluated the performance of three distinct models: an autoencoder, a U-Net, and a convolutional neural network (CNN), comparing their effectiveness in predicting wildfire occurrences. The results indicated that the CNN model demonstrated superior performance, achieving a fire detection accuracy of 82%, which is approximately 10% higher than the best-performing model in similar studies. This accuracy, coupled with the model’s ability to balance various performance metrics and learnable weights, positions it as a promising tool for real-time wildfire detection. The findings underscore the significant potential of optimized machine learning approaches in predicting extreme events, such as wildfires, and improving fire management strategies. Achieving 82% detection accuracy in real-world applications could drastically reduce response times, minimize the damage caused by wildfires, and enhance resource allocation for firefighting efforts, emphasizing the importance of continued research in this domain. © 2025 by the authors.;"active fire detection; deep learning; semantic segmentation; wildfire";NULL;"Optimizing Deep Learning Models for Fire Detection, Classification, and Segmentation Using Satellite Images Earth observation (EO) satellites offer significant potential in wildfire detection and assessment due to their ability to provide fine spatial, temporal, and spectral resolutions. Over the past decade, satellite data have been systematically utilized to monitor wildfire dynamics and evaluate their impacts, leading to substantial advancements in wildfire management strategies. The present study contributes to this field by enhancing the frequency and accuracy of wildfire detection through advanced techniques for detecting, classifying, and segmenting wildfires using satellite imagery. Publicly available multi-sensor satellite data, such as Landsat, Sentinel-1, and Sentinel-2, from 2018 to 2020 were employed, providing temporal observation frequencies of up to five days, which represents a 25% increase compared to traditional monitoring approaches. Sophisticated algorithms were developed and implemented to improve the accuracy of fire detection while minimizing false alarms. The study evaluated the performance of three distinct models: an autoencoder, a U-Net, and a convolutional neural network (CNN), comparing their effectiveness in predicting wildfire occurrences. The results indicated that the CNN model demonstrated superior performance, achieving a fire detection accuracy of 82%, which is approximately 10% higher than the best-performing model in similar studies. This accuracy, coupled with the model’s ability to balance various performance metrics and learnable weights, positions it as a promising tool for real-time wildfire detection. The findings underscore the significant potential of optimized machine learning approaches in predicting extreme events, such as wildfires, and improving fire management strategies. Achieving 82% detection accuracy in real-world applications could drastically reduce response times, minimize the damage caused by wildfires, and enhance resource allocation for firefighting efforts, emphasizing the importance of continued research in this domain. © 2025 by the authors. active fire detection; deep learning; semantic segmentation; wildfire NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
55;Hybrid double ensemble empirical mode decomposition and K-Nearest Neighbors model with improved particle swarm optimization for water level forecasting;Water level forecasting plays a vital role in environmental protection and flood management because reliable predictions allow for the deployment of early warning systems to alert the public to minimize the impacts of flooding. This study presents an enhanced approach for weekly water level and flood prediction by integrating data decomposition techniques with machine learning models. Specifically, Ensemble Empirical Mode Decomposition (EEMD) was applied to disaggregate the original water level data into distinct Intrinsic Mode Functions (IMFs) to simplify complexity and enhance periodicity detection. A secondary decomposition was performed on the high-frequency IMF 1, derived from EEMD, to further refine the data features. The K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) models, optimized using Improved Particle Swarm Optimization (PSO), were employed for forecasting. The effectiveness of these hybrid models was evaluated using various performance metrics, revealing that the DEEMD-KNN-PSO and DEEMD-SVM-PSO models significantly outperformed other single decomposition and standalone models. Among these, the DEEMD-KNN-PSO model demonstrated superior accuracy in predicting water levels, showcasing its potential for reliable flood prediction in the Klang River region of Sri Muda, Malaysia. This approach highlights the value of data decomposition and machine learning optimization for improving water level prediction accuracy. © 2024 The Authors;"Artificial Intelligence; Flood Forecasting; Machine Learning; Predictive Analytics; Water Level";"Adversarial machine learning; Flood control; Information management; Nearest neighbor search; Prediction models; Predictive analytics; Support vector machines; Swarm intelligence; Data decomposition; Empirical Mode Decomposition; Flood forecasting; Flood prediction; Machine-learning; Nearest-neighbour; Particle swarm; Swarm optimization; Water level forecasting; Water level prediction; Empirical mode decomposition";"Hybrid double ensemble empirical mode decomposition and K-Nearest Neighbors model with improved particle swarm optimization for water level forecasting Water level forecasting plays a vital role in environmental protection and flood management because reliable predictions allow for the deployment of early warning systems to alert the public to minimize the impacts of flooding. This study presents an enhanced approach for weekly water level and flood prediction by integrating data decomposition techniques with machine learning models. Specifically, Ensemble Empirical Mode Decomposition (EEMD) was applied to disaggregate the original water level data into distinct Intrinsic Mode Functions (IMFs) to simplify complexity and enhance periodicity detection. A secondary decomposition was performed on the high-frequency IMF 1, derived from EEMD, to further refine the data features. The K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) models, optimized using Improved Particle Swarm Optimization (PSO), were employed for forecasting. The effectiveness of these hybrid models was evaluated using various performance metrics, revealing that the DEEMD-KNN-PSO and DEEMD-SVM-PSO models significantly outperformed other single decomposition and standalone models. Among these, the DEEMD-KNN-PSO model demonstrated superior accuracy in predicting water levels, showcasing its potential for reliable flood prediction in the Klang River region of Sri Muda, Malaysia. This approach highlights the value of data decomposition and machine learning optimization for improving water level prediction accuracy. © 2024 The Authors Artificial Intelligence; Flood Forecasting; Machine Learning; Predictive Analytics; Water Level Adversarial machine learning; Flood control; Information management; Nearest neighbor search; Prediction models; Predictive analytics; Support vector machines; Swarm intelligence; Data decomposition; Empirical Mode Decomposition; Flood forecasting; Flood prediction; Machine-learning; Nearest-neighbour; Particle swarm; Swarm optimization; Water level forecasting; Water level prediction; Empirical mode decomposition";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
56;Domain knowledge-guided intelligent recognition of multi-type potential landslides;Accurate identification of potential landslides is essential for early warning and prevention. While effective, traditional detection methods suffer from high workload, subjectivity, and inaccuracies due to inaccessible landslide distributions. This paper addresses these limitations by introducing a novel data-driven approach that leverages domain knowledge systematically. The approach is based on a three-stage embedding framework, integrating domain knowledge related to potential landslide characteristics, to enhance the recognition of various potential landslide types. Firstly, domain knowledge for potential landslide identification is constructed from the perspective of hazard characteristics. Then, the corresponding knowledge rules are embedded in the preparation, modeling, and prediction stages respectively. Tested in Zhenyuan County, Yunnan Province, China, the domain knowledge-guided random forest (DKRF) model demonstrates significantly improved interpretability and accuracy in classification. It increased accuracy and recall rates by 3 % and 15 %, respectively, and reduced missed and false classifications by 15 % and 3 %. This study not only advances the field of landslide research by offering a cost-effective investigative tool but also contributes to the development of interpretable machine-learning applications. © 2025;"Data-driven approach; Early warning; Landslide identification; Random forest; Three-stage embedding framework";"Embeddings; Landslides; Random forests; Data-driven approach; Detection methods; Domain knowledge; Early warning; Embeddings; Intelligent recognition; Landslide distributions; Landslide identification; Random forests; Three-stage embedding framework; Domain Knowledge";"Domain knowledge-guided intelligent recognition of multi-type potential landslides Accurate identification of potential landslides is essential for early warning and prevention. While effective, traditional detection methods suffer from high workload, subjectivity, and inaccuracies due to inaccessible landslide distributions. This paper addresses these limitations by introducing a novel data-driven approach that leverages domain knowledge systematically. The approach is based on a three-stage embedding framework, integrating domain knowledge related to potential landslide characteristics, to enhance the recognition of various potential landslide types. Firstly, domain knowledge for potential landslide identification is constructed from the perspective of hazard characteristics. Then, the corresponding knowledge rules are embedded in the preparation, modeling, and prediction stages respectively. Tested in Zhenyuan County, Yunnan Province, China, the domain knowledge-guided random forest (DKRF) model demonstrates significantly improved interpretability and accuracy in classification. It increased accuracy and recall rates by 3 % and 15 %, respectively, and reduced missed and false classifications by 15 % and 3 %. This study not only advances the field of landslide research by offering a cost-effective investigative tool but also contributes to the development of interpretable machine-learning applications. © 2025 Data-driven approach; Early warning; Landslide identification; Random forest; Three-stage embedding framework Embeddings; Landslides; Random forests; Data-driven approach; Detection methods; Domain knowledge; Early warning; Embeddings; Intelligent recognition; Landslide distributions; Landslide identification; Random forests; Three-stage embedding framework; Domain Knowledge";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
57;Urban Road Anomaly Monitoring Using Vision–Language Models for Enhanced Safety Management;Abnormal phenomena on urban roads, including uneven surfaces, garbage, traffic congestion, floods, fallen trees, fires, and traffic accidents, present significant risks to public safety and infrastructure, necessitating real-time monitoring and early warning systems. This study develops Urban Road Anomaly Visual Large Language Models (URA-VLMs), a generative AI-based framework designed for the monitoring of diverse urban road anomalies. The InternVL was selected as a foundational model due to its adaptability for this monitoring purpose. The URA-VLMs framework features dedicated modules for anomaly detection, flood depth estimation, and safety level assessment, utilizing multi-step prompting and retrieval-augmented generation (RAG) for precise and adaptive analysis. A comprehensive dataset of 3034 annotated images depicting various urban road scenarios was developed to evaluate the models. Experimental results demonstrate the system’s effectiveness, achieving an overall anomaly detection accuracy of 93.20%, outperforming state-of-the-art models such as InternVL2.5 and ResNet34. By facilitating early detection and real-time decision-making, this generative AI approach offers a scalable and robust solution that contributes to a smarter, safer road environment. © 2025 by the authors.;"real-time monitoring; road anomaly; safety management; urban road safety; vision large language model";"Advanced traffic management systems; Fire alarm systems; Highway accidents; Highway administration; Motor transportation; Public risks; Risk assessment; Urban transportation; Visual languages; Abnormal phenomenon; Anomaly detection; Language model; Real time monitoring; Road anomaly; Road safety; Safety management; Urban road; Urban road safety; Vision large language model; Traffic congestion";"Urban Road Anomaly Monitoring Using Vision–Language Models for Enhanced Safety Management Abnormal phenomena on urban roads, including uneven surfaces, garbage, traffic congestion, floods, fallen trees, fires, and traffic accidents, present significant risks to public safety and infrastructure, necessitating real-time monitoring and early warning systems. This study develops Urban Road Anomaly Visual Large Language Models (URA-VLMs), a generative AI-based framework designed for the monitoring of diverse urban road anomalies. The InternVL was selected as a foundational model due to its adaptability for this monitoring purpose. The URA-VLMs framework features dedicated modules for anomaly detection, flood depth estimation, and safety level assessment, utilizing multi-step prompting and retrieval-augmented generation (RAG) for precise and adaptive analysis. A comprehensive dataset of 3034 annotated images depicting various urban road scenarios was developed to evaluate the models. Experimental results demonstrate the system’s effectiveness, achieving an overall anomaly detection accuracy of 93.20%, outperforming state-of-the-art models such as InternVL2.5 and ResNet34. By facilitating early detection and real-time decision-making, this generative AI approach offers a scalable and robust solution that contributes to a smarter, safer road environment. © 2025 by the authors. real-time monitoring; road anomaly; safety management; urban road safety; vision large language model Advanced traffic management systems; Fire alarm systems; Highway accidents; Highway administration; Motor transportation; Public risks; Risk assessment; Urban transportation; Visual languages; Abnormal phenomenon; Anomaly detection; Language model; Real time monitoring; Road anomaly; Road safety; Safety management; Urban road; Urban road safety; Vision large language model; Traffic congestion";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
58;AI-Powered Digital Twin Technology for Highway System Slope Stability Risk Monitoring;This research proposes an artificial intelligence (AI)-powered digital twin framework for highway slope stability risk monitoring and prediction. For highway slope stability, a digital twin replicates the geological and structural conditions of highway slopes while continuously integrating real-time monitoring data to refine and enhance slope modeling. The framework employs instance segmentation and a random forest model to identify embankments and slopes with high landslide susceptibility scores. Additionally, artificial neural network (ANN) models are trained on historical drilling data to predict 3D subsurface soil type point clouds and groundwater depth maps. The USCS soil classification-based machine learning model achieved an accuracy score of 0.8, calculated by dividing the number of correct soil class predictions by the total number of predictions. The groundwater depth regression model achieved an RMSE of 2.32. These predicted values are integrated as input parameters for seepage and slope stability analyses, ultimately calculating the factor of safety (FoS) under predicted rainfall infiltration scenarios. The proposed methodology automates the identification of embankments and slopes using sub-meter resolution Light Detection and Ranging (LiDAR)-derived digital elevation models (DEMs) and generates critical soil properties and pore water pressure data for slope stability analysis. This enables the provision of early warnings for potential slope failures, facilitating timely interventions and risk mitigation. © 2025 by the authors.;"3D geological model; digital asset inventory; digital twin; geotechnical; highway system; instance segmentation; machine learning; slope stability; subsurface exploration";NULL;"AI-Powered Digital Twin Technology for Highway System Slope Stability Risk Monitoring This research proposes an artificial intelligence (AI)-powered digital twin framework for highway slope stability risk monitoring and prediction. For highway slope stability, a digital twin replicates the geological and structural conditions of highway slopes while continuously integrating real-time monitoring data to refine and enhance slope modeling. The framework employs instance segmentation and a random forest model to identify embankments and slopes with high landslide susceptibility scores. Additionally, artificial neural network (ANN) models are trained on historical drilling data to predict 3D subsurface soil type point clouds and groundwater depth maps. The USCS soil classification-based machine learning model achieved an accuracy score of 0.8, calculated by dividing the number of correct soil class predictions by the total number of predictions. The groundwater depth regression model achieved an RMSE of 2.32. These predicted values are integrated as input parameters for seepage and slope stability analyses, ultimately calculating the factor of safety (FoS) under predicted rainfall infiltration scenarios. The proposed methodology automates the identification of embankments and slopes using sub-meter resolution Light Detection and Ranging (LiDAR)-derived digital elevation models (DEMs) and generates critical soil properties and pore water pressure data for slope stability analysis. This enables the provision of early warnings for potential slope failures, facilitating timely interventions and risk mitigation. © 2025 by the authors. 3D geological model; digital asset inventory; digital twin; geotechnical; highway system; instance segmentation; machine learning; slope stability; subsurface exploration NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
59;Generative super-resolution AI accelerates nanoscale analysis of cells;Super-resolution microscopy (SRM) surpasses Abbe’s diffraction limit, thus enabling nanoscale observation of cells. However, SRM techniques, such as stochastic optical reconstruction microscopy (STORM), suffer from long acquisition times which can significantly impact imaging throughput. To address this issue, we adapted the enhanced super-resolution generative adversarial network from natural to microscopy images. Our goal is to generate super-resolution images from widefield microscopy images in shorter times. We implemented this for imaging microtubules of cells to obtain STORM-like images. Different models were trained by using transfer learning and progressive fine-tuning. The generated images, evaluated by peak signal-to-noise ratio, structural similarity index and expert human validation, prove that this deep learning approach is suitable for microscopy, allowing for 4x-higher throughput of nanoscale imaging compared to unsupported techniques. © 2025 The Author(s). Published by IOP Publishing Ltd.;"deep learning; GAN; super-resolution microscopy";"Image enhancement; Image reconstruction; Image resolution; Deep learning; Diffraction limits; GAN; Microscopy images; Nano scale; Nanoscale analysis; Optical reconstruction; Stochastics; Super-resolution microscopy; Superresolution; Generative adversarial networks";"Generative super-resolution AI accelerates nanoscale analysis of cells Super-resolution microscopy (SRM) surpasses Abbe’s diffraction limit, thus enabling nanoscale observation of cells. However, SRM techniques, such as stochastic optical reconstruction microscopy (STORM), suffer from long acquisition times which can significantly impact imaging throughput. To address this issue, we adapted the enhanced super-resolution generative adversarial network from natural to microscopy images. Our goal is to generate super-resolution images from widefield microscopy images in shorter times. We implemented this for imaging microtubules of cells to obtain STORM-like images. Different models were trained by using transfer learning and progressive fine-tuning. The generated images, evaluated by peak signal-to-noise ratio, structural similarity index and expert human validation, prove that this deep learning approach is suitable for microscopy, allowing for 4x-higher throughput of nanoscale imaging compared to unsupported techniques. © 2025 The Author(s). Published by IOP Publishing Ltd. deep learning; GAN; super-resolution microscopy Image enhancement; Image reconstruction; Image resolution; Deep learning; Diffraction limits; GAN; Microscopy images; Nano scale; Nanoscale analysis; Optical reconstruction; Stochastics; Super-resolution microscopy; Superresolution; Generative adversarial networks";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;-1;NULL;-1;NULL
60;Stochastic finite fault simulation of 2023 Mw 7.8 and Mw 7.5 Turkey earthquakes and its application to regional buildings damage estimation at Kahramanmaras City;On February 6, 2023, an Mw 7.8 earthquake occurred in southern Turkey, and only nine hours later, an Mw 7.5 earthquake occurred 95 km north of the first earthquake epicenter. This study employed stochastic finite fault method to simulate the ground motions from the earthquake doublet. The input parameters of source, path, site are mostly determined by regression of station records. The simulated ground motions are validated by comparing with eight station records, and results show that simulated PGA, waveform, PSA curve, duration match with those from station records with minor discrepancies. In addition, goodness-of-fit evaluation is also performed. Regional building damage estimation results show that severely damaged and collapsed buildings increased from 28 to 42% after the second earthquake, and 1/4 buildings damage state experienced one-level jump, which indicates that the second earthquake might significantly intensify buildings damage and should be carefully evaluated within an earthquake doublet context. The stochastic finite fault simulation in this study could provide a basis for future studies on the Turkey earthquake doublet, and the regional buildings damage estimation could be helpful for improvement of earthquake rescue and disaster mitigation policies. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"2023 Turkey earthquake; Earthquake doublet; Ground motion simulation; Regional buildings damage estimation; Stochastic finite fault simulation";"Kahramanmaras; Turkey; Buildings; Earthquakes; Faulting; 2023 turkey earthquake; Building damage; Damage estimation; Earthquake doublet; Fault's simulations; Ground motions simulations; Regional building damage estimation; Stochastic finite fault simulation; Stochastics; Turkey earthquake; collapse structure; computer simulation; disaster management; earthquake damage; earthquake epicenter; earthquake event; earthquake magnitude; ground motion; mitigation; Stochastic systems";"Stochastic finite fault simulation of 2023 Mw 7.8 and Mw 7.5 Turkey earthquakes and its application to regional buildings damage estimation at Kahramanmaras City On February 6, 2023, an Mw 7.8 earthquake occurred in southern Turkey, and only nine hours later, an Mw 7.5 earthquake occurred 95 km north of the first earthquake epicenter. This study employed stochastic finite fault method to simulate the ground motions from the earthquake doublet. The input parameters of source, path, site are mostly determined by regression of station records. The simulated ground motions are validated by comparing with eight station records, and results show that simulated PGA, waveform, PSA curve, duration match with those from station records with minor discrepancies. In addition, goodness-of-fit evaluation is also performed. Regional building damage estimation results show that severely damaged and collapsed buildings increased from 28 to 42% after the second earthquake, and 1/4 buildings damage state experienced one-level jump, which indicates that the second earthquake might significantly intensify buildings damage and should be carefully evaluated within an earthquake doublet context. The stochastic finite fault simulation in this study could provide a basis for future studies on the Turkey earthquake doublet, and the regional buildings damage estimation could be helpful for improvement of earthquake rescue and disaster mitigation policies. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. 2023 Turkey earthquake; Earthquake doublet; Ground motion simulation; Regional buildings damage estimation; Stochastic finite fault simulation Kahramanmaras; Turkey; Buildings; Earthquakes; Faulting; 2023 turkey earthquake; Building damage; Damage estimation; Earthquake doublet; Fault's simulations; Ground motions simulations; Regional building damage estimation; Stochastic finite fault simulation; Stochastics; Turkey earthquake; collapse structure; computer simulation; disaster management; earthquake damage; earthquake epicenter; earthquake event; earthquake magnitude; ground motion; mitigation; Stochastic systems";-1;Não Classificado;NULL;1.1;Geological;3;Response
61;Deep Learning for Early Earthquake Detection: Application of Convolutional Neural Networks for P-Wave Detection;Featured Application: Earthquake Detection, Earthquake Early Warning System (EWS), Processing of Seismic data. Early detection of earthquakes is essential for minimizing potential damage and ensuring public safety. Recent advancements in deep learning, particularly convolutional neural networks (CNNs), provide a promising alternative for analyzing seismic waves. In contrast, traditional methods such as the short-term average/long-term average (STA/LTA) algorithm and the Akaike information criterion (AIC) have limitations in detecting primary (P) waves in high-noise conditions, caused by industrial and anthropogenic disturbances. This study presents a CNN-based automatic P-wave detection model tailored for the Almaty city region. The seismic dataset used in this research was obtained from the IRIS database and includes data collected from seven stations within a 333 km radius of Almaty, Kazakhstan. The proposed model achieves a recall rate of 89.1% and an accuracy of 94.1% compared to other deep learning-based models. Experimental results demonstrate that this method enhances the reliability of automatic early earthquake warning systems and improves the accuracy of P-wave detection. The research outputs presented for the local region are unique. Applying CNNs in seismic monitoring facilitates the development of efficient automated systems that minimize risks and improve response measures for natural disasters. © 2025 by the authors.;"convolutional neural networks; deep learning; earthquake early warning; P-wave detection; seismic monitoring";"Alarm systems; Deep neural networks; Earthquakes; Public risks; Risk assessment; Seismic response; Seismic waves; Shear waves; Akaike's information criterions; Convolutional neural network; Deep learning; Earthquake detection; Earthquake early warning; Earthquake early warning systems; P waves; P-wave detections; Public safety; Seismic monitoring; Convolutional neural networks";"Deep Learning for Early Earthquake Detection: Application of Convolutional Neural Networks for P-Wave Detection Featured Application: Earthquake Detection, Earthquake Early Warning System (EWS), Processing of Seismic data. Early detection of earthquakes is essential for minimizing potential damage and ensuring public safety. Recent advancements in deep learning, particularly convolutional neural networks (CNNs), provide a promising alternative for analyzing seismic waves. In contrast, traditional methods such as the short-term average/long-term average (STA/LTA) algorithm and the Akaike information criterion (AIC) have limitations in detecting primary (P) waves in high-noise conditions, caused by industrial and anthropogenic disturbances. This study presents a CNN-based automatic P-wave detection model tailored for the Almaty city region. The seismic dataset used in this research was obtained from the IRIS database and includes data collected from seven stations within a 333 km radius of Almaty, Kazakhstan. The proposed model achieves a recall rate of 89.1% and an accuracy of 94.1% compared to other deep learning-based models. Experimental results demonstrate that this method enhances the reliability of automatic early earthquake warning systems and improves the accuracy of P-wave detection. The research outputs presented for the local region are unique. Applying CNNs in seismic monitoring facilitates the development of efficient automated systems that minimize risks and improve response measures for natural disasters. © 2025 by the authors. convolutional neural networks; deep learning; earthquake early warning; P-wave detection; seismic monitoring Alarm systems; Deep neural networks; Earthquakes; Public risks; Risk assessment; Seismic response; Seismic waves; Shear waves; Akaike's information criterions; Convolutional neural network; Deep learning; Earthquake detection; Earthquake early warning; Earthquake early warning systems; P waves; P-wave detections; Public safety; Seismic monitoring; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
62;Peak Ground Acceleration Prediction for Earthquake Early Warning with Multivariable Long Short-Term Memory Networks and Temporal Transformers;Peru’s seismic history, characterized by devastating earthquakes resulting in significant loss of life and property damage, underscores the urgency of effective early warning systems. Notably, events like the 1746 and 2007 Pisco earthquakes highlight the vulnerability of the region to seismic activity. In this context, this work presents a novel approach to earthquake early warning systems using deep learning architectures, specifically Long Short-Term Memory (LSTM) networks, and Temporal Fusion Transformer (TFT) networks. The study focuses on predicting Peak Ground Acceleration (PGA), a crucial parameter for issuing timely alerts to mitigate earthquake hazards. Using a comprehensive dataset comprising 5045 seismic records from various locations in Peru, the study employs LSTM and TFT networks to predict PGA values. Data preprocessing involves homogenizing acceleration records and dividing them into training, validation, and testing sets. Results indicate that both LSTM and TFT networks demonstrate promising performance across different time windows (5, 30, and 60 s). For LSTM networks, the 60-s time window yields the most accurate predictions, with validation accuracy reaching 98.015% and testing accuracy at 88.89%. Meanwhile, TFT networks achieve competitive results, particularly with 30-s time windows, showing validation accuracy of 96.03% and testing accuracy of 91.32%. The findings underscore the potential of deep learning architectures in enhancing early warning systems, contributing to more effective disaster preparedness and response strategies in earthquake-prone regions. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Deep Learning; Earthquake Early Warning; Peak Ground Acceleration; PGA Prediction; Seismic Data";"Earthquake effects; Earthquake engineering; Electric transformer testing; Seismic response; Deep learning; Early Warning System; Earthquake early warning; Learning architectures; Memory network; Peak ground acceleration; Peak ground acceleration prediction; Seismic data; Short term memory; Long short-term memory";"Peak Ground Acceleration Prediction for Earthquake Early Warning with Multivariable Long Short-Term Memory Networks and Temporal Transformers Peru’s seismic history, characterized by devastating earthquakes resulting in significant loss of life and property damage, underscores the urgency of effective early warning systems. Notably, events like the 1746 and 2007 Pisco earthquakes highlight the vulnerability of the region to seismic activity. In this context, this work presents a novel approach to earthquake early warning systems using deep learning architectures, specifically Long Short-Term Memory (LSTM) networks, and Temporal Fusion Transformer (TFT) networks. The study focuses on predicting Peak Ground Acceleration (PGA), a crucial parameter for issuing timely alerts to mitigate earthquake hazards. Using a comprehensive dataset comprising 5045 seismic records from various locations in Peru, the study employs LSTM and TFT networks to predict PGA values. Data preprocessing involves homogenizing acceleration records and dividing them into training, validation, and testing sets. Results indicate that both LSTM and TFT networks demonstrate promising performance across different time windows (5, 30, and 60 s). For LSTM networks, the 60-s time window yields the most accurate predictions, with validation accuracy reaching 98.015% and testing accuracy at 88.89%. Meanwhile, TFT networks achieve competitive results, particularly with 30-s time windows, showing validation accuracy of 96.03% and testing accuracy of 91.32%. The findings underscore the potential of deep learning architectures in enhancing early warning systems, contributing to more effective disaster preparedness and response strategies in earthquake-prone regions. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Deep Learning; Earthquake Early Warning; Peak Ground Acceleration; PGA Prediction; Seismic Data Earthquake effects; Earthquake engineering; Electric transformer testing; Seismic response; Deep learning; Early Warning System; Earthquake early warning; Learning architectures; Memory network; Peak ground acceleration; Peak ground acceleration prediction; Seismic data; Short term memory; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
63;Building occupancy type classification and uncertainty estimation using machine learning and open data;Federal and local agencies have identified a need to create building databases to help ensure that critical infrastructure and residential buildings are accounted for in disaster preparedness and to aid the decision-making processes in subsequent recovery efforts. To respond effectively, we need to understand the built environment - where people live, work, and the critical infrastructure they rely on. Yet, a major discrepancy exists in the way data about buildings are collected across the United SStates There is no harmonization in what data are recorded by city, county, or state governments, let alone at the national scale. We demonstrate how existing open-source datasets can be spatially integrated and subsequently used as training for machine learning (ML) models to predict building occupancy type, a major component needed for disaster preparedness and decision -making. Multiple ML algorithms are compared. We address strategies to handle significant class imbalance and introduce Bayesian neural networks to handle prediction uncertainty. The 100-year flood in North Carolina is provided as a practical application in disaster preparedness.  © The Author(s), 2025.;"Bayesian neural network; building type classification; flood risk; machine learning; open data";NULL;"Building occupancy type classification and uncertainty estimation using machine learning and open data Federal and local agencies have identified a need to create building databases to help ensure that critical infrastructure and residential buildings are accounted for in disaster preparedness and to aid the decision-making processes in subsequent recovery efforts. To respond effectively, we need to understand the built environment - where people live, work, and the critical infrastructure they rely on. Yet, a major discrepancy exists in the way data about buildings are collected across the United SStates There is no harmonization in what data are recorded by city, county, or state governments, let alone at the national scale. We demonstrate how existing open-source datasets can be spatially integrated and subsequently used as training for machine learning (ML) models to predict building occupancy type, a major component needed for disaster preparedness and decision -making. Multiple ML algorithms are compared. We address strategies to handle significant class imbalance and introduce Bayesian neural networks to handle prediction uncertainty. The 100-year flood in North Carolina is provided as a practical application in disaster preparedness.  © The Author(s), 2025. Bayesian neural network; building type classification; flood risk; machine learning; open data NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
64;Mining and Analyzing the Evolution of Public Opinion in Extreme Disaster Events from Social Media: A Case Study of the 2022 Yingde Flood in China;Natural disasters have caused significant economic losses and casualties. Obtaining detailed disaster information and understanding public opinion during disasters are crucial for devising effective policies and ensuring timely disaster responses. With the widespread use of social media, it has become an important channel for extracting disaster information. However, accurately obtaining and revealing public opinion from social media remains challenging. This study combines the biterm topic model and support vector machine to analyze topic features. Additionally, sentiment features are analyzed using the Generative Pre-trained Transformer-3.5 model. These techniques are employed to build a social media-based flood information mining model capable of detecting the spatiotemporal distribution of public sentiment and discussion topics, including significant events impacting public sentiment. Using the 2022 Yingde flood as a case study, we explored the evolutionary patterns of public opinion on floods across three dimensions: time, space, and content. The study also explored the correlation between flooding and public opinion through geographic visualization and statistical analysis. The results indicated a precision of 89.2% and 80.2% for topic and sentiment classification, respectively. Temporally, the public response to flooding was primarily concentrated during heavy rainfall and flooding, varying with disaster severity. Furthermore, significant events or statements by public figures can greatly influence public responses to flooding. Spatially, the public response to flooding focused mainly in major urban areas and severely affected regions. In terms of content, a strong correlation was revealed between sentiments, topic distribution, and the disaster scenario. The findings can be used to analyze disaster conditions and public opinion in depth, and as a supplement of existing methods of extracting disaster information, it can enhance situational awareness for disaster emergency management and provide a reference basis for emergency relief efforts. © 2024 American Society of Civil Engineers.;"Emergency Management; Generative pretrained transformer; Natural disaster; Natural language processing; Public opinion; Social media";"China; Guangdong; Yingde; Distribution transformers; Emergency services; Information management; Disaster Information; Emergency management; Floodings; Generative pretrained transformer; Language processing; Natural disasters; Natural language processing; Natural languages; Public opinions; Social media; disaster management; extreme event; flood; flooding; natural disaster; perception; rainfall; social media; support vector machine; visualization; Risk management";"Mining and Analyzing the Evolution of Public Opinion in Extreme Disaster Events from Social Media: A Case Study of the 2022 Yingde Flood in China Natural disasters have caused significant economic losses and casualties. Obtaining detailed disaster information and understanding public opinion during disasters are crucial for devising effective policies and ensuring timely disaster responses. With the widespread use of social media, it has become an important channel for extracting disaster information. However, accurately obtaining and revealing public opinion from social media remains challenging. This study combines the biterm topic model and support vector machine to analyze topic features. Additionally, sentiment features are analyzed using the Generative Pre-trained Transformer-3.5 model. These techniques are employed to build a social media-based flood information mining model capable of detecting the spatiotemporal distribution of public sentiment and discussion topics, including significant events impacting public sentiment. Using the 2022 Yingde flood as a case study, we explored the evolutionary patterns of public opinion on floods across three dimensions: time, space, and content. The study also explored the correlation between flooding and public opinion through geographic visualization and statistical analysis. The results indicated a precision of 89.2% and 80.2% for topic and sentiment classification, respectively. Temporally, the public response to flooding was primarily concentrated during heavy rainfall and flooding, varying with disaster severity. Furthermore, significant events or statements by public figures can greatly influence public responses to flooding. Spatially, the public response to flooding focused mainly in major urban areas and severely affected regions. In terms of content, a strong correlation was revealed between sentiments, topic distribution, and the disaster scenario. The findings can be used to analyze disaster conditions and public opinion in depth, and as a supplement of existing methods of extracting disaster information, it can enhance situational awareness for disaster emergency management and provide a reference basis for emergency relief efforts. © 2024 American Society of Civil Engineers. Emergency Management; Generative pretrained transformer; Natural disaster; Natural language processing; Public opinion; Social media China; Guangdong; Yingde; Distribution transformers; Emergency services; Information management; Disaster Information; Emergency management; Floodings; Generative pretrained transformer; Language processing; Natural disasters; Natural language processing; Natural languages; Public opinions; Social media; disaster management; extreme event; flood; flooding; natural disaster; perception; rainfall; social media; support vector machine; visualization; Risk management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
65;Earthquake prediction optimization using deep learning hybrid RNN-LSTM model for seismicity analysis;Earthquakes are among the most destructive natural disasters, posing severe risks to human life and infrastructure. Accurate and reliable earthquake forecasting systems are crucial for effective disaster management and mitigation. Recent advancements in machine learning and deep learning present promising pathways for enhancing earthquake prediction accuracy. This study provides an in-depth investigation of machine learning methods for earthquake forecasting, emphasizing their critical role in disaster prevention. Four deep learning models are evaluated: Recurrent Neural Networks (RNN), Long Short-Term Memory networks (LSTM), AdaBoost, and a hybrid RNN-LSTM model. The RNN-LSTM hybrid model demonstrates exceptional performance by leveraging the strength of LSTM in capturing long-term dependencies and RNN in detecting short-term patterns, allowing for a comprehensive analysis of seismic activity. Among these models, the RNN-LSTM hybrid stands out, achieving an impressive accuracy rate of 98 %, significantly surpassing the other models. These results highlight the potential of machine learning technologies to improve earthquake prediction precision. The proposed approach enhances current forecasting methods, offering more accurate and reliable earthquake predictions. This research makes a substantial contribution to disaster preparedness and mitigation. © 2025 Elsevier Ltd;"Casualty prediction; Earthquake; Earthquake prediction; Ensemble learning";"Adversarial machine learning; Casualty prediction; Disaster mitigation; Earthquake forecasting; Earthquake prediction; Ensemble learning; Memory network; Network models; Neural-networks; Optimisations; Short term memory; Prediction models";"Earthquake prediction optimization using deep learning hybrid RNN-LSTM model for seismicity analysis Earthquakes are among the most destructive natural disasters, posing severe risks to human life and infrastructure. Accurate and reliable earthquake forecasting systems are crucial for effective disaster management and mitigation. Recent advancements in machine learning and deep learning present promising pathways for enhancing earthquake prediction accuracy. This study provides an in-depth investigation of machine learning methods for earthquake forecasting, emphasizing their critical role in disaster prevention. Four deep learning models are evaluated: Recurrent Neural Networks (RNN), Long Short-Term Memory networks (LSTM), AdaBoost, and a hybrid RNN-LSTM model. The RNN-LSTM hybrid model demonstrates exceptional performance by leveraging the strength of LSTM in capturing long-term dependencies and RNN in detecting short-term patterns, allowing for a comprehensive analysis of seismic activity. Among these models, the RNN-LSTM hybrid stands out, achieving an impressive accuracy rate of 98 %, significantly surpassing the other models. These results highlight the potential of machine learning technologies to improve earthquake prediction precision. The proposed approach enhances current forecasting methods, offering more accurate and reliable earthquake predictions. This research makes a substantial contribution to disaster preparedness and mitigation. © 2025 Elsevier Ltd Casualty prediction; Earthquake; Earthquake prediction; Ensemble learning Adversarial machine learning; Casualty prediction; Disaster mitigation; Earthquake forecasting; Earthquake prediction; Ensemble learning; Memory network; Network models; Neural-networks; Optimisations; Short term memory; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
66;A post-processing machine learning framework for bias-correcting National Water Model outputs by accounting for dominant streamflow drivers;While the National Water Model (NWM) provides high-resolution, large-scale streamflow data across the United States, its effectiveness as a key water resources management tool in the drought-prone Western US needs further investigation. Previous studies revealed that the NWM has limitations in controlled basins, impacted by reservoir operations and diversions not explicitly included within the model framework. Responding to the observed reduction in model skill throughout the Western US, we developed a model agnostic post-processing machine learning (PP-ML) framework to account for the impacts of water resources management and regionally dominant hydrological processes on model performance. For our case application of the PP-ML framework, we use daily NWM v2.1 retrospective flow rates as the hydrological model and input upstream reservoir storage, SNOTEL snow water equivalent, and catchment characteristics. Applying the PP-ML framework in the contributing Great Salt Lake watersheds, a key watershed of interest due to its drought-prone nature, we observed a 65%, 335%, and 25% improvement in the median Kling-Gupta Efficiency, Percent Bias, and Root Mean Square Error, respectively, for 30 gauged locations compared to the NWM outputs. Comparing model skills across different flow regimes and station types revealed a substantial (225%) improvement in low-flow estimates at stations with extensive upstream water infrastructure, such as those impacted by reservoir operations, as well as in catchments within negligible water management activities. The research underscores how post-processing hydrological model outputs with ML can account for the effects of water management activities on streamflow estimates, most notably without explicitly incorporating infrastructure rulesets, and demonstrate its capability in bias-correcting streamflow forecasts in response to the regionally dominant streamflow drivers. © 2025;"Drought; Great Salt Lake; Machine learning; National Water Model; Post-processing; Water resources management";"Information management; Lakes; Resource allocation; River diversion; Runoff; Water management; Great Salt Lake; Learning frameworks; Machine-learning; Model outputs; National water model; Post-processing; Processing machines; Reservoir operation; Water models; Water resources management; Catchments";"A post-processing machine learning framework for bias-correcting National Water Model outputs by accounting for dominant streamflow drivers While the National Water Model (NWM) provides high-resolution, large-scale streamflow data across the United States, its effectiveness as a key water resources management tool in the drought-prone Western US needs further investigation. Previous studies revealed that the NWM has limitations in controlled basins, impacted by reservoir operations and diversions not explicitly included within the model framework. Responding to the observed reduction in model skill throughout the Western US, we developed a model agnostic post-processing machine learning (PP-ML) framework to account for the impacts of water resources management and regionally dominant hydrological processes on model performance. For our case application of the PP-ML framework, we use daily NWM v2.1 retrospective flow rates as the hydrological model and input upstream reservoir storage, SNOTEL snow water equivalent, and catchment characteristics. Applying the PP-ML framework in the contributing Great Salt Lake watersheds, a key watershed of interest due to its drought-prone nature, we observed a 65%, 335%, and 25% improvement in the median Kling-Gupta Efficiency, Percent Bias, and Root Mean Square Error, respectively, for 30 gauged locations compared to the NWM outputs. Comparing model skills across different flow regimes and station types revealed a substantial (225%) improvement in low-flow estimates at stations with extensive upstream water infrastructure, such as those impacted by reservoir operations, as well as in catchments within negligible water management activities. The research underscores how post-processing hydrological model outputs with ML can account for the effects of water management activities on streamflow estimates, most notably without explicitly incorporating infrastructure rulesets, and demonstrate its capability in bias-correcting streamflow forecasts in response to the regionally dominant streamflow drivers. © 2025 Drought; Great Salt Lake; Machine learning; National Water Model; Post-processing; Water resources management Information management; Lakes; Resource allocation; River diversion; Runoff; Water management; Great Salt Lake; Learning frameworks; Machine-learning; Model outputs; National water model; Post-processing; Processing machines; Reservoir operation; Water models; Water resources management; Catchments";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
67;A Comparative Study of a Deep Reinforcement Learning Solution and Alternative Deep Learning Models for Wildfire Prediction;Wildfires pose an escalating threat to ecosystems and human settlements, making accurate forecasting essential for early mitigation. This study compared three deep learning models for wildfire prediction: Deep Reinforcement Learning (DRL) with Actor–Critic architecture, Convolutional Neural Network (CNN), and Transformer-based models. The models were trained and evaluated using historical data from Chile (2000–2023), including wildfire occurrences, meteorological variables, topography, and vegetation indices. After preprocessing and class balancing, each model was tested over 100 experimental runs. All models achieved outstanding performance, with F1-Scores exceeding 0.999 and perfect AUC-ROC scores. The Transformer model showed a slight advantage over the CNN (99.94%) and Actor–Critic DRL (99.93%) in accuracy. Feature importance analysis identified wind speed, temperature, and vegetation indices as the most influential variables. While DRL offers theoretical benefits for adaptive decision-making, Transformer architectures more effectively capture spatiotemporal dependencies in wildfire dynamics. The findings can support the integration of deep learning models into early warning systems, contributing to proactive wildfire risk management. Future work will include validation with diverse regional datasets, real-time deployment, and collaboration with emergency response agencies. © 2025 by the authors.;"actor–critic; AI-driven risk assessment; convolutional neural networks; deep reinforcement learning; fire behavior modeling; machine learning; transformer models; wildfire prediction";"Adversarial machine learning; Contrastive Learning; Convolutional neural networks; Prediction models; Premixed flames; Reinforcement learning; Risk assessment; Risk management; Actor critic; AI-driven risk assessment; Convolutional neural network; Fire behavior modeling; Learning models; Machine-learning; Reinforcement learnings; Risks assessments; Transformer modeling; Wildfire prediction; Deep reinforcement learning";"A Comparative Study of a Deep Reinforcement Learning Solution and Alternative Deep Learning Models for Wildfire Prediction Wildfires pose an escalating threat to ecosystems and human settlements, making accurate forecasting essential for early mitigation. This study compared three deep learning models for wildfire prediction: Deep Reinforcement Learning (DRL) with Actor–Critic architecture, Convolutional Neural Network (CNN), and Transformer-based models. The models were trained and evaluated using historical data from Chile (2000–2023), including wildfire occurrences, meteorological variables, topography, and vegetation indices. After preprocessing and class balancing, each model was tested over 100 experimental runs. All models achieved outstanding performance, with F1-Scores exceeding 0.999 and perfect AUC-ROC scores. The Transformer model showed a slight advantage over the CNN (99.94%) and Actor–Critic DRL (99.93%) in accuracy. Feature importance analysis identified wind speed, temperature, and vegetation indices as the most influential variables. While DRL offers theoretical benefits for adaptive decision-making, Transformer architectures more effectively capture spatiotemporal dependencies in wildfire dynamics. The findings can support the integration of deep learning models into early warning systems, contributing to proactive wildfire risk management. Future work will include validation with diverse regional datasets, real-time deployment, and collaboration with emergency response agencies. © 2025 by the authors. actor–critic; AI-driven risk assessment; convolutional neural networks; deep reinforcement learning; fire behavior modeling; machine learning; transformer models; wildfire prediction Adversarial machine learning; Contrastive Learning; Convolutional neural networks; Prediction models; Premixed flames; Reinforcement learning; Risk assessment; Risk management; Actor critic; AI-driven risk assessment; Convolutional neural network; Fire behavior modeling; Learning models; Machine-learning; Reinforcement learnings; Risks assessments; Transformer modeling; Wildfire prediction; Deep reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.4;Climatological;2;Preparation
68;Deep Learning-Based River Flow Forecasting with MLPs: Comparative Exploratory Analysis Applied to the Tejo and the Mondego Rivers;This paper presents an innovative service for river flow forecasting and its demonstration in two dam-controlled rivers in Portugal, Tejo, and Mondego rivers, based on using Multilayer Perceptron (MLP) models to predict and forecast river flow. The main goal is to create and improve AI models that operate as remote services, providing precise and timely river flow predictions for the next 3 days. This paper examines the use of MLP architectures to predict river discharge using comprehensive hydrological data from Portugal’s National Water Resources Information System (Sistema Nacional de Informação de Recursos Hídricos, SNIRH), demonstrated for the Tejo and Mondego river basins. The methodology is described in detail, including data preparation, model training, and forecasting processes, and provides a comparative study of the MLP model’s performance in both case studies. The analysis shows that MLP models attain acceptable accuracy in short-term river flow forecasts for the selected scenarios and datasets, adeptly reflecting discharge patterns and peak occurrences. These models seek to enhance water resources management and decision-making by amalgamating modern data-driven methodologies with established hydrological and meteorological data sources, facilitating better flood mitigation and sustainable water resource planning as well as accurate boundary conditions for downstream forecast systems. © 2025 by the authors.;"artificial intelligence; deep learning; MLP; river flow forecasting; SNIRH";"Deep learning; Geophysical prospecting; Information management; Resource allocation; River control; Water management; Weather forecasting; Deep learning; Exploratory analysis; Hydrological data; Mondego River; Multilayers perceptrons; Portugal; Remote services; River flow; River flow forecasting; Sistema nacional de informacao de recursos hídrico; Decision making";"Deep Learning-Based River Flow Forecasting with MLPs: Comparative Exploratory Analysis Applied to the Tejo and the Mondego Rivers This paper presents an innovative service for river flow forecasting and its demonstration in two dam-controlled rivers in Portugal, Tejo, and Mondego rivers, based on using Multilayer Perceptron (MLP) models to predict and forecast river flow. The main goal is to create and improve AI models that operate as remote services, providing precise and timely river flow predictions for the next 3 days. This paper examines the use of MLP architectures to predict river discharge using comprehensive hydrological data from Portugal’s National Water Resources Information System (Sistema Nacional de Informação de Recursos Hídricos, SNIRH), demonstrated for the Tejo and Mondego river basins. The methodology is described in detail, including data preparation, model training, and forecasting processes, and provides a comparative study of the MLP model’s performance in both case studies. The analysis shows that MLP models attain acceptable accuracy in short-term river flow forecasts for the selected scenarios and datasets, adeptly reflecting discharge patterns and peak occurrences. These models seek to enhance water resources management and decision-making by amalgamating modern data-driven methodologies with established hydrological and meteorological data sources, facilitating better flood mitigation and sustainable water resource planning as well as accurate boundary conditions for downstream forecast systems. © 2025 by the authors. artificial intelligence; deep learning; MLP; river flow forecasting; SNIRH Deep learning; Geophysical prospecting; Information management; Resource allocation; River control; Water management; Weather forecasting; Deep learning; Exploratory analysis; Hydrological data; Mondego River; Multilayers perceptrons; Portugal; Remote services; River flow; River flow forecasting; Sistema nacional de informacao de recursos hídrico; Decision making";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
69;Demystifying SAR with attention;Synthetic Aperture Radar (SAR) imagery is indispensable for earth observation, offering the ability to capture data under challenging conditions such as cloud cover and darkness. However, its grayscale format and speckle noise hinder interpretability and pose significant challenges for traditional processing methods. This study introduces an innovative framework for SAR image colorization, leveraging an Attention-Based WGAN-GP (Wasserstein GAN with Gradient Penalty). The model incorporates multi-head self-attention mechanisms to enhance feature extraction, capture long-range dependencies, and dynamically suppress noise through a novel variance-based attention adjustment mechanism. Extensive evaluations on Sentinel-1 and Sentinel-2 datasets across diverse terrains, including agriculture, urban areas, barren land, and grasslands, demonstrate the model's superiority over existing approaches. It achieves an LPIPS score of 0.27, SSIM of 0.76, and an average inference time of 0.22 s, showcasing its ability to preserve spatial coherence and perceptual quality even in complex, noisy environments. This capability enables real-time applications in disaster management, flood monitoring, and urban planning, providing actionable insights and advancing the state-of-the-art in SAR image processing. © 2025 Elsevier Ltd;"Attention; Deep learning; Generative adversarial networks; Image colorization; Image restoration; Multihead attention; Noise; SAR; Sentinel";"Radar imaging; Adversarial networks; Attention; Deep learning; Image colorizations; Multihead; Multihead attention; Noise; Sentinel; Synthetic Aperture Radar Imagery; Synthetic aperture radar images; Generative adversarial networks";"Demystifying SAR with attention Synthetic Aperture Radar (SAR) imagery is indispensable for earth observation, offering the ability to capture data under challenging conditions such as cloud cover and darkness. However, its grayscale format and speckle noise hinder interpretability and pose significant challenges for traditional processing methods. This study introduces an innovative framework for SAR image colorization, leveraging an Attention-Based WGAN-GP (Wasserstein GAN with Gradient Penalty). The model incorporates multi-head self-attention mechanisms to enhance feature extraction, capture long-range dependencies, and dynamically suppress noise through a novel variance-based attention adjustment mechanism. Extensive evaluations on Sentinel-1 and Sentinel-2 datasets across diverse terrains, including agriculture, urban areas, barren land, and grasslands, demonstrate the model's superiority over existing approaches. It achieves an LPIPS score of 0.27, SSIM of 0.76, and an average inference time of 0.22 s, showcasing its ability to preserve spatial coherence and perceptual quality even in complex, noisy environments. This capability enables real-time applications in disaster management, flood monitoring, and urban planning, providing actionable insights and advancing the state-of-the-art in SAR image processing. © 2025 Elsevier Ltd Attention; Deep learning; Generative adversarial networks; Image colorization; Image restoration; Multihead attention; Noise; SAR; Sentinel Radar imaging; Adversarial networks; Attention; Deep learning; Image colorizations; Multihead; Multihead attention; Noise; Sentinel; Synthetic Aperture Radar Imagery; Synthetic aperture radar images; Generative adversarial networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
70;Multi-agent deep reinforcement learning-based truck-drone collaborative routing with dynamic emergency response;In emergency disaster response, the dynamic nature and uncertainty of resource transportation pose significant challenges for vehicle routing planning. We address a truck-drone collaborative routing problem in humanitarian logistics, where a set of truck-drone tandems collaboratively deliver relief resources from a distribution center to a set of affected areas which is dynamically updated as disaster changes. In the truck-drone collaborative mode, as each truck performs the delivery services and serves as a mobile depot for the drone associated with it, the drone launches from its associated truck at a node, delivers relief resources to one affected area, and returns to rendezvous with the truck at the node or another node along the truck route. We cast the problem as a Markov game model with an event-driven method, which can effectively capture the dynamic changes in the states and node information of trucks and drones during relief resources delivery. To solve the model, we develop a multi-agent deep reinforcement learning algorithm, which combines prioritized experience replay and invalid action masking to improve the sample efficiency and reduce the decision space. We conduct extensive numerical studies to validate the effectiveness of the proposed method by comparing it with existing solution methods and two well-known heuristic rules, and discuss the impacts of some model parameters on the solution performance. We also assess the advantages of the truck-drone collaborative mode over the truck/helicopter-only mode through a case study of the 2008 Wenchuan earthquake. © 2025 Elsevier Ltd;"Dynamic routing; Humanitarian logistics; Markov game; Multi-agent deep reinforcement learning; Truck-drone collaborative mode";"China; Sichuan; Wenchuan; Automobiles; Dynamic routing algorithms; Markov processes; Reinforcement learning; Truck transportation; Trucks; Vehicle routing; Affected area; Dynamic routing; Emergency response; Humanitarian logistics; Markov games; Multi agent; Multi-agent deep reinforcement learning; Reinforcement learnings; Routings; Truck-drone collaborative mode; algorithm; machine learning; parameterization; performance assessment; routing; Deep reinforcement learning";"Multi-agent deep reinforcement learning-based truck-drone collaborative routing with dynamic emergency response In emergency disaster response, the dynamic nature and uncertainty of resource transportation pose significant challenges for vehicle routing planning. We address a truck-drone collaborative routing problem in humanitarian logistics, where a set of truck-drone tandems collaboratively deliver relief resources from a distribution center to a set of affected areas which is dynamically updated as disaster changes. In the truck-drone collaborative mode, as each truck performs the delivery services and serves as a mobile depot for the drone associated with it, the drone launches from its associated truck at a node, delivers relief resources to one affected area, and returns to rendezvous with the truck at the node or another node along the truck route. We cast the problem as a Markov game model with an event-driven method, which can effectively capture the dynamic changes in the states and node information of trucks and drones during relief resources delivery. To solve the model, we develop a multi-agent deep reinforcement learning algorithm, which combines prioritized experience replay and invalid action masking to improve the sample efficiency and reduce the decision space. We conduct extensive numerical studies to validate the effectiveness of the proposed method by comparing it with existing solution methods and two well-known heuristic rules, and discuss the impacts of some model parameters on the solution performance. We also assess the advantages of the truck-drone collaborative mode over the truck/helicopter-only mode through a case study of the 2008 Wenchuan earthquake. © 2025 Elsevier Ltd Dynamic routing; Humanitarian logistics; Markov game; Multi-agent deep reinforcement learning; Truck-drone collaborative mode China; Sichuan; Wenchuan; Automobiles; Dynamic routing algorithms; Markov processes; Reinforcement learning; Truck transportation; Trucks; Vehicle routing; Affected area; Dynamic routing; Emergency response; Humanitarian logistics; Markov games; Multi agent; Multi-agent deep reinforcement learning; Reinforcement learnings; Routings; Truck-drone collaborative mode; algorithm; machine learning; parameterization; performance assessment; routing; Deep reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.1;Geological;3;Response
71;End-to-end time-dependent probabilistic assessment of landslide hazards using hybrid deep learning simulator;Early warning detection of landslide hazards often requires real-time or near real-time predictions, which can be challenging due to the presence of multiple geo-uncertainties and time-variant external environmental loadings. The propagation of these uncertainties at the system level for understanding the spatiotemporal behavior of slopes often requires time-consuming numerical calculations, significantly hindering the establishment of an early warning system. This paper presents a hybrid deep learning simulator, which fuses parallel convolutional neural networks (CNNs) and long short-term memory (LSTM) networks through attention mechanisms, termed PCLA-Net, to facilitate time-dependent probabilistic assessment of landslide hazards. PCLA-Net features two novelties. First, it is capable of simultaneously handling both temporal and spatial information. CNNs specialize in interpreting spatial data, while LSTM excels in handling time-variant data. Coupled with two attention mechanisms, the two modules are combined to probabilistically predict the spatiotemporal behavior of slopes. Second, PCLA-Net realizes end-to-end predictions. In this paper, the Liangshuijing landslide in the Three Gorges Reservoir area of China is used to illustrate PCLA-Net. It is first validated followed by a comparison with existing techniques to demonstrate its improved predictive capabilities. The proposed PCLA-Net simulator can achieve the same level of accuracy with at least 50% reduction in computation resources. © 2024 The Author(s);"Attention mechanisms; Convolutional neural networks; Landslide hazards; Long short-term memory networks; Spatial variability; Time-dependent reliability";"China; Three Gorges Reservoir; Deep neural networks; Long short-term memory; Attention mechanisms; Convolutional neural network; End to end; Landslide hazard; Long short-term memory network; Memory network; Short term memory; Spatial variability; Time dependent; Time dependent reliability; accuracy assessment; artificial neural network; computer simulation; early warning system; geological hazard; hazard assessment; landslide; machine learning; numerical model; probability; time dependent behavior; Convolutional neural networks";"End-to-end time-dependent probabilistic assessment of landslide hazards using hybrid deep learning simulator Early warning detection of landslide hazards often requires real-time or near real-time predictions, which can be challenging due to the presence of multiple geo-uncertainties and time-variant external environmental loadings. The propagation of these uncertainties at the system level for understanding the spatiotemporal behavior of slopes often requires time-consuming numerical calculations, significantly hindering the establishment of an early warning system. This paper presents a hybrid deep learning simulator, which fuses parallel convolutional neural networks (CNNs) and long short-term memory (LSTM) networks through attention mechanisms, termed PCLA-Net, to facilitate time-dependent probabilistic assessment of landslide hazards. PCLA-Net features two novelties. First, it is capable of simultaneously handling both temporal and spatial information. CNNs specialize in interpreting spatial data, while LSTM excels in handling time-variant data. Coupled with two attention mechanisms, the two modules are combined to probabilistically predict the spatiotemporal behavior of slopes. Second, PCLA-Net realizes end-to-end predictions. In this paper, the Liangshuijing landslide in the Three Gorges Reservoir area of China is used to illustrate PCLA-Net. It is first validated followed by a comparison with existing techniques to demonstrate its improved predictive capabilities. The proposed PCLA-Net simulator can achieve the same level of accuracy with at least 50% reduction in computation resources. © 2024 The Author(s) Attention mechanisms; Convolutional neural networks; Landslide hazards; Long short-term memory networks; Spatial variability; Time-dependent reliability China; Three Gorges Reservoir; Deep neural networks; Long short-term memory; Attention mechanisms; Convolutional neural network; End to end; Landslide hazard; Long short-term memory network; Memory network; Short term memory; Spatial variability; Time dependent; Time dependent reliability; accuracy assessment; artificial neural network; computer simulation; early warning system; geological hazard; hazard assessment; landslide; machine learning; numerical model; probability; time dependent behavior; Convolutional neural networks";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;2;Preparation
72;Flood prediction in an ungauged watershed: A case study of the naengcheon watershed, Korea;Over the decades, a variety of physically-based, data-driven, and hybrid flood prediction models have been developed, demonstrating high performance by leveraging extensive datasets, including topography, surface roughness, and hydrological data. Despite advancements, the use of these models remains largely limited to gauged watersheds due to their reliance on measured discharge data for accurately modeling infiltration processes, and the use of consistent infiltration parameters further restricts prediction accuracy. To overcome this limitation, this study introduces a straightforward technique that utilizes parameters from an adjacent watershed with comprehensive data collection to account for rainfall loss due to infiltration, employing event-by-event infiltration parameters to enhance prediction accuracy. By integrating diverse watershed data, including rainfall-runoff and GLDAS-Noah data, with the Long Short-Term Memory (LSTM) network—a high-performance deep learning model widely used in hydrology—we achieved reliable parameter estimation in adjacent gauged watersheds. By transferring these estimated parameter values to the target ungauged watershed, leveraging the spatial proximity between adjacent watersheds, we effectively estimated rainfall loss and simulated a physically-based model for streamflow prediction, even in an ungauged watershed. For a gauged watershed, we achieved an accuracy with a Nash-Sutcliffe Efficiency (NSE) of 0.7–0.97 and a coefficient of determination (R²) of 0.7–0.98 in predicting flood hydrographs for most events. For an ungauged watershed, where observed runoff data do not exist, we employed indirect verification methods using alternative data sources such as news reports and flood inundation maps. We first generated flood maps based on our channel routing results and then compared them with a flood inundation map from a news report. In this case, we quantified the accuracy using the confusion matrix, and the metrics were measured as Accuracy = 0.72 and Recall = 0.58. This technique offers significant potential by integrating watershed data with deep learning models like LSTM to enhance flood prediction in data-scarce regions, making it applicable for ungauged watersheds through spatial proximity. By improving disaster preparedness and management with more reliable flood forecasting and efficient resource allocation, it helps mitigate the impact of floods and inspires further advancements in flood prediction models. © 2025 The Authors;"Deep learning; Flood prediction; Physically-based; Ungauged watershed";"Data accuracy; Prediction models; Rain gages; Runoff; Deep learning; Flood prediction; Infiltration parameters; Learning models; Performance; Physically based; Prediction accuracy; Prediction modelling; Short term memory; Ungauged watershed; Resource allocation";"Flood prediction in an ungauged watershed: A case study of the naengcheon watershed, Korea Over the decades, a variety of physically-based, data-driven, and hybrid flood prediction models have been developed, demonstrating high performance by leveraging extensive datasets, including topography, surface roughness, and hydrological data. Despite advancements, the use of these models remains largely limited to gauged watersheds due to their reliance on measured discharge data for accurately modeling infiltration processes, and the use of consistent infiltration parameters further restricts prediction accuracy. To overcome this limitation, this study introduces a straightforward technique that utilizes parameters from an adjacent watershed with comprehensive data collection to account for rainfall loss due to infiltration, employing event-by-event infiltration parameters to enhance prediction accuracy. By integrating diverse watershed data, including rainfall-runoff and GLDAS-Noah data, with the Long Short-Term Memory (LSTM) network—a high-performance deep learning model widely used in hydrology—we achieved reliable parameter estimation in adjacent gauged watersheds. By transferring these estimated parameter values to the target ungauged watershed, leveraging the spatial proximity between adjacent watersheds, we effectively estimated rainfall loss and simulated a physically-based model for streamflow prediction, even in an ungauged watershed. For a gauged watershed, we achieved an accuracy with a Nash-Sutcliffe Efficiency (NSE) of 0.7–0.97 and a coefficient of determination (R²) of 0.7–0.98 in predicting flood hydrographs for most events. For an ungauged watershed, where observed runoff data do not exist, we employed indirect verification methods using alternative data sources such as news reports and flood inundation maps. We first generated flood maps based on our channel routing results and then compared them with a flood inundation map from a news report. In this case, we quantified the accuracy using the confusion matrix, and the metrics were measured as Accuracy = 0.72 and Recall = 0.58. This technique offers significant potential by integrating watershed data with deep learning models like LSTM to enhance flood prediction in data-scarce regions, making it applicable for ungauged watersheds through spatial proximity. By improving disaster preparedness and management with more reliable flood forecasting and efficient resource allocation, it helps mitigate the impact of floods and inspires further advancements in flood prediction models. © 2025 The Authors Deep learning; Flood prediction; Physically-based; Ungauged watershed Data accuracy; Prediction models; Rain gages; Runoff; Deep learning; Flood prediction; Infiltration parameters; Learning models; Performance; Physically based; Prediction accuracy; Prediction modelling; Short term memory; Ungauged watershed; Resource allocation";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;2;Preparation
73;Post-earthquake structural damage assessment, lessons learned, and addressing objections following the 2023 Kahramanmaras, Turkey earthquakes;This paper provides a comprehensive examination of post-earthquake structural damage assessment efforts following the Kahramanmaras, Turkey, earthquakes that occurred on February 6, 2023. Drawing on global damage assessment protocols, the study compares and analyzes the methods implemented in the aftermath of the earthquakes, offering insights into lessons learned and challenges faced. The analysis of objections raised regarding the assessment efforts reveals significant changes in structures with moderate and severe damage, emphasizing the need for continuous improvement in assessment strategies. The paper advocates for a realistic and two-stage application method, consideration of crack type and cause, and active involvement of local communities in the assessment process. Furthermore, the study identifies key issues in the current earthquake damage assessment methodology and proposes solutions, including a more precise classification system, regular volunteer training, consideration of secondary disaster risks, and effective communication methods. The paper concludes by underscoring the importance of effective damage assessment in disaster management, addressing objections from the affected population, and continual enhancement of strategies to improve resilience in earthquake-prone regions. © The Author(s) 2025.;"Disaster response; Kahramanmaras earthquakes; Objections in damage assessment; Post-earthquake damage assessment; Structural resilience";"Kahramanmaras; Turkey; Disaster prevention; Disasters; Earthquake effects; Earthquake engineering; Damage assessments; Disaster-response; Earthquake damages; Kahramanmara earthquake; Objection in damage assessment; Post-earthquake damage assessment; Structural damage assessments; Structural resilience; Turkey earthquake; disaster management; earthquake damage; earthquake event; ecosystem resilience; identification key; natural disaster; risk assessment; Structural Mechanics";"Post-earthquake structural damage assessment, lessons learned, and addressing objections following the 2023 Kahramanmaras, Turkey earthquakes This paper provides a comprehensive examination of post-earthquake structural damage assessment efforts following the Kahramanmaras, Turkey, earthquakes that occurred on February 6, 2023. Drawing on global damage assessment protocols, the study compares and analyzes the methods implemented in the aftermath of the earthquakes, offering insights into lessons learned and challenges faced. The analysis of objections raised regarding the assessment efforts reveals significant changes in structures with moderate and severe damage, emphasizing the need for continuous improvement in assessment strategies. The paper advocates for a realistic and two-stage application method, consideration of crack type and cause, and active involvement of local communities in the assessment process. Furthermore, the study identifies key issues in the current earthquake damage assessment methodology and proposes solutions, including a more precise classification system, regular volunteer training, consideration of secondary disaster risks, and effective communication methods. The paper concludes by underscoring the importance of effective damage assessment in disaster management, addressing objections from the affected population, and continual enhancement of strategies to improve resilience in earthquake-prone regions. © The Author(s) 2025. Disaster response; Kahramanmaras earthquakes; Objections in damage assessment; Post-earthquake damage assessment; Structural resilience Kahramanmaras; Turkey; Disaster prevention; Disasters; Earthquake effects; Earthquake engineering; Damage assessments; Disaster-response; Earthquake damages; Kahramanmara earthquake; Objection in damage assessment; Post-earthquake damage assessment; Structural damage assessments; Structural resilience; Turkey earthquake; disaster management; earthquake damage; earthquake event; ecosystem resilience; identification key; natural disaster; risk assessment; Structural Mechanics";-1;Não Classificado;NULL;1.1;Geological;3;Response
74;Assessing wildfire susceptibility and spatial patterns in diverse forest ecosystems across China: An integrated geospatial analysis;"Understanding the spatial and temporal distribution characteristics of fires, their driving factors and accurately predicting fire occurrences are essential for effective forest management. Therefore, it is essential to identify and predict areas susceptible to fires, particularly in a country like China, where environmental and social conditions have undergone significant changes. In this study, we analyzed the spatial patterns of fires in distinct forest ecosystems across China. By incorporating RS and GIS technologies, and machine learning methodologies, we examined the factors influencing fires and developed a susceptibility model for different forest ecosystems. To generate fire susceptibility maps, we employed three machine learning models to establish connections between fire occurrences data and 17 predictor variables including climate, topography, vegetation, and human disturbances, namely artificial neural network, random forest, and the extreme gradient boosting models. The results showed that the fire points in different forest ecosystems showed a significant clustering distribution in space, and the driving factors of fire were different. We observed satisfactory performance across all the fire prediction models employed. Specially, extreme gradient boosting model exhibited superior performance with an AUC = 0.82–0.95; accuracy = 0.79–0.87; recall = 0.78–0.89; and F-Measure = 0.78–0.86. Forest fires in Heilongjiang Province are mainly caused by vegetation factors, while in Sichuan, human factors are the primary cause of fire incidents. Topographical factors play a crucial role in influencing the occurrence of forest fires in Shanxi and Fujian. Climate factors play a crucial role in Guangdong and Yunnan. The temporal and spatial patterns of fires in various ecosystems could be analyzed in combination with forest fire factors, providing important scientific information for regional forest fire early warning and monitoring. © 2025 Elsevier Ltd";"Fire susceptibility; Forest management; Geospatial analysis; Wildfires; XGBoost";"Premixed flames; Driving factors; Fire occurrences; Fire susceptibility; Forest ecosystem; Forest fires; Geo-spatial analysis; Gradient boosting; Spatial patterns; Wildfire; Xgboost; Forest ecology";"Assessing wildfire susceptibility and spatial patterns in diverse forest ecosystems across China: An integrated geospatial analysis Understanding the spatial and temporal distribution characteristics of fires, their driving factors and accurately predicting fire occurrences are essential for effective forest management. Therefore, it is essential to identify and predict areas susceptible to fires, particularly in a country like China, where environmental and social conditions have undergone significant changes. In this study, we analyzed the spatial patterns of fires in distinct forest ecosystems across China. By incorporating RS and GIS technologies, and machine learning methodologies, we examined the factors influencing fires and developed a susceptibility model for different forest ecosystems. To generate fire susceptibility maps, we employed three machine learning models to establish connections between fire occurrences data and 17 predictor variables including climate, topography, vegetation, and human disturbances, namely artificial neural network, random forest, and the extreme gradient boosting models. The results showed that the fire points in different forest ecosystems showed a significant clustering distribution in space, and the driving factors of fire were different. We observed satisfactory performance across all the fire prediction models employed. Specially, extreme gradient boosting model exhibited superior performance with an AUC = 0.82–0.95; accuracy = 0.79–0.87; recall = 0.78–0.89; and F-Measure = 0.78–0.86. Forest fires in Heilongjiang Province are mainly caused by vegetation factors, while in Sichuan, human factors are the primary cause of fire incidents. Topographical factors play a crucial role in influencing the occurrence of forest fires in Shanxi and Fujian. Climate factors play a crucial role in Guangdong and Yunnan. The temporal and spatial patterns of fires in various ecosystems could be analyzed in combination with forest fire factors, providing important scientific information for regional forest fire early warning and monitoring. © 2025 Elsevier Ltd Fire susceptibility; Forest management; Geospatial analysis; Wildfires; XGBoost Premixed flames; Driving factors; Fire occurrences; Fire susceptibility; Forest ecosystem; Forest fires; Geo-spatial analysis; Gradient boosting; Spatial patterns; Wildfire; Xgboost; Forest ecology";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
75;A local patch regression-based generative model for urban flood prediction in data-poor areas;The increasing frequency of extreme weather events driven by climate changes underscores the urgent need for real-time urban pluvial flood prediction methods that leverage artificial intelligence (AI) technologies. However, these data-driven approaches often encounter significant challenges due to data scarcity and dynamic environments. To enable accurate predictions even in data-poor and environmentally changing scenarios, this study proposes a local patch regression-based generative model (LPR-GM) that strategically achieves a fit-for-purpose implementation and high prediction accuracy with available datasets through two primary methodologies: local patch analysis and latent space regression. Local patch analysis facilitates accurate prediction of flood depths in specific flood-prone areas, even under data-poor conditions. In addition, regression-based sampling, referred to as latent space regression, enhances robustness to potential outliers in the training dataset, which can severely hinder the learning process, particularly under data-poor conditions. The proposed LPR-GM was evaluated in three distinct flood-frequent urban areas, and the results demonstrate that LPR-GM outperformed other baseline algorithms across several quantitative criteria. We believe that LPR-GM offers a practical and realistic solution for early warning systems in urban pluvial flood management, particularly in data-poor conditions. © 2025 Elsevier Ltd;"Artificial intelligence; Data scarcity; Dynamic environment; Urban pluvial flood prediction";"Data accuracy; Generative adversarial networks; Accurate prediction; Condition; Data scarcity; Dynamic environments; Flood prediction; Generative model; Patch analysis; Pluvials; Urban floods; Urban pluvial flood prediction; Prediction models";"A local patch regression-based generative model for urban flood prediction in data-poor areas The increasing frequency of extreme weather events driven by climate changes underscores the urgent need for real-time urban pluvial flood prediction methods that leverage artificial intelligence (AI) technologies. However, these data-driven approaches often encounter significant challenges due to data scarcity and dynamic environments. To enable accurate predictions even in data-poor and environmentally changing scenarios, this study proposes a local patch regression-based generative model (LPR-GM) that strategically achieves a fit-for-purpose implementation and high prediction accuracy with available datasets through two primary methodologies: local patch analysis and latent space regression. Local patch analysis facilitates accurate prediction of flood depths in specific flood-prone areas, even under data-poor conditions. In addition, regression-based sampling, referred to as latent space regression, enhances robustness to potential outliers in the training dataset, which can severely hinder the learning process, particularly under data-poor conditions. The proposed LPR-GM was evaluated in three distinct flood-frequent urban areas, and the results demonstrate that LPR-GM outperformed other baseline algorithms across several quantitative criteria. We believe that LPR-GM offers a practical and realistic solution for early warning systems in urban pluvial flood management, particularly in data-poor conditions. © 2025 Elsevier Ltd Artificial intelligence; Data scarcity; Dynamic environment; Urban pluvial flood prediction Data accuracy; Generative adversarial networks; Accurate prediction; Condition; Data scarcity; Dynamic environments; Flood prediction; Generative model; Patch analysis; Pluvials; Urban floods; Urban pluvial flood prediction; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
76;PSLSA v2.0: An automatic Python package integrating machine learning models for regional landslide susceptibility assessment;Accurate landslide susceptibility assessments (LSA) are crucial for civil protection and land use planning. This study introduces PSLSA v2.0 as an open-source Python package that can conduct LSA automatically. It integrates six sophisticated machine learning algorithms (C5.0, SVM, LR, RF, MLP, XGBoost), and allows arbitrary combinations of influencing factors to generate landslide susceptibility index (LSI). We demonstrate how factor contribution and hyperparameter optimization as additional outputs can enhance the model interpretability. We apply PSLSA to a case study focused from Linzhi City in the Tibetan Plateau of China, that has undergone significant engineering modifications on its slopes. The results reveal that slope and aspect are the dominant factors in determining landslide susceptibility. All the six algorithms have an accuracy of over 80%. Although the distribution patterns of LSI vary, the C5.0 model is set apart with the best performance. PSLSA provides a powerful tool for stakeholders especially the non-geohazard professionals. © 2025;"Engineering interventions; Hyperparameter optimization; Landslide susceptibility assessment; Machine learning; Python; Tibetan plateau";"China; Nyingchi; Xizang; Adversarial machine learning; Open source software; Python; Civil protection; Engineering intervention; Hyper-parameter optimizations; Integrating machines; Land Use Planning; Landslide susceptibility; Landslide susceptibility assessments; Machine learning models; Machine-learning; Tibetan Plateau; land use planning; landslide; machine learning; optimization; risk assessment; slope dynamics; stakeholder; Landslides";"PSLSA v2.0: An automatic Python package integrating machine learning models for regional landslide susceptibility assessment Accurate landslide susceptibility assessments (LSA) are crucial for civil protection and land use planning. This study introduces PSLSA v2.0 as an open-source Python package that can conduct LSA automatically. It integrates six sophisticated machine learning algorithms (C5.0, SVM, LR, RF, MLP, XGBoost), and allows arbitrary combinations of influencing factors to generate landslide susceptibility index (LSI). We demonstrate how factor contribution and hyperparameter optimization as additional outputs can enhance the model interpretability. We apply PSLSA to a case study focused from Linzhi City in the Tibetan Plateau of China, that has undergone significant engineering modifications on its slopes. The results reveal that slope and aspect are the dominant factors in determining landslide susceptibility. All the six algorithms have an accuracy of over 80%. Although the distribution patterns of LSI vary, the C5.0 model is set apart with the best performance. PSLSA provides a powerful tool for stakeholders especially the non-geohazard professionals. © 2025 Engineering interventions; Hyperparameter optimization; Landslide susceptibility assessment; Machine learning; Python; Tibetan plateau China; Nyingchi; Xizang; Adversarial machine learning; Open source software; Python; Civil protection; Engineering intervention; Hyper-parameter optimizations; Integrating machines; Land Use Planning; Landslide susceptibility; Landslide susceptibility assessments; Machine learning models; Machine-learning; Tibetan Plateau; land use planning; landslide; machine learning; optimization; risk assessment; slope dynamics; stakeholder; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
77;CCTV image-based classification of blocked trash screens;"This study introduces image-based classification techniques to identify whether trash screens in urban rivers are blocked. The study obtained 755 images from a CCTV camera surveying a trash screen located on an urban river at Tongwynlais in Cardiff. Manual quality control reduced the dataset to 577 images, labelled as either blocked (80%) or unblocked (20%). The performance of a logistic regression for classification of images was investigated using three different subsets of the labelled images: (1) the original dataset, (2) a balanced but under-sampled dataset with equal number of blocked and unblocked images, and (3) an augmented dataset with an equal number of blocked and unblocked images using Gaussian noise augmentation to increase the number of unblocked images. Results show that our data-augmentation method enhanced model accuracy by 8%, successfully classifying images as blocked or unblocked with an accuracy of 88%; by overcoming the bias in the dataset these results also highlight potential solutions to overcome the challenges of operating this methodology across a network of cameras. This enables authorities in both data rich and data scarce regions the ability to take advantage of machine learning to open up the next generation of a distributed, data-driven flood warning systems, protecting people, infrastructure and the environment. © 2024 The Author(s). Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd.";"data augmentation; image analysis; machine learning; trash screens; urban flood risk";"Cardiff; accuracy assessment; data processing; early warning system; flash flood; image analysis; image classification; machine learning; quality control; remote sensing; surveying";"CCTV image-based classification of blocked trash screens This study introduces image-based classification techniques to identify whether trash screens in urban rivers are blocked. The study obtained 755 images from a CCTV camera surveying a trash screen located on an urban river at Tongwynlais in Cardiff. Manual quality control reduced the dataset to 577 images, labelled as either blocked (80%) or unblocked (20%). The performance of a logistic regression for classification of images was investigated using three different subsets of the labelled images: (1) the original dataset, (2) a balanced but under-sampled dataset with equal number of blocked and unblocked images, and (3) an augmented dataset with an equal number of blocked and unblocked images using Gaussian noise augmentation to increase the number of unblocked images. Results show that our data-augmentation method enhanced model accuracy by 8%, successfully classifying images as blocked or unblocked with an accuracy of 88%; by overcoming the bias in the dataset these results also highlight potential solutions to overcome the challenges of operating this methodology across a network of cameras. This enables authorities in both data rich and data scarce regions the ability to take advantage of machine learning to open up the next generation of a distributed, data-driven flood warning systems, protecting people, infrastructure and the environment. © 2024 The Author(s). Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd. data augmentation; image analysis; machine learning; trash screens; urban flood risk Cardiff; accuracy assessment; data processing; early warning system; flash flood; image analysis; image classification; machine learning; quality control; remote sensing; surveying";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
78;DeepFlood for Inundated Vegetation High-Resolution Dataset for Accurate Flood Mapping and Segmentation;Rapid and accurate assessment of flood extent is important for effective disaster response, mitigation planning, and resource allocation. Traditional flood mapping methods encounter challenges in scalability and transferability. However, the emergence of deep learning, particularly convolutional neural networks (CNNs), revolutionizes flood mapping by autonomously learning intricate spatial patterns and semantic features directly from raw data. DeepFlood is introduced to address the essential requirement for high-quality training datasets. This is a novel dataset comprising high-resolution manned and unmanned aerial imagery and Synthetic Aperture Radar (SAR) imagery, enriched with detailed labels including inundated vegetation, one of the most challenging areas for flood mapping. DeepFlood enables multi-modal flood mapping approaches and mitigates limitations in existing datasets by providing comprehensive annotations and diverse landscape coverage. We evaluate several semantic segmentation architectures on DeepFlood, demonstrating its usability and efficacy in post-disaster flood mapping scenarios. © The Author(s) 2025.;NULL;"article; convolutional neural network; deep learning; disaster response; human; imagery; learning; mitigation; resource allocation; vegetation";"DeepFlood for Inundated Vegetation High-Resolution Dataset for Accurate Flood Mapping and Segmentation Rapid and accurate assessment of flood extent is important for effective disaster response, mitigation planning, and resource allocation. Traditional flood mapping methods encounter challenges in scalability and transferability. However, the emergence of deep learning, particularly convolutional neural networks (CNNs), revolutionizes flood mapping by autonomously learning intricate spatial patterns and semantic features directly from raw data. DeepFlood is introduced to address the essential requirement for high-quality training datasets. This is a novel dataset comprising high-resolution manned and unmanned aerial imagery and Synthetic Aperture Radar (SAR) imagery, enriched with detailed labels including inundated vegetation, one of the most challenging areas for flood mapping. DeepFlood enables multi-modal flood mapping approaches and mitigates limitations in existing datasets by providing comprehensive annotations and diverse landscape coverage. We evaluate several semantic segmentation architectures on DeepFlood, demonstrating its usability and efficacy in post-disaster flood mapping scenarios. © The Author(s) 2025. NULL article; convolutional neural network; deep learning; disaster response; human; imagery; learning; mitigation; resource allocation; vegetation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
79;Predicting landslide surge waves from large-scale physical experimental using machine learning;The impoundment of a hydropower station can cause water levels in reservoir areas to rise, potentially triggering nearby landslides and generating surge waves that pose significant threats to navigation and hydropower infrastructure. Traditional methods for predicting landslide-induced surge waves often struggle to accurately capture peak wave heights and their evolving trends. To address this challenge, this study employs machine learning approaches to enhance the prediction of surge wave characteristics by integrating insight from physical model experimental data. Initially, we utilized multi-peak Gaussian functions to fit the experimental surge wave data, enabling us to characterize surge wave run-up through derived fitting equations. Building on these findings, we developed three machine learning models—Random Forest, Long Short-Term Memory, and Gated Recurrent Unit (GRU)—to predict surge wave behavior. Among these, the GRU model outperformed others, demonstrating exceptional accuracy in capturing the critical first and second wave peaks, which are crucial for disaster mitigation. This study underscores the GRU model's robustness in predicting surge wave dynamics, presenting it as a valuable tool for mitigating risks associated with landslide-induced surge waves. By combining physical modeling, experimental data, and advanced machine learning techniques, this research establishes an innovative framework for enhancing reservoir management and disaster prevention efforts. © 2025 Author(s).;NULL;"Electron diffraction; Information management; Light transmission; Long short-term memory; Petroleum reservoir evaluation; Reservoirs (water); Shock waves; Hydropower stations; Large-scales; Machine learning approaches; Machine-learning; Multi-peaks; Peak wave height; Physical modelling; Reservoir area; Surge wave; Wave characteristics; Landslides";"Predicting landslide surge waves from large-scale physical experimental using machine learning The impoundment of a hydropower station can cause water levels in reservoir areas to rise, potentially triggering nearby landslides and generating surge waves that pose significant threats to navigation and hydropower infrastructure. Traditional methods for predicting landslide-induced surge waves often struggle to accurately capture peak wave heights and their evolving trends. To address this challenge, this study employs machine learning approaches to enhance the prediction of surge wave characteristics by integrating insight from physical model experimental data. Initially, we utilized multi-peak Gaussian functions to fit the experimental surge wave data, enabling us to characterize surge wave run-up through derived fitting equations. Building on these findings, we developed three machine learning models—Random Forest, Long Short-Term Memory, and Gated Recurrent Unit (GRU)—to predict surge wave behavior. Among these, the GRU model outperformed others, demonstrating exceptional accuracy in capturing the critical first and second wave peaks, which are crucial for disaster mitigation. This study underscores the GRU model's robustness in predicting surge wave dynamics, presenting it as a valuable tool for mitigating risks associated with landslide-induced surge waves. By combining physical modeling, experimental data, and advanced machine learning techniques, this research establishes an innovative framework for enhancing reservoir management and disaster prevention efforts. © 2025 Author(s). NULL Electron diffraction; Information management; Light transmission; Long short-term memory; Petroleum reservoir evaluation; Reservoirs (water); Shock waves; Hydropower stations; Large-scales; Machine learning approaches; Machine-learning; Multi-peaks; Peak wave height; Physical modelling; Reservoir area; Surge wave; Wave characteristics; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
80;Effect of uncertainties in breach location and breach mechanisms on risk-related classification of off-stream reservoirs;Off-stream reservoirs are artificial water storage structures that increase the flood risk of an area. In some places, related risk reduction plans are based on a risk classification of these structures, which follows local water resource management regulations. These classification methods typically follow deterministic qualitative guidelines that do not account for uncertainties. This study introduces a fourth-step probabilistic approach that accounts for uncertainties related to simultaneous breach formation and breaking point location of off-stream reservoirs, and proposes an alternative visualisation for their classification. The methodology is applied to a set of Spanish off-stream reservoirs that are classified according to the Spanish normative. Results show that different breaking points and breach formations generate diverse classifications that can affect risk reduction plans. Additionally, we demonstrate that the proposed visualisation can be used for various purposes, including the case of the evolution of the categorisation in time, due to land use changes, which could be used by decision-makers to understand which off-stream reservoir requires a category update. These findings introduce a novel approach to managing uncertainties, which is crucial for developing resilient flood management strategies and contributes to the innovation discourse in flood risk management. © 2024 The Author(s). Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd.;"classification; dams and reservoirs; hazard; probabilistic flood maps; uncertainty analysis";"classification; dam construction; flood control; hydrological hazard; reservoir; risk assessment; storage structure; uncertainty analysis; visualization; water storage";"Effect of uncertainties in breach location and breach mechanisms on risk-related classification of off-stream reservoirs Off-stream reservoirs are artificial water storage structures that increase the flood risk of an area. In some places, related risk reduction plans are based on a risk classification of these structures, which follows local water resource management regulations. These classification methods typically follow deterministic qualitative guidelines that do not account for uncertainties. This study introduces a fourth-step probabilistic approach that accounts for uncertainties related to simultaneous breach formation and breaking point location of off-stream reservoirs, and proposes an alternative visualisation for their classification. The methodology is applied to a set of Spanish off-stream reservoirs that are classified according to the Spanish normative. Results show that different breaking points and breach formations generate diverse classifications that can affect risk reduction plans. Additionally, we demonstrate that the proposed visualisation can be used for various purposes, including the case of the evolution of the categorisation in time, due to land use changes, which could be used by decision-makers to understand which off-stream reservoir requires a category update. These findings introduce a novel approach to managing uncertainties, which is crucial for developing resilient flood management strategies and contributes to the innovation discourse in flood risk management. © 2024 The Author(s). Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd. classification; dams and reservoirs; hazard; probabilistic flood maps; uncertainty analysis classification; dam construction; flood control; hydrological hazard; reservoir; risk assessment; storage structure; uncertainty analysis; visualization; water storage";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
81;INTEGRATING DRONE TECHNOLOGY AND MACHINE LEARNING FOR ENHANCED FLOOD RISK PREDICTION;This study investigates the integration of high-resolution multispectral and topographic data obtained through drone technology with machine learning to enhance flood risk prediction. Using a multispectral GeoTIFF file covering a designated flood-prone area, critical feature such as the Normalized Difference Vegetation Index (NDVI), slope, and Terrain Ruggedness Index (TRI) were extracted to train a logistic regression model. The model achieved an accuracy of 86.35% and an ROC-AUC score of 0.98, demonstrating strong predictive performance in distinguishing flood-prone from non-flood-prone areas. Feature importance analysis identified low NDVI and high terrain ruggedness as significant predictors of increased flood susceptibility. Despite its strengths, the model showed a tendency to overpredict flood risk, resulting in a higher falsepositive rate. This highlights the need for further refinement, including the incorporation of additional data sources such as historical flood records and rainfall data, as well as the exploration of advanced machine learning models to improve precision and reliability. Overall, this study demonstrates the potential of integrating drone-derived data with machine learning for flood risk management. The proposed approach offers a scalable solution for real-time flood prediction, providing actionable insights for improving disaster preparedness and response in flood-prone regions. © Little Lion Scientific.;"Disaster Management; Drone Technology; Flood Risk Prediction; Machine Learning; Multispectral Data";NULL;"INTEGRATING DRONE TECHNOLOGY AND MACHINE LEARNING FOR ENHANCED FLOOD RISK PREDICTION This study investigates the integration of high-resolution multispectral and topographic data obtained through drone technology with machine learning to enhance flood risk prediction. Using a multispectral GeoTIFF file covering a designated flood-prone area, critical feature such as the Normalized Difference Vegetation Index (NDVI), slope, and Terrain Ruggedness Index (TRI) were extracted to train a logistic regression model. The model achieved an accuracy of 86.35% and an ROC-AUC score of 0.98, demonstrating strong predictive performance in distinguishing flood-prone from non-flood-prone areas. Feature importance analysis identified low NDVI and high terrain ruggedness as significant predictors of increased flood susceptibility. Despite its strengths, the model showed a tendency to overpredict flood risk, resulting in a higher falsepositive rate. This highlights the need for further refinement, including the incorporation of additional data sources such as historical flood records and rainfall data, as well as the exploration of advanced machine learning models to improve precision and reliability. Overall, this study demonstrates the potential of integrating drone-derived data with machine learning for flood risk management. The proposed approach offers a scalable solution for real-time flood prediction, providing actionable insights for improving disaster preparedness and response in flood-prone regions. © Little Lion Scientific. Disaster Management; Drone Technology; Flood Risk Prediction; Machine Learning; Multispectral Data NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
82;Deep-TCP: Multi-source data fusion for deep learning-powered tropical cyclone intensity prediction to enhance urban sustainability;Tropical cyclones (TC) exert a profound impact on cities, causing extensive damage and losses. Thus, TC Intensity Prediction is crucial for creating sustainable cities as it enables proactive measures to be taken, including evacuation planning, infrastructure reinforcement, and emergency response coordination. In this study, we propose a Deep learning-powered TC Intensity Prediction (Deep-TCP) framework. In particular, Deep-TCP contains a data constraint module for fusing data features from multiple sources and establishing a unified global representation. To capture the spatiotemporal attributes, a Spatial-Temporal Attention (ST-Attention) module is built to distill insights from environmental variables. To improve the robustness and stability of the predictions, an encoder-decoder module that utilizes the ConvGPU unit is introduced to enhance feature maps. Then, a novel feature enhancement module is built to bolster the generalization capability and solve the dependency attenuation. The results demonstrate that the Deep-TCP framework significantly outperforms various benchmarks. Additionally, it effectively predicts multiple TC categories within the 6–24 h timeframe, showing strong capability in predicting changing trends. The reliable prediction results are potentially beneficial for disaster management and urban planning, significantly enhancing urban sustainability by improving preparedness and response strategies. © 2024 Elsevier B.V.;"Data fusion; Deep learning; Spatial-temporal attention; Sustainable city; Tropical cyclone";"Deep reinforcement learning; Sustainable city; Deep learning; Intensity prediction; Multi-Sources; Source data; Spatial temporals; Spatial-temporal attention; Sustainable cities; Tropical cyclone; Tropical cyclone intensity; Urban sustainability; Tropical cyclone";"Deep-TCP: Multi-source data fusion for deep learning-powered tropical cyclone intensity prediction to enhance urban sustainability Tropical cyclones (TC) exert a profound impact on cities, causing extensive damage and losses. Thus, TC Intensity Prediction is crucial for creating sustainable cities as it enables proactive measures to be taken, including evacuation planning, infrastructure reinforcement, and emergency response coordination. In this study, we propose a Deep learning-powered TC Intensity Prediction (Deep-TCP) framework. In particular, Deep-TCP contains a data constraint module for fusing data features from multiple sources and establishing a unified global representation. To capture the spatiotemporal attributes, a Spatial-Temporal Attention (ST-Attention) module is built to distill insights from environmental variables. To improve the robustness and stability of the predictions, an encoder-decoder module that utilizes the ConvGPU unit is introduced to enhance feature maps. Then, a novel feature enhancement module is built to bolster the generalization capability and solve the dependency attenuation. The results demonstrate that the Deep-TCP framework significantly outperforms various benchmarks. Additionally, it effectively predicts multiple TC categories within the 6–24 h timeframe, showing strong capability in predicting changing trends. The reliable prediction results are potentially beneficial for disaster management and urban planning, significantly enhancing urban sustainability by improving preparedness and response strategies. © 2024 Elsevier B.V. Data fusion; Deep learning; Spatial-temporal attention; Sustainable city; Tropical cyclone Deep reinforcement learning; Sustainable city; Deep learning; Intensity prediction; Multi-Sources; Source data; Spatial temporals; Spatial-temporal attention; Sustainable cities; Tropical cyclone; Tropical cyclone intensity; Urban sustainability; Tropical cyclone";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
83;Rapid mapping of landslides using satellite SAR imagery: A progressive learning approach;Rapid detection of landslides after an exceptional event is critical for planning effective disaster management. Previous works have typically used machine learning-based methods, including the recently popular deep-learning approaches, to identify characteristics surface features from satellite remote sensing data, especially from optical images. However, data acquisition from optical images is not possible in cloudy conditions, leading to unpredictable delays in any mapping task from future events. These methods also rely on large manually labelled inventories for training, which is often not available before the event. In this work, we propose an active training strategy to generate a landslide map after an event using the first available synthetic-aperture radar (SAR) image and improve it once subsequent cloud-free optical images are acquired. The proposed active learning workflow can start with a small (∼100m2) and incomplete inventory,- and can grow the extent and completeness in iterative steps with manual updates after each step. This significantly reduces the slow manual mapping typically required for generating a large training inventory. We designed our experiments to map the landslides triggered by the Mw 6.6 Hokkaido Eastern Iburi earthquake of 2018 in Japan using sequentially ALOS-2 (SAR) and PlanetScope (Optical) scenes in the order they are acquired. The choice of active learning prioritizes speed over accuracy. However, we note only a modest reduction in performance (∼10% drop in F1 and MCC scores), with our method allowing a preliminary landslide inventory to be completed within a single day. This is of major importance in disaster response, improving performance and reducing the potential subjectivity associated with manual mapping. © 2025 The Authors;"Landslide; Machine learning; Rapid mapping; Satellite SAR";"Active learning; Contrastive Learning; Deep learning; Image acquisition; Image enhancement; Network security; Optical tomography; Radar imaging; Active Learning; Learning approach; Machine-learning; Manual mapping; Optical image; Progressive learning; Rapid detection; Rapid mapping; Satellite synthetic-aperture radar; Synthetic Aperture Radar Imagery; Satellite imagery";"Rapid mapping of landslides using satellite SAR imagery: A progressive learning approach Rapid detection of landslides after an exceptional event is critical for planning effective disaster management. Previous works have typically used machine learning-based methods, including the recently popular deep-learning approaches, to identify characteristics surface features from satellite remote sensing data, especially from optical images. However, data acquisition from optical images is not possible in cloudy conditions, leading to unpredictable delays in any mapping task from future events. These methods also rely on large manually labelled inventories for training, which is often not available before the event. In this work, we propose an active training strategy to generate a landslide map after an event using the first available synthetic-aperture radar (SAR) image and improve it once subsequent cloud-free optical images are acquired. The proposed active learning workflow can start with a small (∼100m2) and incomplete inventory,- and can grow the extent and completeness in iterative steps with manual updates after each step. This significantly reduces the slow manual mapping typically required for generating a large training inventory. We designed our experiments to map the landslides triggered by the Mw 6.6 Hokkaido Eastern Iburi earthquake of 2018 in Japan using sequentially ALOS-2 (SAR) and PlanetScope (Optical) scenes in the order they are acquired. The choice of active learning prioritizes speed over accuracy. However, we note only a modest reduction in performance (∼10% drop in F1 and MCC scores), with our method allowing a preliminary landslide inventory to be completed within a single day. This is of major importance in disaster response, improving performance and reducing the potential subjectivity associated with manual mapping. © 2025 The Authors Landslide; Machine learning; Rapid mapping; Satellite SAR Active learning; Contrastive Learning; Deep learning; Image acquisition; Image enhancement; Network security; Optical tomography; Radar imaging; Active Learning; Learning approach; Machine-learning; Manual mapping; Optical image; Progressive learning; Rapid detection; Rapid mapping; Satellite synthetic-aperture radar; Synthetic Aperture Radar Imagery; Satellite imagery";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
84;Dust storm detection for ground-based stations with imbalanced machine learning;Dust storms, common meteorological hazard in arid and semi-arid regions, have significant environmental and societal impacts. Rapid and accurate detecting dust storms is critical for early warning systems. Over the past few decades, dust storm detection primarily relied on satellite remote sensing techniques using multi-channel imagery, but these methods have limitations in temporal resolution. With the recent expansion of China's observation network, the dense distribution of ground-based sensors offers a promising data source for real-time dust storm detection. This study proposes a machine learning approach to detect dust storms using ground-based sensor networks. By combining undersampling strategies and ensemble algorithms, this method improves model's performance in detecting dust storms. Compared with the state-of-the-art models, this approach improves the Recall rates for different dust storm levels by 24.32% and the G-Mean by 18.58%, achieving superior dust storm detection performance. This approach can offer the near-real-time, hourly updated dust storm detection products. © 2025 Elsevier Ltd;"Dust storm warning; Dust storms; Imbalanced classification; Machine learning; Real-time detection";"China; Arid and semi-arid regions; Dust storm; Dust storm warning; Ground based sensors; Ground-based stations; Imbalanced classification; Machine-learning; Real-time detection; Societal impacts; algorithm; detection method; dust storm; ground-based measurement; machine learning; meteorological hazard; real time; remote sensing; satellite data; satellite sensor; warning system; Tropical cyclone";"Dust storm detection for ground-based stations with imbalanced machine learning Dust storms, common meteorological hazard in arid and semi-arid regions, have significant environmental and societal impacts. Rapid and accurate detecting dust storms is critical for early warning systems. Over the past few decades, dust storm detection primarily relied on satellite remote sensing techniques using multi-channel imagery, but these methods have limitations in temporal resolution. With the recent expansion of China's observation network, the dense distribution of ground-based sensors offers a promising data source for real-time dust storm detection. This study proposes a machine learning approach to detect dust storms using ground-based sensor networks. By combining undersampling strategies and ensemble algorithms, this method improves model's performance in detecting dust storms. Compared with the state-of-the-art models, this approach improves the Recall rates for different dust storm levels by 24.32% and the G-Mean by 18.58%, achieving superior dust storm detection performance. This approach can offer the near-real-time, hourly updated dust storm detection products. © 2025 Elsevier Ltd Dust storm warning; Dust storms; Imbalanced classification; Machine learning; Real-time detection China; Arid and semi-arid regions; Dust storm; Dust storm warning; Ground based sensors; Ground-based stations; Imbalanced classification; Machine-learning; Real-time detection; Societal impacts; algorithm; detection method; dust storm; ground-based measurement; machine learning; meteorological hazard; real time; remote sensing; satellite data; satellite sensor; warning system; Tropical cyclone";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
85;Navigating the Challenges of Rainfall Variability: Precipitation Forecasting using Coalesce Model;This study introduces a coalesce forecasting model tailored for flood-prone regions, specifically focusing on Bihar, India. Research has revealed significant disparities in rainfall patterns across various zones such as Tirhut, Patna, and Munger zones experiencing greater mean rainfall than Bhagalpur and Kosi. To evaluate the forecasting capabilities, coalescing methods were applied which includes the autoregressive integrated moving average (ARIMA), exponential smoothing state space (ETS), neural network autoregressive (NNAR), and seasonal-trend decomposition. Moreover, Loess (STL) methods, and trigonometric seasonality, Box‒Cox transformation, ARMA errors, and trend and seasonal components (TBATS) were also employed to contrast the benchmark models such as the seasonal naïve, naïve, and mean methods. These methods were evaluated using error evaluators such as residual error, root mean square error (RMSE), mean absolute error (MAE), mean absolute scaled error (MASE), and autocorrelation of errors at lag 1 (ACF1) to determine the performance of these techniques. Additionally, statistical tests, such as the Box–Pierce and Box–Ljung tests, supported these findings. Among the error evaluators and forecasting models, the ETS and NNAR models remain the top choices for Saran-Tirhut-Bhagalpur and Munger-Magadh-Kosi, respectively, effectively capturing rainfall patterns and minimizing residual errors, as indicated by low RMSE values. Moreover, ARIMA and TBATS remain the top choices for Patna, Purnia and Darbhanga, respectively, followed by ETS model. In addition, the STL model secured the second position for Saran, Tirhut, Bhagalpur, and Purnia zones. This research emphasizes the importance of understanding regional rainfall dynamics for effective flood risk management and climate adaptation strategies. This study provides valuable tools for water resource management and agricultural planning in Bihar amidst climate variability challenges. It advocates for rainfall trend analysis followed by forecasting to achieve more precise water resource management and planning. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"Bihar; ETS; Flood; Forecasting; Machine learning; NNAR; Precipitation; Rainfall";"Bhagalpur; Bihar; India; Kosi; Nepal; Patna; Autocorrelation; Benchmarking; Rain; Resource allocation; Risk assessment; Weather forecasting; Auto-regressive; Bihar; ETS; Forecasting models; Machine-learning; Moving averages; Neural network autoregressive; Neural-networks; Rainfall patterns; Residual error; climate change; flood; forecasting method; machine learning; rainfall; trend analysis; Risk management";"Navigating the Challenges of Rainfall Variability: Precipitation Forecasting using Coalesce Model This study introduces a coalesce forecasting model tailored for flood-prone regions, specifically focusing on Bihar, India. Research has revealed significant disparities in rainfall patterns across various zones such as Tirhut, Patna, and Munger zones experiencing greater mean rainfall than Bhagalpur and Kosi. To evaluate the forecasting capabilities, coalescing methods were applied which includes the autoregressive integrated moving average (ARIMA), exponential smoothing state space (ETS), neural network autoregressive (NNAR), and seasonal-trend decomposition. Moreover, Loess (STL) methods, and trigonometric seasonality, Box‒Cox transformation, ARMA errors, and trend and seasonal components (TBATS) were also employed to contrast the benchmark models such as the seasonal naïve, naïve, and mean methods. These methods were evaluated using error evaluators such as residual error, root mean square error (RMSE), mean absolute error (MAE), mean absolute scaled error (MASE), and autocorrelation of errors at lag 1 (ACF1) to determine the performance of these techniques. Additionally, statistical tests, such as the Box–Pierce and Box–Ljung tests, supported these findings. Among the error evaluators and forecasting models, the ETS and NNAR models remain the top choices for Saran-Tirhut-Bhagalpur and Munger-Magadh-Kosi, respectively, effectively capturing rainfall patterns and minimizing residual errors, as indicated by low RMSE values. Moreover, ARIMA and TBATS remain the top choices for Patna, Purnia and Darbhanga, respectively, followed by ETS model. In addition, the STL model secured the second position for Saran, Tirhut, Bhagalpur, and Purnia zones. This research emphasizes the importance of understanding regional rainfall dynamics for effective flood risk management and climate adaptation strategies. This study provides valuable tools for water resource management and agricultural planning in Bihar amidst climate variability challenges. It advocates for rainfall trend analysis followed by forecasting to achieve more precise water resource management and planning. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. Bihar; ETS; Flood; Forecasting; Machine learning; NNAR; Precipitation; Rainfall Bhagalpur; Bihar; India; Kosi; Nepal; Patna; Autocorrelation; Benchmarking; Rain; Resource allocation; Risk assessment; Weather forecasting; Auto-regressive; Bihar; ETS; Forecasting models; Machine-learning; Moving averages; Neural network autoregressive; Neural-networks; Rainfall patterns; Residual error; climate change; flood; forecasting method; machine learning; rainfall; trend analysis; Risk management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
86;Denoising diffusion probabilistic model for accurate super-resolution in landslide and debris flow disaster remote sensing images;The low-resolution effect of satellite and drone remote sensing images of landslide and debris flow disasters under adverse weather or complex scenarios, as well as the issues of gradient explosion, mode collapse, and artifacts in deep learning-based generative adversarial network (GAN) super-resolution models, present significant challenges. This paper proposes an optical remote sensing super-resolution reconstruction method for landslide and debris flow disasters based on a denoising diffusion probabilistic model (DDPM). The method adds Gaussian noise in the forward process to convert the image into a Gaussian noise distribution, and in the reverse process, generates high-resolution images using a reverse process, thus achieving high-quality super-resolution reconstruction of remote sensing images of landslides and debris flows. The reconstructed images demonstrate enhanced texture, terrain, and landform details. Additionally, this method addresses the problem of insufficient training datasets for precise super-resolution images in disaster intelligence recognition. Experiments were conducted on landslide and debris flow image datasets from regions such as Sichuan and Guizhou in China. The experimental results demonstrate that the super-resolution reconstruction results based on the denoising diffusion probabilistic model exhibit significant improvements in peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM), outperforming traditional deep learning methods. This study provides a new technical solution for obtaining accurate high-resolution remote sensing image data.  Copyright © 2025 SPIE.;"Denoising diffusion probabilistic model; High-resolution remote sensing images; Landslide and debris flow disasters; Super-Resolution";"Gaussian distribution; Gaussian noise (electronic); Health risks; Image denoising; Image enhancement; Image reconstruction; Image resolution; Image texture; Optical remote sensing; Risk analysis; Risk assessment; Weather satellites; De-noising; Debris flows; Denoising diffusion probabilistic model; High-resolution remote sensing images; Landslide and debris flow disaster; Probabilistic models; Remote sensing images; Reverse process; Super-resolution reconstruction; Superresolution; Landslides";"Denoising diffusion probabilistic model for accurate super-resolution in landslide and debris flow disaster remote sensing images The low-resolution effect of satellite and drone remote sensing images of landslide and debris flow disasters under adverse weather or complex scenarios, as well as the issues of gradient explosion, mode collapse, and artifacts in deep learning-based generative adversarial network (GAN) super-resolution models, present significant challenges. This paper proposes an optical remote sensing super-resolution reconstruction method for landslide and debris flow disasters based on a denoising diffusion probabilistic model (DDPM). The method adds Gaussian noise in the forward process to convert the image into a Gaussian noise distribution, and in the reverse process, generates high-resolution images using a reverse process, thus achieving high-quality super-resolution reconstruction of remote sensing images of landslides and debris flows. The reconstructed images demonstrate enhanced texture, terrain, and landform details. Additionally, this method addresses the problem of insufficient training datasets for precise super-resolution images in disaster intelligence recognition. Experiments were conducted on landslide and debris flow image datasets from regions such as Sichuan and Guizhou in China. The experimental results demonstrate that the super-resolution reconstruction results based on the denoising diffusion probabilistic model exhibit significant improvements in peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM), outperforming traditional deep learning methods. This study provides a new technical solution for obtaining accurate high-resolution remote sensing image data.  Copyright © 2025 SPIE. Denoising diffusion probabilistic model; High-resolution remote sensing images; Landslide and debris flow disasters; Super-Resolution Gaussian distribution; Gaussian noise (electronic); Health risks; Image denoising; Image enhancement; Image reconstruction; Image resolution; Image texture; Optical remote sensing; Risk analysis; Risk assessment; Weather satellites; De-noising; Debris flows; Denoising diffusion probabilistic model; High-resolution remote sensing images; Landslide and debris flow disaster; Probabilistic models; Remote sensing images; Reverse process; Super-resolution reconstruction; Superresolution; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
87;An enhanced deep reinforcement learning approach for efficient, effective, and equitable disaster relief distribution;Efficient disaster response, especially within the critical initial 72 h, is crucial for saving lives. However, allocating relief goods effectively to affected areas remains a complex challenge due to uncertainty, limited resources, and dynamic needs. This study addresses this challenge by proposing a multi-period integer nonlinear programming model for efficient, effective, and equitable distribution of relief goods during disaster response phase. To optimize relief allocation within entire 72-h, a novel decision-making approach is proposed that leverages the proximal policy optimization (PPO) algorithm. It uses deep residual neural networks for state-value and optimal action prediction with 5 value and 4 policy residual layers. Additionally, an algorithm-agnostic termination criterion based on episodic reward stall ensures effective convergence detection without requiring prior knowledge of optimal solution. The provided model and solution methods are validated through 30 hypothetical problem instances and a realistic earthquake response case study. The results demonstrate the superiority of proposed approach compared to traditional methods like dynamic programming, state-action-reward-state-action (SARSA), and Q-learning, in terms of both solution quality and sample efficiency. Notably, the deep residual networks and proposed termination criterion enable the PPO algorithm to achieve an average optimality gap of less than 10% for the majority of instances with consistent hyperparameters, while exhibiting significant sample efficiency gains, particularly for large-scale problems. This research empowers disaster managers with an efficient and timely relief delivery plan, ultimately contributing to saving lives in the face of disaster. Moreover, proposed termination criterion may improve the performance of reinforcement learning in other application areas. © 2025 Elsevier Ltd;"Disaster response; Proximal policy optimization; Q-learning; Reinforcement learning; Relief distribution; Solution quality";"Disaster-response; Optimization algorithms; Policy optimization; Proximal policy optimization; Q-learning; Reinforcement learning approach; Reinforcement learnings; Relief distribution; Solution quality; Termination criteria; Deep reinforcement learning";"An enhanced deep reinforcement learning approach for efficient, effective, and equitable disaster relief distribution Efficient disaster response, especially within the critical initial 72 h, is crucial for saving lives. However, allocating relief goods effectively to affected areas remains a complex challenge due to uncertainty, limited resources, and dynamic needs. This study addresses this challenge by proposing a multi-period integer nonlinear programming model for efficient, effective, and equitable distribution of relief goods during disaster response phase. To optimize relief allocation within entire 72-h, a novel decision-making approach is proposed that leverages the proximal policy optimization (PPO) algorithm. It uses deep residual neural networks for state-value and optimal action prediction with 5 value and 4 policy residual layers. Additionally, an algorithm-agnostic termination criterion based on episodic reward stall ensures effective convergence detection without requiring prior knowledge of optimal solution. The provided model and solution methods are validated through 30 hypothetical problem instances and a realistic earthquake response case study. The results demonstrate the superiority of proposed approach compared to traditional methods like dynamic programming, state-action-reward-state-action (SARSA), and Q-learning, in terms of both solution quality and sample efficiency. Notably, the deep residual networks and proposed termination criterion enable the PPO algorithm to achieve an average optimality gap of less than 10% for the majority of instances with consistent hyperparameters, while exhibiting significant sample efficiency gains, particularly for large-scale problems. This research empowers disaster managers with an efficient and timely relief delivery plan, ultimately contributing to saving lives in the face of disaster. Moreover, proposed termination criterion may improve the performance of reinforcement learning in other application areas. © 2025 Elsevier Ltd Disaster response; Proximal policy optimization; Q-learning; Reinforcement learning; Relief distribution; Solution quality Disaster-response; Optimization algorithms; Policy optimization; Proximal policy optimization; Q-learning; Reinforcement learning approach; Reinforcement learnings; Relief distribution; Solution quality; Termination criteria; Deep reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;3;Response
88;NDMI-Derived Field-Scale Soil Moisture Prediction Using ERA5 and LSTM for Precision Agriculture;Accurate soil moisture prediction is fundamental to precision agriculture, facilitating optimal irrigation scheduling, efficient water resource allocation, and enhanced crop productivity. This study employs a Long Short-Term Memory (LSTM) deep learning model, integrated with high-resolution ERA5 remote sensing data, to improve soil moisture estimation at the field scale. Soil moisture dynamics were analyzed across six commercial potato production sites in Quebec—Goulet, DBolduc, PBolduc, BNiquet, Lalancette, and Gou-new—over a five-year period. The model exhibited high predictive accuracy, with correlation coefficients (R) ranging from 0.991 to 0.998 and Nash–Sutcliffe efficiency (NSE) values reaching 0.996, indicating strong agreement between observed and predicted soil moisture variability. The Willmott index (WI) exceeded 0.995, reinforcing the model’s reliability. The integration of NDMI assessments further validated the predictions, demonstrating a strong correlation between NDMI values and LSTM-based soil moisture estimates. These findings confirm the effectiveness of deep learning in capturing spatiotemporal variations in soil moisture, underscoring the potential of AI-driven models for real-time soil moisture monitoring and irrigation optimization. This research study provides a scientifically robust framework for enhancing data-driven agricultural water management, promoting sustainable irrigation practices, and improving resilience to soil moisture variability in agricultural systems. © 2025 by the authors.;"agricultural water management; drought monitoring; LSTM deep learning; NDMI; precision agriculture; remote sensing; soil moisture modeling";"Canada; Quebec [Canada]; alternative agriculture; crop plant; crop production; farming system; precision agriculture; remote sensing; satellite data; soil moisture; spatiotemporal analysis; water management";"NDMI-Derived Field-Scale Soil Moisture Prediction Using ERA5 and LSTM for Precision Agriculture Accurate soil moisture prediction is fundamental to precision agriculture, facilitating optimal irrigation scheduling, efficient water resource allocation, and enhanced crop productivity. This study employs a Long Short-Term Memory (LSTM) deep learning model, integrated with high-resolution ERA5 remote sensing data, to improve soil moisture estimation at the field scale. Soil moisture dynamics were analyzed across six commercial potato production sites in Quebec—Goulet, DBolduc, PBolduc, BNiquet, Lalancette, and Gou-new—over a five-year period. The model exhibited high predictive accuracy, with correlation coefficients (R) ranging from 0.991 to 0.998 and Nash–Sutcliffe efficiency (NSE) values reaching 0.996, indicating strong agreement between observed and predicted soil moisture variability. The Willmott index (WI) exceeded 0.995, reinforcing the model’s reliability. The integration of NDMI assessments further validated the predictions, demonstrating a strong correlation between NDMI values and LSTM-based soil moisture estimates. These findings confirm the effectiveness of deep learning in capturing spatiotemporal variations in soil moisture, underscoring the potential of AI-driven models for real-time soil moisture monitoring and irrigation optimization. This research study provides a scientifically robust framework for enhancing data-driven agricultural water management, promoting sustainable irrigation practices, and improving resilience to soil moisture variability in agricultural systems. © 2025 by the authors. agricultural water management; drought monitoring; LSTM deep learning; NDMI; precision agriculture; remote sensing; soil moisture modeling Canada; Quebec [Canada]; alternative agriculture; crop plant; crop production; farming system; precision agriculture; remote sensing; satellite data; soil moisture; spatiotemporal analysis; water management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
89;Attention-enhanced deep learning model for reconstruction and downscaling of thermocline depth in the tropical Indian Ocean;Accurate estimation of high-resolution thermocline depth is important for investigating ocean processes and climate variability on multiple scales. Due to the sparse coverage and high costs associated with in situ observations, reconstructing ocean interior structure from sea surface data serves as a valuable alternative. In this study, a new deep learning model named Enhanced Block Attention Module-Convolutional Neural Network (EBAM-CNN) was proposed to reconstruct thermocline depth in the tropical Indian Ocean (TIO) from 1993 to 2022. Absolute dynamic topography (ADT), sea surface temperature (SST), and sea surface wind (SSW), along with geographic information (latitude and longitude) and temporal data, were employed as input variables. In comparison with the traditional convolutional neural network (CNN) model, the proposed model demonstrates better performance, with an overall Root Mean Square Error (RMSE) of 5.29 m and a Pearson Correlation Coefficient (R) of 0.87. In addition, this study employs a downscaling approach to reconstruct higher-resolution thermocline depth data. An analysis of the downscaling results confirmed that the proposed framework effectively reconstructed mesoscale sea subsurface features from high-resolution surface observations, significantly enhancing thermocline depth estimates and providing robust data support for oceanic and climatic research. © 2025 The Author(s);"Deep learning; Downscaling; Satellite observations; Thermocline depth; Tropical Indian Ocean";"Indian Ocean; Indian Ocean (Tropical); Deep reinforcement learning; Submarine geophysics; Tropical cyclone; Accurate estimation; Convolutional neural network; Deep learning; Down-scaling; High resolution; Learning models; Satellite observations; Sea surfaces; Thermocline depth; Tropical Indian ocean; artificial neural network; data set; downscaling; estimation method; machine learning; satellite imagery; thermocline; Convolutional neural networks";"Attention-enhanced deep learning model for reconstruction and downscaling of thermocline depth in the tropical Indian Ocean Accurate estimation of high-resolution thermocline depth is important for investigating ocean processes and climate variability on multiple scales. Due to the sparse coverage and high costs associated with in situ observations, reconstructing ocean interior structure from sea surface data serves as a valuable alternative. In this study, a new deep learning model named Enhanced Block Attention Module-Convolutional Neural Network (EBAM-CNN) was proposed to reconstruct thermocline depth in the tropical Indian Ocean (TIO) from 1993 to 2022. Absolute dynamic topography (ADT), sea surface temperature (SST), and sea surface wind (SSW), along with geographic information (latitude and longitude) and temporal data, were employed as input variables. In comparison with the traditional convolutional neural network (CNN) model, the proposed model demonstrates better performance, with an overall Root Mean Square Error (RMSE) of 5.29 m and a Pearson Correlation Coefficient (R) of 0.87. In addition, this study employs a downscaling approach to reconstruct higher-resolution thermocline depth data. An analysis of the downscaling results confirmed that the proposed framework effectively reconstructed mesoscale sea subsurface features from high-resolution surface observations, significantly enhancing thermocline depth estimates and providing robust data support for oceanic and climatic research. © 2025 The Author(s) Deep learning; Downscaling; Satellite observations; Thermocline depth; Tropical Indian Ocean Indian Ocean; Indian Ocean (Tropical); Deep reinforcement learning; Submarine geophysics; Tropical cyclone; Accurate estimation; Convolutional neural network; Deep learning; Down-scaling; High resolution; Learning models; Satellite observations; Sea surfaces; Thermocline depth; Tropical Indian ocean; artificial neural network; data set; downscaling; estimation method; machine learning; satellite imagery; thermocline; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
90;Investigating the Latency of Lightning-Caused Fires in Boreal Coniferous Forests Using Random Forest Methodology;This study investigates the latency of lightning-caused fires in the boreal coniferous forests of the Greater Khingan Mountains, employing advanced machine learning techniques to analyze the relationship between meteorological factors, lightning characteristics, and fire ignition and smoldering processes. Using the Random Forest Model (RFM) combined with Recursive Feature Elimination with Cross-Validation (RFECV) and SHapley Additive exPlanations (SHAP), the study identifies key factors influencing fire latency. Two methods, Min distance and Min latency, were used to determine ignition lightning, with the Min distance method proving more reliable. The results show that lightning-caused fires cluster spatially and peak temporally between May and July, aligning with lightning activity. The Fine Fuel Moisture Code (FFMC) and precipitation were identified as the most influential factors. This study underscores the importance of fuel moisture and weather conditions in determining latency of lightning-caused fire, offering valuable insights for enhancing early warning systems. Despite limitations in data resolution and the exclusion of topographic factors, this study advances our understanding of lightning-fire latency mechanisms and provides a foundation for more effective wildfire management strategies under climate change. © 2025 by the authors.;"ignition lightning; latency; lightning-caused fire; Random Forest Model; RFECV; SHAP";NULL;"Investigating the Latency of Lightning-Caused Fires in Boreal Coniferous Forests Using Random Forest Methodology This study investigates the latency of lightning-caused fires in the boreal coniferous forests of the Greater Khingan Mountains, employing advanced machine learning techniques to analyze the relationship between meteorological factors, lightning characteristics, and fire ignition and smoldering processes. Using the Random Forest Model (RFM) combined with Recursive Feature Elimination with Cross-Validation (RFECV) and SHapley Additive exPlanations (SHAP), the study identifies key factors influencing fire latency. Two methods, Min distance and Min latency, were used to determine ignition lightning, with the Min distance method proving more reliable. The results show that lightning-caused fires cluster spatially and peak temporally between May and July, aligning with lightning activity. The Fine Fuel Moisture Code (FFMC) and precipitation were identified as the most influential factors. This study underscores the importance of fuel moisture and weather conditions in determining latency of lightning-caused fire, offering valuable insights for enhancing early warning systems. Despite limitations in data resolution and the exclusion of topographic factors, this study advances our understanding of lightning-fire latency mechanisms and provides a foundation for more effective wildfire management strategies under climate change. © 2025 by the authors. ignition lightning; latency; lightning-caused fire; Random Forest Model; RFECV; SHAP NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
91;Landslide susceptibility assessment for the Darjeeling Toy Train route: a GIS and machine learning approach;Landslide susceptibility mapping is crucial for reducing risks in culturally and historically significant areas like the Darjeeling Toy Train route, a UNESCO World Heritage site. In this study, the risk of landslides along this road is evaluated using Geographic Information System (GIS) tools and advanced machine learning models, such as Support Vector Machine (SVM), Gradient Boosting Machine (GBM), Logistic Regression, and Classification and Regression Trees (CART). It uses a set of 512 landslide and non-landslide sites, with a 70:30 split between training and testing. Within the research area, thirteen topographical, hydrological, and geological factors linked to landslides are shown as GIS layers to make maps of landslide susceptibility (LSM). The study area particularly vulnerable to various types of landslides, including debris slides, rock falls, and soil slips. ROC–AUC results show that the SVM model did the best (0.813), followed by GBM (0.807), Logistic Regression (0.797), and CART (0.781). SVM had the highest accuracy rate at 83.2%, followed by GBM at 81.5% and LR at 80.3%. CART had the lowest overall accuracy rate at 78.6%. Furthermore, confusion matrix analysis showed that SVM and Logistic Regression were better at finding actual landslide-prone areas, with 84.6% and 82.1% recall rates, respectively. This made them more accurate in predicting high-risk areas. Susceptibility levels were categorized, revealing high-risk areas like Darjeeling and Rishihat and safer areas like Kurseong and Mohanbari. For lowering the risk of landslides and protecting this historic route, these results are very useful for land management and disaster preparation. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.;"CART; Darjeeling Toy Train; Disaster preparedness; Gradient boosting machine (GBM); Landslide-prone areas; Logistic regression; Support vector machine (SVM)";"Darjeeling; India; West Bengal; Adaptive boosting; Adversarial machine learning; Highway administration; Landslides; Railroad accidents; Risk assessment; Soil testing; Support vector regression; Transportation routes; Trees (mathematics); Classification trees; Darjeeling toy train; Disaster preparedness; Gradient boosting; Gradient boosting machine; Landslide-prone areas; Logistics regressions; Regression trees; Support vector machine; Support vectors machine; debris flow; disaster management; GIS; landslide; machine learning; mapping; regression analysis; rockfall; support vector machine; Logistic regression";"Landslide susceptibility assessment for the Darjeeling Toy Train route: a GIS and machine learning approach Landslide susceptibility mapping is crucial for reducing risks in culturally and historically significant areas like the Darjeeling Toy Train route, a UNESCO World Heritage site. In this study, the risk of landslides along this road is evaluated using Geographic Information System (GIS) tools and advanced machine learning models, such as Support Vector Machine (SVM), Gradient Boosting Machine (GBM), Logistic Regression, and Classification and Regression Trees (CART). It uses a set of 512 landslide and non-landslide sites, with a 70:30 split between training and testing. Within the research area, thirteen topographical, hydrological, and geological factors linked to landslides are shown as GIS layers to make maps of landslide susceptibility (LSM). The study area particularly vulnerable to various types of landslides, including debris slides, rock falls, and soil slips. ROC–AUC results show that the SVM model did the best (0.813), followed by GBM (0.807), Logistic Regression (0.797), and CART (0.781). SVM had the highest accuracy rate at 83.2%, followed by GBM at 81.5% and LR at 80.3%. CART had the lowest overall accuracy rate at 78.6%. Furthermore, confusion matrix analysis showed that SVM and Logistic Regression were better at finding actual landslide-prone areas, with 84.6% and 82.1% recall rates, respectively. This made them more accurate in predicting high-risk areas. Susceptibility levels were categorized, revealing high-risk areas like Darjeeling and Rishihat and safer areas like Kurseong and Mohanbari. For lowering the risk of landslides and protecting this historic route, these results are very useful for land management and disaster preparation. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024. CART; Darjeeling Toy Train; Disaster preparedness; Gradient boosting machine (GBM); Landslide-prone areas; Logistic regression; Support vector machine (SVM) Darjeeling; India; West Bengal; Adaptive boosting; Adversarial machine learning; Highway administration; Landslides; Railroad accidents; Risk assessment; Soil testing; Support vector regression; Transportation routes; Trees (mathematics); Classification trees; Darjeeling toy train; Disaster preparedness; Gradient boosting; Gradient boosting machine; Landslide-prone areas; Logistics regressions; Regression trees; Support vector machine; Support vectors machine; debris flow; disaster management; GIS; landslide; machine learning; mapping; regression analysis; rockfall; support vector machine; Logistic regression";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
92;Empirical and machine learning-based approaches to identify rainfall thresholds for landslide prediction: a case study of Kerala, India;"Kerala, a state in India, experiences one of the highest incidences of rainfall-induced landslides. Historical data has been collected and analyzed to devise thresholds for the early detection of landslides. Two empirical approaches based on the relationships between rainfall intensity and duration, as well as cumulative rainfall and duration, have been utilized to identify early warning thresholds for landslides. Five machine learning-based approaches were employed to determine these thresholds. Among the classifiers tested, the K-Nearest Neighbour (KNN) classifier with K=5 demonstrated the highest prediction accuracy compared to other methods in the study. © The Author(s) 2025.; For the safe and resilient development of cities, disaster risk reduction plays a crucial role, aligning with sustainable development goal 11 of the United Nations. Supporting this objective, the present study developed a machine learning (ML) classifier-based threshold model to determine rainfall thresholds for predicting impending landslides in Kerala, India, using historical data. Using a dataset of 64 rainfall-induced landslide events recorded since the year 2000, rainfall data were collected up to 15 days prior to each landslide to support empirical analysis of intensity-duration and event rainfall-duration thresholds. In cases where exact rainfall durations were unavailable, classification machine learning (ML) models, including K-nearest neighbours (KNN), random forest (RF), gradient boosting machine (GBM), support vector machine (SVM), and logistic regression, were used to determine threshold reliability. Among these, the KNN model with 5 Neighbours achieved the highest performance, with an ROC-AUC of 0.9 and an accuracy of 82%. This model, saved as a pickle file, serves as a core filter in the development of a landslide early warning system. This paper presents the model development and performance comparisons, contributing to a practical, community-centred solution for landslide disaster resilience in Kerala. © The Author(s) 2025.";"Empirical models; Machine Learning; Rainfall threshold; Rainfall-induced landslide";"Contrastive Learning; Sustainable development; Case-studies; Empirical model; High incidence; Historical data; Landslide prediction; Learning-based approach; Machine-learning; Rainfall duration; Rainfall induced landslides; Rainfall thresholds; Adversarial machine learning";"Empirical and machine learning-based approaches to identify rainfall thresholds for landslide prediction: a case study of Kerala, India Kerala, a state in India, experiences one of the highest incidences of rainfall-induced landslides. Historical data has been collected and analyzed to devise thresholds for the early detection of landslides. Two empirical approaches based on the relationships between rainfall intensity and duration, as well as cumulative rainfall and duration, have been utilized to identify early warning thresholds for landslides. Five machine learning-based approaches were employed to determine these thresholds. Among the classifiers tested, the K-Nearest Neighbour (KNN) classifier with K=5 demonstrated the highest prediction accuracy compared to other methods in the study. © The Author(s) 2025.; For the safe and resilient development of cities, disaster risk reduction plays a crucial role, aligning with sustainable development goal 11 of the United Nations. Supporting this objective, the present study developed a machine learning (ML) classifier-based threshold model to determine rainfall thresholds for predicting impending landslides in Kerala, India, using historical data. Using a dataset of 64 rainfall-induced landslide events recorded since the year 2000, rainfall data were collected up to 15 days prior to each landslide to support empirical analysis of intensity-duration and event rainfall-duration thresholds. In cases where exact rainfall durations were unavailable, classification machine learning (ML) models, including K-nearest neighbours (KNN), random forest (RF), gradient boosting machine (GBM), support vector machine (SVM), and logistic regression, were used to determine threshold reliability. Among these, the KNN model with 5 Neighbours achieved the highest performance, with an ROC-AUC of 0.9 and an accuracy of 82%. This model, saved as a pickle file, serves as a core filter in the development of a landslide early warning system. This paper presents the model development and performance comparisons, contributing to a practical, community-centred solution for landslide disaster resilience in Kerala. © The Author(s) 2025. Empirical models; Machine Learning; Rainfall threshold; Rainfall-induced landslide Contrastive Learning; Sustainable development; Case-studies; Empirical model; High incidence; Historical data; Landslide prediction; Learning-based approach; Machine-learning; Rainfall duration; Rainfall induced landslides; Rainfall thresholds; Adversarial machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
93;Post-tornado automated building damage evaluation and recovery prediction by integrating remote sensing, deep learning, and restoration models;This study introduces a novel methodology that integrates remote sensing, deep learning, and restoration models to streamline building damage assessment and recovery time predictions following tornado events. In contrast to existing research primarily focused on pre-hazard mitigation and preparedness, this study advances the field by extending the application of engineering models to the post-hazard emergency response and recovery phase. The novelty lies in utilizing remote sensing and deep learning to automate the generation of large-scale maps for tornado damage. Then, building damage evaluation is integrated with restoration models for rapid estimations of post-disaster restoration time and cost. Through a comprehensive application study focused on the 2011 Joplin Tornado, the methodology is demonstrated to be fully automated. The predictions were validated against historical reports, highlighting the methodology's effectiveness in generating accurate damage evaluation and restoration predictions. This study stands out as the first to leverage remote sensing imagery-based damage evaluation to extend the utility of regional risk assessment beyond pre-tornado planning, thus enhancing post-tornado disaster response and recovery efforts. © 2025 Elsevier Ltd;"Building damage evaluation; Deep learning; Post-disaster recovery; Remote sensing; Restoration repair time and cost; Tornado damage assessment";"Damage detection; Disasters; Hazards; Restoration; Building damage; Building damage evaluation; Damage assessments; Damage evaluation; Deep learning; Disaster recovery; Post disasters; Post-disaster recovery; Remote-sensing; Repair costs; Repair time; Restoration repair time and cost; Tornado damage; Tornado damage assessment; building; disaster management; environmental restoration; machine learning; prediction; remote sensing; repair; storm damage; tornado; Risk assessment";"Post-tornado automated building damage evaluation and recovery prediction by integrating remote sensing, deep learning, and restoration models This study introduces a novel methodology that integrates remote sensing, deep learning, and restoration models to streamline building damage assessment and recovery time predictions following tornado events. In contrast to existing research primarily focused on pre-hazard mitigation and preparedness, this study advances the field by extending the application of engineering models to the post-hazard emergency response and recovery phase. The novelty lies in utilizing remote sensing and deep learning to automate the generation of large-scale maps for tornado damage. Then, building damage evaluation is integrated with restoration models for rapid estimations of post-disaster restoration time and cost. Through a comprehensive application study focused on the 2011 Joplin Tornado, the methodology is demonstrated to be fully automated. The predictions were validated against historical reports, highlighting the methodology's effectiveness in generating accurate damage evaluation and restoration predictions. This study stands out as the first to leverage remote sensing imagery-based damage evaluation to extend the utility of regional risk assessment beyond pre-tornado planning, thus enhancing post-tornado disaster response and recovery efforts. © 2025 Elsevier Ltd Building damage evaluation; Deep learning; Post-disaster recovery; Remote sensing; Restoration repair time and cost; Tornado damage assessment Damage detection; Disasters; Hazards; Restoration; Building damage; Building damage evaluation; Damage assessments; Damage evaluation; Deep learning; Disaster recovery; Post disasters; Post-disaster recovery; Remote-sensing; Repair costs; Repair time; Restoration repair time and cost; Tornado damage; Tornado damage assessment; building; disaster management; environmental restoration; machine learning; prediction; remote sensing; repair; storm damage; tornado; Risk assessment";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;4;Recovery
94;Fast and Accurate Evacuation Planning Algorithm with Bayesian Optimization;In this work, we propose a method for generating an evacuation plan at a high speed to realize safe and swift evacuation in the event of a large-scale disaster such as an earthquake and its accompanying tsunami. Existing conventional methods have several problems. Simulation-based methods that use agents and methods that use existing time expansion networks have high computational costs, which makes it difficult for evacuation routes to be immediately changed according to the effects of disasters such as collapsed buildings and roads. Although heuristics with reduced calculation costs are also being researched, they may result in very long evacuation completion times and cannot generate optimal evacuation plans. We guarantee the optimal solution by reducing the number of maximum flow problem calculations, which constitute the bottleneck for methods using the existing time expansion network, through the use of the Bayesian optimization machine learning method. This reduces the calculation cost of the entire algorithm. The performance of our method is evaluated from the two viewpoints of the evacuation completion time, which indicates the quality of the evacuation plan, and the time required for the generation by the solution of the algorithm in computer experiments under multiple scenarios. In addition, the impact of the number of evacuees and the locations of the sinks are analyzed. We show that our method can quickly generate an optimal evacuation plan. © 2025 Copyright held by the owner/author(s). ;"Bayesian optimization; Evacuation plan; Network flow";"Adaptive boosting; Machine learning; Bayesian optimization; Calculation cost; Completion time; Conventional methods; Evacuation planning; Evacuation plans; High Speed; Large scale disasters; Networks flows; Planning algorithms; Disasters";"Fast and Accurate Evacuation Planning Algorithm with Bayesian Optimization In this work, we propose a method for generating an evacuation plan at a high speed to realize safe and swift evacuation in the event of a large-scale disaster such as an earthquake and its accompanying tsunami. Existing conventional methods have several problems. Simulation-based methods that use agents and methods that use existing time expansion networks have high computational costs, which makes it difficult for evacuation routes to be immediately changed according to the effects of disasters such as collapsed buildings and roads. Although heuristics with reduced calculation costs are also being researched, they may result in very long evacuation completion times and cannot generate optimal evacuation plans. We guarantee the optimal solution by reducing the number of maximum flow problem calculations, which constitute the bottleneck for methods using the existing time expansion network, through the use of the Bayesian optimization machine learning method. This reduces the calculation cost of the entire algorithm. The performance of our method is evaluated from the two viewpoints of the evacuation completion time, which indicates the quality of the evacuation plan, and the time required for the generation by the solution of the algorithm in computer experiments under multiple scenarios. In addition, the impact of the number of evacuees and the locations of the sinks are analyzed. We show that our method can quickly generate an optimal evacuation plan. © 2025 Copyright held by the owner/author(s).  Bayesian optimization; Evacuation plan; Network flow Adaptive boosting; Machine learning; Bayesian optimization; Calculation cost; Completion time; Conventional methods; Evacuation planning; Evacuation plans; High Speed; Large scale disasters; Networks flows; Planning algorithms; Disasters";-1;Não Classificado;NULL;1.1;Geological;2;Preparation
95;DFTQuake: Tripartite Fourier attention and dendrite network for real-time early prediction of earthquake magnitude and peak ground acceleration;Earthquakes are extremely destructive natural disasters, causing ground shaking, tsunamis, fissures, and landslides that result in loss of life. Early prediction of earthquake magnitude and peak ground acceleration (PGA) at a specific station using real-time recording can offer vital lead time, which can be utilized to save lives. In this paper, a novel deep-learning model, namely, Dendrite Fourier Transform Earthquake prediction (DFTQuake), has been designed specifically for earthquake magnitude and PGA prediction. The proposed model highlights the tripartite Fourier transform attention mechanism and dendrite connection. Fourier transform in feature maps is applied in the model to obtain periodic information, and the conventionally utilized fully connected layer at the end of the deep learning pipeline is replaced by a dendrite neural network. The DFTQuake model can predict the magnitude of an earthquake by utilizing the early three seconds of the primary waveform. A large number of strong motion records with train, test, and validation datasets of 17081, 6438, and 2760 records have been selected for this work. DFTQuake model outperforms the existing state-of-the-art artificial intelligence models by 13.16% and 34.21%, with a mean absolute error of 0.66 in Japan meteorological agency unit and 0.25 gal for early magnitude and PGA prediction. The model's effectiveness is demonstrated through real-time predictions of magnitude and PGA at various recording sites for seven distinct earthquake events, including the damaging 2024 Noto earthquake. The model's efficacy is further established by testing the model in India and nearby countries earthquake data. © 2025;"Dendrite neural network; Fourier attention; Magnitude; Onsite earthquake early warning; Peak ground acceleration; Single station";"Deep neural networks; Earthquake effects; Fourier transforms; Multilayer neural networks; Dendrite neural network; Earthquake early warning; Earthquake prediction; Fourier; Fourier attention; Magnitude; Neural-networks; Onsite earthquake early warning; Peak ground acceleration; Single station; Disasters";"DFTQuake: Tripartite Fourier attention and dendrite network for real-time early prediction of earthquake magnitude and peak ground acceleration Earthquakes are extremely destructive natural disasters, causing ground shaking, tsunamis, fissures, and landslides that result in loss of life. Early prediction of earthquake magnitude and peak ground acceleration (PGA) at a specific station using real-time recording can offer vital lead time, which can be utilized to save lives. In this paper, a novel deep-learning model, namely, Dendrite Fourier Transform Earthquake prediction (DFTQuake), has been designed specifically for earthquake magnitude and PGA prediction. The proposed model highlights the tripartite Fourier transform attention mechanism and dendrite connection. Fourier transform in feature maps is applied in the model to obtain periodic information, and the conventionally utilized fully connected layer at the end of the deep learning pipeline is replaced by a dendrite neural network. The DFTQuake model can predict the magnitude of an earthquake by utilizing the early three seconds of the primary waveform. A large number of strong motion records with train, test, and validation datasets of 17081, 6438, and 2760 records have been selected for this work. DFTQuake model outperforms the existing state-of-the-art artificial intelligence models by 13.16% and 34.21%, with a mean absolute error of 0.66 in Japan meteorological agency unit and 0.25 gal for early magnitude and PGA prediction. The model's effectiveness is demonstrated through real-time predictions of magnitude and PGA at various recording sites for seven distinct earthquake events, including the damaging 2024 Noto earthquake. The model's efficacy is further established by testing the model in India and nearby countries earthquake data. © 2025 Dendrite neural network; Fourier attention; Magnitude; Onsite earthquake early warning; Peak ground acceleration; Single station Deep neural networks; Earthquake effects; Fourier transforms; Multilayer neural networks; Dendrite neural network; Earthquake early warning; Earthquake prediction; Fourier; Fourier attention; Magnitude; Neural-networks; Onsite earthquake early warning; Peak ground acceleration; Single station; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
96;Prediction of long-period ground motion responses for high-rise buildings using physics-assisted fully convolutional neural network;Far-field long-period ground motions (LPGMs) have a significant long-duration characteristic and can cause severe damages to high-rise buildings in the LPGM potential area. Rapid and accurate prediction of LPGM-induced responses is crucial for earthquake emergency management and loss assessment in large cities. However, dynamic-based methods for calculating structural responses involve intricate physical modeling process, resulting in low computational efficiency and failing to meet the prediction timeliness requirement. Furthermore, current data-driven methods cannot satisfy the prediction accuracy requirement due to the harsh formation condition causing scarcity and particularity of LPGM records. To this end, this paper develops a physics-assisted fully convolutional neural network (PhyFCN) for predicting LPGM-induced response of high-rise buildings. Central to this method lies in encoding the complex seismic motion equation into FCN for formulating an innovative physical loss function. The utilization of this function not only significantly enhances the prediction accuracy and robustness, but also remarkably reduces the dependence on the large number of training samples, i.e., a small number of training samples is sufficient for acquiring the desired parameters. This method integrates strengths of both dynamic-based methods and data-driven methods, enabling it to simultaneously fulfill requirements of prediction timeliness and accuracy. Numerical examples based on two different buildings are employed to verify the effectiveness and superiority of PhyFCN. Results suggest that the incorporation of the physics-assisted mechanism into FCN, even when only one training sample is available, can sharply increase the R value by 28.4 %, while those of MSE and MAE are decreased by 60.2 % and 37.6 %, respectively. © 2025 Elsevier Ltd;"Fully convolutional neural network; High-rise buildings; Long period ground motion; Physical loss function; Response prediction";"Earthquakes; Prediction models; Risk management; Tall buildings; Convolutional neural network; Data-driven methods; Fully convolutional neural network; High rise building; Long-period ground motions; Loss functions; Physical loss function; Prediction accuracy; Response prediction; Training sample; Convolutional neural networks";"Prediction of long-period ground motion responses for high-rise buildings using physics-assisted fully convolutional neural network Far-field long-period ground motions (LPGMs) have a significant long-duration characteristic and can cause severe damages to high-rise buildings in the LPGM potential area. Rapid and accurate prediction of LPGM-induced responses is crucial for earthquake emergency management and loss assessment in large cities. However, dynamic-based methods for calculating structural responses involve intricate physical modeling process, resulting in low computational efficiency and failing to meet the prediction timeliness requirement. Furthermore, current data-driven methods cannot satisfy the prediction accuracy requirement due to the harsh formation condition causing scarcity and particularity of LPGM records. To this end, this paper develops a physics-assisted fully convolutional neural network (PhyFCN) for predicting LPGM-induced response of high-rise buildings. Central to this method lies in encoding the complex seismic motion equation into FCN for formulating an innovative physical loss function. The utilization of this function not only significantly enhances the prediction accuracy and robustness, but also remarkably reduces the dependence on the large number of training samples, i.e., a small number of training samples is sufficient for acquiring the desired parameters. This method integrates strengths of both dynamic-based methods and data-driven methods, enabling it to simultaneously fulfill requirements of prediction timeliness and accuracy. Numerical examples based on two different buildings are employed to verify the effectiveness and superiority of PhyFCN. Results suggest that the incorporation of the physics-assisted mechanism into FCN, even when only one training sample is available, can sharply increase the R value by 28.4 %, while those of MSE and MAE are decreased by 60.2 % and 37.6 %, respectively. © 2025 Elsevier Ltd Fully convolutional neural network; High-rise buildings; Long period ground motion; Physical loss function; Response prediction Earthquakes; Prediction models; Risk management; Tall buildings; Convolutional neural network; Data-driven methods; Fully convolutional neural network; High rise building; Long-period ground motions; Loss functions; Physical loss function; Prediction accuracy; Response prediction; Training sample; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
97;Adversarial Robustness for Deep Learning-Based Wildfire Prediction Models;"Rapidly growing wildfires have recently devastated societal assets, exposing a critical need for early warning systems to expedite relief efforts. Smoke detection using camera-based Deep Neural Networks (DNNs) offers a promising solution for wildfire prediction. However, the rarity of smoke across time and space limits training data, raising model overfitting and bias concerns. Current DNNs, primarily Convolutional Neural Networks (CNNs) and transformers, complicate robustness evaluation due to architectural differences. To address these challenges, we introduce WARP (Wildfire Adversarial Robustness Procedure), the first model-agnostic framework for evaluating wildfire detection models’ adversarial robustness. WARP addresses inherent limitations in data diversity by generating adversarial examples through image-global and -local perturbations. Global and local attacks superimpose Gaussian noise and PNG patches onto image inputs, respectively; this suits both CNNs and transformers while generating realistic adversarial scenarios. Using WARP, we assessed real-time CNNs and Transformers, uncovering key vulnerabilities. At times, transformers exhibited over 70% precision degradation under global attacks, while both models generally struggled to differentiate cloud-like PNG patches from real smoke during local attacks. To enhance model robustness, we proposed four wildfire-oriented data augmentation techniques based on WARP’s methodology and results, which diversify smoke image data and improve model precision and robustness. These advancements represent a substantial step toward developing a reliable early wildfire warning system, which may be our first safeguard against wildfire destruction. © 2025 by the authors.";"adversarial robustness; computer vision; deep neural networks; noise; smoke; wildfire";NULL;"Adversarial Robustness for Deep Learning-Based Wildfire Prediction Models Rapidly growing wildfires have recently devastated societal assets, exposing a critical need for early warning systems to expedite relief efforts. Smoke detection using camera-based Deep Neural Networks (DNNs) offers a promising solution for wildfire prediction. However, the rarity of smoke across time and space limits training data, raising model overfitting and bias concerns. Current DNNs, primarily Convolutional Neural Networks (CNNs) and transformers, complicate robustness evaluation due to architectural differences. To address these challenges, we introduce WARP (Wildfire Adversarial Robustness Procedure), the first model-agnostic framework for evaluating wildfire detection models’ adversarial robustness. WARP addresses inherent limitations in data diversity by generating adversarial examples through image-global and -local perturbations. Global and local attacks superimpose Gaussian noise and PNG patches onto image inputs, respectively; this suits both CNNs and transformers while generating realistic adversarial scenarios. Using WARP, we assessed real-time CNNs and Transformers, uncovering key vulnerabilities. At times, transformers exhibited over 70% precision degradation under global attacks, while both models generally struggled to differentiate cloud-like PNG patches from real smoke during local attacks. To enhance model robustness, we proposed four wildfire-oriented data augmentation techniques based on WARP’s methodology and results, which diversify smoke image data and improve model precision and robustness. These advancements represent a substantial step toward developing a reliable early wildfire warning system, which may be our first safeguard against wildfire destruction. © 2025 by the authors. adversarial robustness; computer vision; deep neural networks; noise; smoke; wildfire NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
98;Hybrid CatBoost and SVR Model for Earthquake Prediction Using the LANL Earthquake Dataset;Earthquakes have the potential to cause catastrophic structural and economic damage. This research explores the application of machine learning for earthquake prediction using LANL (Los Alamos National Laboratory) dataset. The data, obtained from a laboratory stick-slip friction experiment, simulate real earthquakes through digitized acoustic signals recorded against the time to failure of a granular layer. We introduced a hybrid model combining CatBoost and Support Vector Regression (SVR) to predict the time of the next earthquake, evaluating its performance against individual CatBoost and SVR models. The hybrid model demonstrated superior accuracy with a Mean Absolute Error (MAE) of 0.0825, outperforming the individual models. We implemented feature engineering to optimize the predictive capability of the models. Additionally, we compared our hybrid model's performance with previous studies to validate its efficacy. Our findings underscore the potential of machine learning, particularly hybrid models, in enhancing earthquake prediction accuracy. This study highlights the robustness and effectiveness of the hybrid CatBoost-SVR model, paving the way for advanced AI algorithms in seismology and contributing to improved disaster preparedness and mitigation strategies. © 2025 Slovene Society Informatika. All rights reserved.;"earthquake prediction; hybrid model; seismology; time to failure";"Disaster prevention; Disasters; Earthquake effects; Prediction models; Support vector regression; Earthquake prediction; Economic damages; Friction experiments; Hybrid model; Los Alamos National Laboratory; Machine-learning; Stick-slip friction; Structural damages; Support vector regression models; Time to failure; Stick-slip";"Hybrid CatBoost and SVR Model for Earthquake Prediction Using the LANL Earthquake Dataset Earthquakes have the potential to cause catastrophic structural and economic damage. This research explores the application of machine learning for earthquake prediction using LANL (Los Alamos National Laboratory) dataset. The data, obtained from a laboratory stick-slip friction experiment, simulate real earthquakes through digitized acoustic signals recorded against the time to failure of a granular layer. We introduced a hybrid model combining CatBoost and Support Vector Regression (SVR) to predict the time of the next earthquake, evaluating its performance against individual CatBoost and SVR models. The hybrid model demonstrated superior accuracy with a Mean Absolute Error (MAE) of 0.0825, outperforming the individual models. We implemented feature engineering to optimize the predictive capability of the models. Additionally, we compared our hybrid model's performance with previous studies to validate its efficacy. Our findings underscore the potential of machine learning, particularly hybrid models, in enhancing earthquake prediction accuracy. This study highlights the robustness and effectiveness of the hybrid CatBoost-SVR model, paving the way for advanced AI algorithms in seismology and contributing to improved disaster preparedness and mitigation strategies. © 2025 Slovene Society Informatika. All rights reserved. earthquake prediction; hybrid model; seismology; time to failure Disaster prevention; Disasters; Earthquake effects; Prediction models; Support vector regression; Earthquake prediction; Economic damages; Friction experiments; Hybrid model; Los Alamos National Laboratory; Machine-learning; Stick-slip friction; Structural damages; Support vector regression models; Time to failure; Stick-slip";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
99;Computer vision-aided damage assessment of corrugated steel plate shear links under earthquake excitations;The rapid assessment of damage states in structural components is a critical step in disaster evaluation and post-earthquake reconstruction. This study proposes a novel computer vision-aided deep learning model to evaluate the damage states of corrugated steel plate (CSP) shear links under earthquake excitations. A high-fidelity continuum finite element model of CSP shear links is developed and validated against physical test results. A comprehensive virtual image dataset of CSP shear links, comprising 46,860 images, is generated based on the validated finite element model through a parametric study. The modified Park-Ang model is employed to quantitatively calculate the damage index of CSP shear links under cyclic loads. The damage states of the CSP shear links are classified into three stages based on the characteristic points of the skeleton curves. The GoogLeNet deep learning network is trained to predict the damage states of CSP shear links, mapping the visual features of buckling waves to the corresponding damage classification labels. The results demonstrate that the proposed model can accurately classify damage states for CSP shear links while providing probability estimates for the identified states. The deep learning model was utilized to classify damaged images of CSP shear links subjected to both quasi-static and dynamic loads, achieving good accuracy. These findings indicate that the vision-based model is effective for classifying the damage states of CSP shear links under post-earthquake scenarios. © 2025 Elsevier Ltd;"Computer vision; Corrugated steel plate shear link; Damage assessment; Damage quantification; Deep learning";"Corrugated steel; Corrugated steel plate shear link; Damage assessments; Damage quantification; Damage state; Deep learning; Earthquake excitation; Learning models; Shear link; Steel plates; computer vision; earthquake damage; finite element method; machine learning; numerical model; steel structure";"Computer vision-aided damage assessment of corrugated steel plate shear links under earthquake excitations The rapid assessment of damage states in structural components is a critical step in disaster evaluation and post-earthquake reconstruction. This study proposes a novel computer vision-aided deep learning model to evaluate the damage states of corrugated steel plate (CSP) shear links under earthquake excitations. A high-fidelity continuum finite element model of CSP shear links is developed and validated against physical test results. A comprehensive virtual image dataset of CSP shear links, comprising 46,860 images, is generated based on the validated finite element model through a parametric study. The modified Park-Ang model is employed to quantitatively calculate the damage index of CSP shear links under cyclic loads. The damage states of the CSP shear links are classified into three stages based on the characteristic points of the skeleton curves. The GoogLeNet deep learning network is trained to predict the damage states of CSP shear links, mapping the visual features of buckling waves to the corresponding damage classification labels. The results demonstrate that the proposed model can accurately classify damage states for CSP shear links while providing probability estimates for the identified states. The deep learning model was utilized to classify damaged images of CSP shear links subjected to both quasi-static and dynamic loads, achieving good accuracy. These findings indicate that the vision-based model is effective for classifying the damage states of CSP shear links under post-earthquake scenarios. © 2025 Elsevier Ltd Computer vision; Corrugated steel plate shear link; Damage assessment; Damage quantification; Deep learning Corrugated steel; Corrugated steel plate shear link; Damage assessments; Damage quantification; Damage state; Deep learning; Earthquake excitation; Learning models; Shear link; Steel plates; computer vision; earthquake damage; finite element method; machine learning; numerical model; steel structure";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
100;Landslide early warning based on retaining wall damage monitoring by real-time video;The frequency and severity of landslides is increasing, resulting in significant damage to people and infrastructure. Landslide early warning systems (LEWS) are becoming a key in damage mitigation of landslides. This study proposed a framework for building an early warning system of cut-slope areas based on monitoring damage of retaining wall using real-time video. Assessment of slope stability relies on observation of damage conditions such as cracks or displacements in the retaining wall. The identification of displacements and cracks in the retaining wall will be detected by a digital camera system integrated with deep learning and image processing techniques within calculation procedures. The performance of the proposed system is assessed using the lab experiments. The accuracies of displacement measurement ranged from 84.0 % to 99.4 %. While deep learning model achieved mean Average Precision values ranging from 0.86 to 0.90, and F1 score values, as the harmonic mean of precision and recall of the deep learning models, belong to the range of 0.83 and 0.85 in identifying cracks, and the dimensions of cracks were determined with the accuracies between 85.0 and 98.8 %. The correlation between retaining wall damage and slope stability is further investigated using numerical simulations. Subsequently, establishing threshold values ​​for both the displacement and the width of cracks in the retaining wall, which enables an early prediction of the occurrence of landslides. © 2024 The Author(s);"Crack; Displacement; Landslide early warning system; Retaining wall";"Displacement measurement; Retaining walls; Walls (structural partitions); Area-based; Cut slope; Damage monitoring; Displacement; Early warning; Early Warning System; Landslide early warning system; Learning models; Monitoring damage; Real time videos; Landslides";"Landslide early warning based on retaining wall damage monitoring by real-time video The frequency and severity of landslides is increasing, resulting in significant damage to people and infrastructure. Landslide early warning systems (LEWS) are becoming a key in damage mitigation of landslides. This study proposed a framework for building an early warning system of cut-slope areas based on monitoring damage of retaining wall using real-time video. Assessment of slope stability relies on observation of damage conditions such as cracks or displacements in the retaining wall. The identification of displacements and cracks in the retaining wall will be detected by a digital camera system integrated with deep learning and image processing techniques within calculation procedures. The performance of the proposed system is assessed using the lab experiments. The accuracies of displacement measurement ranged from 84.0 % to 99.4 %. While deep learning model achieved mean Average Precision values ranging from 0.86 to 0.90, and F1 score values, as the harmonic mean of precision and recall of the deep learning models, belong to the range of 0.83 and 0.85 in identifying cracks, and the dimensions of cracks were determined with the accuracies between 85.0 and 98.8 %. The correlation between retaining wall damage and slope stability is further investigated using numerical simulations. Subsequently, establishing threshold values ​​for both the displacement and the width of cracks in the retaining wall, which enables an early prediction of the occurrence of landslides. © 2024 The Author(s) Crack; Displacement; Landslide early warning system; Retaining wall Displacement measurement; Retaining walls; Walls (structural partitions); Area-based; Cut slope; Damage monitoring; Displacement; Early warning; Early Warning System; Landslide early warning system; Learning models; Monitoring damage; Real time videos; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
101;Flood Inundation Range Prediction Method Based on SRR-Informer;Flood forecasting methods based on deep learning rely on a large number of observational data, and are facing serious challenges in areas with scarce data. Aiming at the problems of flood inundated range prediction in areas with scarce data, this paper proposes a flood inundated range prediction method based on spatial reduction reconstruction (SRR) and improved deep attention neural network (Informer) to reduce the data requirements of the two core parts of water level simulation and spatial modeling. It alleviates the problem of insufficient observational data for flood inundation range prediction in areas with scarce data. This method obtains the dependency relationship of long time series data through Informer model, and inserts the built-in input selection layer to reduce the number of parameters in the model, reducing the data requirement of water level simulation. At the same time, the SRR algorithm is used to select the representative locations that are easy to be inundated in the basin, which reduces the number of locations required for spatial modeling of flood inundation range and the corresponding data requirements. The experimental results show that this method can improve the accuracy of flood inundation range prediction and speed up the efficiency of the model. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Data scarcity; Deep learning; Flood inundation range forecast";"Data accuracy; Data requirements; Data scarcity; Deep learning; Flood forecasting; Flood inundation range forecast; Observational data; Prediction methods; Spatial modelling; Spatial reduction; Water-level simulation; Prediction models";"Flood Inundation Range Prediction Method Based on SRR-Informer Flood forecasting methods based on deep learning rely on a large number of observational data, and are facing serious challenges in areas with scarce data. Aiming at the problems of flood inundated range prediction in areas with scarce data, this paper proposes a flood inundated range prediction method based on spatial reduction reconstruction (SRR) and improved deep attention neural network (Informer) to reduce the data requirements of the two core parts of water level simulation and spatial modeling. It alleviates the problem of insufficient observational data for flood inundation range prediction in areas with scarce data. This method obtains the dependency relationship of long time series data through Informer model, and inserts the built-in input selection layer to reduce the number of parameters in the model, reducing the data requirement of water level simulation. At the same time, the SRR algorithm is used to select the representative locations that are easy to be inundated in the basin, which reduces the number of locations required for spatial modeling of flood inundation range and the corresponding data requirements. The experimental results show that this method can improve the accuracy of flood inundation range prediction and speed up the efficiency of the model. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Data scarcity; Deep learning; Flood inundation range forecast Data accuracy; Data requirements; Data scarcity; Deep learning; Flood forecasting; Flood inundation range forecast; Observational data; Prediction methods; Spatial modelling; Spatial reduction; Water-level simulation; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
102;A novel framework for dynamic and quantitative mapping of damage severity due to compound Drought–Heatwave impacts on tea Plantations, integrating Sentinel-2 and UAV images;In 2022, China experienced a historically rare compound drought–heatwave (CDH) event, which had more severe impacts on vegetation compared with individual extreme events. However, quantitatively mapping the damage severity of CDH on tea tree using satellite data remains a significant challenge. Here we proposed a novel framework for dynamic and quantitative mapping of tea trees damage severity caused by CDH in 2022 using Sentinel-2 and Unmanned Aerial Vehicle (UAV) data. The Extreme Gradient Boosting (XGBoost) was selected as the optimal machine learning algorithm to extract tea plantations using Sentinel-2 data from XGBoost, Random Forest (RF), Logistic regression (LR), and Naive Bayes. The User's Accuracy and Producer's Accuracy for the extraction of tea plantations are 92.20 % and 93.51 %, respectively. UAV images with 2.5 cm spatial resolution were utilized to detect the tea trees damaged caused by the CDH in 2022. A new index, named the CDH damage severity index (CDH_DSI), was proposed to quantitatively evaluate the damage severity of CDH on tea trees at pixel level, with a spatial resolution of 10 m x 10 m. Based on the results of tea plantations and damaged tea trees detection, UAV-derived CDH_DSI was calculated and used as ground truth data. Then, The XGBoost was selected as the optimal CDH_DSI prediction model from XGBoost, RF, and LR with the Sentnel-2 derived vegetation indices and spectral reflectance as predictors. The coefficient of determination was 0.81 and root mean squared error was 7.61 %. Finally, dynamic and quantitative CDH_DSI maps were generated with the optimal CDH_DSI prediction model. The results show that 50 percent of tea plantations in Wuyi were damaged by the prolonged CDH event in 2022. These results can be attributed to precipitation deficits and heatwaves. Given that more severe CDH events are projected for the future, quantifying their impacts can provide decision-making support for disaster mitigation and prevention. © 2024 The Author(s);"CDH Damage Severity Index (CDH_DSI); Compound Drought-Heatwave (CDH); Sentinel-2; Tea; Unmanned Aerial Vehicle";"China; Adaptive boosting; Air cushion vehicles; Amphibious aircraft; Decision trees; Deforestation; Health risks; Logistic regression; Photomapping; Random forests; Resource allocation; Risk analysis; Risk assessment; Risk management; Vegetation mapping; Aerial vehicle; Compound drought-heatwave; Compound drought-heatwave damage severity index; Dynamic mapping; Heatwaves; Sentinel-2; Severity index; Tea plantations; Unmanned aerial vehicle; algorithm; machine learning; pixel; satellite data; Sentinel; spatial resolution; spectral reflectance; unmanned vehicle; vegetation type; Unmanned aerial vehicles (UAV)";"A novel framework for dynamic and quantitative mapping of damage severity due to compound Drought–Heatwave impacts on tea Plantations, integrating Sentinel-2 and UAV images In 2022, China experienced a historically rare compound drought–heatwave (CDH) event, which had more severe impacts on vegetation compared with individual extreme events. However, quantitatively mapping the damage severity of CDH on tea tree using satellite data remains a significant challenge. Here we proposed a novel framework for dynamic and quantitative mapping of tea trees damage severity caused by CDH in 2022 using Sentinel-2 and Unmanned Aerial Vehicle (UAV) data. The Extreme Gradient Boosting (XGBoost) was selected as the optimal machine learning algorithm to extract tea plantations using Sentinel-2 data from XGBoost, Random Forest (RF), Logistic regression (LR), and Naive Bayes. The User's Accuracy and Producer's Accuracy for the extraction of tea plantations are 92.20 % and 93.51 %, respectively. UAV images with 2.5 cm spatial resolution were utilized to detect the tea trees damaged caused by the CDH in 2022. A new index, named the CDH damage severity index (CDH_DSI), was proposed to quantitatively evaluate the damage severity of CDH on tea trees at pixel level, with a spatial resolution of 10 m x 10 m. Based on the results of tea plantations and damaged tea trees detection, UAV-derived CDH_DSI was calculated and used as ground truth data. Then, The XGBoost was selected as the optimal CDH_DSI prediction model from XGBoost, RF, and LR with the Sentnel-2 derived vegetation indices and spectral reflectance as predictors. The coefficient of determination was 0.81 and root mean squared error was 7.61 %. Finally, dynamic and quantitative CDH_DSI maps were generated with the optimal CDH_DSI prediction model. The results show that 50 percent of tea plantations in Wuyi were damaged by the prolonged CDH event in 2022. These results can be attributed to precipitation deficits and heatwaves. Given that more severe CDH events are projected for the future, quantifying their impacts can provide decision-making support for disaster mitigation and prevention. © 2024 The Author(s) CDH Damage Severity Index (CDH_DSI); Compound Drought-Heatwave (CDH); Sentinel-2; Tea; Unmanned Aerial Vehicle China; Adaptive boosting; Air cushion vehicles; Amphibious aircraft; Decision trees; Deforestation; Health risks; Logistic regression; Photomapping; Random forests; Resource allocation; Risk analysis; Risk assessment; Risk management; Vegetation mapping; Aerial vehicle; Compound drought-heatwave; Compound drought-heatwave damage severity index; Dynamic mapping; Heatwaves; Sentinel-2; Severity index; Tea plantations; Unmanned aerial vehicle; algorithm; machine learning; pixel; satellite data; Sentinel; spatial resolution; spectral reflectance; unmanned vehicle; vegetation type; Unmanned aerial vehicles (UAV)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
103;Deep Learning for Disaster Detection: A Framework for Automated Multimodal Data Classification;Natural disasters such as earthquakes, floods are becoming more frequent and severe due to climate change and urban expansion. The ability to quickly detect and assess the severity of such events is necessary requirements for timely disaster response and mitigation. Traditionally, disaster detection relied on isolated data sources like weather reports, seismic sensors, and ground reports. However, the availability of multi-modal data has opened up opportunities for more holistic monitoring systems in disasters. Traditional disaster detection methods remain limited by sheer volumes and complexity, making automated systems a necessity for real-time analysis. This project will explore the concepts of deep learning frameworks for automating classification and analysis of multimodal data for disaster scenarios. DL, deep learning, with powerful tools automatically examines and classifies such enormous heterogeneity in a vast data field. Therefore, from the perspective of DL algorithms and architectures, extraction of meaningful patterns from multimodal data becomes feasible towards rapid and correct disaster detection. © 2025 IEEE.;"Deep learning; Disaster detection; Machine studying; Multimodal data; Neural networks";"Adversarial machine learning; Earthquakes; Neural networks; Data classification; Deep learning; Disaster detection; Disaster-response; Machine studying; Multi-modal; Multimodal data; Natural disasters; Neural-networks; Urban expansion; Deep learning";"Deep Learning for Disaster Detection: A Framework for Automated Multimodal Data Classification Natural disasters such as earthquakes, floods are becoming more frequent and severe due to climate change and urban expansion. The ability to quickly detect and assess the severity of such events is necessary requirements for timely disaster response and mitigation. Traditionally, disaster detection relied on isolated data sources like weather reports, seismic sensors, and ground reports. However, the availability of multi-modal data has opened up opportunities for more holistic monitoring systems in disasters. Traditional disaster detection methods remain limited by sheer volumes and complexity, making automated systems a necessity for real-time analysis. This project will explore the concepts of deep learning frameworks for automating classification and analysis of multimodal data for disaster scenarios. DL, deep learning, with powerful tools automatically examines and classifies such enormous heterogeneity in a vast data field. Therefore, from the perspective of DL algorithms and architectures, extraction of meaningful patterns from multimodal data becomes feasible towards rapid and correct disaster detection. © 2025 IEEE. Deep learning; Disaster detection; Machine studying; Multimodal data; Neural networks Adversarial machine learning; Earthquakes; Neural networks; Data classification; Deep learning; Disaster detection; Disaster-response; Machine studying; Multi-modal; Multimodal data; Natural disasters; Neural-networks; Urban expansion; Deep learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
104;Real-Time Wildfire-Prone Area Monitoring and Early Warning System Based on Two-Stage YOLO-Based Smoke Detection Model;Wildfires present a severe risk to environmental sustainability, public health, and economic stability, especially in wildfire-prone areas, e.g., northern Thailand. This paper proposes a real-time wildfire-prone area monitoring and early warning system with a detection technique to address these challenges. First, we developed a web application that combines IoT cameras with a YOLOv5-based smoke detection model to monitor, detect, and notify users about potential wildfires. Second, we propose a two-stage smoke detection framework that leverages Gaussian filtering and a dual-stage YOLOv5 pipeline to reduce false predictions and improve detection accuracy. Using the FireSpot dataset comprising annotated images of early-stage wildfires, the system achieved a balanced accuracy of 98.62%, significantly surpassing baseline models with an improvement of 4.02%. Currently deployed in three municipalities in Chiang Mai, Thailand, the system has proven effective in providing timely alerts and enhancing wildfire management. By integrating IoT technology and machine learning, this study offers a scalable and practical solution for early-stage wildfire detection to mitigate environmental and human health impacts. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.;NULL;"Fire alarm systems; Smoke detectors; Area monitoring; Detection models; Early Warning System; Economic stability; Environmental sustainability; Northern Thailand; Public economics; Real- time; Smoke detection; WEB application; Premixed flames";"Real-Time Wildfire-Prone Area Monitoring and Early Warning System Based on Two-Stage YOLO-Based Smoke Detection Model Wildfires present a severe risk to environmental sustainability, public health, and economic stability, especially in wildfire-prone areas, e.g., northern Thailand. This paper proposes a real-time wildfire-prone area monitoring and early warning system with a detection technique to address these challenges. First, we developed a web application that combines IoT cameras with a YOLOv5-based smoke detection model to monitor, detect, and notify users about potential wildfires. Second, we propose a two-stage smoke detection framework that leverages Gaussian filtering and a dual-stage YOLOv5 pipeline to reduce false predictions and improve detection accuracy. Using the FireSpot dataset comprising annotated images of early-stage wildfires, the system achieved a balanced accuracy of 98.62%, significantly surpassing baseline models with an improvement of 4.02%. Currently deployed in three municipalities in Chiang Mai, Thailand, the system has proven effective in providing timely alerts and enhancing wildfire management. By integrating IoT technology and machine learning, this study offers a scalable and practical solution for early-stage wildfire detection to mitigate environmental and human health impacts. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025. NULL Fire alarm systems; Smoke detectors; Area monitoring; Detection models; Early Warning System; Economic stability; Environmental sustainability; Northern Thailand; Public economics; Real- time; Smoke detection; WEB application; Premixed flames";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
105;Machine learning application to disaster damage repair cost modelling of residential buildings;Restoring residential buildings following earthquake damage requires a significant level of resources. Being able to predict these resource requirements in advance and accurately improves the effectiveness of disaster preparedness and subsequent recovery activities. This research explored how the latest ML algorithms could be used for antecedent earthquake loss modelling. A cost database for repairing residential buildings damaged by the Emilia Romagna earthquake in Italy was analysed using six state-of-the-art ML models to explore their ability to predict repair cost rates(cost per floor area) for a domestic building damaged by earthquakes. A Gradient Boost Regression model outperformed five other models in predicting earthquake damage repair cost rate. The performance of this model was significantly accurate and covers about 76% of the cases. A further SHAP analysis revealed that operational level, damage level and non-housing area of the buildings as top 3 important features when predicting the resultant damage repair cost rate. Overall this research advanced antecedent earthquake loss modelling approaches to increase the accuracy of estimates by incorporating more variables than the widely used damage level based simple methodology. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.;"cost modelling; damage repair costs; disaster preparedness; Earthquake; machine learning";"Emilia-Romagna; Italy; Cost benefit analysis; Disaster prevention; Earthquake effects; Earthquake engineering; Cost models; Cost rates; Damage repair; Damage repair cost; Disaster preparedness; Earthquake damages; Earthquake loss modelling; Machine-learning; Repair costs; Residential building; building; cost analysis; disaster management; earthquake damage; machine learning; modeling; Disasters";"Machine learning application to disaster damage repair cost modelling of residential buildings Restoring residential buildings following earthquake damage requires a significant level of resources. Being able to predict these resource requirements in advance and accurately improves the effectiveness of disaster preparedness and subsequent recovery activities. This research explored how the latest ML algorithms could be used for antecedent earthquake loss modelling. A cost database for repairing residential buildings damaged by the Emilia Romagna earthquake in Italy was analysed using six state-of-the-art ML models to explore their ability to predict repair cost rates(cost per floor area) for a domestic building damaged by earthquakes. A Gradient Boost Regression model outperformed five other models in predicting earthquake damage repair cost rate. The performance of this model was significantly accurate and covers about 76% of the cases. A further SHAP analysis revealed that operational level, damage level and non-housing area of the buildings as top 3 important features when predicting the resultant damage repair cost rate. Overall this research advanced antecedent earthquake loss modelling approaches to increase the accuracy of estimates by incorporating more variables than the widely used damage level based simple methodology. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. cost modelling; damage repair costs; disaster preparedness; Earthquake; machine learning Emilia-Romagna; Italy; Cost benefit analysis; Disaster prevention; Earthquake effects; Earthquake engineering; Cost models; Cost rates; Damage repair; Damage repair cost; Disaster preparedness; Earthquake damages; Earthquake loss modelling; Machine-learning; Repair costs; Residential building; building; cost analysis; disaster management; earthquake damage; machine learning; modeling; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
106;Analytical Fragility Curve Construction for Coastal Masonry Buildings: Height-Dependent Structural Vulnerability Evaluation;In recent years, the vulnerability of coastal buildings to tsunamis has gained significant attention within the scientific community. Estimating the probability of building damage and formulating effective risk mitigation strategies have become imperative to mitigate potential losses. While empirical fragility curves based on post-tsunami field investigations offer valuable insights, a predictive approach based on analytical fragility curves applicable across various building types is crucial for civil protection and risk reduction efforts. The present study investigates the influence of building height on the structural vulnerability of masonry buildings. For this purpose, an approach based on Monte Carlo simulations was adopted to analyze distinct classes of buildings, highlighting the advantages of this method in directly assessing uncertainties without complex regression analyses. The assessment of height influence across buildings with random geometries and material properties allows for the evaluation of the accuracy and applicability of fragility curves for masonry structures in tsunami-prone regions. The research not only provides insights into the effectiveness of the procedure for constructing analytical fragility curves but also offers valuable insights into structural vulnerability as a function of the number of storeys. These findings can be instrumental for more precise risk evaluations and in formulating measures to safeguard coastal communities from the devastating repercussions of tsunamis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"coastal resilience; masonry structures; Monte Carlo simulation; structural height; tsunami actions; tsunami fragility";"Buildings; Masonry materials; Risk analysis; Risk perception; Shore protection; Building height; Coastal resilience; Fragility curves; Masonry building; Masonry structures; Monte Carlo's simulation; Structural height; Structural vulnerability; Tsunami action; Tsunami fragility; Risk assessment";"Analytical Fragility Curve Construction for Coastal Masonry Buildings: Height-Dependent Structural Vulnerability Evaluation In recent years, the vulnerability of coastal buildings to tsunamis has gained significant attention within the scientific community. Estimating the probability of building damage and formulating effective risk mitigation strategies have become imperative to mitigate potential losses. While empirical fragility curves based on post-tsunami field investigations offer valuable insights, a predictive approach based on analytical fragility curves applicable across various building types is crucial for civil protection and risk reduction efforts. The present study investigates the influence of building height on the structural vulnerability of masonry buildings. For this purpose, an approach based on Monte Carlo simulations was adopted to analyze distinct classes of buildings, highlighting the advantages of this method in directly assessing uncertainties without complex regression analyses. The assessment of height influence across buildings with random geometries and material properties allows for the evaluation of the accuracy and applicability of fragility curves for masonry structures in tsunami-prone regions. The research not only provides insights into the effectiveness of the procedure for constructing analytical fragility curves but also offers valuable insights into structural vulnerability as a function of the number of storeys. These findings can be instrumental for more precise risk evaluations and in formulating measures to safeguard coastal communities from the devastating repercussions of tsunamis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. coastal resilience; masonry structures; Monte Carlo simulation; structural height; tsunami actions; tsunami fragility Buildings; Masonry materials; Risk analysis; Risk perception; Shore protection; Building height; Coastal resilience; Fragility curves; Masonry building; Masonry structures; Monte Carlo's simulation; Structural height; Structural vulnerability; Tsunami action; Tsunami fragility; Risk assessment";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
107;Intelligent Prediction of Flood Disaster Risk Levels Based on Knowledge Graph and Graph Neural Networks;Flash flood disasters pose a significant threat to human life and property, making accurate prediction of risk levels crucial for disaster prevention and mitigation. This study introduces an innovative artificial intelligence approach based on knowledge graphs and graph neural networks. The method integrates multi-source data to construct a knowledge graph, which is then modeled using graph neural networks. We evaluate the model's performance using metrics such as accuracy, precision, recall, F1 score, and AUC. Under five-fold cross-validation, the AUC reached 0.84, with all performance indicators showing good results, indicating significant performance improvement. Experimental results demonstrate high prediction accuracy when tested on a dataset containing 9000 records. Compared with the three classical models in traditional machine learning, such as RF, SVM and ANN, the performance of this model is improved, and it is better than the traditional model. Through case analysis, risk levels in multiple regions were accurately predicted. Additionally, statistical analysis of flood disaster warning levels and flash flood risk zoning across cities in Jiangxi Province provides a visual representation of flood risk distribution and risk level proportions in different cities, offering strong reference for flood prevention, disaster mitigation, and urban planning. This method provides important scientific support for precise flash flood disaster prediction and risk management.  © 2013 IEEE.;"flood disaster; graph neural network; Knowledge graph; risk level prediction";"Graph neural networks; Prediction models; Flash-floods; Flood disaster; Flood risks; Graph neural networks; Human lives; Intelligent prediction; Knowledge graphs; Performance; Risk level prediction; Risk levels; Knowledge graph";"Intelligent Prediction of Flood Disaster Risk Levels Based on Knowledge Graph and Graph Neural Networks Flash flood disasters pose a significant threat to human life and property, making accurate prediction of risk levels crucial for disaster prevention and mitigation. This study introduces an innovative artificial intelligence approach based on knowledge graphs and graph neural networks. The method integrates multi-source data to construct a knowledge graph, which is then modeled using graph neural networks. We evaluate the model's performance using metrics such as accuracy, precision, recall, F1 score, and AUC. Under five-fold cross-validation, the AUC reached 0.84, with all performance indicators showing good results, indicating significant performance improvement. Experimental results demonstrate high prediction accuracy when tested on a dataset containing 9000 records. Compared with the three classical models in traditional machine learning, such as RF, SVM and ANN, the performance of this model is improved, and it is better than the traditional model. Through case analysis, risk levels in multiple regions were accurately predicted. Additionally, statistical analysis of flood disaster warning levels and flash flood risk zoning across cities in Jiangxi Province provides a visual representation of flood risk distribution and risk level proportions in different cities, offering strong reference for flood prevention, disaster mitigation, and urban planning. This method provides important scientific support for precise flash flood disaster prediction and risk management.  © 2013 IEEE. flood disaster; graph neural network; Knowledge graph; risk level prediction Graph neural networks; Prediction models; Flash-floods; Flood disaster; Flood risks; Graph neural networks; Human lives; Intelligent prediction; Knowledge graphs; Performance; Risk level prediction; Risk levels; Knowledge graph";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
108;Transformer-Based Deep Learning Approach for Wildfire Burn Severity Estimation Using Drone Imagery in South Korea;The frequency and scale of large wildfires are increasing worldwide due to the rise in extreme weather events. Wildfires not only cause direct damage to human life and property but also lead to complex environmental issues, including forest ecosystem destruction and acceleration of climate change through massive greenhouse gas emissions. In South Korea, where forests cover 70% of the territory, accurate assessment and effective management of wildfire damage have emerged as critical challenges. Notably, 99.5% of domestic wildfires are small to medium fires under 100 ha, and their cumulative impact on forest ecosystems is substantial. Traditional burn severity assessment methods based on spectral indices such as normalized difference vegetation index (NDVI) and normalized burn ratio (NBR) have limitations in providing consistent evaluation criteria, as appropriate thresholds vary depending on forest types and regional characteristics. Therefore, this study developed a Transformer-based semantic segmentation model to classify burn severity into three levels: surface fire, crown scorch, and crown fire. To mitigate the class imbalance issues, we conducted experiments with three different sampling approaches for the unburned class, achieving a mean intersection over union (mIoU) of 0.748 with the final model. Notably, the model demonstrated its practical applicability by achieving over 90% prediction accuracy in validating small wildfires under 1ha. This study presents a novel methodology for rapid and accurate estimation of burn severity using deep learning-based segmentation. It is expected to provide a foundation for establishing burn severity assessment systems and effective forest restoration planning tailored to South Korea’s forest characteristics. Copyright © 2024 Korean Society of Remote Sensing.;"Burn severity; Deep learning; Drone image; Swin transformer";NULL;"Transformer-Based Deep Learning Approach for Wildfire Burn Severity Estimation Using Drone Imagery in South Korea The frequency and scale of large wildfires are increasing worldwide due to the rise in extreme weather events. Wildfires not only cause direct damage to human life and property but also lead to complex environmental issues, including forest ecosystem destruction and acceleration of climate change through massive greenhouse gas emissions. In South Korea, where forests cover 70% of the territory, accurate assessment and effective management of wildfire damage have emerged as critical challenges. Notably, 99.5% of domestic wildfires are small to medium fires under 100 ha, and their cumulative impact on forest ecosystems is substantial. Traditional burn severity assessment methods based on spectral indices such as normalized difference vegetation index (NDVI) and normalized burn ratio (NBR) have limitations in providing consistent evaluation criteria, as appropriate thresholds vary depending on forest types and regional characteristics. Therefore, this study developed a Transformer-based semantic segmentation model to classify burn severity into three levels: surface fire, crown scorch, and crown fire. To mitigate the class imbalance issues, we conducted experiments with three different sampling approaches for the unburned class, achieving a mean intersection over union (mIoU) of 0.748 with the final model. Notably, the model demonstrated its practical applicability by achieving over 90% prediction accuracy in validating small wildfires under 1ha. This study presents a novel methodology for rapid and accurate estimation of burn severity using deep learning-based segmentation. It is expected to provide a foundation for establishing burn severity assessment systems and effective forest restoration planning tailored to South Korea’s forest characteristics. Copyright © 2024 Korean Society of Remote Sensing. Burn severity; Deep learning; Drone image; Swin transformer NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;4;Recovery
109;Changes in vegetation ecosystem carbon sinks and their response to drought in the karst concentration distribution area of Asia;"Changes in net ecosystem productivity (NEP) in karst areas can have a significant impact on terrestrial ecosystem carbon cycling, yet quantifying changes in vegetation NEP and its response to factors such as drought and hydroclimate remains a difficult challenge because of its special climatic and hydrological conditions. We used remote sensing data to estimate vegetation NEP in the Asian karst concentrated distribution area (AKC), analyzed its spatial and temporal variations annually (2000−2020) and during rainy season (May–November), established the drought fluorescence monitoring index (DFMI), and used a ridge regression model to explore the response mechanism of vegetation NEP to dry and wet conditions response mechanism. The results showed the following: (1) Compared with the annual changes, the vegetation NEP changes in the rainy season differed significantly on the karst geographic divisions, in which there was a significant increasing trend in Southwest China (SC) and its karst areas, while a significant decreasing trend in the Indochina Peninsula (IP) and its karst areas. (2) DFMI was the main driver of vegetation NEP change, of which the contributions were 38.05 % and 32.82 % at the annual scale and in the rainy season, respectively, which drove the increase in SC vegetation NEP, and the decrease in IP; note that the increase in vapor pressure deficit (VPD) was the key factor causing the decrease in NEP in the IP karst area during the rainy season. (3) In the lagged effect of drought on vegetation NEP, the time scale of the lag was found to be 1 month. The study revealed differences in the changes in the vegetation carbon sinks in different karst geographic divisions. We obtained a new finding: a significant trend of decreasing vegetation NEP in the IP and its karst area was influenced by the long-term effects of changes in DFMI and VPD. Therefore, the variability of different karst areas, as well as changes in drought and water resources, should be considered in carbon-cycle regulation and vegetation restoration efforts in karst areas. © 2024";"Carbon sinks; Climatic hydrological factors; Drought; Karst; Net ecosystem productivity; Spatial and temporal variability";"China; Indochina; carbon sink; concentration (composition); drought; ecosystem dynamics; hydrological response; karst; net ecosystem production; spatiotemporal analysis; terrestrial ecosystem; vapor pressure; vegetation dynamics";"Changes in vegetation ecosystem carbon sinks and their response to drought in the karst concentration distribution area of Asia Changes in net ecosystem productivity (NEP) in karst areas can have a significant impact on terrestrial ecosystem carbon cycling, yet quantifying changes in vegetation NEP and its response to factors such as drought and hydroclimate remains a difficult challenge because of its special climatic and hydrological conditions. We used remote sensing data to estimate vegetation NEP in the Asian karst concentrated distribution area (AKC), analyzed its spatial and temporal variations annually (2000−2020) and during rainy season (May–November), established the drought fluorescence monitoring index (DFMI), and used a ridge regression model to explore the response mechanism of vegetation NEP to dry and wet conditions response mechanism. The results showed the following: (1) Compared with the annual changes, the vegetation NEP changes in the rainy season differed significantly on the karst geographic divisions, in which there was a significant increasing trend in Southwest China (SC) and its karst areas, while a significant decreasing trend in the Indochina Peninsula (IP) and its karst areas. (2) DFMI was the main driver of vegetation NEP change, of which the contributions were 38.05 % and 32.82 % at the annual scale and in the rainy season, respectively, which drove the increase in SC vegetation NEP, and the decrease in IP; note that the increase in vapor pressure deficit (VPD) was the key factor causing the decrease in NEP in the IP karst area during the rainy season. (3) In the lagged effect of drought on vegetation NEP, the time scale of the lag was found to be 1 month. The study revealed differences in the changes in the vegetation carbon sinks in different karst geographic divisions. We obtained a new finding: a significant trend of decreasing vegetation NEP in the IP and its karst area was influenced by the long-term effects of changes in DFMI and VPD. Therefore, the variability of different karst areas, as well as changes in drought and water resources, should be considered in carbon-cycle regulation and vegetation restoration efforts in karst areas. © 2024 Carbon sinks; Climatic hydrological factors; Drought; Karst; Net ecosystem productivity; Spatial and temporal variability China; Indochina; carbon sink; concentration (composition); drought; ecosystem dynamics; hydrological response; karst; net ecosystem production; spatiotemporal analysis; terrestrial ecosystem; vapor pressure; vegetation dynamics";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
110;Model Updating of Cultural Heritage Buildings Through Swarm Intelligence Algorithms;Masonry buildings constitute a great Italian historical and cultural heritage, but they were also severely damaged by earthquakes over the centuries. Therefore, to assess their structural and seismic performance, it is crucial to gain a refined and trustworthy numerical model adopting model updating techniques, sometimes based on artificial intelligence algorithms. This article deals with the development and the updating of a finite element model of an historical church that account for the presence of both the seismic damage and securing systems. The model updating is performed adopting the particle swarm optimization algorithm and is based on the comparison between numerical and experimental modal parameters, the latter achieved by an extensive dynamic test campaign. The obtained calibrated numerical model has been adopted to support the restoration work design, as well as the design of a structural health monitoring system that has been permanently installed on the church. © 2023 Taylor & Francis.;"Ambient vibration tests; cultural heritage building; historical masonry church; machine learning algorithm; model updating; particle swarm optimization";"Learning algorithms; Machine learning; Masonry materials; Modal analysis; Numerical models; Particle swarm optimization (PSO); Seismology; Structural health monitoring; Swarm intelligence; Ambient vibration test; Cultural heritage building; Cultural heritages; Heritage buildings; Historical masonry church; Machine learning algorithms; Masonry churches; Model updating; Particle swarm; Particle swarm optimization; Swarm optimization; artificial intelligence; cultural heritage; machine learning; masonry; monitoring system; numerical model; optimization; testing method; Religious buildings";"Model Updating of Cultural Heritage Buildings Through Swarm Intelligence Algorithms Masonry buildings constitute a great Italian historical and cultural heritage, but they were also severely damaged by earthquakes over the centuries. Therefore, to assess their structural and seismic performance, it is crucial to gain a refined and trustworthy numerical model adopting model updating techniques, sometimes based on artificial intelligence algorithms. This article deals with the development and the updating of a finite element model of an historical church that account for the presence of both the seismic damage and securing systems. The model updating is performed adopting the particle swarm optimization algorithm and is based on the comparison between numerical and experimental modal parameters, the latter achieved by an extensive dynamic test campaign. The obtained calibrated numerical model has been adopted to support the restoration work design, as well as the design of a structural health monitoring system that has been permanently installed on the church. © 2023 Taylor & Francis. Ambient vibration tests; cultural heritage building; historical masonry church; machine learning algorithm; model updating; particle swarm optimization Learning algorithms; Machine learning; Masonry materials; Modal analysis; Numerical models; Particle swarm optimization (PSO); Seismology; Structural health monitoring; Swarm intelligence; Ambient vibration test; Cultural heritage building; Cultural heritages; Heritage buildings; Historical masonry church; Machine learning algorithms; Masonry churches; Model updating; Particle swarm; Particle swarm optimization; Swarm optimization; artificial intelligence; cultural heritage; machine learning; masonry; monitoring system; numerical model; optimization; testing method; Religious buildings";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
111;Use of artificial intelligence in banknote reconstruction;"Banknotes may be damaged during various events, such as floods, fires, insect infestations, and mechanical or manual shredding. Disaster victims might need to perform banknote reconstruction when applying for currency exchange, or investigative agencies might need to conduct such reconstruction during evidence collection. When the number of banknote fragments is small, they can be manually assembled; however, when this number is large, manual assembly becomes increasingly difficult and time-consuming. Therefore, an automated and effective method is required for banknote reconstruction. The process of banknote reconstruction can be considered similar to solving a large-scale jigsaw puzzle. This study employed an artificial intelligence (AI) system to reconstruct damaged banknotes. A robotic arm was used to replace manual separation and automated digital image processing techniques, and AI image registration technology, deep learning, and logical operations were utilized. A deep convolutional neural network was used to estimate the relative homography between images, and fragmented banknotes were mapped to a reference banknote for image transformation, thereby reconstructing the damaged banknotes. Additionally, a repetitive matching method was established to optimize the matching results to achieve the best possible mapping and enhance validation efficiency. © 2024, Intelektual Pustaka Media Utama. All rights reserved.";"Banknote reconstruction; Deep learning; Image reconstruction; Image registration; Jigsaw puzzle";NULL;"Use of artificial intelligence in banknote reconstruction Banknotes may be damaged during various events, such as floods, fires, insect infestations, and mechanical or manual shredding. Disaster victims might need to perform banknote reconstruction when applying for currency exchange, or investigative agencies might need to conduct such reconstruction during evidence collection. When the number of banknote fragments is small, they can be manually assembled; however, when this number is large, manual assembly becomes increasingly difficult and time-consuming. Therefore, an automated and effective method is required for banknote reconstruction. The process of banknote reconstruction can be considered similar to solving a large-scale jigsaw puzzle. This study employed an artificial intelligence (AI) system to reconstruct damaged banknotes. A robotic arm was used to replace manual separation and automated digital image processing techniques, and AI image registration technology, deep learning, and logical operations were utilized. A deep convolutional neural network was used to estimate the relative homography between images, and fragmented banknotes were mapped to a reference banknote for image transformation, thereby reconstructing the damaged banknotes. Additionally, a repetitive matching method was established to optimize the matching results to achieve the best possible mapping and enhance validation efficiency. © 2024, Intelektual Pustaka Media Utama. All rights reserved. Banknote reconstruction; Deep learning; Image reconstruction; Image registration; Jigsaw puzzle NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;4;Recovery
112;Streamflow simulation and forecasting using remote sensing and machine learning techniques;The study investigates the integration of remote sensing data and machine learning (ML) techniques in streamflow simulation and forecasting in Klang River Basin. The motivation behind the research lies in the need for improved accuracy in streamflow prediction to support effective water resource management and flood control in Malaysia. Five ML models, including K-Nearest Neighbours (KNN), Support Vector Machines (SVM), Random Forests (RF), Artificial Neural Network (ANN), and Long Short-Term Memory (LSTM) were tested, with 25 different model configurations trained on datasets from Malaysia's Department of Irrigation and Drainage (DID) and climate data from NASA's Giovanni Portal. Among the models, RF − III showed the best performance with SMAPE, MdAPE and Lp values of 0.36 and 0.37 and 0.908, respectively. Further analysis underscores air temperature's substantial impact on streamflow prediction, deviating significantly from the benchmark value. The findings contribute to advancements in streamflow forecasting, offering potential applications in flood management and water resource planning for the region. © 2024 The Author(s);"Data-driven modelling; Machine learning; Rainfall-runoff; Water resource planning";"Long short-term memory; Nearest neighbor search; Rain; Resource allocation; Rivers; Runoff; Support vector machines; Water management; Data-driven model; Machine learning techniques; Machine-learning; Malaysia; Rainfall runoff; Random forests; Streamflow forecasting; Streamflow prediction; Streamflow simulations; Water resources planning; Adversarial machine learning";"Streamflow simulation and forecasting using remote sensing and machine learning techniques The study investigates the integration of remote sensing data and machine learning (ML) techniques in streamflow simulation and forecasting in Klang River Basin. The motivation behind the research lies in the need for improved accuracy in streamflow prediction to support effective water resource management and flood control in Malaysia. Five ML models, including K-Nearest Neighbours (KNN), Support Vector Machines (SVM), Random Forests (RF), Artificial Neural Network (ANN), and Long Short-Term Memory (LSTM) were tested, with 25 different model configurations trained on datasets from Malaysia's Department of Irrigation and Drainage (DID) and climate data from NASA's Giovanni Portal. Among the models, RF − III showed the best performance with SMAPE, MdAPE and Lp values of 0.36 and 0.37 and 0.908, respectively. Further analysis underscores air temperature's substantial impact on streamflow prediction, deviating significantly from the benchmark value. The findings contribute to advancements in streamflow forecasting, offering potential applications in flood management and water resource planning for the region. © 2024 The Author(s) Data-driven modelling; Machine learning; Rainfall-runoff; Water resource planning Long short-term memory; Nearest neighbor search; Rain; Resource allocation; Rivers; Runoff; Support vector machines; Water management; Data-driven model; Machine learning techniques; Machine-learning; Malaysia; Rainfall runoff; Random forests; Streamflow forecasting; Streamflow prediction; Streamflow simulations; Water resources planning; Adversarial machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
113;Spatiotemporal changes of ecosystem health and the impact of its driving factors on the Loess Plateau in China;The Loess Plateau is renowned for its fragile ecosystem, and understanding the changes and driving impacts of ecosystem health (EH) is crucial for formulating environmental protection policies for the region. Soil erosion, as a key limiting factor of the region's ecosystem, must be considered when evaluating EH in this area. Based on this idea, a new framework for assessing EH is proposed. The efficient machine learning model (light gradient boosting machine model) and the variable explanation model (SHapley Additive exPlanation model) are combined to quantify the functional relationships between various driving factors and EH, thereby exploring the impacts of climatic factors, socioeconomic development (SED), and ecological restoration projects (ERP) on EH. The study found that: (1) From 1995 to 2020, EH in the Loess Plateau increased from 0.42 to 0.58, and the forms of spatial clustering changed. Moreover, there are significant differences in the speed and patterns of EH improvement in different regions, especially after 2010. (2) The importance of precipitation (PRE), SED, ERP, and temperature (TEM) is 33 %, 26 %, 24 %, and 17 %, respectively. The driving impacts exhibit non-monotonic polynomial relationships of third, fourth, second, and fourth degrees, indicating that the mechanisms through which EH is influenced by these factors can change. (3) Counties with higher response to PRE (>0.32) are mainly located in the central region. Counties with higher response to SED (>0.30) are primarily in areas with higher population and urbanization. Counties with higher response to ERP (>0.36) are mainly in the eastern region. Counties with higher response to TEM (>0.26) are primarily in the southern and western regions. The results of this paper provide new insights for EH research in areas with fragile ecosystems and are of significant importance for the ecological civilization construction of the Loess Plateau. © 2024;"Driving factors; Ecosystem health; Loess Plateau; Soil erosion; Spatiotemporal changes";"China; Loess Plateau; Abiotic; Driving factors; Ecological restoration; Ecosystem health; High response; Loess Plateau; Protection policy; Restoration project; Socio-economic development; Soil erosion; Spatio-temporal changes; ecological impact; ecosystem health; environmental protection; limiting factor; machine learning; soil erosion; spatiotemporal analysis; Soil erosion";"Spatiotemporal changes of ecosystem health and the impact of its driving factors on the Loess Plateau in China The Loess Plateau is renowned for its fragile ecosystem, and understanding the changes and driving impacts of ecosystem health (EH) is crucial for formulating environmental protection policies for the region. Soil erosion, as a key limiting factor of the region's ecosystem, must be considered when evaluating EH in this area. Based on this idea, a new framework for assessing EH is proposed. The efficient machine learning model (light gradient boosting machine model) and the variable explanation model (SHapley Additive exPlanation model) are combined to quantify the functional relationships between various driving factors and EH, thereby exploring the impacts of climatic factors, socioeconomic development (SED), and ecological restoration projects (ERP) on EH. The study found that: (1) From 1995 to 2020, EH in the Loess Plateau increased from 0.42 to 0.58, and the forms of spatial clustering changed. Moreover, there are significant differences in the speed and patterns of EH improvement in different regions, especially after 2010. (2) The importance of precipitation (PRE), SED, ERP, and temperature (TEM) is 33 %, 26 %, 24 %, and 17 %, respectively. The driving impacts exhibit non-monotonic polynomial relationships of third, fourth, second, and fourth degrees, indicating that the mechanisms through which EH is influenced by these factors can change. (3) Counties with higher response to PRE (>0.32) are mainly located in the central region. Counties with higher response to SED (>0.30) are primarily in areas with higher population and urbanization. Counties with higher response to ERP (>0.36) are mainly in the eastern region. Counties with higher response to TEM (>0.26) are primarily in the southern and western regions. The results of this paper provide new insights for EH research in areas with fragile ecosystems and are of significant importance for the ecological civilization construction of the Loess Plateau. © 2024 Driving factors; Ecosystem health; Loess Plateau; Soil erosion; Spatiotemporal changes China; Loess Plateau; Abiotic; Driving factors; Ecological restoration; Ecosystem health; High response; Loess Plateau; Protection policy; Restoration project; Socio-economic development; Soil erosion; Spatio-temporal changes; ecological impact; ecosystem health; environmental protection; limiting factor; machine learning; soil erosion; spatiotemporal analysis; Soil erosion";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
114;Coupling SWAT and LSTM for Improving Daily Streamflow Simulation in a Humid and Semi-humid River Basin;Simulation of watershed streamflow is essential for the prevention and control of flood and drought disasters. To improve streamflow simulation, a coupled SWAT-LSTM model was constructed by combining a conceptual process-based hydrological model—Soil and Water Assessment Tool (SWAT)—with a machine learning model—Long Short-Term Memory (LSTM). The coupled model was applied to simulate the daily streamflow of the upper Huaihe River above the Xixian station from 1962 to 2010, which identified the optimal explanatory variables of the model and reduced streamflow simulation errors. Furthermore, four machine learning models, back propagation (BP) neural network, gated recurrent unit (GRU), support vector regression (SVR) and extreme gradient boosting (XGBoost), were chosen to assess the effectiveness of coupling SWAT with LSTM in streamflow simulation. Results showed that the coupled SWAT-LSTM model performed satisfactorily in streamflow simulation in the study area, with NSE reaching 0.90 and 0.85 in calibration and validation periods, respectively. The coupled model showed a significant improvement in simulating flood peak and average streamflow in each period, with mean NSE increasing by 0.24 compared to the standalone SWAT model. In comparison to other coupled models (i.e., SWAT-BP, SWAT-GRU, SWAT-SVR, and SWAT-XGB), the mean NSE of SWAT-LSTM exhibited an improvement of 0.02–0.16 during validation period. Furthermore, the coupled model effectively avoided the overfitting problem and had better generalization performance. The findings of this study offer new ideas for streamflow simulation of watersheds and provide references for water resources management and planning. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"Coupled modeling; LSTM; Machine learning; Streamflow simulation; SWAT";"Adversarial machine learning; Contrastive Learning; Resource allocation; Rivers; Support vector regression; Coupled models; Machine learning models; Machine-learning; Memory modeling; River basins; Short term memory; Soil and Water assessment tools; Streamflow simulations; Support vector regressions; Validation periods; flood; humidity; machine learning; river basin; soil and water assessment tool; streamflow; Long short-term memory";"Coupling SWAT and LSTM for Improving Daily Streamflow Simulation in a Humid and Semi-humid River Basin Simulation of watershed streamflow is essential for the prevention and control of flood and drought disasters. To improve streamflow simulation, a coupled SWAT-LSTM model was constructed by combining a conceptual process-based hydrological model—Soil and Water Assessment Tool (SWAT)—with a machine learning model—Long Short-Term Memory (LSTM). The coupled model was applied to simulate the daily streamflow of the upper Huaihe River above the Xixian station from 1962 to 2010, which identified the optimal explanatory variables of the model and reduced streamflow simulation errors. Furthermore, four machine learning models, back propagation (BP) neural network, gated recurrent unit (GRU), support vector regression (SVR) and extreme gradient boosting (XGBoost), were chosen to assess the effectiveness of coupling SWAT with LSTM in streamflow simulation. Results showed that the coupled SWAT-LSTM model performed satisfactorily in streamflow simulation in the study area, with NSE reaching 0.90 and 0.85 in calibration and validation periods, respectively. The coupled model showed a significant improvement in simulating flood peak and average streamflow in each period, with mean NSE increasing by 0.24 compared to the standalone SWAT model. In comparison to other coupled models (i.e., SWAT-BP, SWAT-GRU, SWAT-SVR, and SWAT-XGB), the mean NSE of SWAT-LSTM exhibited an improvement of 0.02–0.16 during validation period. Furthermore, the coupled model effectively avoided the overfitting problem and had better generalization performance. The findings of this study offer new ideas for streamflow simulation of watersheds and provide references for water resources management and planning. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. Coupled modeling; LSTM; Machine learning; Streamflow simulation; SWAT Adversarial machine learning; Contrastive Learning; Resource allocation; Rivers; Support vector regression; Coupled models; Machine learning models; Machine-learning; Memory modeling; River basins; Short term memory; Soil and Water assessment tools; Streamflow simulations; Support vector regressions; Validation periods; flood; humidity; machine learning; river basin; soil and water assessment tool; streamflow; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
115;A Hybrid Damaged Building Sample Generation Method Based on Cross-Scale Fusion Generative Model for Destroyed Building Detection After Earthquake;The advent of remote sensing and deep learning has rendered the automated detection of postearthquake building damage a necessity for effective disaster response. However, the inherent unpredictability of earthquakes and the limited revisit frequency of satellites frequently result in inadequate postdisaster data for effective model training. Furthermore, conventional data augmentation techniques have the potential to lead to model overfitting, thereby further complicating the development of robust detection systems. To address the challenge, this article introduces an innovative end-to-end data augmentation approach that combines a multichannel fusion Wasserstein generative adversarial network (MCFWGAN) with a Mask-CutMix sample augmentation technique. This approach first introduces a multichannel fusion (MCF) mechanism into the WGAN framework, constructing the MCFWGAN model, which enhances global feature extraction capabilities while strengthening the interdependencies among image channels. After generating images, the Mask-CutMix method integrates these generated images into large-scale remote sensing imagery, thereby augmenting the dataset. The integrated image dataset contains a small amount of pre-earthquake and postearthquake data. Finally, a target detection network is employed to extract damaged buildings from the images. Two study areas were selected to implement data augmentation and target detection. Experimental results demonstrate that the proposed data augmentation method outperforms traditional techniques in terms of accuracy metrics for target detection. Furthermore, to address the issues of missed and false detections of intact buildings within severely damaged areas, the use of style transfer-based sample augmentation was shown to effectively improve accuracy metrics. The experimental results demonstrate that the proposed data enhancement method outperforms the traditional technique with respect to the target detection evaluation metric. In Study Area 1, the detection accuracy is enhanced by 5.08% in comparison to the conventional data enhancement method. In Study Area 2, the mean accuracy is elevated by 3.32%. The data augmentation method proposed in this study not only increases the number of samples but also enhances the diversity of samples representing damaged buildings. The MCFWGAN model is particularly well-suited for generating images of damaged buildings in complex postearthquake scenarios, offering significant potential for the accurate acquisition of postdisaster information and emergency rescue operations.  © 1980-2012 IEEE.;"Convolutional neural network (CNN); generative adversarial network (GAN); postearthquake buildings; sample expansion; style transfer; target detection";"Earthquake effects; Earthquake engineering; Window screens; Adversarial networks; Data augmentation; GAN; Multi channel; Post disasters; Post earthquake building; Sample expansion; Study areas; Style transfer; Targets detection; artificial neural network; building; data processing; detection method; disaster management; earthquake damage; remote sensing; satellite data; Generative adversarial networks";"A Hybrid Damaged Building Sample Generation Method Based on Cross-Scale Fusion Generative Model for Destroyed Building Detection After Earthquake The advent of remote sensing and deep learning has rendered the automated detection of postearthquake building damage a necessity for effective disaster response. However, the inherent unpredictability of earthquakes and the limited revisit frequency of satellites frequently result in inadequate postdisaster data for effective model training. Furthermore, conventional data augmentation techniques have the potential to lead to model overfitting, thereby further complicating the development of robust detection systems. To address the challenge, this article introduces an innovative end-to-end data augmentation approach that combines a multichannel fusion Wasserstein generative adversarial network (MCFWGAN) with a Mask-CutMix sample augmentation technique. This approach first introduces a multichannel fusion (MCF) mechanism into the WGAN framework, constructing the MCFWGAN model, which enhances global feature extraction capabilities while strengthening the interdependencies among image channels. After generating images, the Mask-CutMix method integrates these generated images into large-scale remote sensing imagery, thereby augmenting the dataset. The integrated image dataset contains a small amount of pre-earthquake and postearthquake data. Finally, a target detection network is employed to extract damaged buildings from the images. Two study areas were selected to implement data augmentation and target detection. Experimental results demonstrate that the proposed data augmentation method outperforms traditional techniques in terms of accuracy metrics for target detection. Furthermore, to address the issues of missed and false detections of intact buildings within severely damaged areas, the use of style transfer-based sample augmentation was shown to effectively improve accuracy metrics. The experimental results demonstrate that the proposed data enhancement method outperforms the traditional technique with respect to the target detection evaluation metric. In Study Area 1, the detection accuracy is enhanced by 5.08% in comparison to the conventional data enhancement method. In Study Area 2, the mean accuracy is elevated by 3.32%. The data augmentation method proposed in this study not only increases the number of samples but also enhances the diversity of samples representing damaged buildings. The MCFWGAN model is particularly well-suited for generating images of damaged buildings in complex postearthquake scenarios, offering significant potential for the accurate acquisition of postdisaster information and emergency rescue operations.  © 1980-2012 IEEE. Convolutional neural network (CNN); generative adversarial network (GAN); postearthquake buildings; sample expansion; style transfer; target detection Earthquake effects; Earthquake engineering; Window screens; Adversarial networks; Data augmentation; GAN; Multi channel; Post disasters; Post earthquake building; Sample expansion; Study areas; Style transfer; Targets detection; artificial neural network; building; data processing; detection method; disaster management; earthquake damage; remote sensing; satellite data; Generative adversarial networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
116;Parametric Study of Rainfall-Induced Instability in Fine-Grained Sandy Soil;This study investigates the stability of fine-grained sandy soil slopes under varying rainfall intensities, durations, and geotechnical properties using a parametric analysis within GeoStudio. A total of 4416 unique parameter combinations were analyzed, incorporating variations in unit weight, cohesion, friction angle, slope inclination, slope height, rainfall intensity, and duration. Results reveal that rainfall intensity is the most influential variable on the factor of safety (FS), with higher intensities (e.g., 360 mm/h) on steeper slopes (e.g., 45°) leading to critical FS values below 1, indicating an imminent risk of failure. Under moderate conditions (e.g., 9 mm/h rainfall on slopes of 26.6°), the FS remains above 2. This dataset provides a valuable foundation for training machine learning models to predict slope stability under diverse environmental conditions, contributing to the development of early warning systems for rainfall-induced landslides. © 2024 by the authors.;"landslides; parametric study; rainfall; slope stability";NULL;"Parametric Study of Rainfall-Induced Instability in Fine-Grained Sandy Soil This study investigates the stability of fine-grained sandy soil slopes under varying rainfall intensities, durations, and geotechnical properties using a parametric analysis within GeoStudio. A total of 4416 unique parameter combinations were analyzed, incorporating variations in unit weight, cohesion, friction angle, slope inclination, slope height, rainfall intensity, and duration. Results reveal that rainfall intensity is the most influential variable on the factor of safety (FS), with higher intensities (e.g., 360 mm/h) on steeper slopes (e.g., 45°) leading to critical FS values below 1, indicating an imminent risk of failure. Under moderate conditions (e.g., 9 mm/h rainfall on slopes of 26.6°), the FS remains above 2. This dataset provides a valuable foundation for training machine learning models to predict slope stability under diverse environmental conditions, contributing to the development of early warning systems for rainfall-induced landslides. © 2024 by the authors. landslides; parametric study; rainfall; slope stability NULL";-1;Não Classificado;NULL;1.1;Geological;2;Preparation
117;Enhancing Cyclone Preparedness: Deep Learning Methods with INSAT-3D Satellite Imagery;Cyclones are enormous storms that bring heavy rain and strong winds. The INSAT 3D satellite accurately tracks cyclones and their progression. The objective of our work is to measure the severity of the cyclone using the generated sequence of photographs. Estimating cyclone strength is essential for timely alerts, risk analysis, emergency management, and understanding cyclone behaviour. It helps with planning, resource allocation and minimising the effects on people’s lives and infrastructure. Current cyclone intensity detection systems lack complexity and dynamics, leading to inaccurate predictions and ineffective reaction measures. Improved strategies for detecting cyclone intensity can improve catastrophe preparedness and response in cyclone-prone locations. This work is centered on the study of cyclone intensity prediction using INSAT-3D satellite images. INSAT stands for “Indian National Satellite System”. Utilizing historical data and atmospheric features captured by INSAT-3D, we employ deep learning models to capture complex temporal correlations. These architectures excel in anticipating cyclone strength, facilitating timely alerts and effective emergency management. The comparative analysis demonstrates that our proposed Convolutional Neural Network model performed better over existing methods. By embracing cyclone dynamics, we provide precise intensity assessments, enhancing catastrophe preparedness and response. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Cyclone estimation; long short-term memory networks; Recurrent neural networks; Satellite imagery";"Convolutional neural networks; Deep neural networks; Disasters; Image recording; Long short-term memory; Resource allocation; Risk analysis; Risk assessment; Risk management; Risk perception; Tropical cyclone; Cyclone estimation; Emergency management; Heavy rains; Learning methods; Long short-term memory network; Memory network; Neural-networks; Preparedness and response; Short term memory; Strong winds; Satellite imagery";"Enhancing Cyclone Preparedness: Deep Learning Methods with INSAT-3D Satellite Imagery Cyclones are enormous storms that bring heavy rain and strong winds. The INSAT 3D satellite accurately tracks cyclones and their progression. The objective of our work is to measure the severity of the cyclone using the generated sequence of photographs. Estimating cyclone strength is essential for timely alerts, risk analysis, emergency management, and understanding cyclone behaviour. It helps with planning, resource allocation and minimising the effects on people’s lives and infrastructure. Current cyclone intensity detection systems lack complexity and dynamics, leading to inaccurate predictions and ineffective reaction measures. Improved strategies for detecting cyclone intensity can improve catastrophe preparedness and response in cyclone-prone locations. This work is centered on the study of cyclone intensity prediction using INSAT-3D satellite images. INSAT stands for “Indian National Satellite System”. Utilizing historical data and atmospheric features captured by INSAT-3D, we employ deep learning models to capture complex temporal correlations. These architectures excel in anticipating cyclone strength, facilitating timely alerts and effective emergency management. The comparative analysis demonstrates that our proposed Convolutional Neural Network model performed better over existing methods. By embracing cyclone dynamics, we provide precise intensity assessments, enhancing catastrophe preparedness and response. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Cyclone estimation; long short-term memory networks; Recurrent neural networks; Satellite imagery Convolutional neural networks; Deep neural networks; Disasters; Image recording; Long short-term memory; Resource allocation; Risk analysis; Risk assessment; Risk management; Risk perception; Tropical cyclone; Cyclone estimation; Emergency management; Heavy rains; Learning methods; Long short-term memory network; Memory network; Neural-networks; Preparedness and response; Short term memory; Strong winds; Satellite imagery";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
118;A decision-making framework for school infrastructure improvement programs;School infrastructure affects the quality of education and the performance of children and youth. Natural hazards such as earthquakes, hurricanes, floods, and landslides, threaten critical infrastructure such as school facilities. Additionally, problems related to the functionality of these facilities are common in the region, such as an inadequate number of classrooms, poor lighting, and insufficient ventilation, among others. At a national level, the decision-making process to prioritize schools’ interventions becomes even more challenging due to limited resources and lack of information. Furthermore, there is a lack of a systematic approach to address the need of improving existing infrastructure taking into consideration limited resources. Considering this, a novel decision-making framework is proposed that prioritizes school infrastructure investment with limited budgets, using clustering procedures, a multi-criteria utility function, and an optimization component. This framework allows better public policy decisions and benefits students in terms of buildings quality with a multi-criteria perspective, improving both safety and functional conditions. The framework is illustrated with a case study applied to the public-school infrastructure in the Dominican Republic. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.;"Budget allocation; clustering; decision-making; disaster risk reduction; multi-criteria; optimization; public-school infrastructure; school’s functionality";"Budget control; Hurricanes; Investments; Multiobjective optimization; Public policy; Budget allocation; Clusterings; Decisions makings; Disaster risk reductions; Multi-criteria; Multi-Criterion; Optimisations; Public schools; Public-school infrastructure; School’s functionality; Decision making";"A decision-making framework for school infrastructure improvement programs School infrastructure affects the quality of education and the performance of children and youth. Natural hazards such as earthquakes, hurricanes, floods, and landslides, threaten critical infrastructure such as school facilities. Additionally, problems related to the functionality of these facilities are common in the region, such as an inadequate number of classrooms, poor lighting, and insufficient ventilation, among others. At a national level, the decision-making process to prioritize schools’ interventions becomes even more challenging due to limited resources and lack of information. Furthermore, there is a lack of a systematic approach to address the need of improving existing infrastructure taking into consideration limited resources. Considering this, a novel decision-making framework is proposed that prioritizes school infrastructure investment with limited budgets, using clustering procedures, a multi-criteria utility function, and an optimization component. This framework allows better public policy decisions and benefits students in terms of buildings quality with a multi-criteria perspective, improving both safety and functional conditions. The framework is illustrated with a case study applied to the public-school infrastructure in the Dominican Republic. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. Budget allocation; clustering; decision-making; disaster risk reduction; multi-criteria; optimization; public-school infrastructure; school’s functionality Budget control; Hurricanes; Investments; Multiobjective optimization; Public policy; Budget allocation; Clusterings; Decisions makings; Disaster risk reductions; Multi-criteria; Multi-Criterion; Optimisations; Public schools; Public-school infrastructure; School’s functionality; Decision making";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;1;Prevention
119;Unleashing the Power of Dynamic Mode Decomposition and Deep Learning for Rainfall Prediction in North-East India;Accurate rainfall forecasting is crucial for effective disaster preparedness and mitigation in the North-East region of India, which is prone to extreme weather events such as floods and landslides. In this study, we investigated the use of two data-driven methods, dynamic mode decomposition (DMD) and long-short-term memory (LSTM), for rainfall forecasting using daily rainfall data collected from the India Meteorological Department in northeast region over a period of 122 years. We conducted a comparative analysis of these methods to determine their relative effectiveness in predicting rainfall patterns. Using historical rainfall data from multiple weather stations, we trained and validated our models to forecast future rainfall patterns. Our results indicate that both DMD and LSTM are effective in forecasting rainfall, with LSTM outperforming DMD in terms of accuracy, revealing that LSTM has the ability to capture complex nonlinear relationships in the data, making it a powerful tool for rainfall forecasting. The study reveals that the DMD method achieved Mean Squared Error (MSE) values ranging from 150.44 mm to 263.34 mm and Mean Absolute Error (MAE) values from 91.34 mm to 154.61 mm. In contrast, the Deep Learning (DL) approach, utilizing LSTM, demonstrated a normalized MAE value of 0.35 and a normalized RMSE value of 0.534. Our findings suggest that data-driven methods such as DMD and deep learning approaches like LSTM can significantly improve rainfall forecasting accuracy in the North-East region of India, helping to mitigate the impact of extreme weather events and enhance the region’s resilience to climate change. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Data driven methods; DMD; forecasting; LSTM; rainfall";"Extreme weather; Mean square error; Rain; Weather forecasting; Data-driven methods; Dynamic mode decompositions; Error values; Extreme weather events; Learning approach; Mean absolute error; Rainfall data; Rainfall forecasting; Rainfall patterns; Short term memory; Dynamic mode decomposition";"Unleashing the Power of Dynamic Mode Decomposition and Deep Learning for Rainfall Prediction in North-East India Accurate rainfall forecasting is crucial for effective disaster preparedness and mitigation in the North-East region of India, which is prone to extreme weather events such as floods and landslides. In this study, we investigated the use of two data-driven methods, dynamic mode decomposition (DMD) and long-short-term memory (LSTM), for rainfall forecasting using daily rainfall data collected from the India Meteorological Department in northeast region over a period of 122 years. We conducted a comparative analysis of these methods to determine their relative effectiveness in predicting rainfall patterns. Using historical rainfall data from multiple weather stations, we trained and validated our models to forecast future rainfall patterns. Our results indicate that both DMD and LSTM are effective in forecasting rainfall, with LSTM outperforming DMD in terms of accuracy, revealing that LSTM has the ability to capture complex nonlinear relationships in the data, making it a powerful tool for rainfall forecasting. The study reveals that the DMD method achieved Mean Squared Error (MSE) values ranging from 150.44 mm to 263.34 mm and Mean Absolute Error (MAE) values from 91.34 mm to 154.61 mm. In contrast, the Deep Learning (DL) approach, utilizing LSTM, demonstrated a normalized MAE value of 0.35 and a normalized RMSE value of 0.534. Our findings suggest that data-driven methods such as DMD and deep learning approaches like LSTM can significantly improve rainfall forecasting accuracy in the North-East region of India, helping to mitigate the impact of extreme weather events and enhance the region’s resilience to climate change. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Data driven methods; DMD; forecasting; LSTM; rainfall Extreme weather; Mean square error; Rain; Weather forecasting; Data-driven methods; Dynamic mode decompositions; Error values; Extreme weather events; Learning approach; Mean absolute error; Rainfall data; Rainfall forecasting; Rainfall patterns; Short term memory; Dynamic mode decomposition";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
120;A data-driven seismic fragility model for post-earthquake repairable performance of highway curved bridge portfolios;The rapid assessment of seismic fragility for highway curved bridges is crucial for enhancing the seismic resilience of transportation infrastructure. However, commonly used performance limit states fail to fully capture post-earthquake rehabilitation requirements. To address this, this paper proposes a data-driven, system-level seismic fragility assessment method for post-earthquake repairable performance of highway curved bridge portfolios. A sample database of curved bridge portfolios is generated using uniform design (UD) and a parametric finite element program, which then drives machine learning (ML) algorithms to develop a seismic fragility prediction model for post-earthquake repairable performance. The model is used to analyze the influence of various structural attributes on the median values of fragility curves (mR) for curved bridges. Additionally, a simplified fragility estimation method is developed based on the analysis results, and its reliability is validated through typical case studies. The results show that the central discrepancy of the UD sampling method is 0.0649, providing reliable and comprehensive data support for the ML prediction model. The fragility prediction model based on an artificial neural network (ANN) exhibits favorable fidelity and robustness. Curved bridges with central angles (α) in the range of 0[sbnd]0.5 rad show significant variations in mR, with a coupling effect observed between α and column height (H). The median absolute percentage error (MAPE) of the simplified fragility parameter calculation formula is below 11 %, offering a preliminary reference for assessing the post-earthquake repairable performance of curved bridges. © 2024 Institution of Structural Engineers;"Curved bridge; Machine learning; Post-earthquake repairable performance; Seismic fragility; Uniform design";"Earthquake effects; Highway bridges; Highway planning; Prediction models; Curved-bridge; Data driven; Machine-learning; Performance; Post-earthquake repairable performance; Prediction modelling; Rapid assessment; Seismic fragility; Seismic resilience; Uniform design; Seismic design";"A data-driven seismic fragility model for post-earthquake repairable performance of highway curved bridge portfolios The rapid assessment of seismic fragility for highway curved bridges is crucial for enhancing the seismic resilience of transportation infrastructure. However, commonly used performance limit states fail to fully capture post-earthquake rehabilitation requirements. To address this, this paper proposes a data-driven, system-level seismic fragility assessment method for post-earthquake repairable performance of highway curved bridge portfolios. A sample database of curved bridge portfolios is generated using uniform design (UD) and a parametric finite element program, which then drives machine learning (ML) algorithms to develop a seismic fragility prediction model for post-earthquake repairable performance. The model is used to analyze the influence of various structural attributes on the median values of fragility curves (mR) for curved bridges. Additionally, a simplified fragility estimation method is developed based on the analysis results, and its reliability is validated through typical case studies. The results show that the central discrepancy of the UD sampling method is 0.0649, providing reliable and comprehensive data support for the ML prediction model. The fragility prediction model based on an artificial neural network (ANN) exhibits favorable fidelity and robustness. Curved bridges with central angles (α) in the range of 0[sbnd]0.5 rad show significant variations in mR, with a coupling effect observed between α and column height (H). The median absolute percentage error (MAPE) of the simplified fragility parameter calculation formula is below 11 %, offering a preliminary reference for assessing the post-earthquake repairable performance of curved bridges. © 2024 Institution of Structural Engineers Curved bridge; Machine learning; Post-earthquake repairable performance; Seismic fragility; Uniform design Earthquake effects; Highway bridges; Highway planning; Prediction models; Curved-bridge; Data driven; Machine-learning; Performance; Post-earthquake repairable performance; Prediction modelling; Rapid assessment; Seismic fragility; Seismic resilience; Uniform design; Seismic design";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
121;AI and IoT Based Flood Detection and Alerting Model at River Dams;Number of people die from flooding each year, both in India and outside. Numerous factors can cause floods, such as excessive rainfall, tsunamis, or the failure of infrastructure used to hold water (such as dams, levels, and retention ponds). One can categorize a flood as either aperiodic or periodic. Periodic floods happen on rivers, while aperiodic floods are caused by heavy rain or water logging during wet seasons. Despite the fact that there are numerous methods for detecting floods, the findings are insufficient because the number of flood-related deaths rises year. Therefore, the development of a suitable technique for flood detection is necessary. Techniques for flood detection based on artificial intelligence (AI) and Internet of Things (IoT) are compared to meet the need. The three main technologies that are typically used to detect flooding are IoT, WSN, and MANET. IoT is the most effective technology among these three, with the ability to communicate and analyze issues. Numerous devices are used in this technique to sense. The precision of sensed data will also depend on where the gadget is installed. This study suggests an AI-based IoT model for early flood detection and risk reduction by warning residents living downstream of the river dam. To predict accurate values, the output data of IoT devices is fed to AI algorithms like Linear Regression, artificial neural networks, and support vector machine. ANN has a lower error rate than the other two. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Artificial Intelligence; Flood detection; IoT; Machine learning; MANET";NULL;"AI and IoT Based Flood Detection and Alerting Model at River Dams Number of people die from flooding each year, both in India and outside. Numerous factors can cause floods, such as excessive rainfall, tsunamis, or the failure of infrastructure used to hold water (such as dams, levels, and retention ponds). One can categorize a flood as either aperiodic or periodic. Periodic floods happen on rivers, while aperiodic floods are caused by heavy rain or water logging during wet seasons. Despite the fact that there are numerous methods for detecting floods, the findings are insufficient because the number of flood-related deaths rises year. Therefore, the development of a suitable technique for flood detection is necessary. Techniques for flood detection based on artificial intelligence (AI) and Internet of Things (IoT) are compared to meet the need. The three main technologies that are typically used to detect flooding are IoT, WSN, and MANET. IoT is the most effective technology among these three, with the ability to communicate and analyze issues. Numerous devices are used in this technique to sense. The precision of sensed data will also depend on where the gadget is installed. This study suggests an AI-based IoT model for early flood detection and risk reduction by warning residents living downstream of the river dam. To predict accurate values, the output data of IoT devices is fed to AI algorithms like Linear Regression, artificial neural networks, and support vector machine. ANN has a lower error rate than the other two. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Artificial Intelligence; Flood detection; IoT; Machine learning; MANET NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
122;Multi-Agent Q-Net Enhanced Coevolutionary Algorithm for Resource Allocation in Emergency Human-Machine Fusion UAV-MEC System;Unmanned aerial vehicle (UAV) assisted communication has emerged as a powerful technology for reliable and flexible emergency communications (e.g., earthquakes, hurricanes and floods), especially when the mobile infrastructure is seriously damaged. UAV assisted mobile edge computing (UAV-MEC) system can be deployed in the natural disaster area as communication relay or air mobile base stations to resume communication and provide computing resources for the users in disaster areas. However, the optimized resource allocation performance of UAV-MEC system can be further guaranteed with the human-fusion decision making. In this paper, we construct an emergency human-machine fusion UAV-MEC system consisting of multiple UAVs equipped with computing resources, and the human-machine decision makings are fused for UAV deployment. In order to solve the resource allocation problem of human-machine fusion UAV-MEC system, we establish an human-machine deep integration model for UAV-MEC system, and the UAVs are dispatched reasonably through human-machine fusion decision makings to maintain efficient communication in emergency communication areas. To minimize task latency and improve the computation efficiency in emergency human-machine fusion UAV-MEC system, we consider the number of dispatched UAVs, deployment plans, flight plans, and simultaneously optimize the task allocation scheme, priority order, and task offloading ratio. We propose a reinforcement learning framework combined with evolutionary algorithms, which is named as multi-agent Q-net enhanced cooperative genetic algorithm (MQCGA), for resource allocation of UAV. Based on neural network forecasts, the greedy rate during training processing can be dynamically controlled, and the learning ability of different agents can be strengthened. Simulation experiments are conducted to evaluate the proposed framework, and the results show that our proposed MQCGA algorithm is significantly superior to other algorithms in terms of latency and energy consumption. Note to Practitioners - With the development of MEC and the popularity of UAVs, the potential of UAV-assisted MEC draw much attention from industrial field. Considering the lack of communication capabilities in a certain area in an unexpected situation, UAVs can be quickly deployed to corresponding locations and provide computing services. In this paper, an UAV-MEC system that integrates human-machine decision-making for emergency communication situations, named human-machine fusion UAV-MEC system, is considered. The system divides the scene into regions, models users and UAVs, and provides detailed deployment schemes, maximizing the practicality and applicability of the scene. In order to improve the communication efficiency in the case of emergency communication, this paper proposes a new resource scheduling algorithm and adds human-machine decision-making to enable UAVs to continuously provide efficient services for a certain area. The experimental results provide practitioners with a theoretical basis, such as the task completion time, UAV energy consumption and computing resource scheduling. Applying the system to actual scenarios also requires two preconditions of the system, one is the information collected by the large UAV, and the other is the communication among the UAVs. These two preconditions facilitate the human-machine fusion UAV-MEC system deployment in practical applications.  © 2024 IEEE.;"cooperative genetic algorithm; emergency environment; Human-machine fusion UAV-MEC; reinforcement learning";"Antennas; Computation offloading; Decision making; Energy utilization; Genetic algorithms; Job analysis; Mobile edge computing; Multi agent systems; Resource allocation; Unmanned aerial vehicles (UAV); Vehicle to vehicle communications; Aerial vehicle; Cooperative genetic algorithm; Decisions makings; Emergency environment; Human-machine; Human-machine fusion unmanned aerial vehicle-MEC; Human-machine systems; Reinforcement learnings; Resource management; Task analysis; Reinforcement learning";"Multi-Agent Q-Net Enhanced Coevolutionary Algorithm for Resource Allocation in Emergency Human-Machine Fusion UAV-MEC System Unmanned aerial vehicle (UAV) assisted communication has emerged as a powerful technology for reliable and flexible emergency communications (e.g., earthquakes, hurricanes and floods), especially when the mobile infrastructure is seriously damaged. UAV assisted mobile edge computing (UAV-MEC) system can be deployed in the natural disaster area as communication relay or air mobile base stations to resume communication and provide computing resources for the users in disaster areas. However, the optimized resource allocation performance of UAV-MEC system can be further guaranteed with the human-fusion decision making. In this paper, we construct an emergency human-machine fusion UAV-MEC system consisting of multiple UAVs equipped with computing resources, and the human-machine decision makings are fused for UAV deployment. In order to solve the resource allocation problem of human-machine fusion UAV-MEC system, we establish an human-machine deep integration model for UAV-MEC system, and the UAVs are dispatched reasonably through human-machine fusion decision makings to maintain efficient communication in emergency communication areas. To minimize task latency and improve the computation efficiency in emergency human-machine fusion UAV-MEC system, we consider the number of dispatched UAVs, deployment plans, flight plans, and simultaneously optimize the task allocation scheme, priority order, and task offloading ratio. We propose a reinforcement learning framework combined with evolutionary algorithms, which is named as multi-agent Q-net enhanced cooperative genetic algorithm (MQCGA), for resource allocation of UAV. Based on neural network forecasts, the greedy rate during training processing can be dynamically controlled, and the learning ability of different agents can be strengthened. Simulation experiments are conducted to evaluate the proposed framework, and the results show that our proposed MQCGA algorithm is significantly superior to other algorithms in terms of latency and energy consumption. Note to Practitioners - With the development of MEC and the popularity of UAVs, the potential of UAV-assisted MEC draw much attention from industrial field. Considering the lack of communication capabilities in a certain area in an unexpected situation, UAVs can be quickly deployed to corresponding locations and provide computing services. In this paper, an UAV-MEC system that integrates human-machine decision-making for emergency communication situations, named human-machine fusion UAV-MEC system, is considered. The system divides the scene into regions, models users and UAVs, and provides detailed deployment schemes, maximizing the practicality and applicability of the scene. In order to improve the communication efficiency in the case of emergency communication, this paper proposes a new resource scheduling algorithm and adds human-machine decision-making to enable UAVs to continuously provide efficient services for a certain area. The experimental results provide practitioners with a theoretical basis, such as the task completion time, UAV energy consumption and computing resource scheduling. Applying the system to actual scenarios also requires two preconditions of the system, one is the information collected by the large UAV, and the other is the communication among the UAVs. These two preconditions facilitate the human-machine fusion UAV-MEC system deployment in practical applications.  © 2024 IEEE. cooperative genetic algorithm; emergency environment; Human-machine fusion UAV-MEC; reinforcement learning Antennas; Computation offloading; Decision making; Energy utilization; Genetic algorithms; Job analysis; Mobile edge computing; Multi agent systems; Resource allocation; Unmanned aerial vehicles (UAV); Vehicle to vehicle communications; Aerial vehicle; Cooperative genetic algorithm; Decisions makings; Emergency environment; Human-machine; Human-machine fusion unmanned aerial vehicle-MEC; Human-machine systems; Reinforcement learnings; Resource management; Task analysis; Reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;3;Response
123;Unveiling the spatial heterogeneity of factors influencing physical and perceived recovery disparities under extreme rainstorms: A geographically weighted machine learning approach;Effectively allocating resources to address both the physical recovery of infrastructure and subjective needs of residents is crucial to safeguard the well-being of disaster-affected communities. Traditional recovery strategies often prioritize infrastructure restoration, leading to disparities between physical recovery and residents’ perceived recovery, shaped by multiple factors. However, understanding the factors influencing recovery disparities and their spatial heterogeneity remains challenging. To fill the gap, this study introduces a geographically weighted machine learning model to explore localized variations in factors shaping recovery disparities. By treating recovery disparities as dependent variables, we analyzed 779 communities affected by the “23·7″ extreme rainstorm in Beijing in 2023. The results reveal that 86.1 % of communities exhibit negative recovery disparities, while 13.9 % display positive disparities. All factors exhibit significant spatial heterogeneity of local coefficients (%IncMSE) across regions. Additionally, some factors show nonlinearity and threshold effects, with high vegetation (>6000), intense precipitation (>400 mm), and high population densities (>17,500 people/km2) causing the largest recovery disparities. This study presents a novel framework for disaster recovery planning that addresses both physical restoration and residents’ perceptions, providing insights for policymakers to reduce recovery gaps and promote resilient recovery processes. © 2024;"Public perception; Recovery disparity; Residents’ needs; Spatial machine learning; Urban resilience";"Beijing [Beijing (ADS)]; Beijing [China]; China; Adversarial machine learning; Contrastive Learning; Disasters; Extreme weather; Population statistics; Rain; Restoration; Machine learning approaches; Machine-learning; Public perception; Recovery disparity; Recovery strategies; Resident’ need; Spatial heterogeneity; Spatial machine learning; Urban resilience; Well being; disaster management; extreme event; heterogeneity; machine learning; perception; public attitude; residential location; spatial analysis; urban area; Storms";"Unveiling the spatial heterogeneity of factors influencing physical and perceived recovery disparities under extreme rainstorms: A geographically weighted machine learning approach Effectively allocating resources to address both the physical recovery of infrastructure and subjective needs of residents is crucial to safeguard the well-being of disaster-affected communities. Traditional recovery strategies often prioritize infrastructure restoration, leading to disparities between physical recovery and residents’ perceived recovery, shaped by multiple factors. However, understanding the factors influencing recovery disparities and their spatial heterogeneity remains challenging. To fill the gap, this study introduces a geographically weighted machine learning model to explore localized variations in factors shaping recovery disparities. By treating recovery disparities as dependent variables, we analyzed 779 communities affected by the “23·7″ extreme rainstorm in Beijing in 2023. The results reveal that 86.1 % of communities exhibit negative recovery disparities, while 13.9 % display positive disparities. All factors exhibit significant spatial heterogeneity of local coefficients (%IncMSE) across regions. Additionally, some factors show nonlinearity and threshold effects, with high vegetation (>6000), intense precipitation (>400 mm), and high population densities (>17,500 people/km2) causing the largest recovery disparities. This study presents a novel framework for disaster recovery planning that addresses both physical restoration and residents’ perceptions, providing insights for policymakers to reduce recovery gaps and promote resilient recovery processes. © 2024 Public perception; Recovery disparity; Residents’ needs; Spatial machine learning; Urban resilience Beijing [Beijing (ADS)]; Beijing [China]; China; Adversarial machine learning; Contrastive Learning; Disasters; Extreme weather; Population statistics; Rain; Restoration; Machine learning approaches; Machine-learning; Public perception; Recovery disparity; Recovery strategies; Resident’ need; Spatial heterogeneity; Spatial machine learning; Urban resilience; Well being; disaster management; extreme event; heterogeneity; machine learning; perception; public attitude; residential location; spatial analysis; urban area; Storms";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;4;Recovery
124;Mitigating Blast-Induced Slope Failure in Railway Infrastructure: A Machine Learning Approach for Risk Assessment and Rapid Decision-Making;The Konkan Railway, excavation work completed in 1998 and serving India since then, faces climate-related challenges such as heavy monsoon rains, rockfalls, and slope failures that threaten railway operations in regard of train accidents and traffic interruptions. While the railway has implemented various geotechnical strengthening measures, concerns arise as climate change leads to more extreme weather events. Flattening slopes helps improve the stability of the railway track by reducing the risk of landslides, soil erosion, and boulder failures. A flatter slope contributes to safer and smoother train operations and is less prone to geological hazards. Railway infrastructure management involves numerous challenges, particularly in ensuring the stability and safety of slopes adjacent to the tracks. This research focuses on applying machine learning (ML) techniques in slope reconstruction works using controlled blasting to address these challenges. Blast-induced slope failure or rockfall during blasting operations are critical concerns that may interrupt normal traffic operations. They need accurate prediction for effective risk mitigation and rescheduling of traffic operations. Initially, at three locations of Konkan Railways, ML models have been developed using 490 datasets with the most inputs out of thirteen parameters using multicollinearity and Logistic Regression techniques based on minimum Akaike Information Criterion (AIC) values. This paper narrates the use of the best-trained Random Forest model for the rapid assessment of such failure during day-to-day blasts at two slope reconstruction sites of the Konkan Railway. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Controlled Blasting; Machine Learning Approach; Railway Infrastructure Management; Slope Reconstruction";"Blasting; Landslides; Logistic regression; Railroad accidents; Railroad transportation; Railroads; Risk management; Risk perception; Weather forecasting; Controlled blasting; Infrastructure managements; Machine learning approaches; Railway infrastructure; Railway infrastructure management; Risks assessments; Rockfalls; Slope failure; Slope reconstruction; Traffic operation; blasting; decision making; machine learning; risk assessment; slope dynamics; slope failure; Risk assessment";"Mitigating Blast-Induced Slope Failure in Railway Infrastructure: A Machine Learning Approach for Risk Assessment and Rapid Decision-Making The Konkan Railway, excavation work completed in 1998 and serving India since then, faces climate-related challenges such as heavy monsoon rains, rockfalls, and slope failures that threaten railway operations in regard of train accidents and traffic interruptions. While the railway has implemented various geotechnical strengthening measures, concerns arise as climate change leads to more extreme weather events. Flattening slopes helps improve the stability of the railway track by reducing the risk of landslides, soil erosion, and boulder failures. A flatter slope contributes to safer and smoother train operations and is less prone to geological hazards. Railway infrastructure management involves numerous challenges, particularly in ensuring the stability and safety of slopes adjacent to the tracks. This research focuses on applying machine learning (ML) techniques in slope reconstruction works using controlled blasting to address these challenges. Blast-induced slope failure or rockfall during blasting operations are critical concerns that may interrupt normal traffic operations. They need accurate prediction for effective risk mitigation and rescheduling of traffic operations. Initially, at three locations of Konkan Railways, ML models have been developed using 490 datasets with the most inputs out of thirteen parameters using multicollinearity and Logistic Regression techniques based on minimum Akaike Information Criterion (AIC) values. This paper narrates the use of the best-trained Random Forest model for the rapid assessment of such failure during day-to-day blasts at two slope reconstruction sites of the Konkan Railway. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Controlled Blasting; Machine Learning Approach; Railway Infrastructure Management; Slope Reconstruction Blasting; Landslides; Logistic regression; Railroad accidents; Railroad transportation; Railroads; Risk management; Risk perception; Weather forecasting; Controlled blasting; Infrastructure managements; Machine learning approaches; Railway infrastructure; Railway infrastructure management; Risks assessments; Rockfalls; Slope failure; Slope reconstruction; Traffic operation; blasting; decision making; machine learning; risk assessment; slope dynamics; slope failure; Risk assessment";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
125;Analysis of vegetation dynamics from 2001 to 2020 in China's Ganzhou rare earth mining area using time series remote sensing and SHAP-enhanced machine learning;Rare earth mining, essential for modern industries and economic growth, often leads to severe environmental degradation. Previous research has explored the ecological impacts of rare earth mining but has not fully investigated the intricate interplay and subdivisions of environmental and anthropogenic factors driving vegetation changes over extended periods. This study addresses this gap by employing time series remote sensing and SHAP-enhanced machine learning to analyze vegetation dynamics in China's Ganzhou rare earth mining area from 2001 to 2020. Using the kNDVI derived from Landsat data, we identified three distinct vegetation trajectory types: pro-environment, des-environment, and res-environment. An ensemble machine learning model combined with SHAP analysis revealed the cropland area proportion, PM10 levels, and shrubland area proportion as the most influential factors affecting vegetation across all mining types. Additionally, after 2012, the palmer drought severity index and downward surface shortwave radiation emerged as positive contributors to vegetation health, while population pressure had a more substantial negative influence in des-environment areas. Our findings highlight spatial heterogeneity in vegetation recovery patterns and highlight the complex interactions among land cover changes, air quality, climate factors, and human activities in shaping vegetation dynamics. This study provides valuable insights for developing targeted, context-specific restoration strategies in rare earth mining areas, contributing to more sustainable mining practices and global environmental management. © 2024;"kNDVI; Machine learning; Rare earth mining area; Restoration; SHapley additive exPlanations";"China; Ganzhou; Jiangxi; drought; environmental management; land cover; Landsat; machine learning; population pressure; rare earth element; remote sensing; shortwave radiation; shrubland; time series; vegetation dynamics";"Analysis of vegetation dynamics from 2001 to 2020 in China's Ganzhou rare earth mining area using time series remote sensing and SHAP-enhanced machine learning Rare earth mining, essential for modern industries and economic growth, often leads to severe environmental degradation. Previous research has explored the ecological impacts of rare earth mining but has not fully investigated the intricate interplay and subdivisions of environmental and anthropogenic factors driving vegetation changes over extended periods. This study addresses this gap by employing time series remote sensing and SHAP-enhanced machine learning to analyze vegetation dynamics in China's Ganzhou rare earth mining area from 2001 to 2020. Using the kNDVI derived from Landsat data, we identified three distinct vegetation trajectory types: pro-environment, des-environment, and res-environment. An ensemble machine learning model combined with SHAP analysis revealed the cropland area proportion, PM10 levels, and shrubland area proportion as the most influential factors affecting vegetation across all mining types. Additionally, after 2012, the palmer drought severity index and downward surface shortwave radiation emerged as positive contributors to vegetation health, while population pressure had a more substantial negative influence in des-environment areas. Our findings highlight spatial heterogeneity in vegetation recovery patterns and highlight the complex interactions among land cover changes, air quality, climate factors, and human activities in shaping vegetation dynamics. This study provides valuable insights for developing targeted, context-specific restoration strategies in rare earth mining areas, contributing to more sustainable mining practices and global environmental management. © 2024 kNDVI; Machine learning; Rare earth mining area; Restoration; SHapley additive exPlanations China; Ganzhou; Jiangxi; drought; environmental management; land cover; Landsat; machine learning; population pressure; rare earth element; remote sensing; shortwave radiation; shrubland; time series; vegetation dynamics";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
126;A Novel Deep Learning Model for Flood Detection from Synthetic Aperture Radar Images;Flooding, a common natural disaster, causes widespread damage globally. Detecting flood extents rapidly and accurately using Synthetic Aperture Radar (SAR) images is crucial for effective disaster response and mitigation. This paper proposes a novel machine learning model specifically designed for SAR image analysis to detect floodwaters. The model leverages change detection techniques and operates on pairs of satellite images captured at different time points. The feature extraction module employs a parallel Siamese architecture with a Swin-Transformer backbone to extract features at various levels. Prior to entering the decoding module, the features undergo enhancement by computing the difference between feature maps at the same level. The decoding process predicts changing regions at each level and integrates them into the final result. Experimental results demonstrate that our proposed model outperforms other methods, achieving a recall of 94.6%, a precision of 96.9%, and an F1-score of 95.7%, with a computational cost of 32.3 G FLOPs. © 2025 by the authors.;"deep learning model; flood detection; Swin-Transformer; Synthetic Aperture Radar (SAR) image; vision transformer";NULL;"A Novel Deep Learning Model for Flood Detection from Synthetic Aperture Radar Images Flooding, a common natural disaster, causes widespread damage globally. Detecting flood extents rapidly and accurately using Synthetic Aperture Radar (SAR) images is crucial for effective disaster response and mitigation. This paper proposes a novel machine learning model specifically designed for SAR image analysis to detect floodwaters. The model leverages change detection techniques and operates on pairs of satellite images captured at different time points. The feature extraction module employs a parallel Siamese architecture with a Swin-Transformer backbone to extract features at various levels. Prior to entering the decoding module, the features undergo enhancement by computing the difference between feature maps at the same level. The decoding process predicts changing regions at each level and integrates them into the final result. Experimental results demonstrate that our proposed model outperforms other methods, achieving a recall of 94.6%, a precision of 96.9%, and an F1-score of 95.7%, with a computational cost of 32.3 G FLOPs. © 2025 by the authors. deep learning model; flood detection; Swin-Transformer; Synthetic Aperture Radar (SAR) image; vision transformer NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
127;IoT-based Intelligent Power Supply Management Using Ensemble Learning for Seismic Observation Stations;Seismic observation stations perform a vital part in monitoring and analyzing seismic activity for early warning and disaster preparedness. This paper investigates the integration of an IoT-based intelligent power supply management model to improve station reliability and effectiveness. Traditional systems often suffer from reliability issues and inadequate monitoring, impacting timely seismic data delivery during critical events. The study employs IoT sensors for real-time monitoring of voltage, current, battery status, and environmental conditions. Data are centralized for analysis, leveraging the SeismoGuard Ensemble classifier—a novel machine learning model combining Random Forest, SVM, and KNN models with a Logistic Regression meta-classifier. The novelty lies in its distinctive blend of Random Forest, SVM, KNN, and Logistic Regression improves predictive accuracy and robustness in power supply handling for seismic observation stations. This approach improves forecasting accuracy and robustness in preventing power failures, achieving high prediction measurements like accuracy (90%), precision (88%), recall (91%), and F1-score (89%). Implementation leads to enhanced data transmission throughput and packet delivery ratio, ensuring reduced downtime and increased resilience during seismic events. Integrating IoT technologies in power supply management offers substantial benefits, including enhanced reliability and operational continuity, vital for effective seismic monitoring and early warning systems. © 2025 Slovene Society Informatika. All rights reserved.;"iot-based intelligent power supply management system; operational continuity; power failures prediction; seismic observation stations; seismoguard ensemble classifier";"Earthquake effects; Network security; Seismic response; Seismographs; Steganography; Uninterruptible power systems; Ensemble-classifier; Failures prediction; Intelligent power; Iot-based intelligent power supply management system; Operational continuity; Power failure; Power failure prediction; Power supply; Seismic observation; Seismic observation station; Seismoguard ensemble classifier; Supply management system; Logistic regression";"IoT-based Intelligent Power Supply Management Using Ensemble Learning for Seismic Observation Stations Seismic observation stations perform a vital part in monitoring and analyzing seismic activity for early warning and disaster preparedness. This paper investigates the integration of an IoT-based intelligent power supply management model to improve station reliability and effectiveness. Traditional systems often suffer from reliability issues and inadequate monitoring, impacting timely seismic data delivery during critical events. The study employs IoT sensors for real-time monitoring of voltage, current, battery status, and environmental conditions. Data are centralized for analysis, leveraging the SeismoGuard Ensemble classifier—a novel machine learning model combining Random Forest, SVM, and KNN models with a Logistic Regression meta-classifier. The novelty lies in its distinctive blend of Random Forest, SVM, KNN, and Logistic Regression improves predictive accuracy and robustness in power supply handling for seismic observation stations. This approach improves forecasting accuracy and robustness in preventing power failures, achieving high prediction measurements like accuracy (90%), precision (88%), recall (91%), and F1-score (89%). Implementation leads to enhanced data transmission throughput and packet delivery ratio, ensuring reduced downtime and increased resilience during seismic events. Integrating IoT technologies in power supply management offers substantial benefits, including enhanced reliability and operational continuity, vital for effective seismic monitoring and early warning systems. © 2025 Slovene Society Informatika. All rights reserved. iot-based intelligent power supply management system; operational continuity; power failures prediction; seismic observation stations; seismoguard ensemble classifier Earthquake effects; Network security; Seismic response; Seismographs; Steganography; Uninterruptible power systems; Ensemble-classifier; Failures prediction; Intelligent power; Iot-based intelligent power supply management system; Operational continuity; Power failure; Power failure prediction; Power supply; Seismic observation; Seismic observation station; Seismoguard ensemble classifier; Supply management system; Logistic regression";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
128;Evaluation of Machine Learning Methods for Fire Risk Assessment from Satellite Imagery;Recognising the critical role forests play in global biodiversity and the increasing threat of wildfires, this work exploits advanced geoscientific technologies and machine learning techniques to improve fire risk prediction and management. The primary objective is to develop a Convolutional Neural Network (CNN) that maps remotely sensed images to fire risk levels using a refined subset of the FireRisk dataset. The employed dataset contains 7,644 images categorised into five fire risk classes. Based on it, this work benchmarks the performance of InceptionResNetV2 and Vision Transformer models, which have been pre-trained on extensive datasets and fine-tuned for fire risk classification. The achieved custom CNN model achieves an accuracy and F1 score of 72%, demonstrating its potential for this application. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"CNN; Emergency management; Fire risk assessment";"Adversarial machine learning; Contrastive Learning; Convolutional neural networks; Risk assessment; Risk management; Convolutional neural network; Emergency management; Fire risk assessment; Fire risks; Geoscientific technologies; Machine learning methods; Machine learning techniques; Risk predictions; Risks management; Technology learning; Premixed flames";"Evaluation of Machine Learning Methods for Fire Risk Assessment from Satellite Imagery Recognising the critical role forests play in global biodiversity and the increasing threat of wildfires, this work exploits advanced geoscientific technologies and machine learning techniques to improve fire risk prediction and management. The primary objective is to develop a Convolutional Neural Network (CNN) that maps remotely sensed images to fire risk levels using a refined subset of the FireRisk dataset. The employed dataset contains 7,644 images categorised into five fire risk classes. Based on it, this work benchmarks the performance of InceptionResNetV2 and Vision Transformer models, which have been pre-trained on extensive datasets and fine-tuned for fire risk classification. The achieved custom CNN model achieves an accuracy and F1 score of 72%, demonstrating its potential for this application. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. CNN; Emergency management; Fire risk assessment Adversarial machine learning; Contrastive Learning; Convolutional neural networks; Risk assessment; Risk management; Convolutional neural network; Emergency management; Fire risk assessment; Fire risks; Geoscientific technologies; Machine learning methods; Machine learning techniques; Risk predictions; Risks management; Technology learning; Premixed flames";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
129;Predicting Monthly River Discharge Using Bayesian Optimisation-Based SVR Model;Accurate prediction of monthly river discharge is essential for effective water resource management, flood control, and environmental planning. In this study, monthly river discharge prediction at Adityapur station in the Subarnarekha River basin is conducted using Support Vector Regression (SVR) in conjunction with Bayesian optimization to optimize SVR model hyperparameters, and the resulting model is compared with a simpler SVR variant. The model has been developed using ten years of monthly discharge data to simulate the period spanning from 2009 to 2019. Model performance is assessed and compared using evaluation metrics including Root Mean Square Error (RMSE), R-squared (R2), and Nash–Sutcliffe Efficiency (NSE). The Results show that the accuracy of the Bayesian optimization-based SVR model is high and better than the SVR model. The R2 values for Bayesian optimization-based SVR and SVR models are 0.9358 and 0.8859 respectively. Similarly, The RMSE values for Bayesian optimization-based SVR and SVR models are 50.106 and 66.84 m3/s and NSE values are 0.9238 and 0.7772 respectively. Based on the results, it is concluded that the model’s efficiency can be increased by optimizing the SVR model’s hyperparameters using Bayesian optimization. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.;"Bayesian optimization; Discharge prediction; Subarnarekha River basin; Support vector regression (SVR)";"Environmental design; Environmental management; Natural resources management; Prediction models; Resource allocation; Rivers; Support vector regression; Bayesian optimization; Discharge predictions; Hyper-parameter; River basins; River discharge; Root mean square errors; Subarnarekha river basin; Support vector regression; Support vector regression models; Support vector regressions; Efficiency";"Predicting Monthly River Discharge Using Bayesian Optimisation-Based SVR Model Accurate prediction of monthly river discharge is essential for effective water resource management, flood control, and environmental planning. In this study, monthly river discharge prediction at Adityapur station in the Subarnarekha River basin is conducted using Support Vector Regression (SVR) in conjunction with Bayesian optimization to optimize SVR model hyperparameters, and the resulting model is compared with a simpler SVR variant. The model has been developed using ten years of monthly discharge data to simulate the period spanning from 2009 to 2019. Model performance is assessed and compared using evaluation metrics including Root Mean Square Error (RMSE), R-squared (R2), and Nash–Sutcliffe Efficiency (NSE). The Results show that the accuracy of the Bayesian optimization-based SVR model is high and better than the SVR model. The R2 values for Bayesian optimization-based SVR and SVR models are 0.9358 and 0.8859 respectively. Similarly, The RMSE values for Bayesian optimization-based SVR and SVR models are 50.106 and 66.84 m3/s and NSE values are 0.9238 and 0.7772 respectively. Based on the results, it is concluded that the model’s efficiency can be increased by optimizing the SVR model’s hyperparameters using Bayesian optimization. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025. Bayesian optimization; Discharge prediction; Subarnarekha River basin; Support vector regression (SVR) Environmental design; Environmental management; Natural resources management; Prediction models; Resource allocation; Rivers; Support vector regression; Bayesian optimization; Discharge predictions; Hyper-parameter; River basins; River discharge; Root mean square errors; Subarnarekha river basin; Support vector regression; Support vector regression models; Support vector regressions; Efficiency";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
130;Block-level spatial integration of population density, social vulnerability, and heavy precipitation reveals intensified urban flooding risk;Under the context of global warming and rapid urbanization, cities worldwide confront the pressing problem of urban waterlogging, hindering progress towards Sustainable Development Goals. Effective planning and mitigation of urban flooding require a comprehensive understanding of the spatial and temporal patterns of rainfall and risk heterogeneity. However, evaluating urban water-logging risk is challenged by the need for city-scale hydrological simulation and generally lacks comprehensive metrics integrating fine-scale datasets. To address these gaps, we developed a simulation method for urban flood hazards by integrating hydrological models and Random Forest algorithms. We then took Shenzhen, a megacity in China, as a case study, and investigated the spatial patterns of urban flooding risk and its determinants at the block level based on the risk assessment framework represented by Hazards-Exposure-Vulnerability (H-E-V) dimensions. We found that socio-economic indicators exhibited spatial clustering, while hazard-related indicators displayed more dispersed patterns. High-risk areas exhibited a highly heterogeneous spatial pattern, predominantly influenced by vulnerability and exposure factors, as well as the spatial mismatch among the three dimensions. Our results emphasize the importance of integrating spatial heterogeneity of exposure and vulnerability into climate adaptation resource allocation, addressing both current and future demands for effective climate mitigation. © 2024 Elsevier Ltd;"H-E-V framework; Random forest; Risk assessment; Shenzhen; Urban flooding";"China; Guangdong; Shenzhen; Flooding risks; Hazard-exposure-vulnerability framework; Population densities; Random forests; Risks assessments; Shenzhen; Social vulnerability; Spatial integrations; Spatial patterns; Urban flooding; algorithm; computer simulation; flood; hydrological modeling; population density; precipitation (climatology); resource allocation; risk assessment; Sustainable Development Goal; urban area; urban planning; vulnerability; Digital elevation model";"Block-level spatial integration of population density, social vulnerability, and heavy precipitation reveals intensified urban flooding risk Under the context of global warming and rapid urbanization, cities worldwide confront the pressing problem of urban waterlogging, hindering progress towards Sustainable Development Goals. Effective planning and mitigation of urban flooding require a comprehensive understanding of the spatial and temporal patterns of rainfall and risk heterogeneity. However, evaluating urban water-logging risk is challenged by the need for city-scale hydrological simulation and generally lacks comprehensive metrics integrating fine-scale datasets. To address these gaps, we developed a simulation method for urban flood hazards by integrating hydrological models and Random Forest algorithms. We then took Shenzhen, a megacity in China, as a case study, and investigated the spatial patterns of urban flooding risk and its determinants at the block level based on the risk assessment framework represented by Hazards-Exposure-Vulnerability (H-E-V) dimensions. We found that socio-economic indicators exhibited spatial clustering, while hazard-related indicators displayed more dispersed patterns. High-risk areas exhibited a highly heterogeneous spatial pattern, predominantly influenced by vulnerability and exposure factors, as well as the spatial mismatch among the three dimensions. Our results emphasize the importance of integrating spatial heterogeneity of exposure and vulnerability into climate adaptation resource allocation, addressing both current and future demands for effective climate mitigation. © 2024 Elsevier Ltd H-E-V framework; Random forest; Risk assessment; Shenzhen; Urban flooding China; Guangdong; Shenzhen; Flooding risks; Hazard-exposure-vulnerability framework; Population densities; Random forests; Risks assessments; Shenzhen; Social vulnerability; Spatial integrations; Spatial patterns; Urban flooding; algorithm; computer simulation; flood; hydrological modeling; population density; precipitation (climatology); resource allocation; risk assessment; Sustainable Development Goal; urban area; urban planning; vulnerability; Digital elevation model";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
131;Leveraging artificial neural networks for robust landslide susceptibility mapping: A geospatial modeling approach in the ecologically sensitive Nilgiri District, Tamil Nadu;Landslides pose a significant threat to the lives and livelihoods of marginalised communities residing in rural areas and the delicate ecological balance of the environment. Implementing advanced technologies is crucial for improving hazard risk assessment and enhancing preparedness measures in regions characterised by diverse topography and complex geological formations. Geospatial applications and modelling techniques have emerged as indispensable in mitigating landslide risks, particularly in environmentally sensitive areas. This study presents a comprehensive approach to landslide susceptibility mapping in the Nilgiri District of Tamil Nadu, India, leveraging the power of Artificial Neural Networks (ANNs) and integrating multi-dimensional geospatial datasets. Integrating ANN-based modelling and geospatial techniques offers significant advantages in terms of statistical robustness, reproducibility, and the ability to analyze the complex interplay of factors influencing landslide hazards quantitatively. The methodology involves rigorous pre-processing and integrating spatial data, including landslide event occurrences as the dependent variable and ten independent parameters influencing landslide susceptibility. These parameters encompass elevation, slope aspect, slope degree, distance to roads, land use patterns, geomorphology, lithology, drainage density, lineament density, and rainfall distribution. Feature extraction and selection techniques are employed to effectively model the complex interactions between these factors and landslide occurrences. This process identifies the most relevant variables influencing landslide susceptibility, enhancing the model's predictive capabilities. The state-of-the-art ANNs are trained using historical landslide occurrence data and the selected influencing factors, enabling the development of a robust and accurate landslide susceptibility model. The performance of the developed model is rigorously evaluated using a comprehensive suite of metrics, including accuracy, precision, and the Area under the Receiver Operating Characteristic (ROC) curve. Preliminary results indicate that the ANN-based landslide susceptibility model outperforms traditional zonation methods, demonstrating higher accuracy and reliability in predicting landslide-prone areas. The resulting Landslide Susceptibility Map (LSM) categorises the study area into five distinct hazard zones, ranging from very high (664.1 ​km2), high (598.9 ​km2), moderate (639.7 ​km2), low (478.9 ​km2) and to very low (170.9 ​km2). Notably, the eastern and central regions of the district emerge as particularly vulnerable to landslide occurrences. The study's findings have far-reaching implications for disaster risk reduction efforts, land-use planning, and sustainable development strategies in the ecologically sensitive Nilgiri District and beyond. © 2024 The Author(s);"Artificial neural networks; Feature importance analysis; Geospatial modeling; Landslide susceptibility mapping; Risk management strategies";NULL;"Leveraging artificial neural networks for robust landslide susceptibility mapping: A geospatial modeling approach in the ecologically sensitive Nilgiri District, Tamil Nadu Landslides pose a significant threat to the lives and livelihoods of marginalised communities residing in rural areas and the delicate ecological balance of the environment. Implementing advanced technologies is crucial for improving hazard risk assessment and enhancing preparedness measures in regions characterised by diverse topography and complex geological formations. Geospatial applications and modelling techniques have emerged as indispensable in mitigating landslide risks, particularly in environmentally sensitive areas. This study presents a comprehensive approach to landslide susceptibility mapping in the Nilgiri District of Tamil Nadu, India, leveraging the power of Artificial Neural Networks (ANNs) and integrating multi-dimensional geospatial datasets. Integrating ANN-based modelling and geospatial techniques offers significant advantages in terms of statistical robustness, reproducibility, and the ability to analyze the complex interplay of factors influencing landslide hazards quantitatively. The methodology involves rigorous pre-processing and integrating spatial data, including landslide event occurrences as the dependent variable and ten independent parameters influencing landslide susceptibility. These parameters encompass elevation, slope aspect, slope degree, distance to roads, land use patterns, geomorphology, lithology, drainage density, lineament density, and rainfall distribution. Feature extraction and selection techniques are employed to effectively model the complex interactions between these factors and landslide occurrences. This process identifies the most relevant variables influencing landslide susceptibility, enhancing the model's predictive capabilities. The state-of-the-art ANNs are trained using historical landslide occurrence data and the selected influencing factors, enabling the development of a robust and accurate landslide susceptibility model. The performance of the developed model is rigorously evaluated using a comprehensive suite of metrics, including accuracy, precision, and the Area under the Receiver Operating Characteristic (ROC) curve. Preliminary results indicate that the ANN-based landslide susceptibility model outperforms traditional zonation methods, demonstrating higher accuracy and reliability in predicting landslide-prone areas. The resulting Landslide Susceptibility Map (LSM) categorises the study area into five distinct hazard zones, ranging from very high (664.1 ​km2), high (598.9 ​km2), moderate (639.7 ​km2), low (478.9 ​km2) and to very low (170.9 ​km2). Notably, the eastern and central regions of the district emerge as particularly vulnerable to landslide occurrences. The study's findings have far-reaching implications for disaster risk reduction efforts, land-use planning, and sustainable development strategies in the ecologically sensitive Nilgiri District and beyond. © 2024 The Author(s) Artificial neural networks; Feature importance analysis; Geospatial modeling; Landslide susceptibility mapping; Risk management strategies NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
132;Geomatics Innovation and Simulation for Landslide Risk Management: The Use of Cellular Automata and Random Forest Automation;Landslides are among the most serious and frequent environmental disasters, involving the fall of large masses of rock and soil that can significantly impact human structures and inhabited areas. Anticipating these events is crucial to reduce risks through real-time monitoring of areas at risk during extreme weather events, such as heavy rains, allowing for early warnings. This study aims to develop a methodology to enhance the prediction of landslide susceptibility, creating a more reliable system for early identification of risk areas. Our project involves creating a model capable of quickly predicting the susceptibility index of specific areas in response to extreme weather events. We represent the terrain using cellular automata and implement a random forest model to analyze and learn from weather patterns. Providing data with high spatial accuracy is vital to identify vulnerable areas and implement preventive measures. The proposed method offers an early warning mechanism by comparing the predicted susceptibility index with the current one, allowing for the issuance of alarms for the entire observed area. This early warning mechanism can be integrated into existing emergency protocols to improve the response to natural disasters. We applied this method to the area of Prunella, a small village in the municipality of Melito di Porto Salvo, known for numerous historical landslides. This approach provides an early warning mechanism, allowing for alarms to be issued for the entire observed area, and it can be integrated into existing emergency protocols to enhance disaster response. © 2024 by the authors.;"cellular automata; landslides; landslides suscettibility map; machine learning; random forest; remote sensing";"Deforestation; Emergency services; Extreme weather; Risk assessment; Risk management; Tropics; Weather forecasting; Cellular automatons; Early-warning mechanisms; Environmental disasters; Extreme weather events; Geomatic; Landslide risk managements; Landslide suscettibility map; Machine-learning; Random forests; Remote-sensing; Landslides";"Geomatics Innovation and Simulation for Landslide Risk Management: The Use of Cellular Automata and Random Forest Automation Landslides are among the most serious and frequent environmental disasters, involving the fall of large masses of rock and soil that can significantly impact human structures and inhabited areas. Anticipating these events is crucial to reduce risks through real-time monitoring of areas at risk during extreme weather events, such as heavy rains, allowing for early warnings. This study aims to develop a methodology to enhance the prediction of landslide susceptibility, creating a more reliable system for early identification of risk areas. Our project involves creating a model capable of quickly predicting the susceptibility index of specific areas in response to extreme weather events. We represent the terrain using cellular automata and implement a random forest model to analyze and learn from weather patterns. Providing data with high spatial accuracy is vital to identify vulnerable areas and implement preventive measures. The proposed method offers an early warning mechanism by comparing the predicted susceptibility index with the current one, allowing for the issuance of alarms for the entire observed area. This early warning mechanism can be integrated into existing emergency protocols to improve the response to natural disasters. We applied this method to the area of Prunella, a small village in the municipality of Melito di Porto Salvo, known for numerous historical landslides. This approach provides an early warning mechanism, allowing for alarms to be issued for the entire observed area, and it can be integrated into existing emergency protocols to enhance disaster response. © 2024 by the authors. cellular automata; landslides; landslides suscettibility map; machine learning; random forest; remote sensing Deforestation; Emergency services; Extreme weather; Risk assessment; Risk management; Tropics; Weather forecasting; Cellular automatons; Early-warning mechanisms; Environmental disasters; Extreme weather events; Geomatic; Landslide risk managements; Landslide suscettibility map; Machine-learning; Random forests; Remote-sensing; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
133;Leveraging Machine Learning Models for Proactive Disaster Forecasting;The increasing frequency and intensity of natural disasters, such as earthquakes, tsunamis, floods, and forest fires, necessitate the development of advanced early warning systems. Current disaster prediction systems are hampered by limitations in sensor technology, which is often expensive and primarily effective for large-scale disasters. This project aims to enhance disaster prediction accuracy and timeliness by utilizing real-time data and advanced machine learning algorithms, including XGBoost, Voting classifier, and Random Forest classifier. Additionally, recognizing the long-term health impacts of poor air quality, the project extends its scope to predict the Air Quality Index (AQI). The project seeks to enable proactive risk management and effective disaster response by integrating real-time environmental data and machine learning models. We aim to improve proactive risk management and disaster response by employing real-time data. The outcomes of this project promise significant advancements in disaster preparedness, ultimately safeguarding human lives and infrastructure. © 2025 IEEE.;"Air Quality Index; Disaster preparedness; Early warning systems; Random Forest; Real-time data; Sensor technology; XGBoost";"Adversarial machine learning; Decision trees; Fire alarm systems; Machine learning; Outages; Random forests; Risk assessment; Air quality indices; Disaster prediction; Disaster preparedness; Early Warning System; Machine learning models; Proactive risk management; Random forests; Real-time data; Sensor technologies; Xgboost; Risk management";"Leveraging Machine Learning Models for Proactive Disaster Forecasting The increasing frequency and intensity of natural disasters, such as earthquakes, tsunamis, floods, and forest fires, necessitate the development of advanced early warning systems. Current disaster prediction systems are hampered by limitations in sensor technology, which is often expensive and primarily effective for large-scale disasters. This project aims to enhance disaster prediction accuracy and timeliness by utilizing real-time data and advanced machine learning algorithms, including XGBoost, Voting classifier, and Random Forest classifier. Additionally, recognizing the long-term health impacts of poor air quality, the project extends its scope to predict the Air Quality Index (AQI). The project seeks to enable proactive risk management and effective disaster response by integrating real-time environmental data and machine learning models. We aim to improve proactive risk management and disaster response by employing real-time data. The outcomes of this project promise significant advancements in disaster preparedness, ultimately safeguarding human lives and infrastructure. © 2025 IEEE. Air Quality Index; Disaster preparedness; Early warning systems; Random Forest; Real-time data; Sensor technology; XGBoost Adversarial machine learning; Decision trees; Fire alarm systems; Machine learning; Outages; Random forests; Risk assessment; Air quality indices; Disaster prediction; Disaster preparedness; Early Warning System; Machine learning models; Proactive risk management; Random forests; Real-time data; Sensor technologies; Xgboost; Risk management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
134;Binary vs Multi-class with Gaussian Filter on Typhoon Image Classification for Intensity Prediction;Strong meteorological events Tropical Cyclones (TCs) pose serious risks to coastal ecosystems and communities. Their strength is usually categorized using a variety of metrics, including wind speed, pressure, and rainfall since it directly corresponds with the possibility of damage and fatalities. An accurate classification of TC severity is essential for disaster preparedness, response plans, and mitigation initiatives. Support vector machines (SVM) {function category}, K-Nearest Neighbors (KNN) {lazy category}, Bayesian networks {Bayes category}, Random forests {Ensemble category}, and decision trees {Tree Category} are among the machine learning classifiers whose performances are compared in this study in binary and multi-class configurations by using Gaussian image processing technique. Performance measures, including time complexity, ROC, PRC, accuracy, precision, recall, and F-measure, were examined. The results indicate that Multi-class with SVM and Multi-class with Random Forest classifiers consistently outperform other models across most metrics, achieving the highest accuracy (0.88) and superior ROC (0.97) and PRC (0.94-0.95) scores. However, SVM models exhibited significantly higher time complexity, particularly in the multi-class with SVM. © 2024 Seventh Sense Research Group®.;"Binary classification; KNN; Multi-class classification; Random forest; Typhoon images";NULL;"Binary vs Multi-class with Gaussian Filter on Typhoon Image Classification for Intensity Prediction Strong meteorological events Tropical Cyclones (TCs) pose serious risks to coastal ecosystems and communities. Their strength is usually categorized using a variety of metrics, including wind speed, pressure, and rainfall since it directly corresponds with the possibility of damage and fatalities. An accurate classification of TC severity is essential for disaster preparedness, response plans, and mitigation initiatives. Support vector machines (SVM) {function category}, K-Nearest Neighbors (KNN) {lazy category}, Bayesian networks {Bayes category}, Random forests {Ensemble category}, and decision trees {Tree Category} are among the machine learning classifiers whose performances are compared in this study in binary and multi-class configurations by using Gaussian image processing technique. Performance measures, including time complexity, ROC, PRC, accuracy, precision, recall, and F-measure, were examined. The results indicate that Multi-class with SVM and Multi-class with Random Forest classifiers consistently outperform other models across most metrics, achieving the highest accuracy (0.88) and superior ROC (0.97) and PRC (0.94-0.95) scores. However, SVM models exhibited significantly higher time complexity, particularly in the multi-class with SVM. © 2024 Seventh Sense Research Group®. Binary classification; KNN; Multi-class classification; Random forest; Typhoon images NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
135;Application of Machine Learning Models to Multi-Parameter Maximum Magnitude Prediction;Magnitude prediction is a key focus in earthquake science research, and using machine learning models to analyze seismic data, identify pre-seismic anomalies, and improve prediction accuracy is of great scientific and practical significance. Taking the southern part of China’s North–South Seismic Belt (20° N~30° N, 96° E~106° E), where strong earthquakes frequently occur, as an example, we used the sliding time window method to calculate 11 seismicity indicators from the earthquake catalog data as the characteristic parameters of the training model, and compared six machine learning models, including the random forest (RF) and long short-term memory (LSTM) models, to select the best-performing LSTM model for predicting the maximum magnitude of an earthquake in the study area in the coming year. The experimental results show that the LSTM model performs exceptionally well in predicting earthquakes of magnitude 5 < ML ≤ 6 within the time window of the test set, with a prediction success rate of 85%. Additionally, the study explores how different time windows, spatial locations, and parameter choices affect model performance. It found that longer time windows and key seismicity parameters, such as the b-value and the square root of total seismic energy, are crucial for improving prediction accuracy. Finally, we propose a magnitude interval-based assessment method to better predict the actual impacts that different magnitudes may cause. This method demonstrates the LSTM model’s potential in predicting moderate to strong earthquakes and offers new approaches for earthquake early warning and disaster mitigation. © 2024 by the authors.;"China’s north–south seismic zone; machine learning; prediction of maximum magnitude; seismicity parameters";"Adversarial machine learning; Contrastive Learning; Earthquakes; Machine learning; Seismic response; China’s north–south seismic zone; Machine learning models; Machine-learning; Maximum magnitudes; Memory modeling; Prediction accuracy; Prediction of maximum magnitude; Seismic zones; Seismicity parameters; Short term memory; Prediction models";"Application of Machine Learning Models to Multi-Parameter Maximum Magnitude Prediction Magnitude prediction is a key focus in earthquake science research, and using machine learning models to analyze seismic data, identify pre-seismic anomalies, and improve prediction accuracy is of great scientific and practical significance. Taking the southern part of China’s North–South Seismic Belt (20° N~30° N, 96° E~106° E), where strong earthquakes frequently occur, as an example, we used the sliding time window method to calculate 11 seismicity indicators from the earthquake catalog data as the characteristic parameters of the training model, and compared six machine learning models, including the random forest (RF) and long short-term memory (LSTM) models, to select the best-performing LSTM model for predicting the maximum magnitude of an earthquake in the study area in the coming year. The experimental results show that the LSTM model performs exceptionally well in predicting earthquakes of magnitude 5 < ML ≤ 6 within the time window of the test set, with a prediction success rate of 85%. Additionally, the study explores how different time windows, spatial locations, and parameter choices affect model performance. It found that longer time windows and key seismicity parameters, such as the b-value and the square root of total seismic energy, are crucial for improving prediction accuracy. Finally, we propose a magnitude interval-based assessment method to better predict the actual impacts that different magnitudes may cause. This method demonstrates the LSTM model’s potential in predicting moderate to strong earthquakes and offers new approaches for earthquake early warning and disaster mitigation. © 2024 by the authors. China’s north–south seismic zone; machine learning; prediction of maximum magnitude; seismicity parameters Adversarial machine learning; Contrastive Learning; Earthquakes; Machine learning; Seismic response; China’s north–south seismic zone; Machine learning models; Machine-learning; Maximum magnitudes; Memory modeling; Prediction accuracy; Prediction of maximum magnitude; Seismic zones; Seismicity parameters; Short term memory; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
136;Intelligent post-earthquake building recovery system: A framework combining BIM and deep learning;Post-earthquake building recovery is always a crucial task after the strike of an earthquake. Conventionally, such building recovery is time-consuming. The availability of building information modeling (BIM) and deep learning enables the possibility of a more efficient building assessment and rehabilitation planning process. In this paper, an intelligent post-earthquake building recovery system that integrates BIM and deep learning is proposed. Deep learning is used for damage classification and recognition, while BIM provides data on all building elements to support the analyses of the building recovery process. The proposed system is expected to assist engineers in building inspection, structural element assessment, and rehabilitation planning. More specifically, the system combines the visual indicators for structural element assessment and semantic segmentation for damage area estimation to achieve damage classification, with a BIM software plug-in for building recovery plan application. Finally, a case study is presented to validate the implementation of the system framework. © 2024 Elsevier Ltd;"Building information modeling (BIM); Building recovery; Deep learning; Earthquake; Recovery plan application";"Building information modeling; Building Information Modelling; Building recovery; Damage classification; Deep learning; Recovery plan application; Recovery plans; Recovery systems; Rehabilitation planning; Structural elements";"Intelligent post-earthquake building recovery system: A framework combining BIM and deep learning Post-earthquake building recovery is always a crucial task after the strike of an earthquake. Conventionally, such building recovery is time-consuming. The availability of building information modeling (BIM) and deep learning enables the possibility of a more efficient building assessment and rehabilitation planning process. In this paper, an intelligent post-earthquake building recovery system that integrates BIM and deep learning is proposed. Deep learning is used for damage classification and recognition, while BIM provides data on all building elements to support the analyses of the building recovery process. The proposed system is expected to assist engineers in building inspection, structural element assessment, and rehabilitation planning. More specifically, the system combines the visual indicators for structural element assessment and semantic segmentation for damage area estimation to achieve damage classification, with a BIM software plug-in for building recovery plan application. Finally, a case study is presented to validate the implementation of the system framework. © 2024 Elsevier Ltd Building information modeling (BIM); Building recovery; Deep learning; Earthquake; Recovery plan application Building information modeling; Building Information Modelling; Building recovery; Damage classification; Deep learning; Recovery plan application; Recovery plans; Recovery systems; Rehabilitation planning; Structural elements";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
137;Leveraging Deep Learning Technique with Meta-Data-Featured Images Captured by Drone for Real-Time Victim Positioning in Flood-Affected Areas;Flooding has long been a major concern for nations frequently affected by natural disasters. The detection and localization of isolated victims for immediate temporary rescue have been a focal point for many researchers aiming to demonstrate the practical value of their solutions. However, accessing flood-affected areas poses significant challenges, as floods severely disrupt local transportation and damage infrastructure, rendering access nearly impossible. In such scenarios, the most feasible rescue approach is aerial intervention using Unmanned Aerial Vehicles (UAVs). Nevertheless, accurately detecting and localizing targets from a UAV’s perspective is a significant challenge. UAVs operate at various altitudes and speeds, resulting in motion blur of targets against densely populated backgrounds. Additionally, target localization is made difficult due to the limited contextual understanding of the surroundings. To address these challenges, we propose training the YOLOv10 model to enhance the detection of small objects, integrated with a GPS-based target localization system. Furthermore, to enable the practical, real-world application of this solution, we introduce a communication infrastructure that facilitates the timely and effective deployment of both the detection and localization models. Field experiments have yielded promising results, with the artificial intelligence model achieving an Average Precision (AP) between 85-95%, and the localization model processing targets at an impressive speed of under 1 ms. The combined detection and localization approach demonstrated a positioning error of only 7-9 meters. Moreover, integrating the AI model and the localization algorithm into a low-latency communication system has shown promising results, providing near-instantaneous outputs with delays ranging from 0.6 to 0.9 seconds from the moment a target enters the frame. © 2025 IEEE.;"Aerial surveillance; Autonomous Search and Rescue; Deep Learning; Drone-based monitoring; Localization algorithms; Real-time data processing; UAVs (Unmanned Aerial Vehicles); Victim detection; Victim localization";"Aerial photography; Aircraft communication; Aircraft detection; Deep learning; Network security; Steganography; Aerial surveillance; Aerial vehicle; Autonomous search and rescue; Autonomous searches; Deep learning; Drone-based monitoring; Localisation; Localization algorithm; Real-time data processing; Search and rescue; Unmanned aerial vehicle; Victim detections; Victim localization; Drones";"Leveraging Deep Learning Technique with Meta-Data-Featured Images Captured by Drone for Real-Time Victim Positioning in Flood-Affected Areas Flooding has long been a major concern for nations frequently affected by natural disasters. The detection and localization of isolated victims for immediate temporary rescue have been a focal point for many researchers aiming to demonstrate the practical value of their solutions. However, accessing flood-affected areas poses significant challenges, as floods severely disrupt local transportation and damage infrastructure, rendering access nearly impossible. In such scenarios, the most feasible rescue approach is aerial intervention using Unmanned Aerial Vehicles (UAVs). Nevertheless, accurately detecting and localizing targets from a UAV’s perspective is a significant challenge. UAVs operate at various altitudes and speeds, resulting in motion blur of targets against densely populated backgrounds. Additionally, target localization is made difficult due to the limited contextual understanding of the surroundings. To address these challenges, we propose training the YOLOv10 model to enhance the detection of small objects, integrated with a GPS-based target localization system. Furthermore, to enable the practical, real-world application of this solution, we introduce a communication infrastructure that facilitates the timely and effective deployment of both the detection and localization models. Field experiments have yielded promising results, with the artificial intelligence model achieving an Average Precision (AP) between 85-95%, and the localization model processing targets at an impressive speed of under 1 ms. The combined detection and localization approach demonstrated a positioning error of only 7-9 meters. Moreover, integrating the AI model and the localization algorithm into a low-latency communication system has shown promising results, providing near-instantaneous outputs with delays ranging from 0.6 to 0.9 seconds from the moment a target enters the frame. © 2025 IEEE. Aerial surveillance; Autonomous Search and Rescue; Deep Learning; Drone-based monitoring; Localization algorithms; Real-time data processing; UAVs (Unmanned Aerial Vehicles); Victim detection; Victim localization Aerial photography; Aircraft communication; Aircraft detection; Deep learning; Network security; Steganography; Aerial surveillance; Aerial vehicle; Autonomous search and rescue; Autonomous searches; Deep learning; Drone-based monitoring; Localisation; Localization algorithm; Real-time data processing; Search and rescue; Unmanned aerial vehicle; Victim detections; Victim localization; Drones";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
138;Onboard FPGA AI/ML Processing on Landsat-8 Satellite Images: A Case Study of Wildfires Detection;Using satellite-acquired data often requires transferring information to control stations for processing, which degrades the quality of service. Rapid information usage is crucial in scenarios like early warning systems for critical structures or environments. Using artificial intelligence and machine learning (AI/ML) onboard satellites can enhance autonomy and control. This paper aims to identify the key requirements for onboard FPGA AI/ML processing without significantly increasing payload complexity and to define a reference architecture based on initial study scenarios. A fire detection data set, consisting of images from Landsat-8 satellites, includes wildfire and control images from Uruguay and Brazil. Four network architectures were evaluated for wildfire detection, with Resnet-18 being the best tradeoff. All networks effectively generalized the task. The proposed system is capable of detecting fires in an area of 51,984 km2 in 176 s. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Artifical Intelligence; Deep Learning; Earth observation; FPGA; Landsat; Machine Learning; Satellite";"Deep learning; Failure analysis; Risk management; Tropics; Artifical intelligence; Artificial intelligence learning; Case-studies; Control station; Deep learning; Earth observations; LANDSAT; Machine-learning; Satellite images; Wildfire detection; Landsat";"Onboard FPGA AI/ML Processing on Landsat-8 Satellite Images: A Case Study of Wildfires Detection Using satellite-acquired data often requires transferring information to control stations for processing, which degrades the quality of service. Rapid information usage is crucial in scenarios like early warning systems for critical structures or environments. Using artificial intelligence and machine learning (AI/ML) onboard satellites can enhance autonomy and control. This paper aims to identify the key requirements for onboard FPGA AI/ML processing without significantly increasing payload complexity and to define a reference architecture based on initial study scenarios. A fire detection data set, consisting of images from Landsat-8 satellites, includes wildfire and control images from Uruguay and Brazil. Four network architectures were evaluated for wildfire detection, with Resnet-18 being the best tradeoff. All networks effectively generalized the task. The proposed system is capable of detecting fires in an area of 51,984 km2 in 176 s. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Artifical Intelligence; Deep Learning; Earth observation; FPGA; Landsat; Machine Learning; Satellite Deep learning; Failure analysis; Risk management; Tropics; Artifical intelligence; Artificial intelligence learning; Case-studies; Control station; Deep learning; Earth observations; LANDSAT; Machine-learning; Satellite images; Wildfire detection; Landsat";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
139;Landslide susceptibility evaluation and determination of critical influencing factors in eastern Sichuan mountainous area, China;Landslide susceptibility evaluation and determination of critical influencing factors is a prerequisite for preventing hazardous risks, especially in landslide-prone mountainous areas. However, in densely vegetated Southwest mountainous areas, identifying assessment approach of shallow landslides susceptibility and their major inducing factors is still a huge challenge. To address this challenge, we applied five advanced machine learning models (Logistic Regression Model, Generalized Additive Model, Random Forest Model, Support Vector Machine Model, Artificial Neural Network Model) to assess the spatial distribution of shallow landslide susceptibility, considering several relevant factors that affect landslide occurrence. These factors include geological, topographic and vegetation factors, as well as four new vegetation factors: stock volume, stand density, average tree age, and stand types. Furthermore, we employed SHAP algorithm and Structural Equation Models to quantify the relative importance and explanatory power of these factors on shallow landslide susceptibility and to clarify the interaction mechanisms among various factors in Huaying Mountain. The results shown that Random Forest Model proves to be the most accurate (95.1 %) in assessing the spatial distribution of shallow landslides susceptibility, followed by the Artificial Neural Network model (78.6 %), the Support Vector Machine model (69.8 %), the Generalized additive model (68.1 %) and the Logistic Regression model (67.6 %).The area with high susceptible landslide possibility was 25.3 km2 occupying 14.8 % of the study region, it is mainly distributed in the west of Tianchi Lake, southeast of Huaying City and west of the study area, along with Xiangyu Railway. Geographical environment and vegetation features were found to significantly explain 67.4 % and 32.6 % of the total effects in shallow landslides susceptibility, respectively. Specifically, the spatial distribution of shallow landslides susceptibility were primarily influenced by geological engineering rock group, distance to faults、stand types and distance to river. Geographical environment factors could indirectly affect changes in vegetation features, thereby indirectly affecting the spatial distribution of shallow landslides susceptibility. Findings from this research could be helpful for scientific decision-making and technical assistance for early warning, prevention, and control of rainstorm-induced landslides in highly vegetation covered areas. © 2024;"Influencing factors; Landslides susceptibility evaluation; Machine learning; SHAP algorithm; Vegetation features";"China; Lake Chaonaqiu; Sichuan; Decision trees; Jurassic; Logistic regression; Random forests; Risk assessment; Support vector regression; Influencing factor; Landslide susceptibility; Landslide susceptibility evaluation; Logistic Regression modeling; Machine-learning; Mountainous area; Shallow landslides susceptibility; SHAP algorithm; Susceptibility evaluations; Vegetation feature; decision making; early warning system; environmental factor; landslide; machine learning; model validation; mountain region; rainstorm; spatial distribution; vegetation cover; Landslides";"Landslide susceptibility evaluation and determination of critical influencing factors in eastern Sichuan mountainous area, China Landslide susceptibility evaluation and determination of critical influencing factors is a prerequisite for preventing hazardous risks, especially in landslide-prone mountainous areas. However, in densely vegetated Southwest mountainous areas, identifying assessment approach of shallow landslides susceptibility and their major inducing factors is still a huge challenge. To address this challenge, we applied five advanced machine learning models (Logistic Regression Model, Generalized Additive Model, Random Forest Model, Support Vector Machine Model, Artificial Neural Network Model) to assess the spatial distribution of shallow landslide susceptibility, considering several relevant factors that affect landslide occurrence. These factors include geological, topographic and vegetation factors, as well as four new vegetation factors: stock volume, stand density, average tree age, and stand types. Furthermore, we employed SHAP algorithm and Structural Equation Models to quantify the relative importance and explanatory power of these factors on shallow landslide susceptibility and to clarify the interaction mechanisms among various factors in Huaying Mountain. The results shown that Random Forest Model proves to be the most accurate (95.1 %) in assessing the spatial distribution of shallow landslides susceptibility, followed by the Artificial Neural Network model (78.6 %), the Support Vector Machine model (69.8 %), the Generalized additive model (68.1 %) and the Logistic Regression model (67.6 %).The area with high susceptible landslide possibility was 25.3 km2 occupying 14.8 % of the study region, it is mainly distributed in the west of Tianchi Lake, southeast of Huaying City and west of the study area, along with Xiangyu Railway. Geographical environment and vegetation features were found to significantly explain 67.4 % and 32.6 % of the total effects in shallow landslides susceptibility, respectively. Specifically, the spatial distribution of shallow landslides susceptibility were primarily influenced by geological engineering rock group, distance to faults、stand types and distance to river. Geographical environment factors could indirectly affect changes in vegetation features, thereby indirectly affecting the spatial distribution of shallow landslides susceptibility. Findings from this research could be helpful for scientific decision-making and technical assistance for early warning, prevention, and control of rainstorm-induced landslides in highly vegetation covered areas. © 2024 Influencing factors; Landslides susceptibility evaluation; Machine learning; SHAP algorithm; Vegetation features China; Lake Chaonaqiu; Sichuan; Decision trees; Jurassic; Logistic regression; Random forests; Risk assessment; Support vector regression; Influencing factor; Landslide susceptibility; Landslide susceptibility evaluation; Logistic Regression modeling; Machine-learning; Mountainous area; Shallow landslides susceptibility; SHAP algorithm; Susceptibility evaluations; Vegetation feature; decision making; early warning system; environmental factor; landslide; machine learning; model validation; mountain region; rainstorm; spatial distribution; vegetation cover; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
140;ICAT-net: a lightweight neural network with optimized coordinate attention and transformer mechanisms for earthquake detection and phase picking;Seismic signal detection is a crucial technology for enhancing the efficiency of earthquake early warning systems. However, existing deep learning-based seismic signal detection models often face limitations in resource-constrained seismic monitoring engineering environments due to the high computational resource demands of the models. To address this issue, this study employs spatial-depth convolution techniques in the downsampling process of seismic signal sequences, effectively minimizing the loss of fine-grained feature information. Concurrently, we leverage the coordinate attention module to enhance the model’s ability to recognize spatial features in seismic signal sequences. To reduce computational costs, we map the keys and values in the transformer architecture to a lower-dimensional subspace, significantly decreasing the demand for computational resources. By employing concatenation operations between the encoder and decoder, the model retains rich contextual information and progressively restores the spatial resolution of the signal during the decoding process. Based on these models, we propose the integration of coordinate attention and transformer network (ICAT-net), an efficient multi-task network designed to simultaneously handle various tasks, including seismic sequence recognition and phase picking. ICAT-net integrates local feature relationships with long-range dependency processing capabilities to meet the requirements of multi-task learning. Experimental results demonstrate that ICAT-net requires only 4.743168G of floating-point operations (FLOPs) and has a parameter count of 0.260755M, while performing excellently in tasks such as seismic waveform detection (DET), P-wave phase picking (Ppk), and S-wave phase picking (Spk). These advantages render ICAT-net particularly suitable for deployment in resource-constrained environments, providing valuable solutions for earthquake monitoring and disaster risk assessment. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.;"Deep learning; Integration of attention; Multi-task learning; Phase picking; Seismic detection";"Deep neural networks; Earthquake effects; Image coding; Image segmentation; Light transmission; Seismic response; Computational resources; Deep learning; Integration of attention; Multitask learning; Phase pickings; Seismic detection; Seismic signals; Signal sequence; Signal's detections; Wave phase; Risk assessment";"ICAT-net: a lightweight neural network with optimized coordinate attention and transformer mechanisms for earthquake detection and phase picking Seismic signal detection is a crucial technology for enhancing the efficiency of earthquake early warning systems. However, existing deep learning-based seismic signal detection models often face limitations in resource-constrained seismic monitoring engineering environments due to the high computational resource demands of the models. To address this issue, this study employs spatial-depth convolution techniques in the downsampling process of seismic signal sequences, effectively minimizing the loss of fine-grained feature information. Concurrently, we leverage the coordinate attention module to enhance the model’s ability to recognize spatial features in seismic signal sequences. To reduce computational costs, we map the keys and values in the transformer architecture to a lower-dimensional subspace, significantly decreasing the demand for computational resources. By employing concatenation operations between the encoder and decoder, the model retains rich contextual information and progressively restores the spatial resolution of the signal during the decoding process. Based on these models, we propose the integration of coordinate attention and transformer network (ICAT-net), an efficient multi-task network designed to simultaneously handle various tasks, including seismic sequence recognition and phase picking. ICAT-net integrates local feature relationships with long-range dependency processing capabilities to meet the requirements of multi-task learning. Experimental results demonstrate that ICAT-net requires only 4.743168G of floating-point operations (FLOPs) and has a parameter count of 0.260755M, while performing excellently in tasks such as seismic waveform detection (DET), P-wave phase picking (Ppk), and S-wave phase picking (Spk). These advantages render ICAT-net particularly suitable for deployment in resource-constrained environments, providing valuable solutions for earthquake monitoring and disaster risk assessment. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024. Deep learning; Integration of attention; Multi-task learning; Phase picking; Seismic detection Deep neural networks; Earthquake effects; Image coding; Image segmentation; Light transmission; Seismic response; Computational resources; Deep learning; Integration of attention; Multitask learning; Phase pickings; Seismic detection; Seismic signals; Signal sequence; Signal's detections; Wave phase; Risk assessment";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;2;Preparation
141;MAP FLOODWATER RADAR IMAGERY USING MACHINE LEARNING ALGORITHMS;Flooding is a widespread and costly natural disaster around the world. Accurately assessing the extent of flooding in near real-time is crucial for governments and humanitarian organizations. This information strengthens early warning systems, evaluates risks, and guides effective relief efforts. Therefore, precise flood mapping is essential for saving lives through improved early warning systems and targeted emergency responses. In this study, radar imagery available on the Planetary Computer Data was utilized to train a U-Net model specifically designed to label flood-affected pixels in an image from a flood event. Different blocks of the U-Net encoder architecture were fine-tuned to identify the most efficient fine-tuned model, and their results were compared. As a result, the model with blocks 1 and 2 being fine-tuned demonstrated the highest Intersection over Union (IoU) score of 78.904%, an increase of 8.663% over the baseline methods. © (2025), (International Islamic University Malaysia). All rights reserved.;"Fine-tunning; Flood mapping; Radar imagery; U-Net";NULL;"MAP FLOODWATER RADAR IMAGERY USING MACHINE LEARNING ALGORITHMS Flooding is a widespread and costly natural disaster around the world. Accurately assessing the extent of flooding in near real-time is crucial for governments and humanitarian organizations. This information strengthens early warning systems, evaluates risks, and guides effective relief efforts. Therefore, precise flood mapping is essential for saving lives through improved early warning systems and targeted emergency responses. In this study, radar imagery available on the Planetary Computer Data was utilized to train a U-Net model specifically designed to label flood-affected pixels in an image from a flood event. Different blocks of the U-Net encoder architecture were fine-tuned to identify the most efficient fine-tuned model, and their results were compared. As a result, the model with blocks 1 and 2 being fine-tuned demonstrated the highest Intersection over Union (IoU) score of 78.904%, an increase of 8.663% over the baseline methods. © (2025), (International Islamic University Malaysia). All rights reserved. Fine-tunning; Flood mapping; Radar imagery; U-Net NULL";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;2;Preparation
142;A Study of Flood Analysis for the Hydrological Flow Using Geoinformatics Technology;"Floods are a major cause of mortality, facility destruction, and serious economic impact on a country. It is a catastrophe, it would be difficult; the government, relevant groups, and the community must implement precautions to mitigate their destructive impacts. Emergency management organizations must execute actions before a flooding catastrophe to reduce risks and prepare an emergency reaction during this kind. To do this, the most cutting-edge technologies must be used to foresee calamities as fast as possible so that suitable reaction strategies may be developed before the calamity. Flooding is unpredictable and depends on meteorological and ecological variables, making it challenging to forecast. This classification system divides three types of remote sensing technologies—multispectral, radar, and coupled with GIS for integrating hydrological and hydraulic models to predict floods. Observing the high NDVI results ensures healthier vegetation, whereas the stressed vegetation carries lower NDVI values. Then, an approach for forecasting floods and a map has been proposed to fill the existing gaps. The results illustrate the NDVI in flood prediction. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.";"Flood detection; GIS; Hydrological model; Remote sensing";"Banks (bodies of water); Disaster prevention; Disasters; Flood damage; Hydraulic models; Tropics; Vegetation mapping; Weather forecasting; Cutting edge technology; Economic impacts; Emergency management; Flood detections; Floodings; Geo-informatics; Hydrological models; Meteorological variables; Reaction strategies; Remote-sensing; Risk management";"A Study of Flood Analysis for the Hydrological Flow Using Geoinformatics Technology Floods are a major cause of mortality, facility destruction, and serious economic impact on a country. It is a catastrophe, it would be difficult; the government, relevant groups, and the community must implement precautions to mitigate their destructive impacts. Emergency management organizations must execute actions before a flooding catastrophe to reduce risks and prepare an emergency reaction during this kind. To do this, the most cutting-edge technologies must be used to foresee calamities as fast as possible so that suitable reaction strategies may be developed before the calamity. Flooding is unpredictable and depends on meteorological and ecological variables, making it challenging to forecast. This classification system divides three types of remote sensing technologies—multispectral, radar, and coupled with GIS for integrating hydrological and hydraulic models to predict floods. Observing the high NDVI results ensures healthier vegetation, whereas the stressed vegetation carries lower NDVI values. Then, an approach for forecasting floods and a map has been proposed to fill the existing gaps. The results illustrate the NDVI in flood prediction. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025. Flood detection; GIS; Hydrological model; Remote sensing Banks (bodies of water); Disaster prevention; Disasters; Flood damage; Hydraulic models; Tropics; Vegetation mapping; Weather forecasting; Cutting edge technology; Economic impacts; Emergency management; Flood detections; Floodings; Geo-informatics; Hydrological models; Meteorological variables; Reaction strategies; Remote-sensing; Risk management";-1;Não Classificado;NULL;1.2;Hydrological;2;Preparation
143;Smart Emergency Management System Using Raspberry Pi Pico, GPS and GSM Modules;This paper proposes a GSM-based children's rescue system designed for earthquake emergencies, emphasizing cost-effectiveness and simplicity while ensuring rapid child rescue. The system operates in three key stages: data acquisition, classification, and alert notification. Sensors collect temperature, pH, and heart rate data, which are then digitized and classified to assess the child's condition. If abnormal values are detected, an alert notification is sent via GSM to parents and rescue teams, providing real-time location data through GPS. The hardware implementation, based on Raspberry Pi, converts analog sensor signals into digital data for processing. The system's efficacy is validated through various scenarios, demonstrating its ability to detect critical conditions and promptly trigger alerts, enhancing child safety during disasters.  © 2025 IEEE.;"Children rescue; Earthquake; GPS and GSM; Monitoring; Raspberry pi";"Analog to digital conversion; Anomaly detection; Data acquisition; Data handling; Digital to analog conversion; Disaster prevention; Disasters; Global positioning system; Information management; Metadata; Risk management; Abnormal values; Child rescue; Classifieds; Condition; Emergency management systems; GPS and GSM; Heart-rate; Raspberry pi; Real-time location; Rescue systems; Earthquakes";"Smart Emergency Management System Using Raspberry Pi Pico, GPS and GSM Modules This paper proposes a GSM-based children's rescue system designed for earthquake emergencies, emphasizing cost-effectiveness and simplicity while ensuring rapid child rescue. The system operates in three key stages: data acquisition, classification, and alert notification. Sensors collect temperature, pH, and heart rate data, which are then digitized and classified to assess the child's condition. If abnormal values are detected, an alert notification is sent via GSM to parents and rescue teams, providing real-time location data through GPS. The hardware implementation, based on Raspberry Pi, converts analog sensor signals into digital data for processing. The system's efficacy is validated through various scenarios, demonstrating its ability to detect critical conditions and promptly trigger alerts, enhancing child safety during disasters.  © 2025 IEEE. Children rescue; Earthquake; GPS and GSM; Monitoring; Raspberry pi Analog to digital conversion; Anomaly detection; Data acquisition; Data handling; Digital to analog conversion; Disaster prevention; Disasters; Global positioning system; Information management; Metadata; Risk management; Abnormal values; Child rescue; Classifieds; Condition; Emergency management systems; GPS and GSM; Heart-rate; Raspberry pi; Real-time location; Rescue systems; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
144;A Mesoscale Eddy Reconstruction Method Based on Deep Learning;Mesoscale eddies (MEs), a prevalent natural phenomenon in the oceans, are pivotal to the oceans temperature and salt structure, as well as the acoustic propagation mechanism. In the offshore environment of the mid- and far-ocean, the effective use of scarce ME environmental data for accurate acoustic field reconstruction is a pressing issue in marine scientific research. To address this, our study introduces a hybrid eddy identification algorithm, merging JCOPE2M high-resolution reanalysis data and AVISO satellite altimeter data. This algorithm aims to capture sound velocity profile sample data of MEs, laying a robust foundation for subsequent analysis and modeling. We then delve into the training and reconstruction of the generative adversarial network model based on these valuable sample data. The primary goal of this step is to characterize the sound field of MEs to more accurately reflect their actual physical properties. To ensure the reliability and validity of the reconstruction method, we design a comprehensive evaluation system with RMSE and SSIM, and convergence zone (CZ) accuracy as the main evaluation indexes. After a series of experimental validations, the MAT model reconstruction method we adopted demonstrated a significant improvement in accuracy. Specifically, the RMSE value of the 2D reconstruction results is as low as 1.9 m/s, the SSIM value is as high as 0.72, and the average accuracy of the CZ is more than 60%. Compared with other previous reconstruction methods, ours is more accurate in characterizing the ME sound field and achieves a significant improvement in the accuracy of field prediction. This result provides a new technical path for ME fine-grained sound field reconstruction and a valuable reference for more underwater acoustic research and application. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.;"CZ; MAT modeling; MEs; Sound field reconstruction";"Aneroid altimeters; Offshore oil wells; Surface discharges; Underwater acoustics; Convergence zones; MAT modeling; Mesoscale eddy; Natural phenomenon; Ocean temperature; Reconstruction method; Salt structures; Sample data; Sound field reconstruction; Temperature structure; Ocean structures";"A Mesoscale Eddy Reconstruction Method Based on Deep Learning Mesoscale eddies (MEs), a prevalent natural phenomenon in the oceans, are pivotal to the oceans temperature and salt structure, as well as the acoustic propagation mechanism. In the offshore environment of the mid- and far-ocean, the effective use of scarce ME environmental data for accurate acoustic field reconstruction is a pressing issue in marine scientific research. To address this, our study introduces a hybrid eddy identification algorithm, merging JCOPE2M high-resolution reanalysis data and AVISO satellite altimeter data. This algorithm aims to capture sound velocity profile sample data of MEs, laying a robust foundation for subsequent analysis and modeling. We then delve into the training and reconstruction of the generative adversarial network model based on these valuable sample data. The primary goal of this step is to characterize the sound field of MEs to more accurately reflect their actual physical properties. To ensure the reliability and validity of the reconstruction method, we design a comprehensive evaluation system with RMSE and SSIM, and convergence zone (CZ) accuracy as the main evaluation indexes. After a series of experimental validations, the MAT model reconstruction method we adopted demonstrated a significant improvement in accuracy. Specifically, the RMSE value of the 2D reconstruction results is as low as 1.9 m/s, the SSIM value is as high as 0.72, and the average accuracy of the CZ is more than 60%. Compared with other previous reconstruction methods, ours is more accurate in characterizing the ME sound field and achieves a significant improvement in the accuracy of field prediction. This result provides a new technical path for ME fine-grained sound field reconstruction and a valuable reference for more underwater acoustic research and application. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025. CZ; MAT modeling; MEs; Sound field reconstruction Aneroid altimeters; Offshore oil wells; Surface discharges; Underwater acoustics; Convergence zones; MAT modeling; Mesoscale eddy; Natural phenomenon; Ocean temperature; Reconstruction method; Salt structures; Sample data; Sound field reconstruction; Temperature structure; Ocean structures";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
145;Spatial Riverbank Erosion Assessment Using an Integrated Model in the Barak Floodplain of Northeast India;"Riverbank erosion assessment is crucial for natural resource management and soil protection. Conventional in-situ measurements yield accurate erosion rate data; however, when applied over a vast region, the method becomes prohibitively expensive and time-consuming. This research aimed to investigate the use of an in-situ technique and machine learning (ML) to quantify bank erosion rates in alluvial rivers in Northeast India. The variables influencing bank erosion were identified and determine using Field observations and a digital elevation model analysis. The dataset was then divided into training and testing. The ML model was trained and verified using data from the Barak River in northeastern India. In-situ jet tests were employed to estimate bank erosion values on the riverbank. The ML model (GBR, DL, KNN) was employed, with the bank erosion rate as the output variable and hydraulic shear stress, bankfull discharge, bank height, soil type, vegetation index, and slope degree as the input variables. Following the training phase, the model was tested. Based on the test outcomes, the machine learning model displayed an acceptable level of capability to estimate bank erosion (R-squared: 0.95, RMSE: 2.36). The optimal model and its inputs were employed to estimate the study area's bank erosion rates. Finally, the optimal model results and ArcGIS tools created a riverbank erosion rate map. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.";"Bank erosion; Barak river; Gradient boosting; Machine learning";"Adaptive boosting; Resource allocation; Soil testing; Bank erosion; Barak river; Erosion rates; Gradient boosting; Integrated modeling; Machine learning models; Machine-learning; Northeast india; Optimal model; Riverbank erosion; Rivers";"Spatial Riverbank Erosion Assessment Using an Integrated Model in the Barak Floodplain of Northeast India Riverbank erosion assessment is crucial for natural resource management and soil protection. Conventional in-situ measurements yield accurate erosion rate data; however, when applied over a vast region, the method becomes prohibitively expensive and time-consuming. This research aimed to investigate the use of an in-situ technique and machine learning (ML) to quantify bank erosion rates in alluvial rivers in Northeast India. The variables influencing bank erosion were identified and determine using Field observations and a digital elevation model analysis. The dataset was then divided into training and testing. The ML model was trained and verified using data from the Barak River in northeastern India. In-situ jet tests were employed to estimate bank erosion values on the riverbank. The ML model (GBR, DL, KNN) was employed, with the bank erosion rate as the output variable and hydraulic shear stress, bankfull discharge, bank height, soil type, vegetation index, and slope degree as the input variables. Following the training phase, the model was tested. Based on the test outcomes, the machine learning model displayed an acceptable level of capability to estimate bank erosion (R-squared: 0.95, RMSE: 2.36). The optimal model and its inputs were employed to estimate the study area's bank erosion rates. Finally, the optimal model results and ArcGIS tools created a riverbank erosion rate map. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025. Bank erosion; Barak river; Gradient boosting; Machine learning Adaptive boosting; Resource allocation; Soil testing; Bank erosion; Barak river; Erosion rates; Gradient boosting; Integrated modeling; Machine learning models; Machine-learning; Northeast india; Optimal model; Riverbank erosion; Rivers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
146;Machine-learning-based tropical cyclone wind field model incorporating multiple meteorological parameters;Multiple hazards caused by tropical cyclones (TCs), such as heavy rains and strong winds, result in substantial property losses and casualties worldwide each year. TC wind field models, describing the development of the wind hazard, are key within early warning realizations and associated risk assessments. Different to conventional parametric, analytical or meteorological numerical models, this study aims to develop a machine-learning-based approach for modeling TC wind fields by incorporating multiple meteorological parameters. The wind field model considers linear and nonlinear modeling respectively, where the input data includes various meteorological parameters such as surface pressure gradient (SPG), geopotential (GEO), boundary layer height (BLH), and forecast surface roughness (FSR). The output data is the TC wind field data of the Regional and Mesoscale Meteorology Branch (RAMMB) extracted by image recognition method, and assimilated with the wind field from the fifth generation of the European Center for Medium-Range Weather Forecasts (ECMWF) atmospheric reanalysis dataset ERA5. In the linear model, various combinations of parameters are considered, yet always yielding unsatisfactory results. The best results in the linear model were obtained using all four parameter combinations, where the root mean square error (RMSE) was 2.60 m/s and the coefficient of determination R2 value was 0.44. To increase performance, three nonlinear machine learning methods—Fully Connected Deep Neural Networks (FC-DNN), Convolutional Neural Networks (CNN), and Transformer—are introduced to the training process. Comparing the wind field continuity, RMSE and R2 between the three models, it is found that the Transformer outperforms all other models, with R2 value of 0.877 and an RMSE of 2.23. As a final step, the trained Transformer model was used to predict the evolution of wind speed of the Typhoon Lekima (1909), in what could serve as effective model validation. © 2024 Elsevier Ltd;"Machine learning; Multiple meteorological parameters; Tropical cyclone; Wind data assimilation; Wind field model";"Atmospheric pressure; Convolutional neural networks; Deep neural networks; Mean square error; Multilayer neural networks; Risk assessment; Tropical cyclone; Weather forecasting; Wind forecasting; Data assimilation; Machine-learning; Meteorological parameters; Multiple meteorological parameter; Root mean square errors; Tropical cyclone; Wind data; Wind data assimilation; Wind field; Wind field modeling";"Machine-learning-based tropical cyclone wind field model incorporating multiple meteorological parameters Multiple hazards caused by tropical cyclones (TCs), such as heavy rains and strong winds, result in substantial property losses and casualties worldwide each year. TC wind field models, describing the development of the wind hazard, are key within early warning realizations and associated risk assessments. Different to conventional parametric, analytical or meteorological numerical models, this study aims to develop a machine-learning-based approach for modeling TC wind fields by incorporating multiple meteorological parameters. The wind field model considers linear and nonlinear modeling respectively, where the input data includes various meteorological parameters such as surface pressure gradient (SPG), geopotential (GEO), boundary layer height (BLH), and forecast surface roughness (FSR). The output data is the TC wind field data of the Regional and Mesoscale Meteorology Branch (RAMMB) extracted by image recognition method, and assimilated with the wind field from the fifth generation of the European Center for Medium-Range Weather Forecasts (ECMWF) atmospheric reanalysis dataset ERA5. In the linear model, various combinations of parameters are considered, yet always yielding unsatisfactory results. The best results in the linear model were obtained using all four parameter combinations, where the root mean square error (RMSE) was 2.60 m/s and the coefficient of determination R2 value was 0.44. To increase performance, three nonlinear machine learning methods—Fully Connected Deep Neural Networks (FC-DNN), Convolutional Neural Networks (CNN), and Transformer—are introduced to the training process. Comparing the wind field continuity, RMSE and R2 between the three models, it is found that the Transformer outperforms all other models, with R2 value of 0.877 and an RMSE of 2.23. As a final step, the trained Transformer model was used to predict the evolution of wind speed of the Typhoon Lekima (1909), in what could serve as effective model validation. © 2024 Elsevier Ltd Machine learning; Multiple meteorological parameters; Tropical cyclone; Wind data assimilation; Wind field model Atmospheric pressure; Convolutional neural networks; Deep neural networks; Mean square error; Multilayer neural networks; Risk assessment; Tropical cyclone; Weather forecasting; Wind forecasting; Data assimilation; Machine-learning; Meteorological parameters; Multiple meteorological parameter; Root mean square errors; Tropical cyclone; Wind data; Wind data assimilation; Wind field; Wind field modeling";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
147;Overview and advancement of power system topology addressing pre- and post-event strategies under abnormal operating conditions;The transition towards increased utilization of renewable energy and electric vehicles (EVs), along with the growing use of various other electrical devices, poses challenges to the stable and resilient operation of electric power systems (EPS), especially in the face of natural phenomena associated with climate change. This means that accurate topology and balanced EPS plays a key role to increase the capacity to respond quickly and in a coordinated manner to disaster situations such as cyber-attacks, earthquakes and floods. In this study, a new approach is presented to quickly and accurately detect topology attacks in EPS, thus contributing to making safer and more resilient. The proposed methods provide insights into maintaining uninterrupted electricity service by enabling EPS management through both post- and pre-event operational strategies. This approach is created by identifying faulty points with the obtained topology information and creating microgrid (MG) groups. Machine learning techniques have been integrated into the data intrusion attack detection (DIAD) system, enabling the detection of manipulated or faulty smart meters (SM). Concurrently, a topology identification (TI)-based graph learning algorithm is propounded to determine the exact fault locations before and after the event. For MV region restoration after determining the TI region, a mixed-integer linear programming (MILP) approach is employed to optimize the load restoration process in the MG regions. This approach aims to minimize losses and restore critical loads to their previous state as quickly as possible using flexible and emergency power balancing systems, including grid-support storage systems (GSSs), photovoltaic systems (PVs), electric vehicle charging stations (EVCS), and mobile generators. Moreover, a detailed compilation is presented under the topics of EPS topology, phase identification (PI) and its effect on power system resiliency (PSR), shedding light on the future development of EPS. © 2024 Elsevier Ltd;"Grid support storage system; Machine learning; Mixed-integer linear programming; Optimization; Phase identification; Power system resiliency; Smart grid; Topology identification";"Uninterruptible power systems; Vehicle-to-grid; Grid support storage system; Grid supports; Integer Linear Programming; Machine-learning; Mixed integer linear; Optimisations; Phase identification; Power; Power system resiliency; Smart grid; Storage systems; Topology identification; Integer linear programming";"Overview and advancement of power system topology addressing pre- and post-event strategies under abnormal operating conditions The transition towards increased utilization of renewable energy and electric vehicles (EVs), along with the growing use of various other electrical devices, poses challenges to the stable and resilient operation of electric power systems (EPS), especially in the face of natural phenomena associated with climate change. This means that accurate topology and balanced EPS plays a key role to increase the capacity to respond quickly and in a coordinated manner to disaster situations such as cyber-attacks, earthquakes and floods. In this study, a new approach is presented to quickly and accurately detect topology attacks in EPS, thus contributing to making safer and more resilient. The proposed methods provide insights into maintaining uninterrupted electricity service by enabling EPS management through both post- and pre-event operational strategies. This approach is created by identifying faulty points with the obtained topology information and creating microgrid (MG) groups. Machine learning techniques have been integrated into the data intrusion attack detection (DIAD) system, enabling the detection of manipulated or faulty smart meters (SM). Concurrently, a topology identification (TI)-based graph learning algorithm is propounded to determine the exact fault locations before and after the event. For MV region restoration after determining the TI region, a mixed-integer linear programming (MILP) approach is employed to optimize the load restoration process in the MG regions. This approach aims to minimize losses and restore critical loads to their previous state as quickly as possible using flexible and emergency power balancing systems, including grid-support storage systems (GSSs), photovoltaic systems (PVs), electric vehicle charging stations (EVCS), and mobile generators. Moreover, a detailed compilation is presented under the topics of EPS topology, phase identification (PI) and its effect on power system resiliency (PSR), shedding light on the future development of EPS. © 2024 Elsevier Ltd Grid support storage system; Machine learning; Mixed-integer linear programming; Optimization; Phase identification; Power system resiliency; Smart grid; Topology identification Uninterruptible power systems; Vehicle-to-grid; Grid support storage system; Grid supports; Integer Linear Programming; Machine-learning; Mixed integer linear; Optimisations; Phase identification; Power; Power system resiliency; Smart grid; Storage systems; Topology identification; Integer linear programming";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
148;Integrating machine learning methods for computing greenhouse gas emissions baselines in agriculture;Addressing climate change requires significant reductions in global greenhouse gas (GHG) emissions. In Australia, agriculture accounts for approximately 16.8% of total emissions. Despite advancements in emissions accounting, critical gaps remain, particularly concerning the data-driven selection of reference years and the use of aggregated metrics. These issues often overlook sectoral dynamics, highlighting the need for context-specific, disaggregated baselines that more accurately reflect the complexities of emission assessment from agriculture. This study presents an analytical methodology that integrates machine learning (ML) techniques, specifically dynamic Generalised Additive Models (GAMs) and Conditional Inference Trees (CITs). Applied to 30 years of CO2-e data from Australian inventories, the ML-based approach predicted significant periods of aggregated emissions: from 1990 to 2005 and from 2006 to 2022, with 2005 identified as a critical transition point. The analysis revealed a significant decline in aggregated emissions, with a median of 70.5 Mt CO2-e over the past five years, down from a peak of 76.4 Mt CO2-e estimated in 2005. By optimising the bias-variance trade-off, the algorithms effectively captured underlying patterns while accounting for variability due to climate-related extreme events, such as the Millennium Drought. Another key contribution of this research is the establishment of accurate and adaptable sector- and activity-level baselines, which enhance decision-making related to resource allocation and performance targets. Although applied to Australian agricultural emissions data, this methodology has global applicability. As climate variability intensifies, its predictive and benchmarking capabilities can improve the utility of historical data, supporting dynamic, data-driven approaches crucial for facilitating the net-zero transition. © 2024 The Authors;"Algorithms; Carbon footprint; Climate change policy; Decision making; Global warming mitigation; Sustainable development";"Adversarial machine learning; Low emission; % reductions; Australia; Climate change policies; Data driven; Decisions makings; Global warming mitigation; Greenhouse gas emissions; Integrating machines; Machine learning methods; Total emissions; Greenhouse gas emissions";"Integrating machine learning methods for computing greenhouse gas emissions baselines in agriculture Addressing climate change requires significant reductions in global greenhouse gas (GHG) emissions. In Australia, agriculture accounts for approximately 16.8% of total emissions. Despite advancements in emissions accounting, critical gaps remain, particularly concerning the data-driven selection of reference years and the use of aggregated metrics. These issues often overlook sectoral dynamics, highlighting the need for context-specific, disaggregated baselines that more accurately reflect the complexities of emission assessment from agriculture. This study presents an analytical methodology that integrates machine learning (ML) techniques, specifically dynamic Generalised Additive Models (GAMs) and Conditional Inference Trees (CITs). Applied to 30 years of CO2-e data from Australian inventories, the ML-based approach predicted significant periods of aggregated emissions: from 1990 to 2005 and from 2006 to 2022, with 2005 identified as a critical transition point. The analysis revealed a significant decline in aggregated emissions, with a median of 70.5 Mt CO2-e over the past five years, down from a peak of 76.4 Mt CO2-e estimated in 2005. By optimising the bias-variance trade-off, the algorithms effectively captured underlying patterns while accounting for variability due to climate-related extreme events, such as the Millennium Drought. Another key contribution of this research is the establishment of accurate and adaptable sector- and activity-level baselines, which enhance decision-making related to resource allocation and performance targets. Although applied to Australian agricultural emissions data, this methodology has global applicability. As climate variability intensifies, its predictive and benchmarking capabilities can improve the utility of historical data, supporting dynamic, data-driven approaches crucial for facilitating the net-zero transition. © 2024 The Authors Algorithms; Carbon footprint; Climate change policy; Decision making; Global warming mitigation; Sustainable development Adversarial machine learning; Low emission; % reductions; Australia; Climate change policies; Data driven; Decisions makings; Global warming mitigation; Greenhouse gas emissions; Integrating machines; Machine learning methods; Total emissions; Greenhouse gas emissions";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
149;A universal adapter in segmentation models for transferable landslide mapping;Efficient landslide mapping is crucial for disaster mitigation and relief. Recently, deep learning methods have shown promising results in landslide mapping using satellite imagery. However, the sample sparsity and geographic diversity of landslides have challenged the transferability of deep learning models. In this paper, we proposed a universal adapter module that can be seamlessly embedded into existing segmentation models for transferable landslide mapping. The adapter can achieve high-accuracy cross-regional landslide segmentation with a small sample set, requiring minimal parameter adjustments. In detail, the pre-trained baseline model freezes its parameters to keep learned knowledge of the source domain, while the lightweight adapter fine-tunes only a few parameters to learn new landslide features of the target domain. Structurally, we introduced an attention mechanism to enhance the feature extraction of the adapter. To validate the proposed adapter module, 4321 landslide samples were prepared, and the Segment Anything Model (SAM) and other baseline models, along with four transfer strategies were selected for controlled experiments. In addition, Sentinel-2 satellite imagery in the Himalayas and Hengduan Mountains, located on the southern and southeastern edges of the Tibetan Plateau was collected for evaluation. The controlled experiments reported that SAM, when combined with our adapter module, achieved a peak mean Intersection over Union (mIoU) of 82.3 %. For other baseline models, integrating the adapter improved mIoU by 2.6 % to 12.9 % compared with traditional strategies on cross-regional landslide mapping. In particular, baseline models with Transformers are more suitable for fine-tuning parameters. Furthermore, the visualized feature maps revealed that fine-tuning shallow encoders can achieve better effects in model transfer. Besides, the proposed adapter can effectively extract landslide features and focus on specific spatial and channel domains with significant features. We also quantified the spectral, scale, and shape features of landslides and analyzed their impacts on segmentation results. Our analysis indicated that weak spectral differences, as well as extreme scale and edge shapes are detrimental to the accuracy of landslide segmentation. Overall, this adapter module provides a new perspective for large-scale transferable landslide mapping. © 2024;"Deep learning; Landslide mapping; Tibetan Plateau; Transfer learning";"China; Hengduan Mountains; Himalayas; Qinghai-Xizang Plateau; Deep learning; Deep reinforcement learning; Disaster prevention; Disasters; Image segmentation; Landslides; Mapping; Baseline models; Controlled experiment; Deep learning; Disaster mitigation; Disaster relief; Fine tuning; Landslide mapping; Segmentation models; Tibetan Plateau; Transfer learning; landslide; machine learning; mapping method; numerical model; satellite imagery; segmentation; Satellite imagery";"A universal adapter in segmentation models for transferable landslide mapping Efficient landslide mapping is crucial for disaster mitigation and relief. Recently, deep learning methods have shown promising results in landslide mapping using satellite imagery. However, the sample sparsity and geographic diversity of landslides have challenged the transferability of deep learning models. In this paper, we proposed a universal adapter module that can be seamlessly embedded into existing segmentation models for transferable landslide mapping. The adapter can achieve high-accuracy cross-regional landslide segmentation with a small sample set, requiring minimal parameter adjustments. In detail, the pre-trained baseline model freezes its parameters to keep learned knowledge of the source domain, while the lightweight adapter fine-tunes only a few parameters to learn new landslide features of the target domain. Structurally, we introduced an attention mechanism to enhance the feature extraction of the adapter. To validate the proposed adapter module, 4321 landslide samples were prepared, and the Segment Anything Model (SAM) and other baseline models, along with four transfer strategies were selected for controlled experiments. In addition, Sentinel-2 satellite imagery in the Himalayas and Hengduan Mountains, located on the southern and southeastern edges of the Tibetan Plateau was collected for evaluation. The controlled experiments reported that SAM, when combined with our adapter module, achieved a peak mean Intersection over Union (mIoU) of 82.3 %. For other baseline models, integrating the adapter improved mIoU by 2.6 % to 12.9 % compared with traditional strategies on cross-regional landslide mapping. In particular, baseline models with Transformers are more suitable for fine-tuning parameters. Furthermore, the visualized feature maps revealed that fine-tuning shallow encoders can achieve better effects in model transfer. Besides, the proposed adapter can effectively extract landslide features and focus on specific spatial and channel domains with significant features. We also quantified the spectral, scale, and shape features of landslides and analyzed their impacts on segmentation results. Our analysis indicated that weak spectral differences, as well as extreme scale and edge shapes are detrimental to the accuracy of landslide segmentation. Overall, this adapter module provides a new perspective for large-scale transferable landslide mapping. © 2024 Deep learning; Landslide mapping; Tibetan Plateau; Transfer learning China; Hengduan Mountains; Himalayas; Qinghai-Xizang Plateau; Deep learning; Deep reinforcement learning; Disaster prevention; Disasters; Image segmentation; Landslides; Mapping; Baseline models; Controlled experiment; Deep learning; Disaster mitigation; Disaster relief; Fine tuning; Landslide mapping; Segmentation models; Tibetan Plateau; Transfer learning; landslide; machine learning; mapping method; numerical model; satellite imagery; segmentation; Satellite imagery";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
150;A Comprehensive Study of Deep Learning and Machine Learning Methods for Land Application and Cover Segmentation;Research in the Delhi NCR area from 2000 to 2024 is compared in this study. Firstly, a flash flood occurred due to the heavy rain in July 2013. Grasslands are among the vegetative types in L ULC, which cannot be univocally classified as either natural in-land or artificial urban. But some areas of grassland, when it came time to classify L ULC, three different machine learning techniques were considered: random forest, support vector machine, and feedforward neural networks. Optimization The model best for random forest performed better than those two others. The study used statistical analysis tools, the 'sustainable development prism' and 'patch-corridor matrix' models, a landscape index, and time series data to explore how BGS changed in new urban areas. The research made use of Landsat datasets and optimized machine learning algorithms to classify L ULC, extract built-up areas, analyse landscape fragmentation and investigate built-up expansion in Delhi NCR by means of frequency analysis. In the EFPCNNM ensemble deep learning model, convolutional neural networks, feature pyramids, and ensemble learning are combined to increase the accuracy of remote sensing picture classification. The EFPCNNM process includes everything necessary for image analysis: pre-processing, feature extraction, feature pyramid creation, and ensemble learning. Exciting new possibilities for environmental monitoring, urban planning and property mapping emerge from this study. Sustainable urban development and resource management require effective monitoring and interpretation of the changes in L ULC, as well as employing remote sensing data alongside machine learning and deep learning technology. © 2025 IEEE.;"Arable Land and Subsidence Seeper Area; Ensemble Convolutional Neural Network Model (EFPCNNM); Land Use and Land Cover (LULC) Classification; Landscape Fragmentation Analysis (LFA) and Frequency Approach (FA); Machine Learning and Deep Learning Approaches; Remote Sensing and GIS Techniques";"Contrastive Learning; Deep neural networks; Deforestation; Feedforward neural networks; Image segmentation; Information management; Random forests; Resource allocation; Support vector machines; Analysis approach; Arable land; Arable land and subsidence seeper area; Convolutional neural network; Ensemble convolutional neural network model (EFPCNNM); Fragmentation analysis; Frequency approach; Land-use and land-cover classifications; Landscape fragmentation; Landscape fragmentation analyze  and frequency approach; Learning approach; Machine learning and deep learning approach; Machine-learning; Neural network model; Remote sensing and GIS; Remote sensing and GIS technique; Convolutional neural networks";"A Comprehensive Study of Deep Learning and Machine Learning Methods for Land Application and Cover Segmentation Research in the Delhi NCR area from 2000 to 2024 is compared in this study. Firstly, a flash flood occurred due to the heavy rain in July 2013. Grasslands are among the vegetative types in L ULC, which cannot be univocally classified as either natural in-land or artificial urban. But some areas of grassland, when it came time to classify L ULC, three different machine learning techniques were considered: random forest, support vector machine, and feedforward neural networks. Optimization The model best for random forest performed better than those two others. The study used statistical analysis tools, the 'sustainable development prism' and 'patch-corridor matrix' models, a landscape index, and time series data to explore how BGS changed in new urban areas. The research made use of Landsat datasets and optimized machine learning algorithms to classify L ULC, extract built-up areas, analyse landscape fragmentation and investigate built-up expansion in Delhi NCR by means of frequency analysis. In the EFPCNNM ensemble deep learning model, convolutional neural networks, feature pyramids, and ensemble learning are combined to increase the accuracy of remote sensing picture classification. The EFPCNNM process includes everything necessary for image analysis: pre-processing, feature extraction, feature pyramid creation, and ensemble learning. Exciting new possibilities for environmental monitoring, urban planning and property mapping emerge from this study. Sustainable urban development and resource management require effective monitoring and interpretation of the changes in L ULC, as well as employing remote sensing data alongside machine learning and deep learning technology. © 2025 IEEE. Arable Land and Subsidence Seeper Area; Ensemble Convolutional Neural Network Model (EFPCNNM); Land Use and Land Cover (LULC) Classification; Landscape Fragmentation Analysis (LFA) and Frequency Approach (FA); Machine Learning and Deep Learning Approaches; Remote Sensing and GIS Techniques Contrastive Learning; Deep neural networks; Deforestation; Feedforward neural networks; Image segmentation; Information management; Random forests; Resource allocation; Support vector machines; Analysis approach; Arable land; Arable land and subsidence seeper area; Convolutional neural network; Ensemble convolutional neural network model (EFPCNNM); Fragmentation analysis; Frequency approach; Land-use and land-cover classifications; Landscape fragmentation; Landscape fragmentation analyze  and frequency approach; Learning approach; Machine learning and deep learning approach; Machine-learning; Neural network model; Remote sensing and GIS; Remote sensing and GIS technique; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
151;SDCINet: A novel cross-task integration network for segmentation and detection of damaged/changed building targets with optical remote sensing imagery;Buildings are primary locations for human activities and key focuses in the military domain. Rapidly detecting damaged/changed buildings (DCB) and conducting detailed assessments can effectively aid urbanization monitoring, disaster response, and humanitarian assistance. Currently, the tasks of object detection (OD) and change detection (CD) for DCB are almost independent of each other, making it difficult to simultaneously determine the location and details of changes. Based on this, we have designed a cross-task network called SDCINet, which integrates OD and CD, and have created four dual-task datasets focused on disasters and urbanization. SDCINet is a novel deep learning dual-task framework composed of a consistency encoder, differentiation decoder, and cross-task global attention collaboration module (CGAC). It is capable of modeling differential feature relationships based on bi-temporal images, performing end-to-end pixel-level prediction, and object bounding box regression. The bi-direction traction function of CGAC is used to deeply couple OD and CD tasks. Additionally, we collected bi-temporal images from 10 locations worldwide that experienced earthquakes, explosions, wars, and conflicts to construct two datasets specifically for damaged building OD and CD. We also constructed two datasets for changed building OD and CD based on two publicly available CD datasets. These four datasets can serve as data benchmarks for dual-task research on DCB. Using these datasets, we conducted extensive performance evaluations of 18 state-of-the-art models from the perspectives of OD, CD, and instance segmentation. Benchmark experimental results demonstrated the superior performance of SDCINet. Ablation experiments and evaluative analyses confirmed the effectiveness and unique value of CGAC. © 2024 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS);"Change detection; Cross-task; Deep learning; Object detection; Remote sensing";"Benchmarking; Change detection; Image segmentation; Military photography; Change detection; Cross-task; Deep learning; Dual-tasks; Human activities; Integration networks; Objects detection; Optical remote-sensing imagery; Remote-sensing; Temporal images; accuracy assessment; building; data set; detection method; disaster; machine learning; remote sensing; segmentation; urbanization; Optical remote sensing";"SDCINet: A novel cross-task integration network for segmentation and detection of damaged/changed building targets with optical remote sensing imagery Buildings are primary locations for human activities and key focuses in the military domain. Rapidly detecting damaged/changed buildings (DCB) and conducting detailed assessments can effectively aid urbanization monitoring, disaster response, and humanitarian assistance. Currently, the tasks of object detection (OD) and change detection (CD) for DCB are almost independent of each other, making it difficult to simultaneously determine the location and details of changes. Based on this, we have designed a cross-task network called SDCINet, which integrates OD and CD, and have created four dual-task datasets focused on disasters and urbanization. SDCINet is a novel deep learning dual-task framework composed of a consistency encoder, differentiation decoder, and cross-task global attention collaboration module (CGAC). It is capable of modeling differential feature relationships based on bi-temporal images, performing end-to-end pixel-level prediction, and object bounding box regression. The bi-direction traction function of CGAC is used to deeply couple OD and CD tasks. Additionally, we collected bi-temporal images from 10 locations worldwide that experienced earthquakes, explosions, wars, and conflicts to construct two datasets specifically for damaged building OD and CD. We also constructed two datasets for changed building OD and CD based on two publicly available CD datasets. These four datasets can serve as data benchmarks for dual-task research on DCB. Using these datasets, we conducted extensive performance evaluations of 18 state-of-the-art models from the perspectives of OD, CD, and instance segmentation. Benchmark experimental results demonstrated the superior performance of SDCINet. Ablation experiments and evaluative analyses confirmed the effectiveness and unique value of CGAC. © 2024 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS) Change detection; Cross-task; Deep learning; Object detection; Remote sensing Benchmarking; Change detection; Image segmentation; Military photography; Change detection; Cross-task; Deep learning; Dual-tasks; Human activities; Integration networks; Objects detection; Optical remote-sensing imagery; Remote-sensing; Temporal images; accuracy assessment; building; data set; detection method; disaster; machine learning; remote sensing; segmentation; urbanization; Optical remote sensing";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;3;Response
152;Deep Learning for Seismic Data Compression in Distributed Acoustic Sensing;Distributed acoustic sensing (DAS) is emerging in seismic monitoring due to its ultradense spatial sampling, durability to harsh environments, and sensitivity to weak ground vibration. Compared with traditional nodal geophones that are normally sparsely distributed, DAS offers unprecedented detectability for small-magnitude earthquake events, very subtle reservoir dynamics, and other weak signals among various applications. The appealing detectability of weak signals is compromised by the terabyte-scale daily continuous record that causes prohibitive storage problems. The current solution is to save only the segmented data of interest, for example, a certain length around a target event. Here, we tackle the urgent storage problem of DAS monitoring by designing a deep learning (DL)-based compression algorithm. The compression algorithm can be split into two major components. The first part is the encoder based on the vision transformer architecture, where the input multichannel DAS dataset goes through an encoding process to output the key features from the input. The second part is the decoder, where the features are optimally combined to reconstruct the data of the original scale. The optimal network parameters are obtained via an unsupervised training process, aiming at minimizing the difference between the reconstructed and input data. In the proposed DL-based compression algorithm, only the decoder's weight parameters and extracted features from the input data through the encoder are saved on the disk, which is sufficient to reconstruct a high-fidelity dataset. The proposed compression algorithm can reach around 50 times the compression rate (CR) for a gigabyte-scale DAS dataset without unsatisfactory reconstruction performance.  © 1980-2012 IEEE.;"Compression; deep learning; reconstruction; seismic";"Acoustic sensing; Compression; Compression algorithms; Deep learning; Detectability; Reconstruction; Seismic; Seismic monitoring; Spatial sampling; Weak signals; acoustic method; algorithm; data set; machine learning; reconstruction; seismic data; Network security";"Deep Learning for Seismic Data Compression in Distributed Acoustic Sensing Distributed acoustic sensing (DAS) is emerging in seismic monitoring due to its ultradense spatial sampling, durability to harsh environments, and sensitivity to weak ground vibration. Compared with traditional nodal geophones that are normally sparsely distributed, DAS offers unprecedented detectability for small-magnitude earthquake events, very subtle reservoir dynamics, and other weak signals among various applications. The appealing detectability of weak signals is compromised by the terabyte-scale daily continuous record that causes prohibitive storage problems. The current solution is to save only the segmented data of interest, for example, a certain length around a target event. Here, we tackle the urgent storage problem of DAS monitoring by designing a deep learning (DL)-based compression algorithm. The compression algorithm can be split into two major components. The first part is the encoder based on the vision transformer architecture, where the input multichannel DAS dataset goes through an encoding process to output the key features from the input. The second part is the decoder, where the features are optimally combined to reconstruct the data of the original scale. The optimal network parameters are obtained via an unsupervised training process, aiming at minimizing the difference between the reconstructed and input data. In the proposed DL-based compression algorithm, only the decoder's weight parameters and extracted features from the input data through the encoder are saved on the disk, which is sufficient to reconstruct a high-fidelity dataset. The proposed compression algorithm can reach around 50 times the compression rate (CR) for a gigabyte-scale DAS dataset without unsatisfactory reconstruction performance.  © 1980-2012 IEEE. Compression; deep learning; reconstruction; seismic Acoustic sensing; Compression; Compression algorithms; Deep learning; Detectability; Reconstruction; Seismic; Seismic monitoring; Spatial sampling; Weak signals; acoustic method; algorithm; data set; machine learning; reconstruction; seismic data; Network security";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;-1;NULL
153;A social context-aware graph-based multimodal attentive learning framework for disaster content classification during emergencies;In times of crisis, the prompt and precise classification of disaster-related information shared on social media platforms is of paramount importance for effective disaster response and public safety. During such critical events, people utilize social media as a medium for communication, sharing multimodal textual and visual content. However, due to the substantial influx of unfiltered and diverse data, humanitarian organizations face challenges in effectively leveraging this information. Numerous methods have been proposed for classifying disaster-related content, but these methods lack modeling users’ credibility, emotional context, and social interaction information, which is crucial for classification. In this context, we propose a method, CrisisSpot, that leverages a Graph-based Neural Network to comprehend intricate relationships between textual and visual modalities and Social Context Features to incorporate user-centric and content-centric information. We also propose Inverted Dual Embedded Attention, which captures both harmonious and contrary patterns present in the data to harness complex interactions and facilitate richer insights in multimodal data. We have developed a multimodal disaster dataset, TSEqD (Turkey-Syria Earthquake Dataset), which is a large annotated dataset for a single disaster event containing 10,352 data samples. Through extensive experimentation, CrisisSpot has demonstrated significant improvements, achieving an average gain of 9.45% and 5.01% in F1-score compared to the state-of-the-art methods on the publicly available CrisisMMD dataset and TSEqD dataset, respectively. © 2024 Elsevier Ltd;"Crisis management; Data fusion; Deep learning; Disaster content classification; Multimodal data";"Content classification; Context-Aware; Crisis management; Deep learning; Disaster content classification; Graph-based; Learning frameworks; Multi-modal; Multimodal data; Social context; Contrastive Learning";"A social context-aware graph-based multimodal attentive learning framework for disaster content classification during emergencies In times of crisis, the prompt and precise classification of disaster-related information shared on social media platforms is of paramount importance for effective disaster response and public safety. During such critical events, people utilize social media as a medium for communication, sharing multimodal textual and visual content. However, due to the substantial influx of unfiltered and diverse data, humanitarian organizations face challenges in effectively leveraging this information. Numerous methods have been proposed for classifying disaster-related content, but these methods lack modeling users’ credibility, emotional context, and social interaction information, which is crucial for classification. In this context, we propose a method, CrisisSpot, that leverages a Graph-based Neural Network to comprehend intricate relationships between textual and visual modalities and Social Context Features to incorporate user-centric and content-centric information. We also propose Inverted Dual Embedded Attention, which captures both harmonious and contrary patterns present in the data to harness complex interactions and facilitate richer insights in multimodal data. We have developed a multimodal disaster dataset, TSEqD (Turkey-Syria Earthquake Dataset), which is a large annotated dataset for a single disaster event containing 10,352 data samples. Through extensive experimentation, CrisisSpot has demonstrated significant improvements, achieving an average gain of 9.45% and 5.01% in F1-score compared to the state-of-the-art methods on the publicly available CrisisMMD dataset and TSEqD dataset, respectively. © 2024 Elsevier Ltd Crisis management; Data fusion; Deep learning; Disaster content classification; Multimodal data Content classification; Context-Aware; Crisis management; Deep learning; Disaster content classification; Graph-based; Learning frameworks; Multi-modal; Multimodal data; Social context; Contrastive Learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
154;Synergistic image and point cloud processing of UAV data for urban flood modeling: point cloud smart thinning and curb mapping;We propose an integrated approach for automatic point cloud thinning and curb mapping in Uncrewed Aerial Vehicle - Structure from Motion (UAV-SfM) point clouds to enhance hydrological modeling in flood-prone urban areas. UAV flights were conducted to generate an initial orthoimage, which was used to train a convolutional neural network (CNN) segmentation model. The trained model was then applied to the UAV images to produce two binary mask sets: one for vegetation and one for streets and sidewalks. These masks were incorporated during photogrammetric 3D reconstruction to estimate camera geometry and generate a dense point cloud. Our results show that vegetation masks did not improve camera geometry estimation. However, by applying UAV masks, we achieved a 15% reduction in total processing time and decreased the number of points by a factor of 2.7. This targeted approach enabled curb detection by focusing on expected curb locations. Curb candidate points were proposed using geometric characteristics of the point cloud, including normal values, linearity, and verticality. Our rule-based method effectively mapped even subtle curb features, providing a rapid, cost-effective solution for large-area curb mapping. Further, we explored the potential of random forest for curb mapping, with promising results. Our approach can support urban flood modeling efforts and strengthen urban resilience for flood-prone communities. Copyright: © 2024 Pedro Alberto Pereira Zamboni et al.;"curb mapping; image segmentation; modeling; point cloud processing";"Unmanned aerial vehicles (UAV); Vegetation mapping; Camera geometry; Cloud processing; Curb mapping; Images segmentations; Model points; Modeling; Point cloud processing; Point-clouds; Thinnings; Urban flood modelling; Image segmentation";"Synergistic image and point cloud processing of UAV data for urban flood modeling: point cloud smart thinning and curb mapping We propose an integrated approach for automatic point cloud thinning and curb mapping in Uncrewed Aerial Vehicle - Structure from Motion (UAV-SfM) point clouds to enhance hydrological modeling in flood-prone urban areas. UAV flights were conducted to generate an initial orthoimage, which was used to train a convolutional neural network (CNN) segmentation model. The trained model was then applied to the UAV images to produce two binary mask sets: one for vegetation and one for streets and sidewalks. These masks were incorporated during photogrammetric 3D reconstruction to estimate camera geometry and generate a dense point cloud. Our results show that vegetation masks did not improve camera geometry estimation. However, by applying UAV masks, we achieved a 15% reduction in total processing time and decreased the number of points by a factor of 2.7. This targeted approach enabled curb detection by focusing on expected curb locations. Curb candidate points were proposed using geometric characteristics of the point cloud, including normal values, linearity, and verticality. Our rule-based method effectively mapped even subtle curb features, providing a rapid, cost-effective solution for large-area curb mapping. Further, we explored the potential of random forest for curb mapping, with promising results. Our approach can support urban flood modeling efforts and strengthen urban resilience for flood-prone communities. Copyright: © 2024 Pedro Alberto Pereira Zamboni et al. curb mapping; image segmentation; modeling; point cloud processing Unmanned aerial vehicles (UAV); Vegetation mapping; Camera geometry; Cloud processing; Curb mapping; Images segmentations; Model points; Modeling; Point cloud processing; Point-clouds; Thinnings; Urban flood modelling; Image segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
155;Tracing the 2018 Sulawesi Earthquake and Tsunami’s Impact on Palu, Indonesia: A Remote Sensing Analysis;The 2018 Sulawesi Earthquake and Tsunami serves as a backdrop for this work, which employs simple and straightforward remote sensing techniques to determine the extent of the destruction and indirectly evaluate the region’s vulnerability to such catastrophic events. Documenting damage from tsunamis is only meaningful shortly after the disaster has occurred because governmental agencies clean up debris and start the recovery process within a few hours after the destruction has occurred, deeming impact estimates unreliable. Sentinel-2 and Maxar WorldView-3 satellite images were used to calculate well-known environmental indices to delineate the tsunami-affected areas in Palu, Indonesia. The use of NDVI, NDSI, and NDWI indices has allowed for a quantifiable measure of the changes in vegetation, soil moisture, and water bodies, providing a clear demarcation of the tsunami’s impact on land cover. The final tsunami inundation map indicates that the areas most affected by the tsunami are found in the urban center, low-lying regions, and along the coast. This work charts the aftermath of one of Indonesia’s recent tsunamis but may also lay the groundwork for an easy, handy, and low-cost approach to quickly identify tsunami-affected zones. While previous studies have used high-resolution remote sensing methods such as LiDAR or SAR, our study emphasizes accessibility and simplicity, making it more feasible for resource-constrained regions or rapid disaster response. The scientific novelty lies in the integration of widely used environmental indices (dNDVI, dNDWI, and dNDSI) with threshold-based Decision Tree classification to delineate tsunami-affected areas. Unlike many studies that rely on advanced or proprietary tools, we demonstrate that comparable results can be achieved with cost-effective open-source data and straightforward methodologies. Additionally, we address the challenge of differentiating tsunami impacts from other phenomena (et, liquefaction) through index-based thresholds and propose a framework that is adaptable to other vulnerable coastal regions. © 2025 by the authors.;"environmental indices; Indonesia; land cover change; natural hazards; remote sensing; tsunami; vulnerability assessment";NULL;"Tracing the 2018 Sulawesi Earthquake and Tsunami’s Impact on Palu, Indonesia: A Remote Sensing Analysis The 2018 Sulawesi Earthquake and Tsunami serves as a backdrop for this work, which employs simple and straightforward remote sensing techniques to determine the extent of the destruction and indirectly evaluate the region’s vulnerability to such catastrophic events. Documenting damage from tsunamis is only meaningful shortly after the disaster has occurred because governmental agencies clean up debris and start the recovery process within a few hours after the destruction has occurred, deeming impact estimates unreliable. Sentinel-2 and Maxar WorldView-3 satellite images were used to calculate well-known environmental indices to delineate the tsunami-affected areas in Palu, Indonesia. The use of NDVI, NDSI, and NDWI indices has allowed for a quantifiable measure of the changes in vegetation, soil moisture, and water bodies, providing a clear demarcation of the tsunami’s impact on land cover. The final tsunami inundation map indicates that the areas most affected by the tsunami are found in the urban center, low-lying regions, and along the coast. This work charts the aftermath of one of Indonesia’s recent tsunamis but may also lay the groundwork for an easy, handy, and low-cost approach to quickly identify tsunami-affected zones. While previous studies have used high-resolution remote sensing methods such as LiDAR or SAR, our study emphasizes accessibility and simplicity, making it more feasible for resource-constrained regions or rapid disaster response. The scientific novelty lies in the integration of widely used environmental indices (dNDVI, dNDWI, and dNDSI) with threshold-based Decision Tree classification to delineate tsunami-affected areas. Unlike many studies that rely on advanced or proprietary tools, we demonstrate that comparable results can be achieved with cost-effective open-source data and straightforward methodologies. Additionally, we address the challenge of differentiating tsunami impacts from other phenomena (et, liquefaction) through index-based thresholds and propose a framework that is adaptable to other vulnerable coastal regions. © 2025 by the authors. environmental indices; Indonesia; land cover change; natural hazards; remote sensing; tsunami; vulnerability assessment NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
156;SegNet-ATT: Cross-Channel and Spatial Attention-Enhanced U-Net for Semantic Segmentation of Flood Affected Areas;Rapid and accurate detection of flood-affected areas is crucial for effective disaster response and relief operations. Traditional methods often fail to provide the necessary speed and precision, underscoring the need for advanced technological solutions. This research introduces SegNet-ATT, an innovative approach utilizing an attention-augmented U-Net architecture to semantic segment flood-affected areas in aerial imagery. The SegNet-ATT model incorporates self-attention, cross-attention, channel attention, and spatial attention mechanisms to enhance feature extraction and contextual understanding. These attention mechanisms enhance the model’s ability to discern subtle distinctions between water and land, which is critical for effective flood segmentation. These regions are then utilized to determine drop zones for potential rescue operations. The results demonstrate the effectiveness of the SegNet-ATT model, which achieved an impressive accuracy of 89.46%. Comparing this with other models like DeepLab v3, we identify that although they demonstrate marginally better performance, they have steeper computational requirements leading to longer runtimes. This research demonstrates the potential of leveraging attention-enhanced deep learning models for quicker and more effective disaster response. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Attention Mechanism; Flood Image Segmentation; SegNet-ATT; Semantic Segmentation";"Aerial photography; Deep learning; Disaster prevention; Disasters; Flood damage; Image enhancement; Affected area; Attention mechanisms; Disaster relief operations; Disaster response operations; Flood image segmentation; Images segmentations; Segnet-ATT; Semantic segmentation; Spatial attention; Technological solution; Semantic Segmentation";"SegNet-ATT: Cross-Channel and Spatial Attention-Enhanced U-Net for Semantic Segmentation of Flood Affected Areas Rapid and accurate detection of flood-affected areas is crucial for effective disaster response and relief operations. Traditional methods often fail to provide the necessary speed and precision, underscoring the need for advanced technological solutions. This research introduces SegNet-ATT, an innovative approach utilizing an attention-augmented U-Net architecture to semantic segment flood-affected areas in aerial imagery. The SegNet-ATT model incorporates self-attention, cross-attention, channel attention, and spatial attention mechanisms to enhance feature extraction and contextual understanding. These attention mechanisms enhance the model’s ability to discern subtle distinctions between water and land, which is critical for effective flood segmentation. These regions are then utilized to determine drop zones for potential rescue operations. The results demonstrate the effectiveness of the SegNet-ATT model, which achieved an impressive accuracy of 89.46%. Comparing this with other models like DeepLab v3, we identify that although they demonstrate marginally better performance, they have steeper computational requirements leading to longer runtimes. This research demonstrates the potential of leveraging attention-enhanced deep learning models for quicker and more effective disaster response. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Attention Mechanism; Flood Image Segmentation; SegNet-ATT; Semantic Segmentation Aerial photography; Deep learning; Disaster prevention; Disasters; Flood damage; Image enhancement; Affected area; Attention mechanisms; Disaster relief operations; Disaster response operations; Flood image segmentation; Images segmentations; Segnet-ATT; Semantic segmentation; Spatial attention; Technological solution; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
157;A Learning on Intensity Prediction of Tropical Cyclone Infrared Images by Gabor Filter on Binary and Multi Class Approach;Tropical cyclone intensity classification is critical for disaster preparedness and resource allocation. Existing methods rely heavily on either manual analysis or computationally intensive deep learning models, which, despite their high accuracy, are often impractical for real-time scenarios. A significant gap exists in the literature where lightweight yet accurate models optimized for real-time applications are underexplored. This study addresses this gap by leveraging Gabor filter-based texture feature extraction combined with machine learning models, enabling precise cyclone intensity classification while balancing computational efficiency and prediction accuracy. By evaluating multiple classifiers, including Random Forest, SVM, and KNN, this study offers a comparative perspective to identify the most effective model for cyclone intensity classification in binary and multi-class setups. © 2024 Seventh Sense Research Group®;"Bayes Net; Intensity; KNN; SVM; Tropical cyclone";NULL;"A Learning on Intensity Prediction of Tropical Cyclone Infrared Images by Gabor Filter on Binary and Multi Class Approach Tropical cyclone intensity classification is critical for disaster preparedness and resource allocation. Existing methods rely heavily on either manual analysis or computationally intensive deep learning models, which, despite their high accuracy, are often impractical for real-time scenarios. A significant gap exists in the literature where lightweight yet accurate models optimized for real-time applications are underexplored. This study addresses this gap by leveraging Gabor filter-based texture feature extraction combined with machine learning models, enabling precise cyclone intensity classification while balancing computational efficiency and prediction accuracy. By evaluating multiple classifiers, including Random Forest, SVM, and KNN, this study offers a comparative perspective to identify the most effective model for cyclone intensity classification in binary and multi-class setups. © 2024 Seventh Sense Research Group® Bayes Net; Intensity; KNN; SVM; Tropical cyclone NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
158;Assessing climate vulnerability and nonlinear rainfall dynamics in complex networks;To decisively confront the far-reaching impacts of climate change, it is crucial to identify regions that are vulnerable and sensitive to changes in rainfall patterns. Understanding sub-regional vulnerability can significantly contribute to alleviating disruptive events such as drought and flood that affect larger areas. To this end, the complex network of rainfall dynamics of the Lake District, Türkiye, one of the most significant regions with the largest freshwater resources in the country, was analysed. The most robust network structure was determined using transfer entropy coupled community detection algorithms, i.e., Optimal, Spinglass, Edge Betweenness, Louvain, and Leading Eigen. Three optimal distinct communities were identified with the Louvain algorithm, and vulnerability analysis was conducted considering internal and external factors such as inter- and external links, topological differences from the globally coupled network, and average communicability within communities. The relative vulnerability indices obtained for Comm-1, Comm-2, and Comm-3 are 1.259, 1.000, and 1.073, respectively, where the most vulnerable community is Comm-1. Furthermore, Comm-1 has the highest in- and cycle-clustering coefficients, where the stations are more influenced by others and have reciprocal and complex relationships. Comm-2 has the highest intercommunication and the largest structural difference from other communities, leading to more invulnerability. To analyse the nonlinear characteristics of communities leading to various degrees of vulnerability, complexity-entropy causality plane, recurrence plot, and recurrence quantification analyses were conducted. While Comm-1 exhibits a wide range of complexity-entropy values associated with diverse rainfall patterns leading to greater vulnerability, Comm-2 demonstrates greater regularity and predictability, characterised by well-defined rainfall regimes and temporal structures. The results could help identify critical nodes and vulnerable communities in a basin to build a reliable early warning system for accurate risk prediction under the impacts of climate change. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.;"Complexity-entropy causality plane; Recurrence plot; Recurrence quantification analysis; Robustness analysis; Transfer entropy; Vulnerability analysis";"Community IS; Complexity-entropy causality plane; Rainfall dynamics; Rainfall patterns; Recurrence plot; Recurrence quantification analysis; Robustness analysis; Transfer entropy; Vulnerability analysis; Vulnerable communities; algorithm; complexity; drought; freshwater environment; precipitation intensity; vulnerability";"Assessing climate vulnerability and nonlinear rainfall dynamics in complex networks To decisively confront the far-reaching impacts of climate change, it is crucial to identify regions that are vulnerable and sensitive to changes in rainfall patterns. Understanding sub-regional vulnerability can significantly contribute to alleviating disruptive events such as drought and flood that affect larger areas. To this end, the complex network of rainfall dynamics of the Lake District, Türkiye, one of the most significant regions with the largest freshwater resources in the country, was analysed. The most robust network structure was determined using transfer entropy coupled community detection algorithms, i.e., Optimal, Spinglass, Edge Betweenness, Louvain, and Leading Eigen. Three optimal distinct communities were identified with the Louvain algorithm, and vulnerability analysis was conducted considering internal and external factors such as inter- and external links, topological differences from the globally coupled network, and average communicability within communities. The relative vulnerability indices obtained for Comm-1, Comm-2, and Comm-3 are 1.259, 1.000, and 1.073, respectively, where the most vulnerable community is Comm-1. Furthermore, Comm-1 has the highest in- and cycle-clustering coefficients, where the stations are more influenced by others and have reciprocal and complex relationships. Comm-2 has the highest intercommunication and the largest structural difference from other communities, leading to more invulnerability. To analyse the nonlinear characteristics of communities leading to various degrees of vulnerability, complexity-entropy causality plane, recurrence plot, and recurrence quantification analyses were conducted. While Comm-1 exhibits a wide range of complexity-entropy values associated with diverse rainfall patterns leading to greater vulnerability, Comm-2 demonstrates greater regularity and predictability, characterised by well-defined rainfall regimes and temporal structures. The results could help identify critical nodes and vulnerable communities in a basin to build a reliable early warning system for accurate risk prediction under the impacts of climate change. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024. Complexity-entropy causality plane; Recurrence plot; Recurrence quantification analysis; Robustness analysis; Transfer entropy; Vulnerability analysis Community IS; Complexity-entropy causality plane; Rainfall dynamics; Rainfall patterns; Recurrence plot; Recurrence quantification analysis; Robustness analysis; Transfer entropy; Vulnerability analysis; Vulnerable communities; algorithm; complexity; drought; freshwater environment; precipitation intensity; vulnerability";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;1;Prevention
159;Natural Disaster Prediction Using Deep Learning;Accurate prediction techniques are essential in an era when natural disasters pose severe dangers to human lives and infrastructure. This article presents a deep learning approach that uses convolutional neural networks (CNNs) to anticipate natural disasters such as floods, earthquakes, wildfires, and storms. The work preprocesses a dataset of 4,429 pictures using scaling, normalization, and augmentation techniques (such as flipping and rotation) to ensure model resilience and diversity in the data. The image sets were separated into training, validation, and test sets to optimize model performance. Several CNN architectures were used to train the model, including Xception, VGG16, and MobileNet. The testing results showed that MobileNet outperformed the other models with a test accuracy of 92.25%. VGG16 finished second with 91.50%, while Xception achieved 91.00%. A thorough performance assessment was produced by examining these models according to precision, recall, and F1 score. The impressive outcomes, especially for MobileNet, are ascribed to its capacity to effectively extract salient characteristics from the pictures, like distinct patterns and textures connected to different kinds of calamities. The study's overall findings demonstrate that CNN-based models can make precise predictions about natural disasters in real-time situations. To improve early warning systems, the models in this work indicate a high degree of accuracy by merging image data from satellite sources and using advanced data augmentation techniques. The results show how deep learning can practically enhance emergency preparedness and response procedures.  © 2025 IEEE.;"Convolutional Neural Networks (CNN); Cyclones; Deep Learning; Disaster Management; Disaster Preparedness; Earthquakes; Environmental Data; Floods; Image Classification; Machine Learning; Natural Disaster Prediction; Pattern Recognition; Prediction Accuracy; Satellite Imagery; Wildfires";"Deep neural networks; Image enhancement; Satellite imagery; Convolutional neural network; Deep learning; Disaster management; Disaster prediction; Disaster preparedness; Environmental data; Images classification; Machine-learning; Natural disaster prediction; Natural disasters; Prediction accuracy; Wildfire; Convolutional neural networks";"Natural Disaster Prediction Using Deep Learning Accurate prediction techniques are essential in an era when natural disasters pose severe dangers to human lives and infrastructure. This article presents a deep learning approach that uses convolutional neural networks (CNNs) to anticipate natural disasters such as floods, earthquakes, wildfires, and storms. The work preprocesses a dataset of 4,429 pictures using scaling, normalization, and augmentation techniques (such as flipping and rotation) to ensure model resilience and diversity in the data. The image sets were separated into training, validation, and test sets to optimize model performance. Several CNN architectures were used to train the model, including Xception, VGG16, and MobileNet. The testing results showed that MobileNet outperformed the other models with a test accuracy of 92.25%. VGG16 finished second with 91.50%, while Xception achieved 91.00%. A thorough performance assessment was produced by examining these models according to precision, recall, and F1 score. The impressive outcomes, especially for MobileNet, are ascribed to its capacity to effectively extract salient characteristics from the pictures, like distinct patterns and textures connected to different kinds of calamities. The study's overall findings demonstrate that CNN-based models can make precise predictions about natural disasters in real-time situations. To improve early warning systems, the models in this work indicate a high degree of accuracy by merging image data from satellite sources and using advanced data augmentation techniques. The results show how deep learning can practically enhance emergency preparedness and response procedures.  © 2025 IEEE. Convolutional Neural Networks (CNN); Cyclones; Deep Learning; Disaster Management; Disaster Preparedness; Earthquakes; Environmental Data; Floods; Image Classification; Machine Learning; Natural Disaster Prediction; Pattern Recognition; Prediction Accuracy; Satellite Imagery; Wildfires Deep neural networks; Image enhancement; Satellite imagery; Convolutional neural network; Deep learning; Disaster management; Disaster prediction; Disaster preparedness; Environmental data; Images classification; Machine-learning; Natural disaster prediction; Natural disasters; Prediction accuracy; Wildfire; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
160;PREDICTION OF STORM SURGE LEVEL USING ARTIFICIAL NEURAL NETWORK: A CASE STUDY FOR TYPHOON HAIYAN;Storm surge is considered as one of the greatest threats to life and property during a tropical cyclone, especially to a community living near the coastal area. The Philippines is particularly susceptible to the effects of coastal calamities like storm surges and tsunamis since it is an archipelago nation. One way to reduce the risk is to improve the ability of the community to monitor and forecast the hazard through technological research. As such, it is imperative to develop a numerical model that can predict and perform necessary calculations before a storm surge strikes in a coastal area. This paper utilized the Artificial Neural Network (ANN) to predict the storm surge level with 2013 Typhoon Haiyan (Yolanda) as a case study. The proposed model is tested, trained, and validated using the available 101 test data collected from the Guiuan station of PAGASA and NAMRIA. The collected data is composed of six (6) input variables and one (1) output variable. The input variables are the following: astronomical tide, central atmospheric pressure, rainfall intensity, wind radius, wind speed, and depth, while the output parameter is the storm surge level. The optimum mathematical model, as determined by the back-propagation technique in the artificial neural network (ANN) model, is Bayesian Regularization with twelve (12) hidden neurons, with a regression coefficient (R) of 0.99386 and a mean squared error (MSE) of 0.0051569, respectively. The results obtained are quite promising and demonstrate the potential application of the ANN model for disaster risk reduction during tropical storm activity. Copyright © Int. J. of GEOMATE All rights reserved, including making copies, unless permission is obtained from the copyright proprietors.;"Artificial neural network; Disaster risk reduction; Numerical modeling; Storm surge; Typhoon Haiyan";NULL;"PREDICTION OF STORM SURGE LEVEL USING ARTIFICIAL NEURAL NETWORK: A CASE STUDY FOR TYPHOON HAIYAN Storm surge is considered as one of the greatest threats to life and property during a tropical cyclone, especially to a community living near the coastal area. The Philippines is particularly susceptible to the effects of coastal calamities like storm surges and tsunamis since it is an archipelago nation. One way to reduce the risk is to improve the ability of the community to monitor and forecast the hazard through technological research. As such, it is imperative to develop a numerical model that can predict and perform necessary calculations before a storm surge strikes in a coastal area. This paper utilized the Artificial Neural Network (ANN) to predict the storm surge level with 2013 Typhoon Haiyan (Yolanda) as a case study. The proposed model is tested, trained, and validated using the available 101 test data collected from the Guiuan station of PAGASA and NAMRIA. The collected data is composed of six (6) input variables and one (1) output variable. The input variables are the following: astronomical tide, central atmospheric pressure, rainfall intensity, wind radius, wind speed, and depth, while the output parameter is the storm surge level. The optimum mathematical model, as determined by the back-propagation technique in the artificial neural network (ANN) model, is Bayesian Regularization with twelve (12) hidden neurons, with a regression coefficient (R) of 0.99386 and a mean squared error (MSE) of 0.0051569, respectively. The results obtained are quite promising and demonstrate the potential application of the ANN model for disaster risk reduction during tropical storm activity. Copyright © Int. J. of GEOMATE All rights reserved, including making copies, unless permission is obtained from the copyright proprietors. Artificial neural network; Disaster risk reduction; Numerical modeling; Storm surge; Typhoon Haiyan NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
161;Evaluation of the Applications of using Global free Digital Elevation Models and GNSS-RTK data for Agricultural purposes in Egypt using Machine Learning;"Agriculture is a vital component of Egypt's economy; therefore, using Digital Elevation Models (DEMs) in agricultural planning in Egypt has significant benefits regarding water management, site appropriateness assessment, flood risk mitigation, and infrastructure construction. It is also essential for planners to make more informed decisions, optimize resource allocation, and support sustainable farming practices. This research paper investigates the accuracy of obtaining DEM data from four free global models (STRM30, ALOS30, COP30, and TanDEM-X90). The global DEM data has been compared to an actual GNSS-RTK DEM data surveyed onsite for two agricultural block areas in Aswan, the southern Government of Egypt. The two blocks are a part of a national project. For Block I and II, the RMSE of the Model STRM30 was 2.92 m and 3.59 m, respectively, indicating a poorer solution. Regarding accuracy, the ALOS30 model ranks third, reporting an RMSE of 2.58 m for block II and 3.30 m for block I. COP30 has an RMSE value of 1.06 m for blocks I and II and.91 m overall. TanDEM-X90 is the most accurate model in this investigation; block I provided an RMSE of 0.90 m with an SD of 0.58 m (SD95% = 0.38 m). After removing the anomalies, the model's stated RMSE for block II was 0.34 m, with an SD value of 0.62 m and 1.03 m. According to the classification using machine learning algorithms, with an accuracy of 84.7% for block I and 85% for block II, TanDEM-X90 is the best solution. © 2025, Assiut University, Faculty of Engineering. All rights reserved.";"Agricultural; DEM; Egypt; GNSS-RTK; Machine Learning";NULL;"Evaluation of the Applications of using Global free Digital Elevation Models and GNSS-RTK data for Agricultural purposes in Egypt using Machine Learning Agriculture is a vital component of Egypt's economy; therefore, using Digital Elevation Models (DEMs) in agricultural planning in Egypt has significant benefits regarding water management, site appropriateness assessment, flood risk mitigation, and infrastructure construction. It is also essential for planners to make more informed decisions, optimize resource allocation, and support sustainable farming practices. This research paper investigates the accuracy of obtaining DEM data from four free global models (STRM30, ALOS30, COP30, and TanDEM-X90). The global DEM data has been compared to an actual GNSS-RTK DEM data surveyed onsite for two agricultural block areas in Aswan, the southern Government of Egypt. The two blocks are a part of a national project. For Block I and II, the RMSE of the Model STRM30 was 2.92 m and 3.59 m, respectively, indicating a poorer solution. Regarding accuracy, the ALOS30 model ranks third, reporting an RMSE of 2.58 m for block II and 3.30 m for block I. COP30 has an RMSE value of 1.06 m for blocks I and II and.91 m overall. TanDEM-X90 is the most accurate model in this investigation; block I provided an RMSE of 0.90 m with an SD of 0.58 m (SD95% = 0.38 m). After removing the anomalies, the model's stated RMSE for block II was 0.34 m, with an SD value of 0.62 m and 1.03 m. According to the classification using machine learning algorithms, with an accuracy of 84.7% for block I and 85% for block II, TanDEM-X90 is the best solution. © 2025, Assiut University, Faculty of Engineering. All rights reserved. Agricultural; DEM; Egypt; GNSS-RTK; Machine Learning NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
162;Data Anomaly Detection of Electric Submersible Pump Based on Self-Learning Neural Network of Sequential Coding;Electric submersible pumps (ESP) often have a wide range of abnormal data when operating under complex conditions, which brings disaster to oilfield production analysis and data mining. In this paper, we propose a vector normalization method based on physical significance constraint from the difficulty of high variability of data distribution of each parameter of ESP The redistributed features fix the lower limit of parameters such as leakage current at −1, which can reflect the equipment significance of the ESP. In order to improve the multimodal temporal feature extraction capability, an improved model of ESP CNN AutoEncoder is proposed. The model uses one dimensional convolution for forward encoding and inverse decoding, and combines reconstruction errors at different scales to dynamically adjust the tolerance of anomaly detection to capture a higher adaptive threshold window. The system is validated on real-time databases in two oil fields and achieved successful application results. Error reconstruction and sensitivity analysis are performed for detection performance to determine tolerance metrics that can dynamically meet different oilfield requirements. In comparison with Isolation Forest model, significant advantages are achieved in the ESP start-up anomalies. Finally, an ESP anomaly detection system with self-learning capability is realized. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"AutoEncoder; Deep anomaly detection; Fault warning; Unsupervised learning";"Deep neural networks; Electric oil well pumping; Leakage currents; Network coding; Normal distribution; Oil well flooding; Unsupervised learning; Abnormal data; Anomaly detection; Auto encoders; Data anomalies; Deep anomaly detection; Electric submersible pumps; Fault warning; Pump-based; Self-learning neural networks; Sequential coding; Submersible pumps";"Data Anomaly Detection of Electric Submersible Pump Based on Self-Learning Neural Network of Sequential Coding Electric submersible pumps (ESP) often have a wide range of abnormal data when operating under complex conditions, which brings disaster to oilfield production analysis and data mining. In this paper, we propose a vector normalization method based on physical significance constraint from the difficulty of high variability of data distribution of each parameter of ESP The redistributed features fix the lower limit of parameters such as leakage current at −1, which can reflect the equipment significance of the ESP. In order to improve the multimodal temporal feature extraction capability, an improved model of ESP CNN AutoEncoder is proposed. The model uses one dimensional convolution for forward encoding and inverse decoding, and combines reconstruction errors at different scales to dynamically adjust the tolerance of anomaly detection to capture a higher adaptive threshold window. The system is validated on real-time databases in two oil fields and achieved successful application results. Error reconstruction and sensitivity analysis are performed for detection performance to determine tolerance metrics that can dynamically meet different oilfield requirements. In comparison with Isolation Forest model, significant advantages are achieved in the ESP start-up anomalies. Finally, an ESP anomaly detection system with self-learning capability is realized. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. AutoEncoder; Deep anomaly detection; Fault warning; Unsupervised learning Deep neural networks; Electric oil well pumping; Leakage currents; Network coding; Normal distribution; Oil well flooding; Unsupervised learning; Abnormal data; Anomaly detection; Auto encoders; Data anomalies; Deep anomaly detection; Electric submersible pumps; Fault warning; Pump-based; Self-learning neural networks; Sequential coding; Submersible pumps";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;-1;NULL
163;Tropical Monsoon Cyclone Intensity Estimator using INSAT 3-D Image;Tropical cyclones (TC) are very destructive meteorological events that mostly affect coastal regions. For early warning and catastrophe management, TC intensity estimation is essential. One important source of real-time data for cyclone behavior prediction is satellite photography, especially that provided by INSAT-3D. Cyclone intensity may be precisely approximated by utilizing machine learning methods like as convolutional neural networks (CNNs) in conjunction with sophisticated image processing techniques like feature extraction and segmentation. The goal of this work is to develop a reliable model for cyclone strength estimates by integrating deep learning methods with INSAT-3D satellite data. The algorithm makes predictions based on visual characteristics such as wind speeds, sea surface temperatures, and cloud patterns. The suggested approach has the potential for real-time application and shows up to 95% accuracy in intensity prediction. By decreasing human interaction in TC estimates and increasing forecast accuracy, this method improves preparedness for disasters. © 2025 IEEE.;"convolutional neural networks; deep learning; INSAT-3D satellite; intensity estimation; Tropical cyclones";"Atmospheric temperature; Atmospheric thermodynamics; Coastal zones; Convolutional neural networks; Deep neural networks; Image segmentation; Submarine geophysics; 3-D image; Coastal regions; Convolutional neural network; Deep learning; Early warning; INSAT-3d satellite; Intensity estimation; Tropical cyclone; Tropical cyclone intensity; Tropical monsoon; Tropical cyclone";"Tropical Monsoon Cyclone Intensity Estimator using INSAT 3-D Image Tropical cyclones (TC) are very destructive meteorological events that mostly affect coastal regions. For early warning and catastrophe management, TC intensity estimation is essential. One important source of real-time data for cyclone behavior prediction is satellite photography, especially that provided by INSAT-3D. Cyclone intensity may be precisely approximated by utilizing machine learning methods like as convolutional neural networks (CNNs) in conjunction with sophisticated image processing techniques like feature extraction and segmentation. The goal of this work is to develop a reliable model for cyclone strength estimates by integrating deep learning methods with INSAT-3D satellite data. The algorithm makes predictions based on visual characteristics such as wind speeds, sea surface temperatures, and cloud patterns. The suggested approach has the potential for real-time application and shows up to 95% accuracy in intensity prediction. By decreasing human interaction in TC estimates and increasing forecast accuracy, this method improves preparedness for disasters. © 2025 IEEE. convolutional neural networks; deep learning; INSAT-3D satellite; intensity estimation; Tropical cyclones Atmospheric temperature; Atmospheric thermodynamics; Coastal zones; Convolutional neural networks; Deep neural networks; Image segmentation; Submarine geophysics; 3-D image; Coastal regions; Convolutional neural network; Deep learning; Early warning; INSAT-3d satellite; Intensity estimation; Tropical cyclone; Tropical cyclone intensity; Tropical monsoon; Tropical cyclone";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
164;An Early Warning Method for Fracturing Accidents Using Joint CNN and LSTM Modeling;The accuracy and effectiveness of fracturing risk identification in oil and gas wells are of vital importance for optimizing resource development, increasing oil and gas production, and planning scientific fracturing construction strategies. In order to effectively prevent downhole fracturing accidents, a discrimination model for fracturing accidents has been established by analyzing on-site fracturing construction data. In this project, the research on sand plug prediction method based on deep learning is carried out to address the problems in fracturing construction, with the goal of improving the safety of fracturing construction and reducing the cost of extraction. First, based on analyzing the morphology of fracturing construction curves, multiple threshold judging methods are used in order to identify the construction stages where sand plugging and pressure tampering accidents may occur. In order to identify fracturing sand plugging accidents, a sand sensitivity index system is established, and the thresholds are optimized using a particle swarm optimization algorithm. Through the method of multi-threshold fusion, accurate identification of sand plugging accidents during fracturing was realized. By analyzing the fracturing construction parameter data and data mining, this paper also reveals the possible hidden information in the time series, and establishes a joint CNN-LSTM network model to predict the time series data of fracturing construction curves 30 s forward. The experimental results show that the joint CNN-LSTM network model can predict the trend more accurately during the fracturing construction process, and the prediction accuracy is greatly improved compared with the traditional LSTM network model. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"accident identification; CNN-LSTM network; early warning; Oil well fracturing construction";"Gas oils; Natural gas wells; Oil well flooding; Risk management; Accident identification; CNN-LSTM network; Early warning; Early-warning method; Fracturing construction; Network models; Oil and gas well; Oil well fracturing construction; Resource development; Risk Identification; Oil field development";"An Early Warning Method for Fracturing Accidents Using Joint CNN and LSTM Modeling The accuracy and effectiveness of fracturing risk identification in oil and gas wells are of vital importance for optimizing resource development, increasing oil and gas production, and planning scientific fracturing construction strategies. In order to effectively prevent downhole fracturing accidents, a discrimination model for fracturing accidents has been established by analyzing on-site fracturing construction data. In this project, the research on sand plug prediction method based on deep learning is carried out to address the problems in fracturing construction, with the goal of improving the safety of fracturing construction and reducing the cost of extraction. First, based on analyzing the morphology of fracturing construction curves, multiple threshold judging methods are used in order to identify the construction stages where sand plugging and pressure tampering accidents may occur. In order to identify fracturing sand plugging accidents, a sand sensitivity index system is established, and the thresholds are optimized using a particle swarm optimization algorithm. Through the method of multi-threshold fusion, accurate identification of sand plugging accidents during fracturing was realized. By analyzing the fracturing construction parameter data and data mining, this paper also reveals the possible hidden information in the time series, and establishes a joint CNN-LSTM network model to predict the time series data of fracturing construction curves 30 s forward. The experimental results show that the joint CNN-LSTM network model can predict the trend more accurately during the fracturing construction process, and the prediction accuracy is greatly improved compared with the traditional LSTM network model. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. accident identification; CNN-LSTM network; early warning; Oil well fracturing construction Gas oils; Natural gas wells; Oil well flooding; Risk management; Accident identification; CNN-LSTM network; Early warning; Early-warning method; Fracturing construction; Network models; Oil and gas well; Oil well fracturing construction; Resource development; Risk Identification; Oil field development";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
165;Post Wildfire Burnt-up Detection Using Siamese UNet;In this article, we present an approach for detecting burnt area due to wild fire in Sentinel-2 images by leveraging the power of Siamese neural networks. By employing a Siamese network, we are able to efficiently encode the feature extraction process for pairs of images. This is achieved by utilizing two branches within the Siamese network, which capture and combine information at different resolutions to make predictions. The weights are shared between these two branches in siamese networks. This design allows to effectively analyze the changes between two remote sensing images, enabling precise identification of areas impacted by forest wildfires in the state of California as part of ChaBuD challenge thereby assisting local authorities in effectively monitoring the impacted regions and facilitating the restoration process. We experimented with various model architectures to train ChaBuD dataset and carefully evaluated the performance. Through rigorous testing and analysis, we have achieved promising results, ultimately obtaining a final private score (IoU) of 0.7495 on the hidden test dataset. The code is available at https://github.com/kavyagupta/chabud. We also deploy the final model as a point solution for anyone to use at https://firemap.io. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"change detection; high resolution satellite imagery; remote sensing; siamese networks";"Change detection; Deforestation; Image coding; Image reconstruction; Network coding; Premixed flames; Satellite imagery; Tropics; Burn up; Burnt areas; Change detection; Features extraction; High resolution satellite imagery; Neural-networks; Power; Remote-sensing; Siamese network; Wild fire; HTTP";"Post Wildfire Burnt-up Detection Using Siamese UNet In this article, we present an approach for detecting burnt area due to wild fire in Sentinel-2 images by leveraging the power of Siamese neural networks. By employing a Siamese network, we are able to efficiently encode the feature extraction process for pairs of images. This is achieved by utilizing two branches within the Siamese network, which capture and combine information at different resolutions to make predictions. The weights are shared between these two branches in siamese networks. This design allows to effectively analyze the changes between two remote sensing images, enabling precise identification of areas impacted by forest wildfires in the state of California as part of ChaBuD challenge thereby assisting local authorities in effectively monitoring the impacted regions and facilitating the restoration process. We experimented with various model architectures to train ChaBuD dataset and carefully evaluated the performance. Through rigorous testing and analysis, we have achieved promising results, ultimately obtaining a final private score (IoU) of 0.7495 on the hidden test dataset. The code is available at https://github.com/kavyagupta/chabud. We also deploy the final model as a point solution for anyone to use at https://firemap.io. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. change detection; high resolution satellite imagery; remote sensing; siamese networks Change detection; Deforestation; Image coding; Image reconstruction; Network coding; Premixed flames; Satellite imagery; Tropics; Burn up; Burnt areas; Change detection; Features extraction; High resolution satellite imagery; Neural-networks; Power; Remote-sensing; Siamese network; Wild fire; HTTP";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;4;Recovery
166;Integrating Intelligent Hydro-informatics into an effective Early Warning System for risk-informed urban flood management;The urban drainage system constantly facing flooding issues in coastal and urban areas. Robust and accurate urban flood management, particularly considering fast-moving compound floods, is crucial to minimize the impact of flood disasters in coastal cities. Till now, Ho Chi Minh City (HCMC) lacks an effective means of urban flood management because of flood risk communication among residents. Existing flood risk communication tools rely on post-disaster flood model outcomes and data. Therefore, this research proposes a real-time Early Urban Flooding Warning System (EUFWS) integrated with a user-friendly web and app interface. The backbone of this system consists of flood models developed using machine learning (ML) algorithms, combined with big data and Web-GIS visualization, with ML serving as the core for constructing the EUFWS. EUFWS offer several key advantages: they are available at all times, accessible from anywhere, and provide a real-time, multi-user working platform. Additionally, the system is flexible, allowing for the easy addition of components and services and scalable, adjusting to workload demands. EUFWS have been successfully deployed in Thu Duc City, Vietnam, as a case study and are operating effectively. EUFWS have been successfully deployed in Thu Duc City, Vietnam, as a case study and are operating effectively. Research results indicate that EUFWS supported decision-makers to be effectively risk informed and make intelligent decisions during urban flood emergencies. This underscores the significant potential of integrating ML and information technology to enhance the management of smart urban drainage systems in flood-prone cities worldwide. © 2024 Elsevier Ltd;"Early warning system; ML model; Open-source; Real-time; Urban flooding";"Ho Chi Minh City; Viet Nam; Flood damage; Information management; Open source software; Risk management; Early Warning System; Flood management; Flood modeling; Flood risk communications; Machine learning models; Open-source; Real- time; Urban drainage systems; Urban flooding; Urban floods; algorithm; early warning system; flood control; flooding; numerical model; real time; risk assessment; urban area; urban drainage; Decision making";"Integrating Intelligent Hydro-informatics into an effective Early Warning System for risk-informed urban flood management The urban drainage system constantly facing flooding issues in coastal and urban areas. Robust and accurate urban flood management, particularly considering fast-moving compound floods, is crucial to minimize the impact of flood disasters in coastal cities. Till now, Ho Chi Minh City (HCMC) lacks an effective means of urban flood management because of flood risk communication among residents. Existing flood risk communication tools rely on post-disaster flood model outcomes and data. Therefore, this research proposes a real-time Early Urban Flooding Warning System (EUFWS) integrated with a user-friendly web and app interface. The backbone of this system consists of flood models developed using machine learning (ML) algorithms, combined with big data and Web-GIS visualization, with ML serving as the core for constructing the EUFWS. EUFWS offer several key advantages: they are available at all times, accessible from anywhere, and provide a real-time, multi-user working platform. Additionally, the system is flexible, allowing for the easy addition of components and services and scalable, adjusting to workload demands. EUFWS have been successfully deployed in Thu Duc City, Vietnam, as a case study and are operating effectively. EUFWS have been successfully deployed in Thu Duc City, Vietnam, as a case study and are operating effectively. Research results indicate that EUFWS supported decision-makers to be effectively risk informed and make intelligent decisions during urban flood emergencies. This underscores the significant potential of integrating ML and information technology to enhance the management of smart urban drainage systems in flood-prone cities worldwide. © 2024 Elsevier Ltd Early warning system; ML model; Open-source; Real-time; Urban flooding Ho Chi Minh City; Viet Nam; Flood damage; Information management; Open source software; Risk management; Early Warning System; Flood management; Flood modeling; Flood risk communications; Machine learning models; Open-source; Real- time; Urban drainage systems; Urban flooding; Urban floods; algorithm; early warning system; flood control; flooding; numerical model; real time; risk assessment; urban area; urban drainage; Decision making";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
167;A Multi-Criteria GIS-Based Approach for Risk Assessment of Slope Instability Driven by Glacier Melting in the Alpine Area;Climate change is resulting in significant transformations in mountain areas all over the world, causing the melting of glacier ice, reduction in snow accumulation, and permafrost loss. Changes in the mountain cryosphere are not only modifying flora and fauna distributions but also affecting the stability of slopes in those regions. For all these reasons, and because of the risks these phenomena pose to the population, the dentification of dangerous areas is a crucial step in the development of risk reduction strategies. While several methods and examples exist that cover the assessment and computation of single sub-components, there is still a lack of application of risk assessment due to glacier melting over large areas in which the final result can be directly employed in the design of risk mitigation policies at regional and municipal levels. This research is focused on landslides and gravitational movements on slopes resulting from rapid glacier melting phenomena in the Valle d’Aosta region in Italy, with the aim of providing a tool that can support spatial planning in response to climate change in Alpine environments. Through the conceptualization and development of a GIS-based and multi-criteria approach, risk is then estimated by defining hazard indices that consider different aspects, combining the experience acquired from studies carried out in various disciplinary fields, to obtain a framework at the regional level. This first assessment is then deepened for the Lys River Valley, where the mapping of hazardous areas was implemented, obtaining a classification of buildings according to their hazard score to estimate the potential damage and total risk relating to possible slope instability events due to ice melt at the local scale. © 2024 by the authors.;"Alpine landscape; disaster risk management; GIS methodology; glacier melting; risk assessment; slope instability; urban planning";"Disaster prevention; Disasters; Glacial geology; Glaciers; Landslides; Permafrost; Risk analysis; Risk management; Risk perception; Snow melting systems; Tropics; Urban planning; Alpine landscape; Disaster risk management; GIS methodology; Glacier melting; Multi-criteria; Multi-Criterion; Regional levels; Risks assessments; Risks management; Slope instability; Risk assessment";"A Multi-Criteria GIS-Based Approach for Risk Assessment of Slope Instability Driven by Glacier Melting in the Alpine Area Climate change is resulting in significant transformations in mountain areas all over the world, causing the melting of glacier ice, reduction in snow accumulation, and permafrost loss. Changes in the mountain cryosphere are not only modifying flora and fauna distributions but also affecting the stability of slopes in those regions. For all these reasons, and because of the risks these phenomena pose to the population, the dentification of dangerous areas is a crucial step in the development of risk reduction strategies. While several methods and examples exist that cover the assessment and computation of single sub-components, there is still a lack of application of risk assessment due to glacier melting over large areas in which the final result can be directly employed in the design of risk mitigation policies at regional and municipal levels. This research is focused on landslides and gravitational movements on slopes resulting from rapid glacier melting phenomena in the Valle d’Aosta region in Italy, with the aim of providing a tool that can support spatial planning in response to climate change in Alpine environments. Through the conceptualization and development of a GIS-based and multi-criteria approach, risk is then estimated by defining hazard indices that consider different aspects, combining the experience acquired from studies carried out in various disciplinary fields, to obtain a framework at the regional level. This first assessment is then deepened for the Lys River Valley, where the mapping of hazardous areas was implemented, obtaining a classification of buildings according to their hazard score to estimate the potential damage and total risk relating to possible slope instability events due to ice melt at the local scale. © 2024 by the authors. Alpine landscape; disaster risk management; GIS methodology; glacier melting; risk assessment; slope instability; urban planning Disaster prevention; Disasters; Glacial geology; Glaciers; Landslides; Permafrost; Risk analysis; Risk management; Risk perception; Snow melting systems; Tropics; Urban planning; Alpine landscape; Disaster risk management; GIS methodology; Glacier melting; Multi-criteria; Multi-Criterion; Regional levels; Risks assessments; Risks management; Slope instability; Risk assessment";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
168;A Climate Adaptation Asset Risk Management Approach for Resilient Roadway Infrastructure;As climate change intensifies, roadway infrastructure is increasingly at risk from extreme weather events including floods, hurricanes, and wildfires. This paper presents a system-of-systems performance-based asset risk management approach, designed to integrate various elements for effective investment prioritization and infrastructure resilience. Central to this approach are an Asset Inventory Database and a Risk Registry Database, supported by a Common Reference Location System (GIS). These components are the foundation for analytical modules to assess vulnerability and resilience based on exposure, sensitivity, and adaptive capacity. The approach includes an actionable framework to support a proactive data-driven performance-based management process for prioritizing investments. The project prioritization process consists of four steps: identifying risk factors, integrating climate data, conducting advanced risk assessments, and project prioritization. The goal is to prioritize resource allocation and develop climate-adaptive risk mitigation management strategies. Key performance indicators (KPIs) are recommended for setting goals, monitoring the outcomes of these strategies, and measuring their benefits. A Climate Impact Vulnerability Score (CIVS) is proposed to assess the susceptibility of infrastructure assets to environmental conditions. The approach also leverages artificial intelligence (AI) tools to analyze roadway infrastructure vulnerabilities and climate risk exposure. A case study applied to bridges using k-means clustering and multi-criteria decision analysis (MCDA) demonstrates the potential of advanced analytical methods in improving decision-making. This research concludes that the approach will contribute to enhancing resource allocation, supporting strategic decisions, aligning goals with budgets prioritizing investments, and strengthening the resilience and sustainability of roadway infrastructure. © 2024 by the authors.;"artificial intelligence tools; bridge project prioritization; climate adaptation; key performance indicators; risk assessments; roadway infrastructure resilience; transportation asset management";NULL;"A Climate Adaptation Asset Risk Management Approach for Resilient Roadway Infrastructure As climate change intensifies, roadway infrastructure is increasingly at risk from extreme weather events including floods, hurricanes, and wildfires. This paper presents a system-of-systems performance-based asset risk management approach, designed to integrate various elements for effective investment prioritization and infrastructure resilience. Central to this approach are an Asset Inventory Database and a Risk Registry Database, supported by a Common Reference Location System (GIS). These components are the foundation for analytical modules to assess vulnerability and resilience based on exposure, sensitivity, and adaptive capacity. The approach includes an actionable framework to support a proactive data-driven performance-based management process for prioritizing investments. The project prioritization process consists of four steps: identifying risk factors, integrating climate data, conducting advanced risk assessments, and project prioritization. The goal is to prioritize resource allocation and develop climate-adaptive risk mitigation management strategies. Key performance indicators (KPIs) are recommended for setting goals, monitoring the outcomes of these strategies, and measuring their benefits. A Climate Impact Vulnerability Score (CIVS) is proposed to assess the susceptibility of infrastructure assets to environmental conditions. The approach also leverages artificial intelligence (AI) tools to analyze roadway infrastructure vulnerabilities and climate risk exposure. A case study applied to bridges using k-means clustering and multi-criteria decision analysis (MCDA) demonstrates the potential of advanced analytical methods in improving decision-making. This research concludes that the approach will contribute to enhancing resource allocation, supporting strategic decisions, aligning goals with budgets prioritizing investments, and strengthening the resilience and sustainability of roadway infrastructure. © 2024 by the authors. artificial intelligence tools; bridge project prioritization; climate adaptation; key performance indicators; risk assessments; roadway infrastructure resilience; transportation asset management NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;1;Prevention
169;CFALEA_LSTM: Adaptive Lotus Effect Algorithm Enabled Long Short-Term Memory for Rainfall Prediction Using Time Series Data;Rainfall prediction (RP) utilizing machine learning (ML) is an imperative tool for agricultural and economic practices. RP is also considered as a challenging chore in weather forecasting. An accurate RP can assist farmers to make knowledgeable decisions regarding crop planting, harvesting and fertilization. It also helps in the prevention of floods, disaster management and water resource management. Therefore, the prediction of precise rainfall is significant for diverse applications of a country. Here, Adaptive Lotus Effect algorithm-based Long Short-Term Memory (ALEA_LSTM) is presented for RP utilizing time series data (TSD). Initially, TSD is obtained from a specific database and thereafter technical indicators for RP are extracted from the considered input TSD. Afterwards, feature selection (FS) is accomplished to choose suitable features employing mutual information (MI). Then, data augmentation (DA) is conducted by the oversampling method to augment the dimensionality of data. At last, RP is executed by Long Short-Term Memory (LSTM) that is trained by the Adaptive Lotus Effect Algorithm (ALEA). Moreover, ALEA is devised by integrating the adaptive concept with the Lotus Effect Algorithm (LEA). In addition, ALEA_LSTM achieved minimal values of Mean absolute percentage error (MAPE) of about 0.359, Mean squared error (MSE) of about 0.133 and Root mean squared error (RMSE) of about 0.364.  © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Long Short-Term Memory (LSTM); Lotus Effect Algorithm (LEA); mutual information (MI); oversampling method; Rainfall prediction (RP)";"Machine learning; Rain; Resource allocation; Water management; Weather forecasting; Long short-term memory; Lotus effect; Lotus effect algorithm; Mutual information; Mutual informations; Over sampling; Oversampling method; Rainfall prediction; Short term memory; Mean square error";"CFALEA_LSTM: Adaptive Lotus Effect Algorithm Enabled Long Short-Term Memory for Rainfall Prediction Using Time Series Data Rainfall prediction (RP) utilizing machine learning (ML) is an imperative tool for agricultural and economic practices. RP is also considered as a challenging chore in weather forecasting. An accurate RP can assist farmers to make knowledgeable decisions regarding crop planting, harvesting and fertilization. It also helps in the prevention of floods, disaster management and water resource management. Therefore, the prediction of precise rainfall is significant for diverse applications of a country. Here, Adaptive Lotus Effect algorithm-based Long Short-Term Memory (ALEA_LSTM) is presented for RP utilizing time series data (TSD). Initially, TSD is obtained from a specific database and thereafter technical indicators for RP are extracted from the considered input TSD. Afterwards, feature selection (FS) is accomplished to choose suitable features employing mutual information (MI). Then, data augmentation (DA) is conducted by the oversampling method to augment the dimensionality of data. At last, RP is executed by Long Short-Term Memory (LSTM) that is trained by the Adaptive Lotus Effect Algorithm (ALEA). Moreover, ALEA is devised by integrating the adaptive concept with the Lotus Effect Algorithm (LEA). In addition, ALEA_LSTM achieved minimal values of Mean absolute percentage error (MAPE) of about 0.359, Mean squared error (MSE) of about 0.133 and Root mean squared error (RMSE) of about 0.364.  © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Long Short-Term Memory (LSTM); Lotus Effect Algorithm (LEA); mutual information (MI); oversampling method; Rainfall prediction (RP) Machine learning; Rain; Resource allocation; Water management; Weather forecasting; Long short-term memory; Lotus effect; Lotus effect algorithm; Mutual information; Mutual informations; Over sampling; Oversampling method; Rainfall prediction; Short term memory; Mean square error";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
170;Predictive Modeling of Water Level in the San Juan River Using Hybrid Neural Networks Integrated with Kalman Smoothing Methods;This study presents an innovative approach to predicting the water level in the San Juan River, Chocó, Colombia, by implementing two hybrid models: nonlinear auto-regressive with exogenous inputs (NARX) and long short-term memory (LSTM). These models combine artificial neural networks with smoothing techniques, including the exponential, Savitzky–Golay, and Rauch–Tung–Striebel (RTS) smoothing filters, with the aim of improving the accuracy of hydrological predictions. Given the high rainfall in the region, the San Juan River experiences significant fluctuations in its water levels, which presents a challenge for accurate prediction. The models were trained using historical data, and various smoothing techniques were applied to optimize data quality and reduce noise. The effectiveness of the models was evaluated using standard regression metrics, such as Nash–Sutcliffe efficiency (NSE), mean square error (MSE), and mean absolute error (MAE), in addition to Kling–Gupta efficiency (KGE). The results show that the combination of neural networks with smoothing filters, especially the RTS filter and smoothed Kalman filter, provided the most accurate predictions, outperforming traditional methods. This research has important implications for water resource management and flood prevention in vulnerable areas such as Chocó. The implementation of these hybrid models will allow local authorities to anticipate changes in water levels and plan preventive measures more effectively, thus reducing the risk of damage from extreme events. In summary, this study establishes a solid foundation for future research in water level prediction, highlighting the importance of integrating advanced technologies in water resources management. © 2024 by the authors.;"filtering techniques; hydrological forecasting; nonlinear models; parameter estimation; time series models";"Flood control; Flood damage; Kalman filters; Prediction models; Resource allocation; Risk assessment; Rivers; Water management; Weather forecasting; Accurate prediction; Filtering technique; Hybrid model; Hydrological forecasting; Non-linear modelling; Parameters estimation; Smoothing filters; Smoothing techniques; Times series models; Water resources management; Long short-term memory";"Predictive Modeling of Water Level in the San Juan River Using Hybrid Neural Networks Integrated with Kalman Smoothing Methods This study presents an innovative approach to predicting the water level in the San Juan River, Chocó, Colombia, by implementing two hybrid models: nonlinear auto-regressive with exogenous inputs (NARX) and long short-term memory (LSTM). These models combine artificial neural networks with smoothing techniques, including the exponential, Savitzky–Golay, and Rauch–Tung–Striebel (RTS) smoothing filters, with the aim of improving the accuracy of hydrological predictions. Given the high rainfall in the region, the San Juan River experiences significant fluctuations in its water levels, which presents a challenge for accurate prediction. The models were trained using historical data, and various smoothing techniques were applied to optimize data quality and reduce noise. The effectiveness of the models was evaluated using standard regression metrics, such as Nash–Sutcliffe efficiency (NSE), mean square error (MSE), and mean absolute error (MAE), in addition to Kling–Gupta efficiency (KGE). The results show that the combination of neural networks with smoothing filters, especially the RTS filter and smoothed Kalman filter, provided the most accurate predictions, outperforming traditional methods. This research has important implications for water resource management and flood prevention in vulnerable areas such as Chocó. The implementation of these hybrid models will allow local authorities to anticipate changes in water levels and plan preventive measures more effectively, thus reducing the risk of damage from extreme events. In summary, this study establishes a solid foundation for future research in water level prediction, highlighting the importance of integrating advanced technologies in water resources management. © 2024 by the authors. filtering techniques; hydrological forecasting; nonlinear models; parameter estimation; time series models Flood control; Flood damage; Kalman filters; Prediction models; Resource allocation; Risk assessment; Rivers; Water management; Weather forecasting; Accurate prediction; Filtering technique; Hybrid model; Hydrological forecasting; Non-linear modelling; Parameters estimation; Smoothing filters; Smoothing techniques; Times series models; Water resources management; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
171;Unsupervised deep learning approach for structural anomaly detection using probabilistic features;Civil structures may deteriorate during their service life due to degradation or damage imposed by natural hazards such as earthquakes, wind, and impact. Structural performance anomaly detection is essential to provide an early warning of structural degradation limit states in order to prevent potential catastrophic failure. Data-driven machine learning approaches have been widely used for this, due to their capability in capturing features sensitive to damage-induced anomalies from structural health monitoring (SHM) data, assuming that such data are available. Although machine learning models have been used, many are challenged by the vast operational and environmental variability that can corrupt SHM data and by (typically) strongly correlated information from different sensors in the SHM data. This paper proposes an unsupervised deep learning approach for the detection of structural anomaly based on a deep convolutional variational autoencoder (DCVAE) for feature extraction coupled with support vector data description (SVDD) for anomaly detection. The proposed DCVAE-SVDD method has several appealing strengths. First, the variational latent encoding is used to capture the features of monitoring data through a probability distribution. The integration of the Kullback–Leibler divergence in the loss function provides accurate estimation of the probability distributions. Second, the DCVAE designed with convolutional and deconvolutional operations utilizes the correlation among multisensor data to avoid loss of correlation features and achieve better performance in feature extraction. Third, the SVDD is utilized to create a minimum-volume hypersphere that contains the anomaly-sensitive statistical features of the state. The hypersphere accurately separates anomaly-sensitive statistical features of reference states of structure from the anomalous ones. A computational frame model and a laboratory grandstand model are used to evaluate the performance of the proposed method for detecting structural anomaly. The results demonstrate the superiority of the proposed DCVAE-SVDD in detection accuracy over the other commonly used structural anomaly detection methods (deep autoencoder combined with SVDD autoregressive model with one-class support vector machine, and principal component analysis). © The Author(s) 2024.;"Structural anomaly detection; structural health monitoring; support vector data description; unsupervised deep learning; variational autoencoder";"Anomaly detection; Convolution; Data description; Deep learning; Extraction; Feature extraction; Learning systems; Principal component analysis; Probability distributions; Support vector machines; Anomaly detection; Auto encoders; Features extraction; Learning approach; Probability: distributions; Structural anomaly; Structural anomaly detection; Support vector data description; Unsupervised deep learning; Variational autoencoder; Structural health monitoring";"Unsupervised deep learning approach for structural anomaly detection using probabilistic features Civil structures may deteriorate during their service life due to degradation or damage imposed by natural hazards such as earthquakes, wind, and impact. Structural performance anomaly detection is essential to provide an early warning of structural degradation limit states in order to prevent potential catastrophic failure. Data-driven machine learning approaches have been widely used for this, due to their capability in capturing features sensitive to damage-induced anomalies from structural health monitoring (SHM) data, assuming that such data are available. Although machine learning models have been used, many are challenged by the vast operational and environmental variability that can corrupt SHM data and by (typically) strongly correlated information from different sensors in the SHM data. This paper proposes an unsupervised deep learning approach for the detection of structural anomaly based on a deep convolutional variational autoencoder (DCVAE) for feature extraction coupled with support vector data description (SVDD) for anomaly detection. The proposed DCVAE-SVDD method has several appealing strengths. First, the variational latent encoding is used to capture the features of monitoring data through a probability distribution. The integration of the Kullback–Leibler divergence in the loss function provides accurate estimation of the probability distributions. Second, the DCVAE designed with convolutional and deconvolutional operations utilizes the correlation among multisensor data to avoid loss of correlation features and achieve better performance in feature extraction. Third, the SVDD is utilized to create a minimum-volume hypersphere that contains the anomaly-sensitive statistical features of the state. The hypersphere accurately separates anomaly-sensitive statistical features of reference states of structure from the anomalous ones. A computational frame model and a laboratory grandstand model are used to evaluate the performance of the proposed method for detecting structural anomaly. The results demonstrate the superiority of the proposed DCVAE-SVDD in detection accuracy over the other commonly used structural anomaly detection methods (deep autoencoder combined with SVDD autoregressive model with one-class support vector machine, and principal component analysis). © The Author(s) 2024. Structural anomaly detection; structural health monitoring; support vector data description; unsupervised deep learning; variational autoencoder Anomaly detection; Convolution; Data description; Deep learning; Extraction; Feature extraction; Learning systems; Principal component analysis; Probability distributions; Support vector machines; Anomaly detection; Auto encoders; Features extraction; Learning approach; Probability: distributions; Structural anomaly; Structural anomaly detection; Support vector data description; Unsupervised deep learning; Variational autoencoder; Structural health monitoring";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;1;Prevention
172;Modeling Seismic Signals of an Earthquake through Symbolic Dynamical System: The Case of Surigao Region, Philippines;Earthquakes pose threats globally, most especially in developing hotspots such as the Philippines, given that it falls under the Pacific Ring of Fire. One of the most seismically active regions within the country is the Surigao region located in northeast Mindanao, where two large earthquakes hit in December 2023 with 7.4 and 6.8 magnitudes. The outcome of this study underscores the need for accurate earthquake forecasting models in such high-risk regions. Conventional methods for seismic signal modeling usually lose the complexity and nonlinear of seismic datasets. This paper uses symbolic regression with genetic programming as a means to come up with a more accurate and explainable model in estimating earthquake magnitudes in the Surigao area. A technique in machine learning, symbolic regression generates mathematical expressions which best fit empirical data, and it doesn't require a predefined model configuration. The developed model identifies critical parameters relevant to seismic phenomena and accurately estimates the magnitudes of earthquakes within the considered region. The model, such as the magnitude of 3.0, as happening in September 2024. The results obtained in this study contribute to the accuracy of forecasting for earthquakes by providing SR as an efficient tool for studying seismic records. These findings have the potential to further improve this effort in disaster preparedness and risk reduction that saves lives and reduces economic losses in earthquake-prone areas. Collaboration is encouraged with a continued comprehensive revision toward research for further exploration of broader applications in seismic hazard analysis. Copyright © 2024 by Author/s and Licensed by JISEM.;"earthquake; genetic programming; magnitude; seismic signal; symbolic regression";NULL;"Modeling Seismic Signals of an Earthquake through Symbolic Dynamical System: The Case of Surigao Region, Philippines Earthquakes pose threats globally, most especially in developing hotspots such as the Philippines, given that it falls under the Pacific Ring of Fire. One of the most seismically active regions within the country is the Surigao region located in northeast Mindanao, where two large earthquakes hit in December 2023 with 7.4 and 6.8 magnitudes. The outcome of this study underscores the need for accurate earthquake forecasting models in such high-risk regions. Conventional methods for seismic signal modeling usually lose the complexity and nonlinear of seismic datasets. This paper uses symbolic regression with genetic programming as a means to come up with a more accurate and explainable model in estimating earthquake magnitudes in the Surigao area. A technique in machine learning, symbolic regression generates mathematical expressions which best fit empirical data, and it doesn't require a predefined model configuration. The developed model identifies critical parameters relevant to seismic phenomena and accurately estimates the magnitudes of earthquakes within the considered region. The model, such as the magnitude of 3.0, as happening in September 2024. The results obtained in this study contribute to the accuracy of forecasting for earthquakes by providing SR as an efficient tool for studying seismic records. These findings have the potential to further improve this effort in disaster preparedness and risk reduction that saves lives and reduces economic losses in earthquake-prone areas. Collaboration is encouraged with a continued comprehensive revision toward research for further exploration of broader applications in seismic hazard analysis. Copyright © 2024 by Author/s and Licensed by JISEM. earthquake; genetic programming; magnitude; seismic signal; symbolic regression NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
173;Severity of Flood Damage Estimation from Aerial Scenery;Accurate assessment of the flood damage and its severity estimation is essential for effective disaster management and related reconstruction works. In our study, we propose a novel approach for estimating the severity of flood damage from aerial scene images. We introduce a fusion network architecture that leverages both RGB and generated pseudo thermal image modalities. Our approach is based on training a CNN head as U-Net model to perform semantic segmentation of images from flood scenes. We show that the feature maps extracted from multimodal data, helps to improve accuracy even though one of the modalities is pseudo generated via CycleGAN. These feature maps are fed into a custom fully connected network for regression, predicting the severity level of flood damage. Our use of CycleGAN to generate thermal images from RGB images, providing additional input modalities for our network. Our approach significantly outperforms baseline methods, showcasing the effectiveness of leveraging multiple modalities for flood damage severity estimation. The results from our regression network show that our fused network outperforms conventional approaches. Our method achieves 0.053 MAE and 0.008 MSE, indicating a substantial enhancement of performance compared to baseline methods. These results show the importance of multimodal fusion via pseudo modality generation, which also offers valuable insights in flood damage assessment. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Disaster management; Flood damage assessment; Regression Analysis; Semantic segmentation; Thermal image generation";"Aerial photography; Aircraft accidents; Antenna feeders; Damage detection; Disaster prevention; Disasters; Flood control; Flood damage; Image coding; Information management; Thermography (imaging); Baseline methods; Damage assessments; Damage estimation; Disaster management; Feature map; Flood damage assessment; Image generations; Semantic segmentation; Thermal image generation; Thermal images; Semantic Segmentation";"Severity of Flood Damage Estimation from Aerial Scenery Accurate assessment of the flood damage and its severity estimation is essential for effective disaster management and related reconstruction works. In our study, we propose a novel approach for estimating the severity of flood damage from aerial scene images. We introduce a fusion network architecture that leverages both RGB and generated pseudo thermal image modalities. Our approach is based on training a CNN head as U-Net model to perform semantic segmentation of images from flood scenes. We show that the feature maps extracted from multimodal data, helps to improve accuracy even though one of the modalities is pseudo generated via CycleGAN. These feature maps are fed into a custom fully connected network for regression, predicting the severity level of flood damage. Our use of CycleGAN to generate thermal images from RGB images, providing additional input modalities for our network. Our approach significantly outperforms baseline methods, showcasing the effectiveness of leveraging multiple modalities for flood damage severity estimation. The results from our regression network show that our fused network outperforms conventional approaches. Our method achieves 0.053 MAE and 0.008 MSE, indicating a substantial enhancement of performance compared to baseline methods. These results show the importance of multimodal fusion via pseudo modality generation, which also offers valuable insights in flood damage assessment. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Disaster management; Flood damage assessment; Regression Analysis; Semantic segmentation; Thermal image generation Aerial photography; Aircraft accidents; Antenna feeders; Damage detection; Disaster prevention; Disasters; Flood control; Flood damage; Image coding; Information management; Thermography (imaging); Baseline methods; Damage assessments; Damage estimation; Disaster management; Feature map; Flood damage assessment; Image generations; Semantic segmentation; Thermal image generation; Thermal images; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;4;Recovery
174;A Hybrid Machine Learning and Kriging Approach for Rainfall Interpolation;The machine learning technique is a computational algorithm that is currently developing and is widely used in various models because it has the advantage of predicting without limited assumptions. Therefore, the purpose of this research is to develop a machine learning method on the kriging interpolation method to overcome the unmet assumptions of stationary and normally distributed rainfall data in East Java. This research is expected to make an important contribution as input for flood hydrograph models, which will improve understanding of flood behavior and support flood early warning systems in East Java. © (2025), (Badih/Ghusayni Ed. & Pub.). All rights reserved.;"Interpolation; Krigingm; Machine Learning";NULL;"A Hybrid Machine Learning and Kriging Approach for Rainfall Interpolation The machine learning technique is a computational algorithm that is currently developing and is widely used in various models because it has the advantage of predicting without limited assumptions. Therefore, the purpose of this research is to develop a machine learning method on the kriging interpolation method to overcome the unmet assumptions of stationary and normally distributed rainfall data in East Java. This research is expected to make an important contribution as input for flood hydrograph models, which will improve understanding of flood behavior and support flood early warning systems in East Java. © (2025), (Badih/Ghusayni Ed. & Pub.). All rights reserved. Interpolation; Krigingm; Machine Learning NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
175;Real-Time Detection of Forest Fires Using FireNet-CNN and Explainable AI Techniques;This study presents FireNet-CNN, an advanced deep-learning model particularly designed for forest fire detection, which significantly surpasses existing methods in terms of reliability, efficiency, and interpretability. FireNet-CNN is compared to popular pre-trained models, including VGG16, VGG19, and Inception V3, across key performance metrics and consistently shows superior results, achieving 99.05% accuracy, 99.41% precision, and 98.28% recall. The model was evaluated using two augmented datasets: Dataset A and Dataset B, which consist of fire and non-fire images sourced from multiple video and image datasets. FireNet-CNN's architecture, which includes 2.75 million parameters and a compact model size of 10.58 MB, has been meticulously optimized for fire detection tasks. As a consequence, the inference time of 0.95 seconds/image enables fast real-time deployment especially suitable for resource-constrained platforms like drones, remote sensors or other types of embedded systems in wooded regions. FireNet-CNN uses synthetic data augmentation based on Stable Diffusion to overcome the limitations of dataset size and class imbalance. This augmentation is critical as it helps the model accurately identify fire instances with a lower false positive rate, which is key for any real-time fire detection system where reliability and dependability are vital. To improve transparency and trust in safety-critical applications, FireNet-CNN incorporates the explainable AI (XAI) techniques, such as Grad-CAM and Saliency Maps. Despite encountering challenges such as reliance on synthetic data and issues of class imbalance, FireNet-CNN has demonstrated promising potential as a viable and effective solution for early wildfire detection. It offers significant insights for future research and practical applications in fire management and disaster response.  © 2025 IEEE.;"deep learning; explainable AI; Forest fire detection; generative AI; wildfire monitoring";"Deep learning; Deforestation; Disasters; Fire detectors; Fire hazards; AI techniques; Class imbalance; Deep learning; Explainable AI; Forest fire detection; Forest fires; Generative AI; Real-time detection; Synthetic data; Wildfire monitoring; Premixed flames";"Real-Time Detection of Forest Fires Using FireNet-CNN and Explainable AI Techniques This study presents FireNet-CNN, an advanced deep-learning model particularly designed for forest fire detection, which significantly surpasses existing methods in terms of reliability, efficiency, and interpretability. FireNet-CNN is compared to popular pre-trained models, including VGG16, VGG19, and Inception V3, across key performance metrics and consistently shows superior results, achieving 99.05% accuracy, 99.41% precision, and 98.28% recall. The model was evaluated using two augmented datasets: Dataset A and Dataset B, which consist of fire and non-fire images sourced from multiple video and image datasets. FireNet-CNN's architecture, which includes 2.75 million parameters and a compact model size of 10.58 MB, has been meticulously optimized for fire detection tasks. As a consequence, the inference time of 0.95 seconds/image enables fast real-time deployment especially suitable for resource-constrained platforms like drones, remote sensors or other types of embedded systems in wooded regions. FireNet-CNN uses synthetic data augmentation based on Stable Diffusion to overcome the limitations of dataset size and class imbalance. This augmentation is critical as it helps the model accurately identify fire instances with a lower false positive rate, which is key for any real-time fire detection system where reliability and dependability are vital. To improve transparency and trust in safety-critical applications, FireNet-CNN incorporates the explainable AI (XAI) techniques, such as Grad-CAM and Saliency Maps. Despite encountering challenges such as reliance on synthetic data and issues of class imbalance, FireNet-CNN has demonstrated promising potential as a viable and effective solution for early wildfire detection. It offers significant insights for future research and practical applications in fire management and disaster response.  © 2025 IEEE. deep learning; explainable AI; Forest fire detection; generative AI; wildfire monitoring Deep learning; Deforestation; Disasters; Fire detectors; Fire hazards; AI techniques; Class imbalance; Deep learning; Explainable AI; Forest fire detection; Forest fires; Generative AI; Real-time detection; Synthetic data; Wildfire monitoring; Premixed flames";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
176;A Search and Detection Autonomous Drone System: From Design to Implementation;Utilizing autonomous drones or unmanned aerial vehicles (UAVs) has shown great advantages over preceding methods in support of urgent scenarios such as search and rescue (SAR) and wildfire detection. In these operations, search efficiency in terms of the amount of time spent to find the target is crucial since with time the survivability of the missing person decreases or wildfire management becomes more difficult with disastrous consequences. In this work, we consider the scenario where a drone is intended to search and detect a missing person (e.g., a hiker or a mountaineer) or a potential fire spot in a given area. To obtain the shortest path to the target, a general framework is provided to model the problem of target detection when the target's location is probabilistically known. To this end, two algorithms are proposed: Path planning and target detection. The path planning algorithm is based on Bayesian inference and the target detection is accomplished by using a residual neural network (ResNet) trained on the image dataset captured by the drone as well as existing pictures and datasets on the web. Through simulation and experiment, the proposed path planning algorithm is compared with two benchmark algorithms. It is shown that the proposed algorithm significantly decreases the average time of the mission.Note to Practitioners - This article is motivated by the need for an efficient path-planning algorithm for drones during specific SAR operations. In particular, situations where someone is lost in a snow-covered hike and a fire spot that is in its initial levels are of interest. In fact, since the target location is not known, it is required that the UAV be able to efficiently search the entire area until it finds the target in the shortest possible time. The proposed Bayesian framework along with the ResNet learning algorithm shows an efficient performance in terms of average time duration and accuracy, respectively. The framework developed in this paper can be extended to a multi-UAV scenario where UAVs coordinate to optimize the overall performance.  © 2004-2012 IEEE.;"Autonomous drones; fire detection; machine learning; path planning; search and rescue (SAR); unmanned aerial vehicles (UAVs)";"Aircraft detection; Antennas; Bayesian networks; Drones; Fires; Inference engines; Learning algorithms; Learning systems; Machine learning; Object detection; Object recognition; Probabilistic logics; Aerial vehicle; Autonomous drone; Fire detection; Machine-learning; Objects detection; Search and rescue; Surveillance; Unmanned aerial vehicle; Wildfire; Motion planning";"A Search and Detection Autonomous Drone System: From Design to Implementation Utilizing autonomous drones or unmanned aerial vehicles (UAVs) has shown great advantages over preceding methods in support of urgent scenarios such as search and rescue (SAR) and wildfire detection. In these operations, search efficiency in terms of the amount of time spent to find the target is crucial since with time the survivability of the missing person decreases or wildfire management becomes more difficult with disastrous consequences. In this work, we consider the scenario where a drone is intended to search and detect a missing person (e.g., a hiker or a mountaineer) or a potential fire spot in a given area. To obtain the shortest path to the target, a general framework is provided to model the problem of target detection when the target's location is probabilistically known. To this end, two algorithms are proposed: Path planning and target detection. The path planning algorithm is based on Bayesian inference and the target detection is accomplished by using a residual neural network (ResNet) trained on the image dataset captured by the drone as well as existing pictures and datasets on the web. Through simulation and experiment, the proposed path planning algorithm is compared with two benchmark algorithms. It is shown that the proposed algorithm significantly decreases the average time of the mission.Note to Practitioners - This article is motivated by the need for an efficient path-planning algorithm for drones during specific SAR operations. In particular, situations where someone is lost in a snow-covered hike and a fire spot that is in its initial levels are of interest. In fact, since the target location is not known, it is required that the UAV be able to efficiently search the entire area until it finds the target in the shortest possible time. The proposed Bayesian framework along with the ResNet learning algorithm shows an efficient performance in terms of average time duration and accuracy, respectively. The framework developed in this paper can be extended to a multi-UAV scenario where UAVs coordinate to optimize the overall performance.  © 2004-2012 IEEE. Autonomous drones; fire detection; machine learning; path planning; search and rescue (SAR); unmanned aerial vehicles (UAVs) Aircraft detection; Antennas; Bayesian networks; Drones; Fires; Inference engines; Learning algorithms; Learning systems; Machine learning; Object detection; Object recognition; Probabilistic logics; Aerial vehicle; Autonomous drone; Fire detection; Machine-learning; Objects detection; Search and rescue; Surveillance; Unmanned aerial vehicle; Wildfire; Motion planning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
177;NEURAL NETWORKS FOR EXTREME QUANTILE REGRESSION WITH AN APPLICATION TO FORECASTING OF FLOOD RISK;Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems. © Institute of Mathematical Statistics, 2024.;"Extreme value theory; generalized Pareto distribution; machine learning; prediction; recurrent neural network";NULL;"NEURAL NETWORKS FOR EXTREME QUANTILE REGRESSION WITH AN APPLICATION TO FORECASTING OF FLOOD RISK Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems. © Institute of Mathematical Statistics, 2024. Extreme value theory; generalized Pareto distribution; machine learning; prediction; recurrent neural network NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
178;Enhancing Multiple Precipitation Data Integration Across a Large-Scale Area: A Deep Learning ResU-Net Framework Without Interpolation;"Accurate and reliable precipitation estimation is crucial for various applications, such as land surface modeling, agricultural management, flood forecasting, and drought monitoring. In situ precipitation datasets, remote sensing-based estimates, and reanalysis products each, however, present varying levels of uncertainty. In this study, we develop a deep learning-based framework to enhance the accuracy and reliability of precipitation estimation by merging multiple products with different spatiotemporal resolutions. While the framework is based on the ResU-Net architecture, it introduces three innovations for reconstructing historical precipitation: 1) using each data source at its inherent spatial and temporal resolutions as input; 2) incorporating data not only from the current time steps but also from antecedent and subsequent time steps; and 3) integrating topographic information to improve the accuracy of spatial pattern reproduction. The proposed model is evaluated using 19 years of daily precipitation data from 1560 observation stations across the vast region of East Asia. Validation is conducted through three extensive sequential experiments involving eight simpler nested models and five comparative models. The results indicate that each of the three innovations improves out-of-sample accuracy. The ResU-Net model, furthermore, exhibits significantly superior performance compared to the comparative models. Specifically, the ResU-Net model shows an improvement of 20.2% and 15.9% in terms of the median Nash-Sutcliffe efficiency coefficient (NSE) over the validation period when compared to the error variance and random forest (RF) models, respectively. Finally, we conclude with a discussion of possible framework improvements by addressing diverse and extreme weather conditions.  © 1980-2012 IEEE.";"Deep learning; East Asia; precipitation merging; reconstructing precipitation; remote sensing product; ResU-Net";"Far East; Deep learning; East Asia; Precipitation data; Precipitation estimation; Precipitation merging; Reconstructing precipitation; Remote sensing product; Remote-sensing; Resu-net; Time step; accuracy assessment; algorithm; model validation; observational method; precipitation (climatology); precipitation assessment; reconstruction; remote sensing; spatiotemporal analysis; Data accuracy";"Enhancing Multiple Precipitation Data Integration Across a Large-Scale Area: A Deep Learning ResU-Net Framework Without Interpolation Accurate and reliable precipitation estimation is crucial for various applications, such as land surface modeling, agricultural management, flood forecasting, and drought monitoring. In situ precipitation datasets, remote sensing-based estimates, and reanalysis products each, however, present varying levels of uncertainty. In this study, we develop a deep learning-based framework to enhance the accuracy and reliability of precipitation estimation by merging multiple products with different spatiotemporal resolutions. While the framework is based on the ResU-Net architecture, it introduces three innovations for reconstructing historical precipitation: 1) using each data source at its inherent spatial and temporal resolutions as input; 2) incorporating data not only from the current time steps but also from antecedent and subsequent time steps; and 3) integrating topographic information to improve the accuracy of spatial pattern reproduction. The proposed model is evaluated using 19 years of daily precipitation data from 1560 observation stations across the vast region of East Asia. Validation is conducted through three extensive sequential experiments involving eight simpler nested models and five comparative models. The results indicate that each of the three innovations improves out-of-sample accuracy. The ResU-Net model, furthermore, exhibits significantly superior performance compared to the comparative models. Specifically, the ResU-Net model shows an improvement of 20.2% and 15.9% in terms of the median Nash-Sutcliffe efficiency coefficient (NSE) over the validation period when compared to the error variance and random forest (RF) models, respectively. Finally, we conclude with a discussion of possible framework improvements by addressing diverse and extreme weather conditions.  © 1980-2012 IEEE. Deep learning; East Asia; precipitation merging; reconstructing precipitation; remote sensing product; ResU-Net Far East; Deep learning; East Asia; Precipitation data; Precipitation estimation; Precipitation merging; Reconstructing precipitation; Remote sensing product; Remote-sensing; Resu-net; Time step; accuracy assessment; algorithm; model validation; observational method; precipitation (climatology); precipitation assessment; reconstruction; remote sensing; spatiotemporal analysis; Data accuracy";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
179;Enhancing Flood Severity Prediction through a Combined Threshold-Based Alert Algorithm and Random Forest Classifier;Flooding is a significant and recurrent is- sue that poses severe risks to communities, economies, and environments. Traditional flood prediction methods, which often rely on fixed thresholds, may lack adaptability to changing conditions. Meanwhile, machine learning (ML) models like Random Forest Classifiers (RFCs) offer advanced predictive capabilities, but may not provide straightforward alert mechanisms. This research proposes a novel approach that integrates a Threshold-Based Alert Algorithm (TBA) with an RFC to enhance flood severity prediction. The combined model leverages the strengths of both methodologies: immediate, interpretable alerts from the TBA and adaptive, data-driven insights from the RFC. Using historical and real-time data from river basins in Wisconsin and New York, the hybrid model demonstrates improved accuracy and reliability over traditional methods and standalone RFCs. The results indicate that this integrated approach offers a more effective tool for flood management and disaster mitigation. © 2025 IEEE.;"data integration; disaster risk reduction; environmental data; Flood forecasting; flood management; hybrid models; machine learning (ML); predictive analytics; random forest classification (RFC); threshold-based alert algorithm (TBA)";"Adversarial machine learning; Prediction models; Predictive analytics; Random forests; Disaster risk reductions; Environmental data; Flood forecasting; Flood management; Hybrid model; Machine learning; Machine-learning; Random forest classification; Threshold-based alert algorithm";"Enhancing Flood Severity Prediction through a Combined Threshold-Based Alert Algorithm and Random Forest Classifier Flooding is a significant and recurrent is- sue that poses severe risks to communities, economies, and environments. Traditional flood prediction methods, which often rely on fixed thresholds, may lack adaptability to changing conditions. Meanwhile, machine learning (ML) models like Random Forest Classifiers (RFCs) offer advanced predictive capabilities, but may not provide straightforward alert mechanisms. This research proposes a novel approach that integrates a Threshold-Based Alert Algorithm (TBA) with an RFC to enhance flood severity prediction. The combined model leverages the strengths of both methodologies: immediate, interpretable alerts from the TBA and adaptive, data-driven insights from the RFC. Using historical and real-time data from river basins in Wisconsin and New York, the hybrid model demonstrates improved accuracy and reliability over traditional methods and standalone RFCs. The results indicate that this integrated approach offers a more effective tool for flood management and disaster mitigation. © 2025 IEEE. data integration; disaster risk reduction; environmental data; Flood forecasting; flood management; hybrid models; machine learning (ML); predictive analytics; random forest classification (RFC); threshold-based alert algorithm (TBA) Adversarial machine learning; Prediction models; Predictive analytics; Random forests; Disaster risk reductions; Environmental data; Flood forecasting; Flood management; Hybrid model; Machine learning; Machine-learning; Random forest classification; Threshold-based alert algorithm";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
180;DisTGranD: Granular event/sub-event classification for disaster response;Efficient crisis management relies on prompt and precise analysis of disaster data from various sources, including social media. The advantage of fine-grained, annotated, class-labeled data is the provision of a diversified range of information compared to high-level label datasets. In this study, we introduce a dataset richly annotated at a low level to more accurately classify crisis-related communication. To this end, we first present DisTGranD, an extensively annotated dataset of over 47,600 tweets related to earthquakes and hurricanes. The dataset uses the Automatic Content Extraction (ACE) standard to provide detailed classification into dual-layer annotation for events and sub-events and identify critical triggers and supporting arguments. The inter-annotator evaluation of DisTGranD demonstrated high agreement among annotators, with Fleiss Kappa scores of 0.90 and 0.93 for event and sub-event types, respectively. Moreover, a transformer-based embedded phrase extraction method showed XLNet achieving an impressive 96% intra-label similarity score for event type and 97% for sub-event type. We further proposed a novel deep learning classification model, RoBiCCus, which achieved ≥90% accuracy and F1-Score in the event and sub-event type classification tasks on our DisTGranD dataset and outperformed other models on publicly available disaster datasets. DisTGranD dataset represents a nuanced class-labeled framework for detecting and classifying disaster-related social media content, which can significantly aid decision-making in disaster response. This robust dataset enables deep-learning models to provide insightful, actionable data during crises. Our annotated dataset and code are publicly available on GitHub 1. © 2024;"Disaster response; Event classification; Fine-grained labels; Phrase extraction; Text annotation; Transformers; X data";"Deep learning; Disaster prevention; Labeled data; Disaster-response; Event Types; Events classification; Fine grained; Fine-grained label; Phrase extraction; Sub-events; Text annotations; Transformer; X data; Disasters";"DisTGranD: Granular event/sub-event classification for disaster response Efficient crisis management relies on prompt and precise analysis of disaster data from various sources, including social media. The advantage of fine-grained, annotated, class-labeled data is the provision of a diversified range of information compared to high-level label datasets. In this study, we introduce a dataset richly annotated at a low level to more accurately classify crisis-related communication. To this end, we first present DisTGranD, an extensively annotated dataset of over 47,600 tweets related to earthquakes and hurricanes. The dataset uses the Automatic Content Extraction (ACE) standard to provide detailed classification into dual-layer annotation for events and sub-events and identify critical triggers and supporting arguments. The inter-annotator evaluation of DisTGranD demonstrated high agreement among annotators, with Fleiss Kappa scores of 0.90 and 0.93 for event and sub-event types, respectively. Moreover, a transformer-based embedded phrase extraction method showed XLNet achieving an impressive 96% intra-label similarity score for event type and 97% for sub-event type. We further proposed a novel deep learning classification model, RoBiCCus, which achieved ≥90% accuracy and F1-Score in the event and sub-event type classification tasks on our DisTGranD dataset and outperformed other models on publicly available disaster datasets. DisTGranD dataset represents a nuanced class-labeled framework for detecting and classifying disaster-related social media content, which can significantly aid decision-making in disaster response. This robust dataset enables deep-learning models to provide insightful, actionable data during crises. Our annotated dataset and code are publicly available on GitHub 1. © 2024 Disaster response; Event classification; Fine-grained labels; Phrase extraction; Text annotation; Transformers; X data Deep learning; Disaster prevention; Labeled data; Disaster-response; Event Types; Events classification; Fine grained; Fine-grained label; Phrase extraction; Sub-events; Text annotations; Transformer; X data; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
181;Comprehensive Monitoring of Construction Spoil Disposal Areas in High-Speed Railways Utilizing Integrated 3S Techniques;High-speed railways are critical infrastructure in many countries, but their construction generates substantial spoil, particularly in mountainous regions dominated by tunnels and slopes, necessitating the establishment and monitoring of spoil disposal areas. Inadequate monitoring of spoil disposal areas can lead to significant environmental issues, including soil erosion and geological hazards such as landslides and debris flows, while also hindering the recycling and reuse of construction spoil, thereby impeding the achievement of circular economy and sustainable development goals for high-speed railways. Although the potential of geographic information systems, remote sensing, and global positioning systems in waste monitoring is increasingly recognized, there remains a critical research gap in their application to spoil disposal areas monitoring within high-speed railway projects. This study proposes an innovative framework integrating geographic information systems, remote sensing, and global positioning systems for monitoring spoil disposal areas during high-speed railway construction across three key scenarios: identification of disturbance boundaries (scenario 1), extraction of soil and water conservation measures (scenario 2), and estimation of spoil volume changes (scenario 3). In scenario 1, disturbance boundaries were identified using Gaofen-1 satellite data through processes such as imagery fusion, unsupervised classification, and spatial analysis. In scenario 2, unmanned aerial vehicle data were employed to extract soil and water conservation measures via visual interpretation and overlay analysis. In scenario 3, Sentinel-1 data were used to analyze elevation changes through the differential interferometric synthetic aperture radar method, followed by the estimation of spoil volume changes. The effectiveness of this integrated framework was validated through a case study. The results demonstrate that the framework can accurately delineate disturbance boundaries, efficiently extract soil and water conservation measures, and estimate dynamic changes in spoil volume with an acceptable error margin (15.5%). These findings highlight the framework’s capability to enhance monitoring accuracy and efficiency. By integrating multi-source data, this framework provides robust support for sustainable resource management, reduces the environmental impact, and advances circular economy practices. This study contributes to the efficient utilization of construction spoil and the sustainable development of high-speed railway projects. © 2025 by the authors.;"circular economy; geographic information systems (GIS); global positioning systems (GPS); high-speed railways (HSRs); remote sensing (RS); spoil disposal areas (SDAs)";"Aerial photography; Energy efficiency; Interlocking signals; Landslides; Radioactive waste disposal; Railroad transportation; Railroads; Resource allocation; Soil conservation; Unmanned aerial vehicles (UAV); Water recycling; Circular economy; Disposal areas; Geographic information; Geographic information system; Global positioning; Global positioning system; High-speed railway; High-speed railways; Positioning system; Remote sensing; Remote-sensing; Spoil disposal area";"Comprehensive Monitoring of Construction Spoil Disposal Areas in High-Speed Railways Utilizing Integrated 3S Techniques High-speed railways are critical infrastructure in many countries, but their construction generates substantial spoil, particularly in mountainous regions dominated by tunnels and slopes, necessitating the establishment and monitoring of spoil disposal areas. Inadequate monitoring of spoil disposal areas can lead to significant environmental issues, including soil erosion and geological hazards such as landslides and debris flows, while also hindering the recycling and reuse of construction spoil, thereby impeding the achievement of circular economy and sustainable development goals for high-speed railways. Although the potential of geographic information systems, remote sensing, and global positioning systems in waste monitoring is increasingly recognized, there remains a critical research gap in their application to spoil disposal areas monitoring within high-speed railway projects. This study proposes an innovative framework integrating geographic information systems, remote sensing, and global positioning systems for monitoring spoil disposal areas during high-speed railway construction across three key scenarios: identification of disturbance boundaries (scenario 1), extraction of soil and water conservation measures (scenario 2), and estimation of spoil volume changes (scenario 3). In scenario 1, disturbance boundaries were identified using Gaofen-1 satellite data through processes such as imagery fusion, unsupervised classification, and spatial analysis. In scenario 2, unmanned aerial vehicle data were employed to extract soil and water conservation measures via visual interpretation and overlay analysis. In scenario 3, Sentinel-1 data were used to analyze elevation changes through the differential interferometric synthetic aperture radar method, followed by the estimation of spoil volume changes. The effectiveness of this integrated framework was validated through a case study. The results demonstrate that the framework can accurately delineate disturbance boundaries, efficiently extract soil and water conservation measures, and estimate dynamic changes in spoil volume with an acceptable error margin (15.5%). These findings highlight the framework’s capability to enhance monitoring accuracy and efficiency. By integrating multi-source data, this framework provides robust support for sustainable resource management, reduces the environmental impact, and advances circular economy practices. This study contributes to the efficient utilization of construction spoil and the sustainable development of high-speed railway projects. © 2025 by the authors. circular economy; geographic information systems (GIS); global positioning systems (GPS); high-speed railways (HSRs); remote sensing (RS); spoil disposal areas (SDAs) Aerial photography; Energy efficiency; Interlocking signals; Landslides; Radioactive waste disposal; Railroad transportation; Railroads; Resource allocation; Soil conservation; Unmanned aerial vehicles (UAV); Water recycling; Circular economy; Disposal areas; Geographic information; Geographic information system; Global positioning; Global positioning system; High-speed railway; High-speed railways; Positioning system; Remote sensing; Remote-sensing; Spoil disposal area";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
182;An improved method to predict man-made slope failure using machine learning tools;Landslide hazards associated with man-made slopes are increasing due to ageing and extreme weather conditions under a changing climate. To effectively mitigate landslide risks, the implementation of regional landslide early warning systems is desirable, albeit challenging, if not impossible, due to the scarcity of reliable landslide data and suitable predictive tools. In this paper, a thorough analysis has been conducted on reasonably reliable and substantial amounts of historical rainfall data, slope features, and landslide inventory of man-made slope failures in Hong Kong. Four different machine learning methods, namely logistic regression (LR), decision tree (DT), random forest (RF), and extreme gradient boosting (XGBoost), have been employed. The predicted number of landslides from the machine learning methods is compared with the predictions made by the current Landslip Warning System in Hong Kong. The effects of rainfall parameters and slope features on model performance are also investigated. The analysed results show that dynamic rainfall conditions are identified as the most influential factors for predicting man-made slope landslide. A combination of 1 and 12 h maximal rolling rainfall (MRR) demonstrates superior performance compared to relying solely on the 24 h MRR. Therefore, this combination is recommended for predicting man-made slope failures in Hong Kong. © 2025 The Author(s).;"machine learning; man-made slopes; rainfall parameters; regional landslide–rainfall correlations";"China; Hong Kong; Adversarial machine learning; Alarm systems; Landslides; Logistic regression; Machine learning; Extreme weather conditions; Hong-kong; Landslide hazard; Learning tool; Machine learning methods; Machine-learning; Man-make slope; Rainfall parameter; Regional landslide–rainfall correlation; Slope failure; climate change; correlation; early warning system; extreme event; landslide; machine learning; rainfall; regression analysis; slope failure; Rain";"An improved method to predict man-made slope failure using machine learning tools Landslide hazards associated with man-made slopes are increasing due to ageing and extreme weather conditions under a changing climate. To effectively mitigate landslide risks, the implementation of regional landslide early warning systems is desirable, albeit challenging, if not impossible, due to the scarcity of reliable landslide data and suitable predictive tools. In this paper, a thorough analysis has been conducted on reasonably reliable and substantial amounts of historical rainfall data, slope features, and landslide inventory of man-made slope failures in Hong Kong. Four different machine learning methods, namely logistic regression (LR), decision tree (DT), random forest (RF), and extreme gradient boosting (XGBoost), have been employed. The predicted number of landslides from the machine learning methods is compared with the predictions made by the current Landslip Warning System in Hong Kong. The effects of rainfall parameters and slope features on model performance are also investigated. The analysed results show that dynamic rainfall conditions are identified as the most influential factors for predicting man-made slope landslide. A combination of 1 and 12 h maximal rolling rainfall (MRR) demonstrates superior performance compared to relying solely on the 24 h MRR. Therefore, this combination is recommended for predicting man-made slope failures in Hong Kong. © 2025 The Author(s). machine learning; man-made slopes; rainfall parameters; regional landslide–rainfall correlations China; Hong Kong; Adversarial machine learning; Alarm systems; Landslides; Logistic regression; Machine learning; Extreme weather conditions; Hong-kong; Landslide hazard; Learning tool; Machine learning methods; Machine-learning; Man-make slope; Rainfall parameter; Regional landslide–rainfall correlation; Slope failure; climate change; correlation; early warning system; extreme event; landslide; machine learning; rainfall; regression analysis; slope failure; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
183;Analysis and Prediction of Grouting Reinforcement Performance of Broken Rock Considering Joint Morphology Characteristics;In tunnel engineering, joint shear slip caused by external disturbances is a key factor contributing to landslides, instability of surrounding rock masses, and related hazards. Therefore, accurately characterizing the macromechanical properties of joints is essential for ensuring engineering safety. Given the significant influence of rock joint morphology on mechanical behavior, this study employs the frequency spectrum fractal dimension (D) and the frequency domain amplitude integral (Rq) as quantitative descriptors of joint morphology. Using Fourier transform techniques, a reconstruction method is developed to model joints with arbitrary shape characteristics. The numerical model is calibrated through 3D printing and direct shear tests. Systematic parameter analysis validates the selected quantitative indices as effective descriptors of joint morphology. Furthermore, multiple machine learning algorithms are employed to construct a robust predictive model. Machine learning, recognized as a rapidly advancing field, plays a pivotal role in data-driven engineering applications due to its powerful analytical capabilities. In this study, six algorithms—Random Forest (RF), Support Vector Regression (SVR), BP Neural Network, GA-BP Neural Network, Genetic Programming (GP), and ANN-based MCD—are evaluated using 300 samples. The performance of each algorithm is assessed through comparative analysis of their predictive accuracy based on correlation coefficients. The results demonstrate that all six algorithms achieve satisfactory predictive performance. Notably, the Random Forest (RF) algorithm excels in rapid and accurate predictions when handling similar training data, while the ANN-based MCD algorithm consistently delivers stable and precise results across diverse datasets. © 2025 by the authors.;"direct shear test; machine learning; numerical simulation; performance prediction; quantitative reconstruction of joints";NULL;"Analysis and Prediction of Grouting Reinforcement Performance of Broken Rock Considering Joint Morphology Characteristics In tunnel engineering, joint shear slip caused by external disturbances is a key factor contributing to landslides, instability of surrounding rock masses, and related hazards. Therefore, accurately characterizing the macromechanical properties of joints is essential for ensuring engineering safety. Given the significant influence of rock joint morphology on mechanical behavior, this study employs the frequency spectrum fractal dimension (D) and the frequency domain amplitude integral (Rq) as quantitative descriptors of joint morphology. Using Fourier transform techniques, a reconstruction method is developed to model joints with arbitrary shape characteristics. The numerical model is calibrated through 3D printing and direct shear tests. Systematic parameter analysis validates the selected quantitative indices as effective descriptors of joint morphology. Furthermore, multiple machine learning algorithms are employed to construct a robust predictive model. Machine learning, recognized as a rapidly advancing field, plays a pivotal role in data-driven engineering applications due to its powerful analytical capabilities. In this study, six algorithms—Random Forest (RF), Support Vector Regression (SVR), BP Neural Network, GA-BP Neural Network, Genetic Programming (GP), and ANN-based MCD—are evaluated using 300 samples. The performance of each algorithm is assessed through comparative analysis of their predictive accuracy based on correlation coefficients. The results demonstrate that all six algorithms achieve satisfactory predictive performance. Notably, the Random Forest (RF) algorithm excels in rapid and accurate predictions when handling similar training data, while the ANN-based MCD algorithm consistently delivers stable and precise results across diverse datasets. © 2025 by the authors. direct shear test; machine learning; numerical simulation; performance prediction; quantitative reconstruction of joints NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
184;Using YOLOv8 for Building Damage Identification in Japan's Noto Region Following Earthquakes: A Deep Learning-Based Approach;In the critical context of enhancing post-disaster assessments in regions frequently affected by earthquakes and typhoons, this study utilizes the YOLOv8 algorithm to innovate the classification of building damage, focusing on the Noto area in Japan. By employing a meticulously augmented dataset and optimizing the model over 200 epochs with the NAdam optimizer, our approach distinguishes itself through its efficiency and accuracy. The exceptional performance of the model is underscored by an overall mAP score of 0.952 and precision nearing 1.00 at higher confidence thresholds, markedly outperforming conventional methods of damage assessment. Moreover, with a recall rate of 0.8, YOLOv8 exhibits strong detection capabilities across various types of structural damage. This research not only introduces an innovative application of YOLOv8 for disaster response but also establishes a new benchmark in emergency management practices, underscoring the potential of advanced deep learning techniques in lessening the impact of natural disasters worldwide. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.;"Building damage identification; Deep learning; Disaster assessment; Object detection; YOLOv8";"Adversarial machine learning; Contrastive Learning; Deep learning; Disasters; Risk management; Building damage; Building damage identification; Damage Identification; Deep learning; Disaster assessment; Learning-based approach; Objects detection; Optimizers; Post disasters; YOLOv8; Earthquakes";"Using YOLOv8 for Building Damage Identification in Japan's Noto Region Following Earthquakes: A Deep Learning-Based Approach In the critical context of enhancing post-disaster assessments in regions frequently affected by earthquakes and typhoons, this study utilizes the YOLOv8 algorithm to innovate the classification of building damage, focusing on the Noto area in Japan. By employing a meticulously augmented dataset and optimizing the model over 200 epochs with the NAdam optimizer, our approach distinguishes itself through its efficiency and accuracy. The exceptional performance of the model is underscored by an overall mAP score of 0.952 and precision nearing 1.00 at higher confidence thresholds, markedly outperforming conventional methods of damage assessment. Moreover, with a recall rate of 0.8, YOLOv8 exhibits strong detection capabilities across various types of structural damage. This research not only introduces an innovative application of YOLOv8 for disaster response but also establishes a new benchmark in emergency management practices, underscoring the potential of advanced deep learning techniques in lessening the impact of natural disasters worldwide. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025. Building damage identification; Deep learning; Disaster assessment; Object detection; YOLOv8 Adversarial machine learning; Contrastive Learning; Deep learning; Disasters; Risk management; Building damage; Building damage identification; Damage Identification; Deep learning; Disaster assessment; Learning-based approach; Objects detection; Optimizers; Post disasters; YOLOv8; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
185;A probabilistic pluvial flood warning model based on nest som using radar reflectivity data;The occurrence and intensity of extreme weather events have increased under climate change, making flooding disasters more likely during the flood season from May to November in Taiwan. The current early warning system for flooding disasters developed by the Water Resources Agency in Taiwan relies on the density of rain gauges, which limits its effectiveness. To improve this system, our research collected historical radar reflectivity and rainfall data in the flood-prone area at the Zhonghua village of Taipei City. An unsupervised neural network called the self-organizing map (Clayton and Emery) is applied to establish the relationship between radar reflectivity and rainfall observations, enabling the analysis of clustering vectors corresponding to pluvial flood disaster events. A Nest SOM-based pluvial flood warning model was proposed for identifying flooding hot zones and delivering probabilistic flood warning information. Based on radar reflectivity characteristics along with corresponding rainfall intensity and frequency, the proposed model was evaluated during extreme events to demonstrate its applicability and provide probabilistic warning information prior to flood disasters. Consequently, the model provides considerable practical value in enhancing flood disaster management. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.;"Pluvial flood; Probabilistic flood warning; Radar reflectivity; Self-organizing map";"Taipei; Taiwan; Conformal mapping; Extreme weather; Flood damage; Radar reflection; Radar warning systems; Rain gages; Self organizing maps; Flood warning; Floodings; Pluvial flood; Pluvials; Probabilistic flood warning; Probabilistics; Radar reflectivities; Reflectivity data; Self-organizing-maps; Warning models; data set; disaster management; flooding; precipitation intensity; probability; radar imagery; raingauge; Rain";"A probabilistic pluvial flood warning model based on nest som using radar reflectivity data The occurrence and intensity of extreme weather events have increased under climate change, making flooding disasters more likely during the flood season from May to November in Taiwan. The current early warning system for flooding disasters developed by the Water Resources Agency in Taiwan relies on the density of rain gauges, which limits its effectiveness. To improve this system, our research collected historical radar reflectivity and rainfall data in the flood-prone area at the Zhonghua village of Taipei City. An unsupervised neural network called the self-organizing map (Clayton and Emery) is applied to establish the relationship between radar reflectivity and rainfall observations, enabling the analysis of clustering vectors corresponding to pluvial flood disaster events. A Nest SOM-based pluvial flood warning model was proposed for identifying flooding hot zones and delivering probabilistic flood warning information. Based on radar reflectivity characteristics along with corresponding rainfall intensity and frequency, the proposed model was evaluated during extreme events to demonstrate its applicability and provide probabilistic warning information prior to flood disasters. Consequently, the model provides considerable practical value in enhancing flood disaster management. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024. Pluvial flood; Probabilistic flood warning; Radar reflectivity; Self-organizing map Taipei; Taiwan; Conformal mapping; Extreme weather; Flood damage; Radar reflection; Radar warning systems; Rain gages; Self organizing maps; Flood warning; Floodings; Pluvial flood; Pluvials; Probabilistic flood warning; Probabilistics; Radar reflectivities; Reflectivity data; Self-organizing-maps; Warning models; data set; disaster management; flooding; precipitation intensity; probability; radar imagery; raingauge; Rain";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.2;Hydrological;2;Preparation
186;QuakeWake: A Novel AI-Based Early Earthquake Warning and Post-Quake Building Safety Guidance System;Millions of people around the world suffer from earthquakes every year. This research introduces an innovative, mobile device-based approach for real-time earthquake detection and prediction. By discerning quake patterns from users’ regular usage patterns, a novel approach that prevents excessively draining battery uses an on-device neural network only when needed to detect earthquake tremors. Cloud servers running an AI module reliably predict the quake intensity and propagation pattern using signals from many users, enabling warning others who have yet to experience these tremors. It also detects buildings at high risk to reinhabit due to high relative floor displacement exceeding the building safety standards. A low-cost, affordable, and highly reliable optional adjunct device on the user’s premise captures tremors with higher accuracy than mobile devices. This enables effective building-wide earthquake warnings and eliminates fatalities due to post-earthquake building structural integrity issues. With a neural network trained with many past earthquake patterns, the mobile devices reliably detected quakes and the AI module accurately detected its propagation with 99% accuracy, warning users along its path. Moreover, the adjunct device adequately captured shifts in the building’s structure and reliably flagged the building as uninhabitable with more than 95% accuracy. © 2025 by SCITEPRESS – Science and Technology Publications, Lda.;"Autonomous Learning; Building Damage; Dynamic Time Warping; Earthquake; Earthquake Early Warning; P-Wave; S-Wave; Supervised Learning; User Collaboration";NULL;"QuakeWake: A Novel AI-Based Early Earthquake Warning and Post-Quake Building Safety Guidance System Millions of people around the world suffer from earthquakes every year. This research introduces an innovative, mobile device-based approach for real-time earthquake detection and prediction. By discerning quake patterns from users’ regular usage patterns, a novel approach that prevents excessively draining battery uses an on-device neural network only when needed to detect earthquake tremors. Cloud servers running an AI module reliably predict the quake intensity and propagation pattern using signals from many users, enabling warning others who have yet to experience these tremors. It also detects buildings at high risk to reinhabit due to high relative floor displacement exceeding the building safety standards. A low-cost, affordable, and highly reliable optional adjunct device on the user’s premise captures tremors with higher accuracy than mobile devices. This enables effective building-wide earthquake warnings and eliminates fatalities due to post-earthquake building structural integrity issues. With a neural network trained with many past earthquake patterns, the mobile devices reliably detected quakes and the AI module accurately detected its propagation with 99% accuracy, warning users along its path. Moreover, the adjunct device adequately captured shifts in the building’s structure and reliably flagged the building as uninhabitable with more than 95% accuracy. © 2025 by SCITEPRESS – Science and Technology Publications, Lda. Autonomous Learning; Building Damage; Dynamic Time Warping; Earthquake; Earthquake Early Warning; P-Wave; S-Wave; Supervised Learning; User Collaboration NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
187;A Hybrid Prediction Model Integrating Artificial Intelligence and Geospatial Analysis for Disaster Management;A natural or man-made disaster must be predicted accurately and in time. Advanced modelling is essential to predict increasingly frequent and intense natural events, enabling better response and recovery strategies. Hybrid machine learning is used to predict floods, assess earthquake damage, and control wildfires. Convolutional Neural Networks (CNNs), Gradient Boosting Machines (GBMs) and Support Vector Machines (SVMs) are used in the proposed model. We evaluate the proposed machine learning model in comparison to conventional statistical analysis and single machine learning techniques. The proposed hybrid model provided 90% correct predictions for flood events in Bangladesh with precision and recall values of 88% and 85%, respectively. In the assessment of earthquake damage in Japan, an accuracy of 92% was achieved with a precision of 90% and a recall of 89% with an F1 score of 89%. In the management of wildfires in California, an accuracy of 88% with a precision of 85% were achieved. The proposed hybrid model outperforms conventional techniques due to its higher reliability in predicting floods.  © 2013 IEEE.;"convolutional neural networks; disaster response optimization; earthquake damage assessment; Flood prediction";"Adversarial machine learning; Contrastive Learning; Convolutional neural networks; Damage detection; Disaster prevention; Disasters; Earthquake effects; Flood damage; Support vector machines; Convolutional neural network; Damage assessments; Disaster response optimization; Disaster-response; Earthquake damage assessment; Earthquake damages; Flood prediction; Hybrid model; Hybrid prediction models; Optimisations; Prediction models";"A Hybrid Prediction Model Integrating Artificial Intelligence and Geospatial Analysis for Disaster Management A natural or man-made disaster must be predicted accurately and in time. Advanced modelling is essential to predict increasingly frequent and intense natural events, enabling better response and recovery strategies. Hybrid machine learning is used to predict floods, assess earthquake damage, and control wildfires. Convolutional Neural Networks (CNNs), Gradient Boosting Machines (GBMs) and Support Vector Machines (SVMs) are used in the proposed model. We evaluate the proposed machine learning model in comparison to conventional statistical analysis and single machine learning techniques. The proposed hybrid model provided 90% correct predictions for flood events in Bangladesh with precision and recall values of 88% and 85%, respectively. In the assessment of earthquake damage in Japan, an accuracy of 92% was achieved with a precision of 90% and a recall of 89% with an F1 score of 89%. In the management of wildfires in California, an accuracy of 88% with a precision of 85% were achieved. The proposed hybrid model outperforms conventional techniques due to its higher reliability in predicting floods.  © 2013 IEEE. convolutional neural networks; disaster response optimization; earthquake damage assessment; Flood prediction Adversarial machine learning; Contrastive Learning; Convolutional neural networks; Damage detection; Disaster prevention; Disasters; Earthquake effects; Flood damage; Support vector machines; Convolutional neural network; Damage assessments; Disaster response optimization; Disaster-response; Earthquake damage assessment; Earthquake damages; Flood prediction; Hybrid model; Hybrid prediction models; Optimisations; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
188;Consistency Regularization for Semi-Supervised Semantic Segmentation of Flood Regions from SAR Images;As one of the most powerful natural catastrophes, floods pose serious risks to people's lives, the integrity of infrastructure, and agricultural landscapes, which increases the toll they take on the economy and society. As a result, it becomes essential to continuously monitor these areas of vulnerability in order to support effective disaster response and mitigation efforts. Accurately defining the extent of floods is a problem for traditional flood mapping approaches, which emphasizes the vital need for modern technologies such as Synthetic Aperture Radar (SAR) imaging. Additionally, there is a need to develop computer-aided tools specifically designed for automatically identifying areas that are vulnerable to flooding using SAR data. Nonetheless, the lack of consistent large datasets presents a barrier that prevents these algorithms from progressing and being used in real-world scenarios. For this reason, the present study aims to develop a semi-supervised semantic segmentation algorithm for accurate flood region delineation in SAR data. In particular, the paper proposes labeling unannotated instances of data using a pseudo-label generation strategy. In order to accomplish this, the study suggests using a self-supervised trained teacher model to generate pseudo-labels and speed up the training procedure. The teacher model is then trained with a student model to efficiently extract features from the labeled data. Furthermore, the study presents a new semantic segmentation technique that uses convolutional neural networks to automatically identify flooded areas in SAR images. A comprehensive assessment conducted on publicly available datasets produces promising results. These results confirm the usefulness and possible relevance of the suggested methodology in enhancing efforts related to flood zone identification and management.  © 2013 IEEE.;"flood mapping; SAR images; semantic segmentation; Semi-supervised learning";"Convolutional neural networks; Labeled data; Mapping; Personnel training; Radar imaging; Self-supervised learning; Semi-supervised learning; Teaching; Agricultural landscapes; Economy and society; Flood mapping; Radar data; Regularisation; Semantic segmentation; Semi-supervised; Semi-supervised learning; Synthetic aperture radar images; Teacher models; Semantic Segmentation";"Consistency Regularization for Semi-Supervised Semantic Segmentation of Flood Regions from SAR Images As one of the most powerful natural catastrophes, floods pose serious risks to people's lives, the integrity of infrastructure, and agricultural landscapes, which increases the toll they take on the economy and society. As a result, it becomes essential to continuously monitor these areas of vulnerability in order to support effective disaster response and mitigation efforts. Accurately defining the extent of floods is a problem for traditional flood mapping approaches, which emphasizes the vital need for modern technologies such as Synthetic Aperture Radar (SAR) imaging. Additionally, there is a need to develop computer-aided tools specifically designed for automatically identifying areas that are vulnerable to flooding using SAR data. Nonetheless, the lack of consistent large datasets presents a barrier that prevents these algorithms from progressing and being used in real-world scenarios. For this reason, the present study aims to develop a semi-supervised semantic segmentation algorithm for accurate flood region delineation in SAR data. In particular, the paper proposes labeling unannotated instances of data using a pseudo-label generation strategy. In order to accomplish this, the study suggests using a self-supervised trained teacher model to generate pseudo-labels and speed up the training procedure. The teacher model is then trained with a student model to efficiently extract features from the labeled data. Furthermore, the study presents a new semantic segmentation technique that uses convolutional neural networks to automatically identify flooded areas in SAR images. A comprehensive assessment conducted on publicly available datasets produces promising results. These results confirm the usefulness and possible relevance of the suggested methodology in enhancing efforts related to flood zone identification and management.  © 2013 IEEE. flood mapping; SAR images; semantic segmentation; Semi-supervised learning Convolutional neural networks; Labeled data; Mapping; Personnel training; Radar imaging; Self-supervised learning; Semi-supervised learning; Teaching; Agricultural landscapes; Economy and society; Flood mapping; Radar data; Regularisation; Semantic segmentation; Semi-supervised; Semi-supervised learning; Synthetic aperture radar images; Teacher models; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
189;An operational IoT-based slope stability forecast using a digital twin;The paper investigates the combined use of real-time hydrological monitoring, publicly available meteorological data and hydrological and geotechnical numerical modelling, to develop data-driven models to forecast the stability of a slope. This study showcases a first attempt to integrate these critical aspects into a fully automatic Internet of Thing (IoT)-based local landslide early warning system (Lo-LEWS). The paper uses a validated hydrological numerical model, back-calculated over real monitored conditions, to evaluate the slope stability. The factor of safety (FoS) was computed coupling the commercial package GeoStudio, using transient SEEP/W and Slope. The analyses were conducted for 5 different 1-year datasets encompassing both historical (2019–2020, 2021–2022, 2022–2023) and future projections (2064–2065, 2095–2096) of meteorological variables. Daily variation of hydrological and meteorological variables, along with vegetation indicators were used as inputs to train data-driven models, using polynomial regression (PR) and Random Forest (RF), to forecast daily FoS values. The trained models proved to be effective and were employed to forecast slope stability for the rolling three days. To accurately forecast the FoS, it was essential to incorporate forecasted hydrological, meteorological and vegetation variables into the analysis. The hydrological variables used as inputs for the data-driven models are forecasted using an open-source Python package for the analysis of hydrogeological time series, called Pastas (Collenteur et al., 2019). This model uses historical and forecasted meteorological and vegetation conditions to, specifically, replicate and forecast the time series of volumetric water content (VWC) and pore water pressure (PWP). The forecasted hydrological variables from Pastas, the forecasted meteorological variables as well as Leaf Area Index (LAI) are used as inputs for the trained data-driven models to forecast the FoS values. Finally, a web-based platform (WBP) has been created that automatically runs once a day and perform the following actions: 1) fetches measured and forecasted data using APIs, 2) runs rolling three days forecast based on collected hydrological, meteorological and vegetation variables, and 3) sends the forecasted values back to the Norwegian Geotechnical Institute (NGI) data platform, NGI Live, making them available for real-time visualization in online dashboards. If FoS, VWC or PWP threshold values are exceeded, text messages and emails are sent to the system managers, enabling them to take appropriate actions. The successful implementation of this framework is the result of a collaborative effort across diverse expertise areas, including geotechnics, hydrology, meteorology, instrumentation, and informatics. © 2024 The Authors;"Early warning; Landslides; Machine learning; Modelling; Real-time monitoring";"Decision trees; Landslides; Open source software; Polynomial approximation; Polynomial regression; Problem oriented languages; Water content; Weather forecasting; Data-driven model; Early warning; Factors of safeties; Geotechnical; Hydrological variables; Machine-learning; Meteorological variables; Modeling; Real time monitoring; Safety values; early warning system; forecasting method; Internet; landslide; porewater; real time; slope stability; threshold; water content; Safety factor";"An operational IoT-based slope stability forecast using a digital twin The paper investigates the combined use of real-time hydrological monitoring, publicly available meteorological data and hydrological and geotechnical numerical modelling, to develop data-driven models to forecast the stability of a slope. This study showcases a first attempt to integrate these critical aspects into a fully automatic Internet of Thing (IoT)-based local landslide early warning system (Lo-LEWS). The paper uses a validated hydrological numerical model, back-calculated over real monitored conditions, to evaluate the slope stability. The factor of safety (FoS) was computed coupling the commercial package GeoStudio, using transient SEEP/W and Slope. The analyses were conducted for 5 different 1-year datasets encompassing both historical (2019–2020, 2021–2022, 2022–2023) and future projections (2064–2065, 2095–2096) of meteorological variables. Daily variation of hydrological and meteorological variables, along with vegetation indicators were used as inputs to train data-driven models, using polynomial regression (PR) and Random Forest (RF), to forecast daily FoS values. The trained models proved to be effective and were employed to forecast slope stability for the rolling three days. To accurately forecast the FoS, it was essential to incorporate forecasted hydrological, meteorological and vegetation variables into the analysis. The hydrological variables used as inputs for the data-driven models are forecasted using an open-source Python package for the analysis of hydrogeological time series, called Pastas (Collenteur et al., 2019). This model uses historical and forecasted meteorological and vegetation conditions to, specifically, replicate and forecast the time series of volumetric water content (VWC) and pore water pressure (PWP). The forecasted hydrological variables from Pastas, the forecasted meteorological variables as well as Leaf Area Index (LAI) are used as inputs for the trained data-driven models to forecast the FoS values. Finally, a web-based platform (WBP) has been created that automatically runs once a day and perform the following actions: 1) fetches measured and forecasted data using APIs, 2) runs rolling three days forecast based on collected hydrological, meteorological and vegetation variables, and 3) sends the forecasted values back to the Norwegian Geotechnical Institute (NGI) data platform, NGI Live, making them available for real-time visualization in online dashboards. If FoS, VWC or PWP threshold values are exceeded, text messages and emails are sent to the system managers, enabling them to take appropriate actions. The successful implementation of this framework is the result of a collaborative effort across diverse expertise areas, including geotechnics, hydrology, meteorology, instrumentation, and informatics. © 2024 The Authors Early warning; Landslides; Machine learning; Modelling; Real-time monitoring Decision trees; Landslides; Open source software; Polynomial approximation; Polynomial regression; Problem oriented languages; Water content; Weather forecasting; Data-driven model; Early warning; Factors of safeties; Geotechnical; Hydrological variables; Machine-learning; Meteorological variables; Modeling; Real time monitoring; Safety values; early warning system; forecasting method; Internet; landslide; porewater; real time; slope stability; threshold; water content; Safety factor";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
190;Revolutionizing Wildfire Detection Through UAV-Driven Fire Monitoring with a Transformer-Based Approach;The rapid detection and accurate localization of wildfires are critical for effective disaster management and response. This study proposes an innovative Unmanned aerial vehicles (UAVs)-based fire detection system leveraging a modified Miti-DETR model tailored to meet the computational constraints of drones. The enhanced architecture incorporates a redesigned AlexNet backbone with residual depthwise separable convolution blocks, significantly reducing computational load while improving feature extraction and accuracy. Furthermore, a novel residual self-attention mechanism addresses convergence issues in transformer networks, ensuring robust feature representation for complex aerial imagery. The model, which was trained on the FLAME dataset encompassing diverse fire scenarios, demonstrates superior performance in terms of Mean Average Precision (mAP) and Intersection over Union (IoU) metrics compared to existing systems. Its capability to detect and localize fires across varied backgrounds highlights its practical application in real-world scenarios. This advancement represents a pivotal step forward in applying deep learning for real-time wildfire detection, with implications for broader emergency management applications. © 2024 by the authors.;"deep learning; drone; environmental monitoring; object detection; real-time imaging; transformers in image processing; wildfire detection; wildfire management";NULL;"Revolutionizing Wildfire Detection Through UAV-Driven Fire Monitoring with a Transformer-Based Approach The rapid detection and accurate localization of wildfires are critical for effective disaster management and response. This study proposes an innovative Unmanned aerial vehicles (UAVs)-based fire detection system leveraging a modified Miti-DETR model tailored to meet the computational constraints of drones. The enhanced architecture incorporates a redesigned AlexNet backbone with residual depthwise separable convolution blocks, significantly reducing computational load while improving feature extraction and accuracy. Furthermore, a novel residual self-attention mechanism addresses convergence issues in transformer networks, ensuring robust feature representation for complex aerial imagery. The model, which was trained on the FLAME dataset encompassing diverse fire scenarios, demonstrates superior performance in terms of Mean Average Precision (mAP) and Intersection over Union (IoU) metrics compared to existing systems. Its capability to detect and localize fires across varied backgrounds highlights its practical application in real-world scenarios. This advancement represents a pivotal step forward in applying deep learning for real-time wildfire detection, with implications for broader emergency management applications. © 2024 by the authors. deep learning; drone; environmental monitoring; object detection; real-time imaging; transformers in image processing; wildfire detection; wildfire management NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;3;Response
191;Reverse logistics for electric vehicles under uncertainty: An intelligent emergency management approach;The frequency and intensity of global disasters, including the COVID-19 pandemic, and natural disasters such as earthquakes, floods, and wildfires, are increasing, necessitating effective emergency logistics management. Climate change significantly contributes to these events, emphasizing the importance of limiting human and environmental impacts. The transportation sector, particularly the automobile industry, ranks second in global carbon emissions, highlighting the need to adopt electric vehicles (EVs) to reduce emissions and minimize the impact of climate change. However, this has led to an increase in demand for lithium-ion batteries. During emergencies, end-of-life (EOL) battery management through reverse logistics is essential because recycling EOL batteries can recover valuable raw materials, decrease landfill waste and costs, and support environmental sustainability. This study proposed a two-phase method for intelligent emergency EV battery reverse logistics management. The first phase employed machine learning to address unpredictable battery demands, whereas the second phase proposed a multi-objective model to minimize carbon emissions through efficient order allocation during uncertain emergencies. The model considers carbon emissions and defect rates as sources of uncertainty, current regulations, and customer environmental awareness. The model is solved using the weighted sum and ε-constraint methods, resulting in non-dominant solutions. The findings indicate that combining the selection of third-party reverse logistics providers (3PRLPs) with optimal order allocation for recycling old batteries during emergencies effectively minimizes environmental impacts and combats climate change. © 2024 Elsevier Ltd;"Battery Industry; Carbon Emission Reduction; Emergency Logistics Operations; Machine Learning; Multi-Objective Optimization";"Lithium-ion batteries; Risk management; Battery industry; Carbon emissions; Carbon emissions reductions; Emergency logistic operation; Emergency logistics; End-of-life batteries; Logistic operations; Machine-learning; Multi-objectives optimization; Reverse logistics; carbon emission; climate change; climate effect; electric vehicle; emission control; machine learning; optimization; uncertainty analysis; Land fill";"Reverse logistics for electric vehicles under uncertainty: An intelligent emergency management approach The frequency and intensity of global disasters, including the COVID-19 pandemic, and natural disasters such as earthquakes, floods, and wildfires, are increasing, necessitating effective emergency logistics management. Climate change significantly contributes to these events, emphasizing the importance of limiting human and environmental impacts. The transportation sector, particularly the automobile industry, ranks second in global carbon emissions, highlighting the need to adopt electric vehicles (EVs) to reduce emissions and minimize the impact of climate change. However, this has led to an increase in demand for lithium-ion batteries. During emergencies, end-of-life (EOL) battery management through reverse logistics is essential because recycling EOL batteries can recover valuable raw materials, decrease landfill waste and costs, and support environmental sustainability. This study proposed a two-phase method for intelligent emergency EV battery reverse logistics management. The first phase employed machine learning to address unpredictable battery demands, whereas the second phase proposed a multi-objective model to minimize carbon emissions through efficient order allocation during uncertain emergencies. The model considers carbon emissions and defect rates as sources of uncertainty, current regulations, and customer environmental awareness. The model is solved using the weighted sum and ε-constraint methods, resulting in non-dominant solutions. The findings indicate that combining the selection of third-party reverse logistics providers (3PRLPs) with optimal order allocation for recycling old batteries during emergencies effectively minimizes environmental impacts and combats climate change. © 2024 Elsevier Ltd Battery Industry; Carbon Emission Reduction; Emergency Logistics Operations; Machine Learning; Multi-Objective Optimization Lithium-ion batteries; Risk management; Battery industry; Carbon emissions; Carbon emissions reductions; Emergency logistic operation; Emergency logistics; End-of-life batteries; Logistic operations; Machine-learning; Multi-objectives optimization; Reverse logistics; carbon emission; climate change; climate effect; electric vehicle; emission control; machine learning; optimization; uncertainty analysis; Land fill";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
192;Seismopredict: Advanced Machine Learning for Earthquake Damage Assessment;The 2015 Gorkha earthquake highlighted the urgent need for predictive models to assess structural damage levels effectively. This study proposes an advanced machine learning framework employing gradient boosting algorithms - XGBoost, CatBoost, and LightGBM - to predict building damage levels. The models utilize construction attributes and geographic features. Key innovations include tailored feature engineering, hyperparameter tuning using GridSearchCV, and a detailed comparative evaluation against traditional methods like Random Forest and Logistic Regression. LightGBM emerged as the best-performing model, achieving 0.85 accuracy and exceptional computational efficiency due to its leaf-wise growth strategy. These findings can significantly aid disaster management by enabling precise risk assessments and efficient resource allocation. The paper provides mathematical foundations, implementation details, and interprets results to validate the approach's practicality.  © 2025 IEEE.;"CatBoost; KNN; LightGBM; Logistic Regression; Random Forest; SVM; XGBoost";"Adaptive boosting; Adversarial machine learning; Contrastive Learning; Disaster prevention; Disasters; Earthquake effects; Earthquake engineering; Logistic regression; Machine learning; Random forests; Resource allocation; Risk management; Catboost; Damage level; Earthquake damages; KNN; Lightgbm; Logistics regressions; Machine-learning; Random forests; SVM; Xgboost; Risk assessment";"Seismopredict: Advanced Machine Learning for Earthquake Damage Assessment The 2015 Gorkha earthquake highlighted the urgent need for predictive models to assess structural damage levels effectively. This study proposes an advanced machine learning framework employing gradient boosting algorithms - XGBoost, CatBoost, and LightGBM - to predict building damage levels. The models utilize construction attributes and geographic features. Key innovations include tailored feature engineering, hyperparameter tuning using GridSearchCV, and a detailed comparative evaluation against traditional methods like Random Forest and Logistic Regression. LightGBM emerged as the best-performing model, achieving 0.85 accuracy and exceptional computational efficiency due to its leaf-wise growth strategy. These findings can significantly aid disaster management by enabling precise risk assessments and efficient resource allocation. The paper provides mathematical foundations, implementation details, and interprets results to validate the approach's practicality.  © 2025 IEEE. CatBoost; KNN; LightGBM; Logistic Regression; Random Forest; SVM; XGBoost Adaptive boosting; Adversarial machine learning; Contrastive Learning; Disaster prevention; Disasters; Earthquake effects; Earthquake engineering; Logistic regression; Machine learning; Random forests; Resource allocation; Risk management; Catboost; Damage level; Earthquake damages; KNN; Lightgbm; Logistics regressions; Machine-learning; Random forests; SVM; Xgboost; Risk assessment";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
193;Harnessing Satellite Imagery for Effective Flood Detection using Convolution Neural Network;Earth Observation has become essential for monitoring geosciences and human activity. With the increasing availability of data, Artificial Intelligence (AI) algorithms have delivered significant advancements in remote sensing. However, weather-related disasters like hurricanes and floods, often accompanied by dense cloud cover, limit the effectiveness of optical imaging. Synthetic Aperture Radar (SAR), which penetrates clouds, offers a robust alternative. This study presents a novel dataset of co-registered optical and SAR image time series for flood event detection. By integrating spatial and temporal information using a ResNet-GRU architecture, we overcome the challenges posed by limited labeled SAR data. Our method achieves state-of-the-art accuracy, demonstrating resilience in detecting floods under adverse conditions. Performance metrics indicate substantial improvements in disaster response capabilities, emphasizing the practical utility of SAR in flood monitoring. This work provides critical insights for researchers and practitioners in leveraging SAR and optical data for disaster management. © 2025 IEEE.;"Flood Detection; Machine Learning; Optical Images; ResNet-GRU; Synthetic Aperture Radar (SAR)";"Convolutional neural networks; Image enhancement; Optical data processing; Optical image storage; Optical remote sensing; Optical tomography; Tropics; Convolution neural network; Earth observations; Flood detections; Geosciences; Human activities; Machine-learning; Optical image; Radar data; Resnet-GRU; Synthetic aperture radar; Radar imaging";"Harnessing Satellite Imagery for Effective Flood Detection using Convolution Neural Network Earth Observation has become essential for monitoring geosciences and human activity. With the increasing availability of data, Artificial Intelligence (AI) algorithms have delivered significant advancements in remote sensing. However, weather-related disasters like hurricanes and floods, often accompanied by dense cloud cover, limit the effectiveness of optical imaging. Synthetic Aperture Radar (SAR), which penetrates clouds, offers a robust alternative. This study presents a novel dataset of co-registered optical and SAR image time series for flood event detection. By integrating spatial and temporal information using a ResNet-GRU architecture, we overcome the challenges posed by limited labeled SAR data. Our method achieves state-of-the-art accuracy, demonstrating resilience in detecting floods under adverse conditions. Performance metrics indicate substantial improvements in disaster response capabilities, emphasizing the practical utility of SAR in flood monitoring. This work provides critical insights for researchers and practitioners in leveraging SAR and optical data for disaster management. © 2025 IEEE. Flood Detection; Machine Learning; Optical Images; ResNet-GRU; Synthetic Aperture Radar (SAR) Convolutional neural networks; Image enhancement; Optical data processing; Optical image storage; Optical remote sensing; Optical tomography; Tropics; Convolution neural network; Earth observations; Flood detections; Geosciences; Human activities; Machine-learning; Optical image; Radar data; Resnet-GRU; Synthetic aperture radar; Radar imaging";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
194;Flood vulnerability index to aid decision making;Floods damage ecosystem of the affected area resulting in destruction, loss of asset and life. The paper proposes a novel k-FVI, (k stands for Kerala and FVI for flood vulnerability index) to aid the decision makers reduce flood vulnerability of 14 districts of Kerala. Instead of usual classification of flood vulnerability indicators under exposure (), sensitivity (), and adaptive capacity (ℂ), k-FVI proposes that, indicators reflecting and preparedness (ℙ) govern pre-flood vulnerability, whereas those of and rehabilitation (ℝ) affects post-flood vulnerability. The division of ℂ indicator into ℙ and ℝ indicators and clubbing them into pre-flood and post-flood vulnerability respectively results into reduced errors. The importance of high dimensional flood indicators is realized by measuring the entropy of affected areas. Use of technique for order preference by similarity to ideal solution (TOPSIS) and entropy-based weights to score flood affected area results in formulating robust k-FVI. The paper also compares k-FVI with existing FVIs in literature. It uses data of 2018 Kerala floods and assesses the flood vulnerability of its 14 districts. The results prove that k-FVI is an effective flood vulnerability score estimator. Variant of k-FVI can be used to obtain vulnerability for any other flood prone areas. © 2024 This is an open access article under the CC BY-SA license;"Decision support system; Disaster management; Flood vulnerability; Multi criteria decision making; Technique for order preference by similarity to ideal solution";NULL;"Flood vulnerability index to aid decision making Floods damage ecosystem of the affected area resulting in destruction, loss of asset and life. The paper proposes a novel k-FVI, (k stands for Kerala and FVI for flood vulnerability index) to aid the decision makers reduce flood vulnerability of 14 districts of Kerala. Instead of usual classification of flood vulnerability indicators under exposure (), sensitivity (), and adaptive capacity (ℂ), k-FVI proposes that, indicators reflecting and preparedness (ℙ) govern pre-flood vulnerability, whereas those of and rehabilitation (ℝ) affects post-flood vulnerability. The division of ℂ indicator into ℙ and ℝ indicators and clubbing them into pre-flood and post-flood vulnerability respectively results into reduced errors. The importance of high dimensional flood indicators is realized by measuring the entropy of affected areas. Use of technique for order preference by similarity to ideal solution (TOPSIS) and entropy-based weights to score flood affected area results in formulating robust k-FVI. The paper also compares k-FVI with existing FVIs in literature. It uses data of 2018 Kerala floods and assesses the flood vulnerability of its 14 districts. The results prove that k-FVI is an effective flood vulnerability score estimator. Variant of k-FVI can be used to obtain vulnerability for any other flood prone areas. © 2024 This is an open access article under the CC BY-SA license Decision support system; Disaster management; Flood vulnerability; Multi criteria decision making; Technique for order preference by similarity to ideal solution NULL";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
195;Collaborative attacks and defense;Ad Hoc Networks, especially Flying Ad Hoc Networks (FANETs), are becoming increasingly crucial in a variety of civil and military applications, including wildfire monitoring and suppression, search and rescue operations in hazardous scenarios, and transport of supplies or personnel to remote locations. Going beyond protecting networks from single forms of attacks, the ongoing research work addresses collaborative attacks (CA) within these networks. A collaborative attack occurs when attackers synchronize their malicious actions against a target network. The collaboration can occur simultaneously, where multiple attackers attempt to compromise the system at the same time or split in time where one attacker gathers the information about the network and subsequently another attacker executes the actual exploit. There is a need to develop a systematic understanding of the threats imposed by collaborative attacks and formulate intelligent and effective defenses. In future, Machine Learning algorithms can be used to analyze large amounts of data to detect advanced threats and reduce false positives/negatives. © 2025 The Author(s).;NULL;"Ad hoc networks; Learning algorithms; Machine learning; Ad-hoc networks; False positive/negative; Large amounts of data; Machine learning algorithms; Remote location; Search and rescue operations; Military applications";"Collaborative attacks and defense Ad Hoc Networks, especially Flying Ad Hoc Networks (FANETs), are becoming increasingly crucial in a variety of civil and military applications, including wildfire monitoring and suppression, search and rescue operations in hazardous scenarios, and transport of supplies or personnel to remote locations. Going beyond protecting networks from single forms of attacks, the ongoing research work addresses collaborative attacks (CA) within these networks. A collaborative attack occurs when attackers synchronize their malicious actions against a target network. The collaboration can occur simultaneously, where multiple attackers attempt to compromise the system at the same time or split in time where one attacker gathers the information about the network and subsequently another attacker executes the actual exploit. There is a need to develop a systematic understanding of the threats imposed by collaborative attacks and formulate intelligent and effective defenses. In future, Machine Learning algorithms can be used to analyze large amounts of data to detect advanced threats and reduce false positives/negatives. © 2025 The Author(s). NULL Ad hoc networks; Learning algorithms; Machine learning; Ad-hoc networks; False positive/negative; Large amounts of data; Machine learning algorithms; Remote location; Search and rescue operations; Military applications";-1;Não Classificado;NULL;1.4;Climatological;3;Response
196;Deep learning-based landslide tsunami run-up prediction from synthetic gage data;The present study proposes a deep learning model based on Long-Short Term Memory (LSTM) that uses gage measurements for prediction of landslide-driven maximum tsunami run-up. In an attempt to overcome the limitation of insufficient real-world data in the field, our methodology refers to analytical models to create a comprehensive dataset employing a time series recorded from an offshore gage as input and its corresponding maximum run-up at the shoreline as output, for different landslide scenarios with pre-determined parameters. The LSTM-based model is then trained using this dataset in order to predict the maximum run-up. The results, with mean values of 0.211 m, 0.149 m, 1.745% and 0.9988 for RMSE, MAE, MAPE and R2, respectively, indicate that our model is both accurate and precise. As the data-driven models such as the one proposed here are often utilized to identify relationships that may not be immediately apparent from the physical models alone, our interdisciplinary approach has the potential to foster the development of innovative solutions and methodologies for addressing complex natural hazards by enhancing early warning systems, preparedness and response to tsunamis. © 2024 The Authors;"Deep learning; Landslide tsunami; LSTM; Maximum run-up; Prediction";"Landslides; Tsunamis; Deep learning; Gauge measurements; Landslide tsunamis; Learning models; Maximum run-up; Model-based OPC; Real-world; Short term memory; Times series; Tsunami run up; early warning system; landslide; natural hazard; prediction; shoreline; tsunami; Prediction models";"Deep learning-based landslide tsunami run-up prediction from synthetic gage data The present study proposes a deep learning model based on Long-Short Term Memory (LSTM) that uses gage measurements for prediction of landslide-driven maximum tsunami run-up. In an attempt to overcome the limitation of insufficient real-world data in the field, our methodology refers to analytical models to create a comprehensive dataset employing a time series recorded from an offshore gage as input and its corresponding maximum run-up at the shoreline as output, for different landslide scenarios with pre-determined parameters. The LSTM-based model is then trained using this dataset in order to predict the maximum run-up. The results, with mean values of 0.211 m, 0.149 m, 1.745% and 0.9988 for RMSE, MAE, MAPE and R2, respectively, indicate that our model is both accurate and precise. As the data-driven models such as the one proposed here are often utilized to identify relationships that may not be immediately apparent from the physical models alone, our interdisciplinary approach has the potential to foster the development of innovative solutions and methodologies for addressing complex natural hazards by enhancing early warning systems, preparedness and response to tsunamis. © 2024 The Authors Deep learning; Landslide tsunami; LSTM; Maximum run-up; Prediction Landslides; Tsunamis; Deep learning; Gauge measurements; Landslide tsunamis; Learning models; Maximum run-up; Model-based OPC; Real-world; Short term memory; Times series; Tsunami run up; early warning system; landslide; natural hazard; prediction; shoreline; tsunami; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
197;AttentionPoolMobileNeXt: An automated construction damage detection model based on a new convolutional neural network and deep feature engineering models;In 2023, Turkiye faced a series of devastating earthquakes and these earthquakes affected millions of people due to damaged constructions. These earthquakes demonstrated the urgent need for advanced automated damage detection models to help people. This study introduces a novel solution to address this challenge through the AttentionPoolMobileNeXt model, derived from a modified MobileNetV2 architecture. To rigorously evaluate the effectiveness of the model, we meticulously curated a dataset comprising instances of construction damage classified into five distinct classes. Upon applying this dataset to the AttentionPoolMobileNeXt model, we obtained an accuracy of 97%. In this work, we have created a dataset consisting of five distinct damage classes, and achieved 97% test accuracy using our proposed AttentionPoolMobileNeXt model. Additionally, the study extends its impact by introducing the AttentionPoolMobileNeXt-based Deep Feature Engineering (DFE) model, further enhancing the classification performance and interpretability of the system. The presented DFE significantly increased the test classification accuracy from 90.17% to 97%, yielding improvement over the baseline model. AttentionPoolMobileNeXt and its DFE counterpart collectively contribute to advancing the state-of-the-art in automated damage detection, offering valuable insights for disaster response and recovery efforts. © The Author(s) 2024.;"AttentionPoolMobileNeXt; Construction damage classification; Deep feature engineering; Image classification";"Automation; Convolutional neural networks; Damage detection; Earthquakes; Feature extraction; Statistical tests; Attentionpoolmobilenext; Automated construction; Construction damage classification; Construction damages; Damage classification; Deep feature engineering; Detection models; Engineering modelling; Feature engineerings; Images classification; Image classification";"AttentionPoolMobileNeXt: An automated construction damage detection model based on a new convolutional neural network and deep feature engineering models In 2023, Turkiye faced a series of devastating earthquakes and these earthquakes affected millions of people due to damaged constructions. These earthquakes demonstrated the urgent need for advanced automated damage detection models to help people. This study introduces a novel solution to address this challenge through the AttentionPoolMobileNeXt model, derived from a modified MobileNetV2 architecture. To rigorously evaluate the effectiveness of the model, we meticulously curated a dataset comprising instances of construction damage classified into five distinct classes. Upon applying this dataset to the AttentionPoolMobileNeXt model, we obtained an accuracy of 97%. In this work, we have created a dataset consisting of five distinct damage classes, and achieved 97% test accuracy using our proposed AttentionPoolMobileNeXt model. Additionally, the study extends its impact by introducing the AttentionPoolMobileNeXt-based Deep Feature Engineering (DFE) model, further enhancing the classification performance and interpretability of the system. The presented DFE significantly increased the test classification accuracy from 90.17% to 97%, yielding improvement over the baseline model. AttentionPoolMobileNeXt and its DFE counterpart collectively contribute to advancing the state-of-the-art in automated damage detection, offering valuable insights for disaster response and recovery efforts. © The Author(s) 2024. AttentionPoolMobileNeXt; Construction damage classification; Deep feature engineering; Image classification Automation; Convolutional neural networks; Damage detection; Earthquakes; Feature extraction; Statistical tests; Attentionpoolmobilenext; Automated construction; Construction damage classification; Construction damages; Damage classification; Deep feature engineering; Detection models; Engineering modelling; Feature engineerings; Images classification; Image classification";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
198;Developing a decision tree model to forecast runup and assess uncertainty in empirical formulations;The coastal zone is a dynamic region that can change rapidly and significantly with respect to the morphology of the beach and incoming wave conditions. Runup forecasts may be improved by adapting a dynamic approach that allows for different runup models to be implemented in response to changes in beach state. Accurately forecasting wave runup is critical to characterize exposure to coastal hazards and provide an early warning against potential erosion and inundation. Here, we developed a decision tree model to produce a weighted ensemble of existing runup models to predict 1.25 years of runup at Duck, North Carolina, USA. We then applied the calibrated decision tree model to reproduce observed runup during the DUNEX experiment in Pea Island, North Carolina, USA. We found that the decision tree approach yielded a prediction that was comparable or greater in accuracy (i.e. higher r2, lower RMSE) than the individual runup models. We also interrogated the decision tree predictions to determine how the individual models perform relative to each other and why certain models perform better than others under the same observed wave and beach conditions. We found that the decision tree approach drew on the processes represented in the individual models in the ensemble to produce a forecast that is accurate and explainable without relying on prior knowledge of the study site(s) or requiring manual adjustments beyond the initial model training. © 2024;"Coastcam; Decision tree; Machine learning; Wave runup";"Duck; North Carolina; Pea Island; United States; Beaches; Coastal zones; Digital elevation model; Miocene; Coastcam; Decision-tree model; Dynamic approaches; Dynamic region; Individual modeling; Machine-learning; North Carolina; Uncertainty; Wave conditions; Wave runup; coastal zone; decision analysis; empirical analysis; forecasting method; machine learning; nearshore dynamics; uncertainty analysis; wave modeling; wave runup; Prediction models";"Developing a decision tree model to forecast runup and assess uncertainty in empirical formulations The coastal zone is a dynamic region that can change rapidly and significantly with respect to the morphology of the beach and incoming wave conditions. Runup forecasts may be improved by adapting a dynamic approach that allows for different runup models to be implemented in response to changes in beach state. Accurately forecasting wave runup is critical to characterize exposure to coastal hazards and provide an early warning against potential erosion and inundation. Here, we developed a decision tree model to produce a weighted ensemble of existing runup models to predict 1.25 years of runup at Duck, North Carolina, USA. We then applied the calibrated decision tree model to reproduce observed runup during the DUNEX experiment in Pea Island, North Carolina, USA. We found that the decision tree approach yielded a prediction that was comparable or greater in accuracy (i.e. higher r2, lower RMSE) than the individual runup models. We also interrogated the decision tree predictions to determine how the individual models perform relative to each other and why certain models perform better than others under the same observed wave and beach conditions. We found that the decision tree approach drew on the processes represented in the individual models in the ensemble to produce a forecast that is accurate and explainable without relying on prior knowledge of the study site(s) or requiring manual adjustments beyond the initial model training. © 2024 Coastcam; Decision tree; Machine learning; Wave runup Duck; North Carolina; Pea Island; United States; Beaches; Coastal zones; Digital elevation model; Miocene; Coastcam; Decision-tree model; Dynamic approaches; Dynamic region; Individual modeling; Machine-learning; North Carolina; Uncertainty; Wave conditions; Wave runup; coastal zone; decision analysis; empirical analysis; forecasting method; machine learning; nearshore dynamics; uncertainty analysis; wave modeling; wave runup; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
199;Assessing the Residual Capacity of Damaged Buildings: A Mechanical Approach for Future Perspectives;Seismic risk in Italy is significant due to its high population density, the presence of large historic centres, the existence of diffused ancient hamlets in inner areas, and a built heritage consisting largely of old buildings. It is imperative to adopt strategic approaches focused on reuse, restoration, and adaptation to preserve historical and cultural values while promoting sustainability by reducing the impact of new urbanization. This paper proposes an empirical-performance method for an expedited assessment of the seismic vulnerability of damaged buildings. A set of 27 abandoned buildings located in the historical centre of Conza della Campania (Italy) and severely damaged by the 1980 Irpinia earthquake, is used as a case study. The study begins with a typological classification of the buildings, from which capacity curves are derived using a mechanical approach. Based on these curves, a second set of residual capacity curves is derived. By acting on the parameters of yielding acceleration and yielding displacement, these new curves enable consideration of the reduction in stiffness and strength linked to the degree of damage observed. Then, two different scenarios are considered, evaluating the impacts of a future earthquake first according to the residual curves and then following seismic adaptation measures. The demolition, transportation and reconstruction costs of the first scenario are compared to the retrofitting costs of the second scenario. The results and the comparative approach proposed provide valuable insights to implement reuse and rehabilitation strategies for the built environment, promoting sustainable regeneration of inner areas. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.;"Building Damage; Capacity Curves; Inner Areas; Seismic Risk; Vulnerability Assessment";"Demolition; Earthquake effects; Environmental remediation; Green buildings; Risk assessment; Seismic response; Building damage; Capacity curves; Future perspectives; High population density; Historic centres; Inner area; Mechanical; Residual capacity; Seismic risk; Vulnerability assessments";"Assessing the Residual Capacity of Damaged Buildings: A Mechanical Approach for Future Perspectives Seismic risk in Italy is significant due to its high population density, the presence of large historic centres, the existence of diffused ancient hamlets in inner areas, and a built heritage consisting largely of old buildings. It is imperative to adopt strategic approaches focused on reuse, restoration, and adaptation to preserve historical and cultural values while promoting sustainability by reducing the impact of new urbanization. This paper proposes an empirical-performance method for an expedited assessment of the seismic vulnerability of damaged buildings. A set of 27 abandoned buildings located in the historical centre of Conza della Campania (Italy) and severely damaged by the 1980 Irpinia earthquake, is used as a case study. The study begins with a typological classification of the buildings, from which capacity curves are derived using a mechanical approach. Based on these curves, a second set of residual capacity curves is derived. By acting on the parameters of yielding acceleration and yielding displacement, these new curves enable consideration of the reduction in stiffness and strength linked to the degree of damage observed. Then, two different scenarios are considered, evaluating the impacts of a future earthquake first according to the residual curves and then following seismic adaptation measures. The demolition, transportation and reconstruction costs of the first scenario are compared to the retrofitting costs of the second scenario. The results and the comparative approach proposed provide valuable insights to implement reuse and rehabilitation strategies for the built environment, promoting sustainable regeneration of inner areas. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025. Building Damage; Capacity Curves; Inner Areas; Seismic Risk; Vulnerability Assessment Demolition; Earthquake effects; Environmental remediation; Green buildings; Risk assessment; Seismic response; Building damage; Capacity curves; Future perspectives; High population density; Historic centres; Inner area; Mechanical; Residual capacity; Seismic risk; Vulnerability assessments";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
200;A post-disaster load supply restoration model for urban integrated energy systems based on multi-energy coordination;Urban integrated energy systems (UIES) have emerged as a promising solution to address the challenges of urban energy supply and consumption. However, UIES are vulnerable to extreme disasters, such as earthquakes, hurricanes, or floods, which can disrupt the energy infrastructure and lead to power outages and energy shortages. This study introduces a novel approach aimed at enhancing the resilience and load recovery capabilities of UIES in the face of extreme events. The proposed approach encompasses several key innovations, including the comprehensive coordination of local energy sources, prioritized restoration of critical loads, and a node-based modeling approach for gas and heat networks. Firstly, by fully integrating electricity, gas, and heat sources, the overall resilience of UIES is significantly improved, ensuring higher levels of load recovery even in the event of energy supply shortages. Secondly, the critical load recovery is prioritized by load classification, and the weights set can fully consider the connection status and load level of nodes to ensure priority provisioning of important node loads. In addition, the node-based modeling approach allows for accurate consideration of the flow of gas and heat in the pipe during modeling of the pipe energy storage. Finally, the case study section conducts simulation experiments with the UIES E33-G20-H6 test system to verify the effectiveness of the proposed approach and to demonstrate its potential in enhancing the post-disaster load recovery capability of UIES. © 2024;"Energy storage; Extreme event; Load supply recovery; Resilience; Urban integrated energy system";"Disasters; Electric energy storage; Electric loads; Energy resources; Outages; Restoration; Critical load; Energy supplies; Extreme events; Integrated energy systems; Load supply recovery; Node-based; Post disasters; Recovery capabilities; Resilience; Urban integrated energy system; disaster relief; energy storage; energy use; extreme event; integrated approach; restoration ecology; Recovery";"A post-disaster load supply restoration model for urban integrated energy systems based on multi-energy coordination Urban integrated energy systems (UIES) have emerged as a promising solution to address the challenges of urban energy supply and consumption. However, UIES are vulnerable to extreme disasters, such as earthquakes, hurricanes, or floods, which can disrupt the energy infrastructure and lead to power outages and energy shortages. This study introduces a novel approach aimed at enhancing the resilience and load recovery capabilities of UIES in the face of extreme events. The proposed approach encompasses several key innovations, including the comprehensive coordination of local energy sources, prioritized restoration of critical loads, and a node-based modeling approach for gas and heat networks. Firstly, by fully integrating electricity, gas, and heat sources, the overall resilience of UIES is significantly improved, ensuring higher levels of load recovery even in the event of energy supply shortages. Secondly, the critical load recovery is prioritized by load classification, and the weights set can fully consider the connection status and load level of nodes to ensure priority provisioning of important node loads. In addition, the node-based modeling approach allows for accurate consideration of the flow of gas and heat in the pipe during modeling of the pipe energy storage. Finally, the case study section conducts simulation experiments with the UIES E33-G20-H6 test system to verify the effectiveness of the proposed approach and to demonstrate its potential in enhancing the post-disaster load recovery capability of UIES. © 2024 Energy storage; Extreme event; Load supply recovery; Resilience; Urban integrated energy system Disasters; Electric energy storage; Electric loads; Energy resources; Outages; Restoration; Critical load; Energy supplies; Extreme events; Integrated energy systems; Load supply recovery; Node-based; Post disasters; Recovery capabilities; Resilience; Urban integrated energy system; disaster relief; energy storage; energy use; extreme event; integrated approach; restoration ecology; Recovery";-1;Não Classificado;NULL;-1;NULL;3;Response
201;Empowering flood forecasting through meteorological and social media data;Floods pose diverse challenges for organisms, infrastructure, and the environment, especially with the increasing frequency of natural disasters. Many existing works focused on traditional flood prediction methods, relying on statistical models which struggle in adapting to the dynamic environmental shifts, resulting in inaccurate forecasts and impeding effective disaster response. To address this, we propose a flood prediction paradigm that integrates both statistical models and social media data analysis techniques. By employing ensemble models combining classifiers such as K-Nearest Neighbor (KNN), Logistic Regression, Random Forest for statistical data analysis, alongside advanced natural language processing techniques such as Bidirectional Encoder Representations from Transformers (BERT), Long Short Term Memory (LSTM), and FastText for tweet analysis, our multimodal framework offers a comprehensive solution for flood forecasting. Leveraging rainfall data and user tweets, this study explores the fusion of diverse datasets to enhance predictive accuracy and provide timely insights into flood risk. The proposed system achieved an accuracy of 97%. Experimental results exhibit that the system outperforms other traditional methods. It captured the dynamic interplay between environmental data and human-generated content by amalgamating the predictive capabilities of diverse classifiers with advanced natural language processing techniques. © Bharati Vidyapeeth's Institute of Computer Applications and Management 2024.;"Feature fusion; Flood forecasting; Multimodal approach; VARMA model";NULL;"Empowering flood forecasting through meteorological and social media data Floods pose diverse challenges for organisms, infrastructure, and the environment, especially with the increasing frequency of natural disasters. Many existing works focused on traditional flood prediction methods, relying on statistical models which struggle in adapting to the dynamic environmental shifts, resulting in inaccurate forecasts and impeding effective disaster response. To address this, we propose a flood prediction paradigm that integrates both statistical models and social media data analysis techniques. By employing ensemble models combining classifiers such as K-Nearest Neighbor (KNN), Logistic Regression, Random Forest for statistical data analysis, alongside advanced natural language processing techniques such as Bidirectional Encoder Representations from Transformers (BERT), Long Short Term Memory (LSTM), and FastText for tweet analysis, our multimodal framework offers a comprehensive solution for flood forecasting. Leveraging rainfall data and user tweets, this study explores the fusion of diverse datasets to enhance predictive accuracy and provide timely insights into flood risk. The proposed system achieved an accuracy of 97%. Experimental results exhibit that the system outperforms other traditional methods. It captured the dynamic interplay between environmental data and human-generated content by amalgamating the predictive capabilities of diverse classifiers with advanced natural language processing techniques. © Bharati Vidyapeeth's Institute of Computer Applications and Management 2024. Feature fusion; Flood forecasting; Multimodal approach; VARMA model NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
202;Landslide susceptibility and building exposure assessment using machine learning models and geospatial analysis techniques;Landslides are among the most dangerous hazards in Asia, posing a significant threat to human lives, infrastructure, and sustainable development. Landslide susceptibility maps provide useful insights into hazard potential but lack quantitative exposure assessments to develop targeted mitigation strategies and resource allocation. This study aims to propose an integrated approach for landslide hazards and exposure evaluation using machine learning models on the Google Earth Engine environment and geospatial analysis techniques. A geospatial database was established to predict hazards and evaluate exposure, including data on topography, geology, hydrology, climate features, land use, and building data. The landslide susceptibility map was created with advanced machine-learning algorithms, including Classification And Regression Tree (CART), Random Forest (RF), Gradient Boosting (GB), and Support Vector Machine (SVM). The ROC curve and the Wilcoxon signed-rank test were employed to evaluate the performance differences among the CART, GB, RF, and SVM models. The results indicated that the RF model demonstrated the highest performance, leading to its selection for creating a landslide susceptibility map. Landslide exposure was evaluated by overlaying the landslide susceptibility map with building data to quantify the number of affected houses by landslides across districts and communes. The analysis results identified the Cho Don district as the most exposed, with 46,237 households located in high and very high landslide susceptibility zones, followed by Ba Be district (39,631 households), Bac Kan city (37,266 households), Bach Thong district (28,495 households), Cho Moi district (28,436 households), Na Ri district (17,723 households), Ngan Son district (14,142 households), and Pac Nam district (13,034 households). These findings enable the identification of the potential consequences of landslides on infrastructure, human settlements, and livelihoods, contributing to the promotion of disaster reduction and prevention strategies. © 2024 COSPAR;"Affected houses; Bac Kan province; Landslide exposure; Landslide susceptibility; Machine learning; Vietnam";"Adaptive boosting; Deforestation; Learning to rank; Random forests; Support vector machines; Trees (mathematics); Tropics; Affected house; Bac kan province; Exposure assessment; Geo-spatial analysis; Landslide exposure; Landslide susceptibility; Machine learning models; Machine-learning; Susceptibility maps; Viet Nam; Landslides";"Landslide susceptibility and building exposure assessment using machine learning models and geospatial analysis techniques Landslides are among the most dangerous hazards in Asia, posing a significant threat to human lives, infrastructure, and sustainable development. Landslide susceptibility maps provide useful insights into hazard potential but lack quantitative exposure assessments to develop targeted mitigation strategies and resource allocation. This study aims to propose an integrated approach for landslide hazards and exposure evaluation using machine learning models on the Google Earth Engine environment and geospatial analysis techniques. A geospatial database was established to predict hazards and evaluate exposure, including data on topography, geology, hydrology, climate features, land use, and building data. The landslide susceptibility map was created with advanced machine-learning algorithms, including Classification And Regression Tree (CART), Random Forest (RF), Gradient Boosting (GB), and Support Vector Machine (SVM). The ROC curve and the Wilcoxon signed-rank test were employed to evaluate the performance differences among the CART, GB, RF, and SVM models. The results indicated that the RF model demonstrated the highest performance, leading to its selection for creating a landslide susceptibility map. Landslide exposure was evaluated by overlaying the landslide susceptibility map with building data to quantify the number of affected houses by landslides across districts and communes. The analysis results identified the Cho Don district as the most exposed, with 46,237 households located in high and very high landslide susceptibility zones, followed by Ba Be district (39,631 households), Bac Kan city (37,266 households), Bach Thong district (28,495 households), Cho Moi district (28,436 households), Na Ri district (17,723 households), Ngan Son district (14,142 households), and Pac Nam district (13,034 households). These findings enable the identification of the potential consequences of landslides on infrastructure, human settlements, and livelihoods, contributing to the promotion of disaster reduction and prevention strategies. © 2024 COSPAR Affected houses; Bac Kan province; Landslide exposure; Landslide susceptibility; Machine learning; Vietnam Adaptive boosting; Deforestation; Learning to rank; Random forests; Support vector machines; Trees (mathematics); Tropics; Affected house; Bac kan province; Exposure assessment; Geo-spatial analysis; Landslide exposure; Landslide susceptibility; Machine learning models; Machine-learning; Susceptibility maps; Viet Nam; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
203;Automatic Methodology for Forest Fire Mapping with SuperDove Imagery;The global increase in wildfires due to climate change highlights the need for accurate wildfire mapping. This study performs a proof of concept on the usefulness of SuperDove imagery for wildfire mapping. To address this topic, we present an automatic methodology that combines the use of various vegetation indices with clustering algorithms (bisecting k-means and k-means) to analyze images before and after fires, with the aim of improving the precision of the burned area and severity assessments. The results demonstrate the potential of using this PlanetScope sensor, showing that the methodology effectively delineates burned areas and classifies them by severity level, in comparison with data from the Copernicus Emergency Management Service (CEMS). Thus, the potential of the SuperDove satellite sensor constellation for fire monitoring is highlighted, despite its limitations regarding radiometric distortion and the absence of Short-Wave Infrared (SWIR) bands, suggesting that the methodology could contribute to better fire management strategies. © 2024 by the authors.;"burned-area mapping; climate change; global warning; k-means; PlanetScope; severity mapping; SuperDove; vegetation index; wildfire";"Fire extinguishers; Fire hazards; K-means clustering; Risk management; Vegetation mapping; Burned areas; Burned-area mapping; Forest fires; Global warnings; K-means; Planetscope; Severity mapping; Superdove; Vegetation index; Wildfire; article; burn; climate change; clustering algorithm; controlled study; diagnosis; electric potential; female; forest fire; imagery; male; methodology; nonhuman; proof of concept; radiometry; sensor; vegetation; wildfire; Premixed flames";"Automatic Methodology for Forest Fire Mapping with SuperDove Imagery The global increase in wildfires due to climate change highlights the need for accurate wildfire mapping. This study performs a proof of concept on the usefulness of SuperDove imagery for wildfire mapping. To address this topic, we present an automatic methodology that combines the use of various vegetation indices with clustering algorithms (bisecting k-means and k-means) to analyze images before and after fires, with the aim of improving the precision of the burned area and severity assessments. The results demonstrate the potential of using this PlanetScope sensor, showing that the methodology effectively delineates burned areas and classifies them by severity level, in comparison with data from the Copernicus Emergency Management Service (CEMS). Thus, the potential of the SuperDove satellite sensor constellation for fire monitoring is highlighted, despite its limitations regarding radiometric distortion and the absence of Short-Wave Infrared (SWIR) bands, suggesting that the methodology could contribute to better fire management strategies. © 2024 by the authors. burned-area mapping; climate change; global warning; k-means; PlanetScope; severity mapping; SuperDove; vegetation index; wildfire Fire extinguishers; Fire hazards; K-means clustering; Risk management; Vegetation mapping; Burned areas; Burned-area mapping; Forest fires; Global warnings; K-means; Planetscope; Severity mapping; Superdove; Vegetation index; Wildfire; article; burn; climate change; clustering algorithm; controlled study; diagnosis; electric potential; female; forest fire; imagery; male; methodology; nonhuman; proof of concept; radiometry; sensor; vegetation; wildfire; Premixed flames";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.4;Climatological;1;Prevention
204;Predictive Model for Determining Saturation Profiles under Pavements during Flood Events;Pavements are highly susceptible to water infiltration during floods, resulting in reduced serviceability, shortened life span, and poor durability. To enhance pavement resilience and mitigate the rising risks associated with flooding events, this study aims to propose a method to predict the short-term temporal evolution of saturation levels within pavement structures resulting from inundation events. The methodology involves conducting a series of finite-element-based hydraulic simulations considering various influential factors such as pavement structure, subgrade type, groundwater table level, and flooding scenario. Time-descriptive indicators, including peak saturation time and restoration time, are calculated from the simulated volumetric water content data. To capture the complex relationships between the input parameters and output indicators, machine learning (ML) methods are employed to construct a predictive model for saturation profiles during flooding events. The results demonstrate that the 2nd-degree polynomial provides the best fit for the saturation changes within the vadose zone, and the random forest algorithm outperforms other ML methods, achieving the highest accuracy in projecting saturation changes during flooding events. The predictive model offers valuable insights for decision-making processes, including determining the optimal timing to reopen submerged roadways to traffic, and evaluating the moisture damage caused by the inundation. Overall, this research contributes to enhancing pavement resilience and enables the design and management of resilient pavements under changing climate conditions and extreme weather events. © The Author(s) 2024.;"geology and geoenvironmental engineering; geotechnical instrumentation and modeling; infrastructure; natural hazards and extreme weather events; numerical modeling; sustainability and resilience";"Decision making; Durability; Floods; Forestry; Groundwater; Weather information services; Extreme weather events; Floodings; Geoenvironmental engineering; Geology engineering; Geotechnical instrumentation; Geotechnical models; Infrastructure; Natural hazard; Natural hazard and extreme weather event; Sustainability and resilience; Pavements";"Predictive Model for Determining Saturation Profiles under Pavements during Flood Events Pavements are highly susceptible to water infiltration during floods, resulting in reduced serviceability, shortened life span, and poor durability. To enhance pavement resilience and mitigate the rising risks associated with flooding events, this study aims to propose a method to predict the short-term temporal evolution of saturation levels within pavement structures resulting from inundation events. The methodology involves conducting a series of finite-element-based hydraulic simulations considering various influential factors such as pavement structure, subgrade type, groundwater table level, and flooding scenario. Time-descriptive indicators, including peak saturation time and restoration time, are calculated from the simulated volumetric water content data. To capture the complex relationships between the input parameters and output indicators, machine learning (ML) methods are employed to construct a predictive model for saturation profiles during flooding events. The results demonstrate that the 2nd-degree polynomial provides the best fit for the saturation changes within the vadose zone, and the random forest algorithm outperforms other ML methods, achieving the highest accuracy in projecting saturation changes during flooding events. The predictive model offers valuable insights for decision-making processes, including determining the optimal timing to reopen submerged roadways to traffic, and evaluating the moisture damage caused by the inundation. Overall, this research contributes to enhancing pavement resilience and enables the design and management of resilient pavements under changing climate conditions and extreme weather events. © The Author(s) 2024. geology and geoenvironmental engineering; geotechnical instrumentation and modeling; infrastructure; natural hazards and extreme weather events; numerical modeling; sustainability and resilience Decision making; Durability; Floods; Forestry; Groundwater; Weather information services; Extreme weather events; Floodings; Geoenvironmental engineering; Geology engineering; Geotechnical instrumentation; Geotechnical models; Infrastructure; Natural hazard; Natural hazard and extreme weather event; Sustainability and resilience; Pavements";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
205;Methods for spatial and temporal detection of forest wildfire disturbance based on time series Eco-environment indicators;Forest wildfire disturbance information extracting − extracting the changes in vegetation and the condition of the burned areas − is essential for post-fire management and effective forest recovery. This study derived ecological indicators from remote sensing time series data. Time series analysis methods and change detection algorithms were applied to assess these indicators, enabling the identification of spatiotemporal information of fire disturbances. We selected the Sen + Mann-Kendall model, Coefficient of variation, Hurst exponent and Slope trend analysis to analyze the long-term impacts of the indicators extracted from Landsat images, including photosynthetic vegetation (PV), non-photosynthetic vegetation (NPV), bare rocky (BR) and normalized burn ratio (NBR). We determined the spatial distribution and timing of wildfires by analyzing the variations and fluctuations in indicators. The variation patterns of the indicators following the fires are as follows: PV and NBR decreased, while NPV and BR initially increased and subsequently decreased. By analyzing the time series analysis results of PV, NPV, BR, and NBR, the spatio-temporal information of the fires could be determined. Additionally, we used the stacked convolution long short-term memory (Stacked ConvLSTM) neural network to extract the burned area. The area extraction accuracy of this algorithm is approximately 98.43 %. Finally, the ensemble empirical mode decomposition (EEMD) was utilized to unmix the monthly mean PV, thereby obtaining the periods of vegetation recovery over multiple years. The recovery period of vegetation post-fire ranges from 3 to 12 months. This study proposes a method for comprehensively extracting information on forest wildfire disturbances at a spatiotemporal scale and discusses the recovery period of vegetation following the wildfires, as well as future development trends. It's crucial for evaluating the impacts on the ecological environment and subsequent restoration. © 2024 The Authors;"Change detection; Ecological environment; Forest wildfire disturbance information; NPV; PV; Time series analysis";"Abiotic; Premixed flames; Burned areas; Change detection; Ecological environments; Forest wildfire disturbance information; Forest wildfires; Non-photosynthetic vegetation; Photosynthetic vegetation; Post-fire; Spatiotemporal information; Time-series analysis; algorithm; decomposition analysis; disturbance; Landsat; remote sensing; spatial distribution; Forest ecology";"Methods for spatial and temporal detection of forest wildfire disturbance based on time series Eco-environment indicators Forest wildfire disturbance information extracting − extracting the changes in vegetation and the condition of the burned areas − is essential for post-fire management and effective forest recovery. This study derived ecological indicators from remote sensing time series data. Time series analysis methods and change detection algorithms were applied to assess these indicators, enabling the identification of spatiotemporal information of fire disturbances. We selected the Sen + Mann-Kendall model, Coefficient of variation, Hurst exponent and Slope trend analysis to analyze the long-term impacts of the indicators extracted from Landsat images, including photosynthetic vegetation (PV), non-photosynthetic vegetation (NPV), bare rocky (BR) and normalized burn ratio (NBR). We determined the spatial distribution and timing of wildfires by analyzing the variations and fluctuations in indicators. The variation patterns of the indicators following the fires are as follows: PV and NBR decreased, while NPV and BR initially increased and subsequently decreased. By analyzing the time series analysis results of PV, NPV, BR, and NBR, the spatio-temporal information of the fires could be determined. Additionally, we used the stacked convolution long short-term memory (Stacked ConvLSTM) neural network to extract the burned area. The area extraction accuracy of this algorithm is approximately 98.43 %. Finally, the ensemble empirical mode decomposition (EEMD) was utilized to unmix the monthly mean PV, thereby obtaining the periods of vegetation recovery over multiple years. The recovery period of vegetation post-fire ranges from 3 to 12 months. This study proposes a method for comprehensively extracting information on forest wildfire disturbances at a spatiotemporal scale and discusses the recovery period of vegetation following the wildfires, as well as future development trends. It's crucial for evaluating the impacts on the ecological environment and subsequent restoration. © 2024 The Authors Change detection; Ecological environment; Forest wildfire disturbance information; NPV; PV; Time series analysis Abiotic; Premixed flames; Burned areas; Change detection; Ecological environments; Forest wildfire disturbance information; Forest wildfires; Non-photosynthetic vegetation; Photosynthetic vegetation; Post-fire; Spatiotemporal information; Time-series analysis; algorithm; decomposition analysis; disturbance; Landsat; remote sensing; spatial distribution; Forest ecology";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;4;Recovery
206;Structural Seismic Response Reconstruction Using Physics-Guided Neural Networks;Reconstruction of data loss in structural seismic responses is important for structural health monitoring to evaluate the safety of structures. A physics-guided neural network that leverages the prior knowledge was proposed for reconstructing structural seismic responses that were inaccessible to measure or missing during earthquakes. The presented methodology consisted of convolutional neural networks with dilated kernel and fully connected neural networks, which were developed to achieve a multitask learning that involved the regression task with measured labeled displacement data and the reconstruction task of seismic response without any labels. To better balance the loss gradient across different tasks, a probabilistic model was introduced to optimize the weight coefficient for each task by quantifying the task-dependent uncertainty based on Bayesian statistics. The weight coefficient for each task can be dynamically updated during the training process, thereby improving the learning efficacy and performance accuracy of the neural networks. The probabilistic model with task-dependent uncertainty was validated to outperform the equal-weighted model (i.e. equal weight for each task) in reconstructing the structural seismic responses based on numerical data, even when the relevant physical information (i.e. Bouc-Wen model) was not complete. © 2024 World Scientific Publishing Company.;"convolutional neural networks; multi-task learning; Physics-guided neural network; probabilistic model; seismic response reconstruction";"Convolution; Convolutional neural networks; Earthquakes; Learning systems; Structural health monitoring; Convolutional neural network; Multitask learning; Neural-networks; Physic-guided neural network; Probabilistic models; Response reconstruction; Seismic response reconstruction; Structural seismic response; Uncertainty; Weight coefficients; Seismic response";"Structural Seismic Response Reconstruction Using Physics-Guided Neural Networks Reconstruction of data loss in structural seismic responses is important for structural health monitoring to evaluate the safety of structures. A physics-guided neural network that leverages the prior knowledge was proposed for reconstructing structural seismic responses that were inaccessible to measure or missing during earthquakes. The presented methodology consisted of convolutional neural networks with dilated kernel and fully connected neural networks, which were developed to achieve a multitask learning that involved the regression task with measured labeled displacement data and the reconstruction task of seismic response without any labels. To better balance the loss gradient across different tasks, a probabilistic model was introduced to optimize the weight coefficient for each task by quantifying the task-dependent uncertainty based on Bayesian statistics. The weight coefficient for each task can be dynamically updated during the training process, thereby improving the learning efficacy and performance accuracy of the neural networks. The probabilistic model with task-dependent uncertainty was validated to outperform the equal-weighted model (i.e. equal weight for each task) in reconstructing the structural seismic responses based on numerical data, even when the relevant physical information (i.e. Bouc-Wen model) was not complete. © 2024 World Scientific Publishing Company. convolutional neural networks; multi-task learning; Physics-guided neural network; probabilistic model; seismic response reconstruction Convolution; Convolutional neural networks; Earthquakes; Learning systems; Structural health monitoring; Convolutional neural network; Multitask learning; Neural-networks; Physic-guided neural network; Probabilistic models; Response reconstruction; Seismic response reconstruction; Structural seismic response; Uncertainty; Weight coefficients; Seismic response";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;2;Preparation
207;Application of Artificial Intelligence to Forecast Drought Index for the Mekong Delta;Droughts have a substantial impact on water supplies, agriculture, and ecosystems worldwide. Agricultural sustainability and production in the Mekong Delta of Vietnam are being jeopardized by droughts caused by climate change. Conventional forecasting methods frequently struggle to comprehend the intricate dynamics of meteorological occurrences connected to drought, necessitating the use of sophisticated prediction techniques. This study assesses the effectiveness of various statistical models (ARIMA), machine learning, and deep learning models (Gradient Boosting, XGBoost, RNN, and LSTM) in forecasting the SPEI over different time periods (1, 3, 6, and 12 months) across six prediction intervals. The models were developed and evaluated using data from 11 meteorological stations spanning from 1985 to 2022. These models incorporated various climatic variables, including precipitation, temperature, humidity, potential evapotranspiration (PET), Southern Oscillation Index (SOI) Anomaly, and sea surface temperature in the NINO4 region (SST_NINO4). The results demonstrate that XGBoost and LSTM models exhibit outstanding performance, showcasing lower error metrics and higher R² values compared to Gradient Boosting and RNN. The performance of the model fluctuated depending on the forecast step, with error metrics often increasing with longer prediction horizons. The use of climatic indices improved the accuracy of the model. These findings are consistent with earlier research on drought episodes in the Mekong Delta and support studies from other areas that show the effectiveness of advanced modeling tools for predicting droughts. The work emphasizes the capacity of machine learning and deep learning models to enhance the precision of drought forecasting, which is vital for efficient water resource management and agricultural planning in places prone to drought. © 2024 by the authors.;"deep learning algorithms; drought forecasting; machine learning algorithms; Mekong delta; standardized precipitation evapotranspiration index; Vietnam; water resource management";"Agriculture; Climate change; Drought; Learning systems; Long short-term memory; Mammals; Oceanography; Resource allocation; Surface waters; Water management; Water supply; Deep learning algorithm; Drought forecasting; Gradient boosting; Learning models; Machine learning algorithms; Machine-learning; Mekong Delta; Standardized precipitation evapotranspiration index; Viet Nam; Water resources management; Evapotranspiration";"Application of Artificial Intelligence to Forecast Drought Index for the Mekong Delta Droughts have a substantial impact on water supplies, agriculture, and ecosystems worldwide. Agricultural sustainability and production in the Mekong Delta of Vietnam are being jeopardized by droughts caused by climate change. Conventional forecasting methods frequently struggle to comprehend the intricate dynamics of meteorological occurrences connected to drought, necessitating the use of sophisticated prediction techniques. This study assesses the effectiveness of various statistical models (ARIMA), machine learning, and deep learning models (Gradient Boosting, XGBoost, RNN, and LSTM) in forecasting the SPEI over different time periods (1, 3, 6, and 12 months) across six prediction intervals. The models were developed and evaluated using data from 11 meteorological stations spanning from 1985 to 2022. These models incorporated various climatic variables, including precipitation, temperature, humidity, potential evapotranspiration (PET), Southern Oscillation Index (SOI) Anomaly, and sea surface temperature in the NINO4 region (SST_NINO4). The results demonstrate that XGBoost and LSTM models exhibit outstanding performance, showcasing lower error metrics and higher R² values compared to Gradient Boosting and RNN. The performance of the model fluctuated depending on the forecast step, with error metrics often increasing with longer prediction horizons. The use of climatic indices improved the accuracy of the model. These findings are consistent with earlier research on drought episodes in the Mekong Delta and support studies from other areas that show the effectiveness of advanced modeling tools for predicting droughts. The work emphasizes the capacity of machine learning and deep learning models to enhance the precision of drought forecasting, which is vital for efficient water resource management and agricultural planning in places prone to drought. © 2024 by the authors. deep learning algorithms; drought forecasting; machine learning algorithms; Mekong delta; standardized precipitation evapotranspiration index; Vietnam; water resource management Agriculture; Climate change; Drought; Learning systems; Long short-term memory; Mammals; Oceanography; Resource allocation; Surface waters; Water management; Water supply; Deep learning algorithm; Drought forecasting; Gradient boosting; Learning models; Machine learning algorithms; Machine-learning; Mekong Delta; Standardized precipitation evapotranspiration index; Viet Nam; Water resources management; Evapotranspiration";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
208;Deep learning model for human-intuitive shoeprint reconstruction;Shoeprint reconstruction is essential in forensic science, but it is also challenging due to various inconsistencies in the patterns, textures, sizes, abrasions, etc. of shoeprints. The computational reconstruction of sharper and more complete shoeprints is conventionally conducted using handcrafted features, and it often requires human intervention. Prior studies using end-to-end machine learning approaches are limited in number and have not achieved a high level of performance. In this paper, we propose a model named ShoeRec, which employs variational autoencoder (VAE) as a component in a U-Net-like architecture to reconstruct missing regions and borders in shoeprint images. ShoeRec incorporates skip connections to preserve key patterns and employs VAE in the bottleneck to facilitate the reconstruction of desired shoeprints with the restoration of detail as perceived by humans. As a U-Net, the model skips the contextual information from the encoder to the decoder, and the compressed features in the latent space via VAE optimize the probabilistic distribution for reconstructing complete shoeprints. The reconstruction operation is automatically tuned according to the objective function, so as to reduce the structural correlation between the original and projected shoeprint and restore the absent information in the desired shoeprints. To the best of our knowledge, ShoeRec is the first deep learning infusion model that specializes in shoeprint reconstruction. The shoeprints reconstructed by ShoeRec have a close match with the originals in terms of both structures and patterns, and ShoeRec outperforms state-of-the-art generative models in human evaluation. © 2024 Elsevier Ltd;"Abrasion; Forensics; Generation; Patterns; Reconstruction; Shoeprint";"Abrasion; Deep learning; Erosion; Forensic science; Learning systems; Probability distributions; Textures; Auto encoders; Computational reconstruction; End to end; Forensic; Generation; Human intervention; Learning models; Pattern; Reconstruction; Shoeprint; Restoration";"Deep learning model for human-intuitive shoeprint reconstruction Shoeprint reconstruction is essential in forensic science, but it is also challenging due to various inconsistencies in the patterns, textures, sizes, abrasions, etc. of shoeprints. The computational reconstruction of sharper and more complete shoeprints is conventionally conducted using handcrafted features, and it often requires human intervention. Prior studies using end-to-end machine learning approaches are limited in number and have not achieved a high level of performance. In this paper, we propose a model named ShoeRec, which employs variational autoencoder (VAE) as a component in a U-Net-like architecture to reconstruct missing regions and borders in shoeprint images. ShoeRec incorporates skip connections to preserve key patterns and employs VAE in the bottleneck to facilitate the reconstruction of desired shoeprints with the restoration of detail as perceived by humans. As a U-Net, the model skips the contextual information from the encoder to the decoder, and the compressed features in the latent space via VAE optimize the probabilistic distribution for reconstructing complete shoeprints. The reconstruction operation is automatically tuned according to the objective function, so as to reduce the structural correlation between the original and projected shoeprint and restore the absent information in the desired shoeprints. To the best of our knowledge, ShoeRec is the first deep learning infusion model that specializes in shoeprint reconstruction. The shoeprints reconstructed by ShoeRec have a close match with the originals in terms of both structures and patterns, and ShoeRec outperforms state-of-the-art generative models in human evaluation. © 2024 Elsevier Ltd Abrasion; Forensics; Generation; Patterns; Reconstruction; Shoeprint Abrasion; Deep learning; Erosion; Forensic science; Learning systems; Probability distributions; Textures; Auto encoders; Computational reconstruction; End to end; Forensic; Generation; Human intervention; Learning models; Pattern; Reconstruction; Shoeprint; Restoration";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
209;An integrated approach for prediction of magnitude using deep learning techniques;Timely estimation of earthquake magnitude plays a crucial role in the early warning systems for earthquakes. Despite the inherent danger associated with earthquake energy, earthquake research necessitates extensive parameter estimation and predictive techniques to account for uncertain trends in earthquake waveforms when determining earthquake magnitudes using a single station. This study introduces an effective solution to tackle the issue through the automatic magnitude deep network (AMagDN) model. The proposed model includes long short-term memory (LSTM), a bidirectional LSTM, an autocorrelation attention mechanism, and a machine learning block that can capture detailed information from the seismic waveform recorded during an earthquake. The unique feature of this model is the use of multivariate time series waveforms derived from recorded accelerograms specifically tailored to their energy significance with magnitude and seven fusion tabular parameters involving source and geospatial features. The proposed model’s training, validation and testing are done using independent 15014, 1287 and 3448 records maintained by the Kyoshin network, Japan, for moderate to great impact earthquakes between 5.5 and 8.0 (MJMA). A comparative study shows that the proposed model outperforms recent state-of-the-art models and common linear relations, reducing mean absolute prediction error by 40% from the second-best model. The multi-stations data are also used for successfully forecasting the magnitudes of two significant earthquakes of 7.7 and 7.3 magnitude (MJMA) using the proposed model. The reliable prediction capabilities of the proposed model for both single and multi-station data clearly demonstrate its utility in reducing earthquake hazards. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.;"Autocorrelation attention; LSTM; Magnitude; Seismic waveforms";"Autocorrelation; Earthquakes; Forecasting; Learning systems; Seismic waves; Uncertainty analysis; Waveform analysis; Auto correlation; Autocorrelation attention; Earthquake magnitudes; Energy; Integrated approach; Learning techniques; Magnitude; Multi-stations; Seismic waveforms; Station data; Long short-term memory";"An integrated approach for prediction of magnitude using deep learning techniques Timely estimation of earthquake magnitude plays a crucial role in the early warning systems for earthquakes. Despite the inherent danger associated with earthquake energy, earthquake research necessitates extensive parameter estimation and predictive techniques to account for uncertain trends in earthquake waveforms when determining earthquake magnitudes using a single station. This study introduces an effective solution to tackle the issue through the automatic magnitude deep network (AMagDN) model. The proposed model includes long short-term memory (LSTM), a bidirectional LSTM, an autocorrelation attention mechanism, and a machine learning block that can capture detailed information from the seismic waveform recorded during an earthquake. The unique feature of this model is the use of multivariate time series waveforms derived from recorded accelerograms specifically tailored to their energy significance with magnitude and seven fusion tabular parameters involving source and geospatial features. The proposed model’s training, validation and testing are done using independent 15014, 1287 and 3448 records maintained by the Kyoshin network, Japan, for moderate to great impact earthquakes between 5.5 and 8.0 (MJMA). A comparative study shows that the proposed model outperforms recent state-of-the-art models and common linear relations, reducing mean absolute prediction error by 40% from the second-best model. The multi-stations data are also used for successfully forecasting the magnitudes of two significant earthquakes of 7.7 and 7.3 magnitude (MJMA) using the proposed model. The reliable prediction capabilities of the proposed model for both single and multi-station data clearly demonstrate its utility in reducing earthquake hazards. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024. Autocorrelation attention; LSTM; Magnitude; Seismic waveforms Autocorrelation; Earthquakes; Forecasting; Learning systems; Seismic waves; Uncertainty analysis; Waveform analysis; Auto correlation; Autocorrelation attention; Earthquake magnitudes; Energy; Integrated approach; Learning techniques; Magnitude; Multi-stations; Seismic waveforms; Station data; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
210;Machine Learning-Based Systems for Early Warning of Rainfall-Induced Landslide;Landslide disasters have inflicted incalculable losses on China's national economy, as well as on lives and property. Notably, 90% of landslide disasters are directly induced by rainfall or have indirect associations with it. In Bazhong City, Sichuan Province, China, the proportion of rainfall-induced landslides accounts for more than 70% of all geological disasters in the region. Our research undertook a susceptibility analysis of multimodal landslide data in Bazhou District of Bazhong City, employing four distinct machine learning methods: decision trees (DTs), random forests (RFs), support vector machines (SVMs), and back-propagation neural networks (BPNNs). Additionally, data from the Tropical Rainfall Measuring Mission (TRMM) 3B42 precipitation product were utilized to develop a rainfall intensity-duration (I-D) model for the Bazhou District. The experimental results indicated that the BPNN achieved the highest overall classification accuracy, reaching 92.00%, which was 3.00% to 6.00% higher than those achieved by other algorithms. The kappa coefficient for BPNN was 0.84, surpassing other algorithms by 0.06 to 0.10. Furthermore, our results demonstrated that the rainfall I-D model had a prediction accuracy of 90.91% for rainfall-induced landslides. Finally, a probability quantification model for landslide triggering factors was established based on the previous two research results, aimed at meteorological warning. Comparisons with five recorded landslide events in 2009 revealed that the experimental outcomes of the meteorological early warning model aligned with the actual inspection results. Therefore, this model can serve as a reliable reference for issuing warnings about rainfall-induced landslides in Bazhou District. © 2024 American Society of Civil Engineers.;"Bazhou District; Landslide susceptibility map; Machine learning; Rainfall threshold; Rainfall-induced landslide; Warning";"Bazhong; China; Sichuan; Backpropagation; Decision trees; Landslides; Learning systems; Neural networks; Rain; Rain gages; Support vector machines; Back-propagation neural networks; Bazhou district; Landslide susceptibility; Landslide susceptibility map; Machine-learning; Rainfall induced landslides; Rainfall intensity durations; Rainfall thresholds; Susceptibility maps; Warning; algorithm; early warning system; landslide; machine learning; prediction; probability; TRMM; Disasters";"Machine Learning-Based Systems for Early Warning of Rainfall-Induced Landslide Landslide disasters have inflicted incalculable losses on China's national economy, as well as on lives and property. Notably, 90% of landslide disasters are directly induced by rainfall or have indirect associations with it. In Bazhong City, Sichuan Province, China, the proportion of rainfall-induced landslides accounts for more than 70% of all geological disasters in the region. Our research undertook a susceptibility analysis of multimodal landslide data in Bazhou District of Bazhong City, employing four distinct machine learning methods: decision trees (DTs), random forests (RFs), support vector machines (SVMs), and back-propagation neural networks (BPNNs). Additionally, data from the Tropical Rainfall Measuring Mission (TRMM) 3B42 precipitation product were utilized to develop a rainfall intensity-duration (I-D) model for the Bazhou District. The experimental results indicated that the BPNN achieved the highest overall classification accuracy, reaching 92.00%, which was 3.00% to 6.00% higher than those achieved by other algorithms. The kappa coefficient for BPNN was 0.84, surpassing other algorithms by 0.06 to 0.10. Furthermore, our results demonstrated that the rainfall I-D model had a prediction accuracy of 90.91% for rainfall-induced landslides. Finally, a probability quantification model for landslide triggering factors was established based on the previous two research results, aimed at meteorological warning. Comparisons with five recorded landslide events in 2009 revealed that the experimental outcomes of the meteorological early warning model aligned with the actual inspection results. Therefore, this model can serve as a reliable reference for issuing warnings about rainfall-induced landslides in Bazhou District. © 2024 American Society of Civil Engineers. Bazhou District; Landslide susceptibility map; Machine learning; Rainfall threshold; Rainfall-induced landslide; Warning Bazhong; China; Sichuan; Backpropagation; Decision trees; Landslides; Learning systems; Neural networks; Rain; Rain gages; Support vector machines; Back-propagation neural networks; Bazhou district; Landslide susceptibility; Landslide susceptibility map; Machine-learning; Rainfall induced landslides; Rainfall intensity durations; Rainfall thresholds; Susceptibility maps; Warning; algorithm; early warning system; landslide; machine learning; prediction; probability; TRMM; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
211;Threshold-based earthquake early warning for high-speed railways using deep learning;Earthquakes are disasters that threaten the operational safety of high-speed railways. To obtain reliable alerts for the earthquake monitoring and early warning systems of high-speed railways, based on magnitude and peak ground acceleration (PGA) thresholds (M = 5.5 and PGA = 40 cm/s2), an earthquake early warning (EEW) method for high-speed railways using deep learning is proposed. And the application of deep learning method in EEW for high-speed railway is explored. We design a single-station deep learning network architecture (named the CT architecture) by combining convolutional neural and transformer networks, and with that architecture, we train two separate models (CT-M and CT-PGA models) using the strong motion data recorded from the Kyoshin Network in Japan, which are used to predict whether the magnitude and PGA exceed the thresholds for issuing an alert. To verify the robustness of the method, we apply it to the M7.3 earthquake and M7.4 earthquake off the coast of Fukushima in 2021–2022. Results show that within 10 s after P-wave arrival, the accuracy of the alert reaches 90 %, and the average observed lead time reaches 18 s. The proposed method displays potential application on EEW systems for high-speed railways. © 2024;"Accuracy of alert; Deep learning; Earthquake early warning; High-speed railway; Magnitude; Peak ground acceleration";"Acceleration; Deep learning; Earthquakes; Learning systems; Railroad transportation; Railroads; Seismic waves; Acceleration threshold; Accuracy of alert; Deep learning; Early Warning System; Earthquake early warning; Earthquake monitoring; High-speed railways; Magnitude; Operational safety; Peak ground acceleration; Network architecture";"Threshold-based earthquake early warning for high-speed railways using deep learning Earthquakes are disasters that threaten the operational safety of high-speed railways. To obtain reliable alerts for the earthquake monitoring and early warning systems of high-speed railways, based on magnitude and peak ground acceleration (PGA) thresholds (M = 5.5 and PGA = 40 cm/s2), an earthquake early warning (EEW) method for high-speed railways using deep learning is proposed. And the application of deep learning method in EEW for high-speed railway is explored. We design a single-station deep learning network architecture (named the CT architecture) by combining convolutional neural and transformer networks, and with that architecture, we train two separate models (CT-M and CT-PGA models) using the strong motion data recorded from the Kyoshin Network in Japan, which are used to predict whether the magnitude and PGA exceed the thresholds for issuing an alert. To verify the robustness of the method, we apply it to the M7.3 earthquake and M7.4 earthquake off the coast of Fukushima in 2021–2022. Results show that within 10 s after P-wave arrival, the accuracy of the alert reaches 90 %, and the average observed lead time reaches 18 s. The proposed method displays potential application on EEW systems for high-speed railways. © 2024 Accuracy of alert; Deep learning; Earthquake early warning; High-speed railway; Magnitude; Peak ground acceleration Acceleration; Deep learning; Earthquakes; Learning systems; Railroad transportation; Railroads; Seismic waves; Acceleration threshold; Accuracy of alert; Deep learning; Early Warning System; Earthquake early warning; Earthquake monitoring; High-speed railways; Magnitude; Operational safety; Peak ground acceleration; Network architecture";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
212;An Experimental Study of the Acoustic Signal Characteristics of Locked-Segment Damage Evolution in a Landslide Model;Three-section landslides are renowned for their immense size, concealed development process, and devastating impact. This study conducted physical model tests to simulate one special geological structure called a three-section-within landslide. The failure process and precursory characteristics of the tested samples were meticulously analyzed using video imagery, micro-seismic (MS) signals, and acoustic emission (AE) signals, with a focus on event activity, intensity, and frequency. A novel classification method based on AE waveform characteristics was proposed, categorizing AE signals into burst signals and continuous signals. The findings reveal distinct differences in the evolution of these signals. Burst signals appeared exclusively during the crack propagation and failure stages. During these stages, the cumulative AE hits of burst signals increased gradually, with amplitude rising and then declining. High-amplitude burst signals were predominantly distributed in the middle- and high-frequency bands. In contrast, cumulative AE hits of continuous signals escalated rapidly, with amplitude monotonously increasing, and high-amplitude continuous signals were primarily distributed in the low-frequency band. The emergence of burst signals and high-frequency AE signals indicated the generation of microcracks, serving as early-warning indicators. Notably, the early-warning points of AE signals were detected earlier than those of video imagery and MS signals. Furthermore, the early-warning point of burst signals occurred earlier than those of continuous signals, and the early-warning point of the classification method preceded that of overall AE signals. © 2024 by the authors.;"acoustic emission; locking section; micro-seismic signal; three-section landslide; video image";"Acoustic emission testing; Acoustic emissions; Image segmentation; Microcracks; Seismology; Acoustic emission signal; Acoustic-emissions; Burst signals; Early warning; Locking section; Micro-seismic; Micro-seismic signal; Seismic signals; Three-section landslide; Video image; article; controlled study; diagnosis; evolution; experimental study; female; human experiment; imagery; landslide; male; nonhuman; physical model; simulation; videorecording; waveform; Landslides";"An Experimental Study of the Acoustic Signal Characteristics of Locked-Segment Damage Evolution in a Landslide Model Three-section landslides are renowned for their immense size, concealed development process, and devastating impact. This study conducted physical model tests to simulate one special geological structure called a three-section-within landslide. The failure process and precursory characteristics of the tested samples were meticulously analyzed using video imagery, micro-seismic (MS) signals, and acoustic emission (AE) signals, with a focus on event activity, intensity, and frequency. A novel classification method based on AE waveform characteristics was proposed, categorizing AE signals into burst signals and continuous signals. The findings reveal distinct differences in the evolution of these signals. Burst signals appeared exclusively during the crack propagation and failure stages. During these stages, the cumulative AE hits of burst signals increased gradually, with amplitude rising and then declining. High-amplitude burst signals were predominantly distributed in the middle- and high-frequency bands. In contrast, cumulative AE hits of continuous signals escalated rapidly, with amplitude monotonously increasing, and high-amplitude continuous signals were primarily distributed in the low-frequency band. The emergence of burst signals and high-frequency AE signals indicated the generation of microcracks, serving as early-warning indicators. Notably, the early-warning points of AE signals were detected earlier than those of video imagery and MS signals. Furthermore, the early-warning point of burst signals occurred earlier than those of continuous signals, and the early-warning point of the classification method preceded that of overall AE signals. © 2024 by the authors. acoustic emission; locking section; micro-seismic signal; three-section landslide; video image Acoustic emission testing; Acoustic emissions; Image segmentation; Microcracks; Seismology; Acoustic emission signal; Acoustic-emissions; Burst signals; Early warning; Locking section; Micro-seismic; Micro-seismic signal; Seismic signals; Three-section landslide; Video image; article; controlled study; diagnosis; evolution; experimental study; female; human experiment; imagery; landslide; male; nonhuman; physical model; simulation; videorecording; waveform; Landslides";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;2;Preparation
213;Electrokinetically propelled digital pendulum for seismic alert;An Electrostatic Induction Pendulum (EIP) has been designed and developed for the first time in the innovative construction and maintenance of earthquake prewarning systems to address the existed issues of massive funding required, incomplete distribution even as well as the lack of coverage in remote or sparsely populated areas. This innovative EIP includes a pendulum and a static-electricity induced sensor which could detect subtle variations in electrical potential during motion. The sensor offers crucial insights into the vibration, damping, and resonance traits of the EIP, with the minimum detectable vibration intensity of 0.17 m/s2. Simultaneously, by introducing external vibrations varied with frequencies, the inherent frequency of the EIP can correspondingly be modified subtly leading energy distribution to be weakened, which lay a groundwork for earthquake monitoring and early warning. Furthermore, an earthquake monitoring and early warning system aided by EIP, data acquisition module, and machine learning is engineered for real-time analysis and decision-making. Once an earthquake occurs, the system automatically sends alerts to relevant departments or individuals to take protective measures in advance. This cost-effective, easily portable, and widely distributed earthquake monitoring solution complements existing systems and can be easily deployed even in remote areas. © 2024 Elsevier B.V.;"Earthquake alert; Electrostatic induction; Machine learning; Pendulum";"Earthquake engineering; Electrostatic devices; Pendulums; Earthquake alert; Earthquake monitoring; Electrical potential; Electrostatic induction; Fundings; Machine-learning; Sparsely populated areas; Vibration intensity; Vibration resonance; Vibration-damping; Earthquakes";"Electrokinetically propelled digital pendulum for seismic alert An Electrostatic Induction Pendulum (EIP) has been designed and developed for the first time in the innovative construction and maintenance of earthquake prewarning systems to address the existed issues of massive funding required, incomplete distribution even as well as the lack of coverage in remote or sparsely populated areas. This innovative EIP includes a pendulum and a static-electricity induced sensor which could detect subtle variations in electrical potential during motion. The sensor offers crucial insights into the vibration, damping, and resonance traits of the EIP, with the minimum detectable vibration intensity of 0.17 m/s2. Simultaneously, by introducing external vibrations varied with frequencies, the inherent frequency of the EIP can correspondingly be modified subtly leading energy distribution to be weakened, which lay a groundwork for earthquake monitoring and early warning. Furthermore, an earthquake monitoring and early warning system aided by EIP, data acquisition module, and machine learning is engineered for real-time analysis and decision-making. Once an earthquake occurs, the system automatically sends alerts to relevant departments or individuals to take protective measures in advance. This cost-effective, easily portable, and widely distributed earthquake monitoring solution complements existing systems and can be easily deployed even in remote areas. © 2024 Elsevier B.V. Earthquake alert; Electrostatic induction; Machine learning; Pendulum Earthquake engineering; Electrostatic devices; Pendulums; Earthquake alert; Earthquake monitoring; Electrical potential; Electrostatic induction; Fundings; Machine-learning; Sparsely populated areas; Vibration intensity; Vibration resonance; Vibration-damping; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
214;Flood Susceptibility Assessment in Urban Areas via Deep Neural Network Approach;Floods, caused by intense rainfall or typhoons, overwhelming urban drainage systems, pose significant threats to urban areas, leading to substantial economic losses and endangering human lives. This study proposes a methodology for flood assessment in urban areas using a multiclass classification approach with a Deep Neural Network (DNN) optimized through hyperparameter tuning with genetic algorithms (GAs) leveraging remote sensing data of a flood dataset for the Ibadan metropolis, Nigeria and Metro Manila, Philippines. The results show that the optimized DNN model significantly improves flood risk assessment accuracy (Ibadan-0.98) compared to datasets containing only location and precipitation data (Manila-0.38). By incorporating soil data into the model, as well as reducing the number of classes, it is able to predict flood risks more accurately, providing insights for proactive flood mitigation strategies and urban planning. © 2024 by the authors.;"disaster risk reduction; floods; multiclass classification; sustainable cities and communities; sustainable urban development; urban environment";"Ibadan; National Capital Region; National Capital Region; Nigeria; Oyo; Philippines; artificial neural network; data set; disaster management; environmental assessment; flood control; mitigation; remote sensing; risk assessment; sustainability; sustainable development; urban area; urban development; urban drainage";"Flood Susceptibility Assessment in Urban Areas via Deep Neural Network Approach Floods, caused by intense rainfall or typhoons, overwhelming urban drainage systems, pose significant threats to urban areas, leading to substantial economic losses and endangering human lives. This study proposes a methodology for flood assessment in urban areas using a multiclass classification approach with a Deep Neural Network (DNN) optimized through hyperparameter tuning with genetic algorithms (GAs) leveraging remote sensing data of a flood dataset for the Ibadan metropolis, Nigeria and Metro Manila, Philippines. The results show that the optimized DNN model significantly improves flood risk assessment accuracy (Ibadan-0.98) compared to datasets containing only location and precipitation data (Manila-0.38). By incorporating soil data into the model, as well as reducing the number of classes, it is able to predict flood risks more accurately, providing insights for proactive flood mitigation strategies and urban planning. © 2024 by the authors. disaster risk reduction; floods; multiclass classification; sustainable cities and communities; sustainable urban development; urban environment Ibadan; National Capital Region; National Capital Region; Nigeria; Oyo; Philippines; artificial neural network; data set; disaster management; environmental assessment; flood control; mitigation; remote sensing; risk assessment; sustainability; sustainable development; urban area; urban development; urban drainage";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
215;A parallel machine learning-based approach for tsunami waves forecasting using regression trees;Following a seismic event, tsunami early warning systems (TEWSs) try to provide precise forecasts of the maximum height of incoming waves at designated target points along the coast. This information is crucial to trigger early warnings in areas where the impact of tsunami waves is predicted to be dangerous (or potentially cause destruction), to help the management of the potential impact of a tsunami as well as reduce environmental destruction and losses of human lives. For such a reason, it is crucial that TEWSs produce predictions with short computation time while maintaining a high prediction accuracy. This paper presents a parallel machine learning approach, based on regression trees, to discover tsunami predictive models from simulation data. In order to achieve the results in a short time, the proposed approach relies on the parallelization of the most time consuming tasks and on incremental learning executions, in order to achieve higher performances in terms of execution time, efficiency and scalability. The experimental evaluation, performed on two real tsunami cases occurred in the Western and Eastern Mediterranean basin in 2003 and 2017, shows reasonable advantages in terms of scalability and execution time, which is an important benefit in a urgent-computing scenarios. © 2024 The Author(s);"Machine learning; Parallel data mining; Regression trees; Tsunami forecasting";"Data mining; Forecasting; Information management; Machine learning; Regression analysis; Scalability; Trees (mathematics); Learning-based approach; Machine-learning; Parallel data mining; Parallel machine; Regression trees; Seismic event; Tsunami early-warning systems; Tsunami forecasting; Tsunami waves; Wave forecasting; Tsunamis";"A parallel machine learning-based approach for tsunami waves forecasting using regression trees Following a seismic event, tsunami early warning systems (TEWSs) try to provide precise forecasts of the maximum height of incoming waves at designated target points along the coast. This information is crucial to trigger early warnings in areas where the impact of tsunami waves is predicted to be dangerous (or potentially cause destruction), to help the management of the potential impact of a tsunami as well as reduce environmental destruction and losses of human lives. For such a reason, it is crucial that TEWSs produce predictions with short computation time while maintaining a high prediction accuracy. This paper presents a parallel machine learning approach, based on regression trees, to discover tsunami predictive models from simulation data. In order to achieve the results in a short time, the proposed approach relies on the parallelization of the most time consuming tasks and on incremental learning executions, in order to achieve higher performances in terms of execution time, efficiency and scalability. The experimental evaluation, performed on two real tsunami cases occurred in the Western and Eastern Mediterranean basin in 2003 and 2017, shows reasonable advantages in terms of scalability and execution time, which is an important benefit in a urgent-computing scenarios. © 2024 The Author(s) Machine learning; Parallel data mining; Regression trees; Tsunami forecasting Data mining; Forecasting; Information management; Machine learning; Regression analysis; Scalability; Trees (mathematics); Learning-based approach; Machine-learning; Parallel data mining; Parallel machine; Regression trees; Seismic event; Tsunami early-warning systems; Tsunami forecasting; Tsunami waves; Wave forecasting; Tsunamis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
216;Spatial heterogeneities of residents' sentiments and their associations with urban functional areas during heat waves– a case study in Beijing;The intensification of global heat wave events is seriously affecting residents' emotional health. Based on social media big data, our research explored the spatial pattern of residents' sentiments during heat waves (SDHW). Besides, their association with urban functional areas (UFAs) was analyzed using the Apriori algorithm of association rule mining. It was found that SDHW in Beijing were characterized by obvious spatial clustering, with hot spots predominately dispersed in urban areas and far suburbs, and cold spots mainly clustered in near suburbs. As for the associations with urban function areas, green space and park areas had significant effects on the positive sentiment in the study area, while a higher percentage of industrial areas had a greater impact on negative SDHW. When it comes to combined UFAs, our results revealed that the green space and park area combined with other functional areas was more closely related to positive SDHW, indicating the significance of promoting positive sentiment. Subdistricts with a lower percentage of residential and traffic areas may have a more negative sentiment. There were two main combined UFAs that have greater impacts on SDHW: the combination of residential and industrial areas, and the combination of residential and public areas. This study contributes to the understanding of improving community planning and governance when heat waves increase, building healthy cities, and enhancing urban emergency management. © The Author(s) 2024.;"Association rules analysis; Beijing; Geographical big data; Sentiment analysis; Urban functional area";"Association rules; Data mining; Housing; Parks; Risk management; Sentiment analysis; Urban planning; Association rule analysis; Beijing; Functional areas; Geographical big data; Green spaces; Heatwaves; Industrial area; Negative sentiments; Sentiment analysis; Urban functional area; Big data";"Spatial heterogeneities of residents' sentiments and their associations with urban functional areas during heat waves– a case study in Beijing The intensification of global heat wave events is seriously affecting residents' emotional health. Based on social media big data, our research explored the spatial pattern of residents' sentiments during heat waves (SDHW). Besides, their association with urban functional areas (UFAs) was analyzed using the Apriori algorithm of association rule mining. It was found that SDHW in Beijing were characterized by obvious spatial clustering, with hot spots predominately dispersed in urban areas and far suburbs, and cold spots mainly clustered in near suburbs. As for the associations with urban function areas, green space and park areas had significant effects on the positive sentiment in the study area, while a higher percentage of industrial areas had a greater impact on negative SDHW. When it comes to combined UFAs, our results revealed that the green space and park area combined with other functional areas was more closely related to positive SDHW, indicating the significance of promoting positive sentiment. Subdistricts with a lower percentage of residential and traffic areas may have a more negative sentiment. There were two main combined UFAs that have greater impacts on SDHW: the combination of residential and industrial areas, and the combination of residential and public areas. This study contributes to the understanding of improving community planning and governance when heat waves increase, building healthy cities, and enhancing urban emergency management. © The Author(s) 2024. Association rules analysis; Beijing; Geographical big data; Sentiment analysis; Urban functional area Association rules; Data mining; Housing; Parks; Risk management; Sentiment analysis; Urban planning; Association rule analysis; Beijing; Functional areas; Geographical big data; Green spaces; Heatwaves; Industrial area; Negative sentiments; Sentiment analysis; Urban functional area; Big data";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.3;Meteorological;1;Prevention
217;Reconstruction of displacement responses of a supertall building during typhoons based on limited field measurements and a physics-informed machine learning model;Severe weather conditions during windstorms may result in unavailability of traditional displacement monitoring techniques for civil structures such as supertall buildings. To address this challenge, this paper develops a long short-term memory model with a physics-informed loss function to initially estimate the missing strain responses of structures during typhoons. Subsequently, the missing or unmeasured displacements of structures during typhoons are reconstructed using the estimated missing strain responses and limited field measurements (i.e., acceleration and strain responses), employing a displacement reconstruction method. The proposed methodology is validated using field measurements on a 600 m supertall building during Typhoon Lionrock, demonstrating the effectiveness in accurately reconstructing the missing displacements of the supertall building under typhoon conditions. Finally, the missing displacements of the supertall building during Super Typhoon Saola are reconstructed, and the accuracy of the reconstructed displacements is verified. This paper aims to offer a novel method for displacement reconstructions of supertall buildings during windstorms based on limited monitoring information, enabling real-time structural integrity monitoring while reducing maintenance costs and downtime. © 2024 Author(s).;NULL;"Buildings; Displacement measurement; Hurricanes; Maintenance; Storms; Condition; Displacement monitoring; Displacement reconstruction; Displacement response; Field measurement; Machine learning models; Monitoring techniques; Severe weather; Strain response; Super tall buildings; Machine learning";"Reconstruction of displacement responses of a supertall building during typhoons based on limited field measurements and a physics-informed machine learning model Severe weather conditions during windstorms may result in unavailability of traditional displacement monitoring techniques for civil structures such as supertall buildings. To address this challenge, this paper develops a long short-term memory model with a physics-informed loss function to initially estimate the missing strain responses of structures during typhoons. Subsequently, the missing or unmeasured displacements of structures during typhoons are reconstructed using the estimated missing strain responses and limited field measurements (i.e., acceleration and strain responses), employing a displacement reconstruction method. The proposed methodology is validated using field measurements on a 600 m supertall building during Typhoon Lionrock, demonstrating the effectiveness in accurately reconstructing the missing displacements of the supertall building under typhoon conditions. Finally, the missing displacements of the supertall building during Super Typhoon Saola are reconstructed, and the accuracy of the reconstructed displacements is verified. This paper aims to offer a novel method for displacement reconstructions of supertall buildings during windstorms based on limited monitoring information, enabling real-time structural integrity monitoring while reducing maintenance costs and downtime. © 2024 Author(s). NULL Buildings; Displacement measurement; Hurricanes; Maintenance; Storms; Condition; Displacement monitoring; Displacement reconstruction; Displacement response; Field measurement; Machine learning models; Monitoring techniques; Severe weather; Strain response; Super tall buildings; Machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
218;Classification of Non-Seismic Tsunami Early Warning Level Using Decision Tree Algorithm;Background: Tsunami caused by volcanic collapse are categorized as non-seismic uncommon events, unlike tsunamis caused by earthquakes, which are common events. The traditional tsunami early warning based on the seismic sensor (e.g. earthquake detectors) may not be applicable to volcanic tsunamis because they do not generate seismic waves. Consequently, these tsunamis cannot be detected in advance, and warnings cannot be issued. New methods should be explored to address these non-seismic tsunamis caused by volcanic collapse. Objective: This study explored the potential of machine learning algorithms in supporting early warning level issuing for non-seismic tsunamis, specifically volcanic tsunamis. The Anak Krakatau volcano event in Indonesia was used as a case study. Methods: This study generated a database of 160 collapse scenarios using numerical simulation as input sequences. A classification model was constructed by defining the worst tsunami elevation and its arrival time at the coast. The database was supervised by labeling the warning levels as targets. Subsequently, a decision tree algorithm was employed to classify the warning levels. Results: The results demonstrated that the classification model performs very well for the Major Tsunami, Minor Tsunami, and Tsunami classes, achieving high precision, recall, and F1-Score with very high accuracy of 98%. However, the macro average indicates uneven performance across classes, as there are instances of ‘No Warning’ in some coastal gauges. Conclusion: To improve the model performance in the ‘No Warning’ class, it is necessary to balance the dataset by including more ‘No Warning’ scenarios, which can be achieved by simulating additional scenarios involving very small-volume collapse. Additionally, exploring additional collapse parameters such as dip angle and outlier volume could contribute to developing a more robust classification model. © 2024 The Authors.;"Classification; Decision Tree; Early Warning; Machine Learning; Volcanic Tsunamis";NULL;"Classification of Non-Seismic Tsunami Early Warning Level Using Decision Tree Algorithm Background: Tsunami caused by volcanic collapse are categorized as non-seismic uncommon events, unlike tsunamis caused by earthquakes, which are common events. The traditional tsunami early warning based on the seismic sensor (e.g. earthquake detectors) may not be applicable to volcanic tsunamis because they do not generate seismic waves. Consequently, these tsunamis cannot be detected in advance, and warnings cannot be issued. New methods should be explored to address these non-seismic tsunamis caused by volcanic collapse. Objective: This study explored the potential of machine learning algorithms in supporting early warning level issuing for non-seismic tsunamis, specifically volcanic tsunamis. The Anak Krakatau volcano event in Indonesia was used as a case study. Methods: This study generated a database of 160 collapse scenarios using numerical simulation as input sequences. A classification model was constructed by defining the worst tsunami elevation and its arrival time at the coast. The database was supervised by labeling the warning levels as targets. Subsequently, a decision tree algorithm was employed to classify the warning levels. Results: The results demonstrated that the classification model performs very well for the Major Tsunami, Minor Tsunami, and Tsunami classes, achieving high precision, recall, and F1-Score with very high accuracy of 98%. However, the macro average indicates uneven performance across classes, as there are instances of ‘No Warning’ in some coastal gauges. Conclusion: To improve the model performance in the ‘No Warning’ class, it is necessary to balance the dataset by including more ‘No Warning’ scenarios, which can be achieved by simulating additional scenarios involving very small-volume collapse. Additionally, exploring additional collapse parameters such as dip angle and outlier volume could contribute to developing a more robust classification model. © 2024 The Authors. Classification; Decision Tree; Early Warning; Machine Learning; Volcanic Tsunamis NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
219;A hybrid SARIMA-Prophet model for predicting historical streamflow time-series of the Sobat River in South Sudan;"Accurate river streamflow forecasting is pivotal for effective water resource planning, infrastructure design, utilization, optimization, and flood planning and warning. Streamflow prediction remains a difficult task due to several factors such as climate change, topography, and lack of observed data in some cases. This paper investigates and evaluates the individual performances of the seasonal auto-regressive integrated moving average (SARIMA) and Prophet models in forecasting the streamflow of the Sobat River and proposes a hybrid SARIMA-Prophet model to leverage the strengths of both approaches. Using the augmented Dickey-Fuller (ADF) and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) tests, the flow of the Sobat River was found to be stationary. The performance of the models was then assessed based on their residual errors and predictive accuracy using the mean absolute error (MAE), root mean squared error (RMSE), and coefficient of determination (R2). Residual analysis and prediction capabilities revealed that Prophet slightly edged SARIMA in terms of prediction efficacy; however, both models struggled to effectively capture extreme values, resulting in significant overestimations and slight underestimations. The hybrid SARIMA-Prophet model significantly reduced residual variability, achieving a lower MAE of 4.047 m3/s, RMSE of 6.17 m3/s, and a higher R2 of 0.92 than did the SARIMA (MAE: 5.39 m3/s, RMSE: 8.70 m3/s, R2: 0.85) and Prophet (MAE: 5.35 m3/s, RMSE: 8.32 m3/s, and R2: 0.86) models. This indicates that the hybrid model handles both long-term patterns and short-term fluctuations more effectively than the individual models. The findings of the present study highlight the potential of hybrid SARIMA-Prophet models for streamflow forecasting in terms of accuracy and reliability, thus contributing to more effective water resource management and planning, particularly in the Sobat River. © The Author(s) 2024.";"Hydrological time-series prediction; Machine learning prediction; Sobat River; South Sudan; Traditional time-series prediction";"Resource allocation; Rivers; Time series; Water management; Weather forecasting; Autoregressive integrated moving average(ARIMA); Hydrological time-series; Hydrological time-series prediction; Machine learning prediction; Machine-learning; Sobat river; South sudan; Time series prediction; Traditional time-series prediction; Prediction models";"A hybrid SARIMA-Prophet model for predicting historical streamflow time-series of the Sobat River in South Sudan Accurate river streamflow forecasting is pivotal for effective water resource planning, infrastructure design, utilization, optimization, and flood planning and warning. Streamflow prediction remains a difficult task due to several factors such as climate change, topography, and lack of observed data in some cases. This paper investigates and evaluates the individual performances of the seasonal auto-regressive integrated moving average (SARIMA) and Prophet models in forecasting the streamflow of the Sobat River and proposes a hybrid SARIMA-Prophet model to leverage the strengths of both approaches. Using the augmented Dickey-Fuller (ADF) and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) tests, the flow of the Sobat River was found to be stationary. The performance of the models was then assessed based on their residual errors and predictive accuracy using the mean absolute error (MAE), root mean squared error (RMSE), and coefficient of determination (R2). Residual analysis and prediction capabilities revealed that Prophet slightly edged SARIMA in terms of prediction efficacy; however, both models struggled to effectively capture extreme values, resulting in significant overestimations and slight underestimations. The hybrid SARIMA-Prophet model significantly reduced residual variability, achieving a lower MAE of 4.047 m3/s, RMSE of 6.17 m3/s, and a higher R2 of 0.92 than did the SARIMA (MAE: 5.39 m3/s, RMSE: 8.70 m3/s, R2: 0.85) and Prophet (MAE: 5.35 m3/s, RMSE: 8.32 m3/s, and R2: 0.86) models. This indicates that the hybrid model handles both long-term patterns and short-term fluctuations more effectively than the individual models. The findings of the present study highlight the potential of hybrid SARIMA-Prophet models for streamflow forecasting in terms of accuracy and reliability, thus contributing to more effective water resource management and planning, particularly in the Sobat River. © The Author(s) 2024. Hydrological time-series prediction; Machine learning prediction; Sobat River; South Sudan; Traditional time-series prediction Resource allocation; Rivers; Time series; Water management; Weather forecasting; Autoregressive integrated moving average(ARIMA); Hydrological time-series; Hydrological time-series prediction; Machine learning prediction; Machine-learning; Sobat river; South sudan; Time series prediction; Traditional time-series prediction; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
220;Reconstruction of tropical cyclone boundary layer wind field using physics-informed machine learning;A physics-informed machine learning model is proposed in this paper to reconstruct the high-fidelity three-dimensional boundary layer wind field of tropical cyclones. The governing equations of the wind field, which incorporate a spatially varying eddy diffusivity coefficient, are derived and embedded within the model's loss function. This integration allows the model to learn the underlying physics of the boundary layer wind field. The model is applied to reconstruct two tropical cyclone events in different oceanic basins. A wide range of observational data from satellite, dropsonde, and Doppler radar records are assimilated into the model. The model's performance is evaluated by comparing its results with observations and a classic linear model. The findings demonstrate that the model's accuracy improves with an increased amount of real data and the introduction of spatially varying eddy diffusivity. Furthermore, the proposed model does not require strict boundary conditions to reconstruct the wind field, offering greater flexibility compared to traditional numerical models. With the assimilation of observational data, the proposed model accurately reconstructs the horizontal, radial, and vertical distributions of the wind field. Compared with the linear model, the proposed model more effectively captures the nonlinearities and asymmetries of the wind field, thus presents more realistic outcomes. © 2024 Author(s).;NULL;"Hurricanes; Tropical engineering; Eddy Diffusivities; Governing equations; High-fidelity; Linear modeling; Machine learning models; Machine-learning; Observational data; Three-dimensional boundary layers; Tropical cyclone; Wind field; Tropical cyclone";"Reconstruction of tropical cyclone boundary layer wind field using physics-informed machine learning A physics-informed machine learning model is proposed in this paper to reconstruct the high-fidelity three-dimensional boundary layer wind field of tropical cyclones. The governing equations of the wind field, which incorporate a spatially varying eddy diffusivity coefficient, are derived and embedded within the model's loss function. This integration allows the model to learn the underlying physics of the boundary layer wind field. The model is applied to reconstruct two tropical cyclone events in different oceanic basins. A wide range of observational data from satellite, dropsonde, and Doppler radar records are assimilated into the model. The model's performance is evaluated by comparing its results with observations and a classic linear model. The findings demonstrate that the model's accuracy improves with an increased amount of real data and the introduction of spatially varying eddy diffusivity. Furthermore, the proposed model does not require strict boundary conditions to reconstruct the wind field, offering greater flexibility compared to traditional numerical models. With the assimilation of observational data, the proposed model accurately reconstructs the horizontal, radial, and vertical distributions of the wind field. Compared with the linear model, the proposed model more effectively captures the nonlinearities and asymmetries of the wind field, thus presents more realistic outcomes. © 2024 Author(s). NULL Hurricanes; Tropical engineering; Eddy Diffusivities; Governing equations; High-fidelity; Linear modeling; Machine learning models; Machine-learning; Observational data; Three-dimensional boundary layers; Tropical cyclone; Wind field; Tropical cyclone";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
221;Situational-aware multi-graph convolutional recurrent network (SA-MGCRN) for travel demand forecasting during wildfires;Natural hazards, such as wildfires, pose a significant threat to communities worldwide. Real-time forecasting of travel demand during wildfire evacuations is crucial for emergency managers and transportation planners to make timely and better-informed decisions. However, few studies focus on accurate travel demand forecasting in large-scale emergency evacuations. To tackle this research gap, the study develops a new methodological framework for modeling highly granular spatiotemporal trip generation in wildfire evacuations by using (a) large-scale GPS data generated by mobile devices and (b) state-of-the-art AI technologies. Based on the travel demand inferred from the GPS data, we develop a new deep learning model, i.e., Situational-Aware Multi-Graph Convolutional Recurrent Network (SA-MGCRN), along with a model updating scheme to achieve real-time forecasting of travel demand during wildfire evacuations. The proposed methodological framework is tested using a real-world case study: the 2019 Kincade Fire in Sonoma County, CA. The results show that SA-MGCRN significantly outperforms all the selected state-of-the-art benchmarks in terms of prediction performance. Our finding suggests that the most important model components of SA-MGCRN are weekend indicator, population change, evacuation order/warning information, and proximity to fire, which are consistent with behavioral theories and empirical findings. SA-MGCRN can be directly used in future wildfire events to assist real-time decision-making and emergency management. © 2024 The Author(s);"AI; GPS data; Real-time; Travel demand forecasting; Wildfire evacuation";"California; Sonoma County; United States; Benchmarking; Premixed flames; Risk management; GPS data; Methodological frameworks; Natural hazard; Real- time; Real-time forecasting; Recurrent networks; State of the art; Travel demand; Travel demand forecasting; Wildfire evacuation; artificial intelligence; fire management; GPS; travel demand; wildfire; Decision making";"Situational-aware multi-graph convolutional recurrent network (SA-MGCRN) for travel demand forecasting during wildfires Natural hazards, such as wildfires, pose a significant threat to communities worldwide. Real-time forecasting of travel demand during wildfire evacuations is crucial for emergency managers and transportation planners to make timely and better-informed decisions. However, few studies focus on accurate travel demand forecasting in large-scale emergency evacuations. To tackle this research gap, the study develops a new methodological framework for modeling highly granular spatiotemporal trip generation in wildfire evacuations by using (a) large-scale GPS data generated by mobile devices and (b) state-of-the-art AI technologies. Based on the travel demand inferred from the GPS data, we develop a new deep learning model, i.e., Situational-Aware Multi-Graph Convolutional Recurrent Network (SA-MGCRN), along with a model updating scheme to achieve real-time forecasting of travel demand during wildfire evacuations. The proposed methodological framework is tested using a real-world case study: the 2019 Kincade Fire in Sonoma County, CA. The results show that SA-MGCRN significantly outperforms all the selected state-of-the-art benchmarks in terms of prediction performance. Our finding suggests that the most important model components of SA-MGCRN are weekend indicator, population change, evacuation order/warning information, and proximity to fire, which are consistent with behavioral theories and empirical findings. SA-MGCRN can be directly used in future wildfire events to assist real-time decision-making and emergency management. © 2024 The Author(s) AI; GPS data; Real-time; Travel demand forecasting; Wildfire evacuation California; Sonoma County; United States; Benchmarking; Premixed flames; Risk management; GPS data; Methodological frameworks; Natural hazard; Real- time; Real-time forecasting; Recurrent networks; State of the art; Travel demand; Travel demand forecasting; Wildfire evacuation; artificial intelligence; fire management; GPS; travel demand; wildfire; Decision making";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;3;Response
222;Interdecadal Variations in Agricultural Drought Monitoring Using Land Surface Temperature and Vegetation Indices: A Case of the Amahlathi Local Municipality in South Africa;Agricultural droughts in South Africa, particularly in the Amahlathi Local Municipality (ALM), significantly impact socioeconomic activities, sustainable livelihoods, and ecosystem services, necessitating urgent attention to improved resilience and food security. The study assessed the interdecadal drought severity and duration in Amahlathi’s agricultural potential zone from 1989 to 2019 using various vegetation indicators. Landsat time series data were used to analyse the land surface temperature (LST), soil-adjusted vegetation index (SAVI), normalized difference vegetation index (NDVI), and standardized precipitation index (SPI). The study utilised GIS-based weighted overlay, multiple linear regression models, and Pearson’s correlation analysis to assess the correlations between LST, NDVI, SAVI, and SPI in response to the agricultural drought extent. The results reveal a consistent negative correlation between LST and NDVI in the ALM, with an increase in vegetation (R2 = 0.9889) and surface temperature. LST accuracy in dry areas increased to 55.8% in 2019, despite dense vegetation and a high average temperature of 40.12 °C, impacting water availability, agricultural land, and local ecosystems. The regression analysis shows a consistent negative correlation between LST and NDVI in the ALM from 1989 to 2019, with the correlation between vegetation and surface temperature increasing since 2019. The SAVI indicates a slight improvement in overall average vegetation health from 0.18 in 1989 to 0.25 in 2009, but a slight decrease to 0.21 in 2019. The SPI at 12 and 24 months indicates that drought severely impacted vegetation cover from 2014 to 2019, with notable recovery during improved wet periods in 1993, 2000, 2003, 2006, 2008, and 2013, possibly due to temporary drought relief. The findings can guide provincial drought monitoring and early warning programs, enhancing drought resilience, productivity, and sustainable livelihoods, especially in farming communities. © 2024 by the authors.;"agricultural drought monitoring; interdecadal variation; LST; NDVI; SAVI";"South Africa; drought; land surface; Landsat; NDVI; regression analysis; surface temperature; vegetation cover";"Interdecadal Variations in Agricultural Drought Monitoring Using Land Surface Temperature and Vegetation Indices: A Case of the Amahlathi Local Municipality in South Africa Agricultural droughts in South Africa, particularly in the Amahlathi Local Municipality (ALM), significantly impact socioeconomic activities, sustainable livelihoods, and ecosystem services, necessitating urgent attention to improved resilience and food security. The study assessed the interdecadal drought severity and duration in Amahlathi’s agricultural potential zone from 1989 to 2019 using various vegetation indicators. Landsat time series data were used to analyse the land surface temperature (LST), soil-adjusted vegetation index (SAVI), normalized difference vegetation index (NDVI), and standardized precipitation index (SPI). The study utilised GIS-based weighted overlay, multiple linear regression models, and Pearson’s correlation analysis to assess the correlations between LST, NDVI, SAVI, and SPI in response to the agricultural drought extent. The results reveal a consistent negative correlation between LST and NDVI in the ALM, with an increase in vegetation (R2 = 0.9889) and surface temperature. LST accuracy in dry areas increased to 55.8% in 2019, despite dense vegetation and a high average temperature of 40.12 °C, impacting water availability, agricultural land, and local ecosystems. The regression analysis shows a consistent negative correlation between LST and NDVI in the ALM from 1989 to 2019, with the correlation between vegetation and surface temperature increasing since 2019. The SAVI indicates a slight improvement in overall average vegetation health from 0.18 in 1989 to 0.25 in 2009, but a slight decrease to 0.21 in 2019. The SPI at 12 and 24 months indicates that drought severely impacted vegetation cover from 2014 to 2019, with notable recovery during improved wet periods in 1993, 2000, 2003, 2006, 2008, and 2013, possibly due to temporary drought relief. The findings can guide provincial drought monitoring and early warning programs, enhancing drought resilience, productivity, and sustainable livelihoods, especially in farming communities. © 2024 by the authors. agricultural drought monitoring; interdecadal variation; LST; NDVI; SAVI South Africa; drought; land surface; Landsat; NDVI; regression analysis; surface temperature; vegetation cover";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
223;Exploring AI Progress in GNSS Remote Sensing: A Deep Learning Based Framework for Real-Time Detection of Earthquake and Tsunami Induced Ionospheric Perturbations;Global Navigation Satellite System Ionospheric Seismology investigates the ionospheric response to earthquakes and tsunamis. These events are known to generate Traveling Ionospheric Disturbances (TIDs) that can be detected through GNSS-derived Total Electron Content (TEC) observations. Real-time TID identification provides a method for tsunami detection, improving tsunami early warning systems (TEWS) by extending coverage to open-ocean regions where buoy-based warning systems are impractical. Scalable and automated TID detection is, hence, essential for TEWS augmentation. In this work, we present an innovative approach to perform automatic real-time TID monitoring and detection, using deep learning insights. We utilize Gramian Angular Difference Fields (GADFs), a technique that transforms time-series into images, in combination with Convolutional Neural Networks (CNNs), starting from VARION (Variometric Approach for Real-time Ionosphere Observation) real-time TEC estimates. We select four tsunamigenic earthquakes that occurred in the Pacific Ocean: the 2010 Maule earthquake, the 2011 Tohoku earthquake, the 2012 Haida-Gwaii, the 2015 Illapel earthquake. The first three events are used for model training, whereas the out-of-sample validation is performed on the last one. The presented framework, being perfectly suitable for real-time applications, achieves 91.7% of F1 score and 84.6% of recall, highlighting its potential. Our approach to improve false positive detection, based on the likelihood of a TID at each time step, ensures robust and high performance as the system scales up, integrating more data for model training. This research lays the foundation for incorporating deep learning into real-time GNSS-TEC analysis, offering a joint and substantial contribution to TEWS progression. © 2024. The Author(s).;"AI4 Geodesy; convolutional neural networks; deep learning; earthquake and tsunami-induced ionospheric disturbances; GNSS-total electron content observations; tsunami early warning systems (TEWS)";"Deep neural networks; Earthquakes; Geodesy; Geodetic satellites; Global positioning system; Image enhancement; Intelligent systems; Ionosphere; Ionospheric electromagnetic wave propagation; Ionospheric measurement; Tropics; AI4 geodesy; Convolutional neural network; Deep learning; Earthquake and tsunami-induced ionospheric disturbance; Earthquake and tsunamis; GNSS-total electron content observation; Ionospheric disturbance; Total electron content; Tsunami early warning system; Tsunami early-warning systems; Convolutional neural networks";"Exploring AI Progress in GNSS Remote Sensing: A Deep Learning Based Framework for Real-Time Detection of Earthquake and Tsunami Induced Ionospheric Perturbations Global Navigation Satellite System Ionospheric Seismology investigates the ionospheric response to earthquakes and tsunamis. These events are known to generate Traveling Ionospheric Disturbances (TIDs) that can be detected through GNSS-derived Total Electron Content (TEC) observations. Real-time TID identification provides a method for tsunami detection, improving tsunami early warning systems (TEWS) by extending coverage to open-ocean regions where buoy-based warning systems are impractical. Scalable and automated TID detection is, hence, essential for TEWS augmentation. In this work, we present an innovative approach to perform automatic real-time TID monitoring and detection, using deep learning insights. We utilize Gramian Angular Difference Fields (GADFs), a technique that transforms time-series into images, in combination with Convolutional Neural Networks (CNNs), starting from VARION (Variometric Approach for Real-time Ionosphere Observation) real-time TEC estimates. We select four tsunamigenic earthquakes that occurred in the Pacific Ocean: the 2010 Maule earthquake, the 2011 Tohoku earthquake, the 2012 Haida-Gwaii, the 2015 Illapel earthquake. The first three events are used for model training, whereas the out-of-sample validation is performed on the last one. The presented framework, being perfectly suitable for real-time applications, achieves 91.7% of F1 score and 84.6% of recall, highlighting its potential. Our approach to improve false positive detection, based on the likelihood of a TID at each time step, ensures robust and high performance as the system scales up, integrating more data for model training. This research lays the foundation for incorporating deep learning into real-time GNSS-TEC analysis, offering a joint and substantial contribution to TEWS progression. © 2024. The Author(s). AI4 Geodesy; convolutional neural networks; deep learning; earthquake and tsunami-induced ionospheric disturbances; GNSS-total electron content observations; tsunami early warning systems (TEWS) Deep neural networks; Earthquakes; Geodesy; Geodetic satellites; Global positioning system; Image enhancement; Intelligent systems; Ionosphere; Ionospheric electromagnetic wave propagation; Ionospheric measurement; Tropics; AI4 geodesy; Convolutional neural network; Deep learning; Earthquake and tsunami-induced ionospheric disturbance; Earthquake and tsunamis; GNSS-total electron content observation; Ionospheric disturbance; Total electron content; Tsunami early warning system; Tsunami early-warning systems; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
224;Analysis of TEC variations and prediction of TEC by RNN during Indonesian earthquakes occurred from 2004 to 2024 and comparison with IRI-2020 model;The Ionosphere, a crucial region of Earth's atmosphere extending from approximately 50 to 1000 km above the Earth's surface, plays a pivotal role in global communication, navigation, and satellite-based technologies. Among its various parameters, Total Electron Content (TEC) stands out as a key metric, representing the integrated electron density along a specific path through the ionosphere. This research focuses on analyzing TEC variations during Indonesian earthquakes between 2004 and 2024 using a Recurrent Neural Network (RNN) model. The study investigates the ionospheric response to seismic activity, particularly in Indonesia, a region prone to earthquakes due to its location within the Pacific Ring of Fire and complex tectonic setting involving multiple lithospheric plates. The RNN model, designed to predict TEC variations during earthquakes, utilizes data on solar and geomagnetic activity, including the Kp index, solar wind, and geomagnetic storm activity, obtained from the OMNIWEB data centre, along with TEC data from the IONOLAB data servers for the BAKO station in Cibinong, Indonesia. The earthquakes considered for observation include significant events such as the 2004 Indian Ocean earthquake and tsunami, the 2012 Wharton Basin earthquake, and subsequent seismic events up to 2024. Statistical metrics such as Mean Bias Deviation (MBD), Relative Error (REL_E), Absolute Error (ABS_E), and Root Mean Square Error (RMSE) are used to evaluate the predictive capability of the RNN model and compare it with the International Reference Ionosphere (IRI) 2020 model, a widely accepted empirical model for describing ionospheric parameters. The comparison reveals the performance of the RNN model in capturing TEC variations during seismic events, providing valuable insights for earthquake monitoring and early warning systems. © 2024 COSPAR;"Earthquake; GPS; IRI-2020; RNN; TEC";"Earthquakes; Errors; Geomagnetism; Ionosphere; Ionospheric measurement; Mean square error; Content variation; Earth's surface; Earth: atmosphere; Global navigation; Indonesia; International reference ionosphere-2020; International reference ionospheres; Recurrent neural network model; Seismic event; Total electron content; Recurrent neural networks";"Analysis of TEC variations and prediction of TEC by RNN during Indonesian earthquakes occurred from 2004 to 2024 and comparison with IRI-2020 model The Ionosphere, a crucial region of Earth's atmosphere extending from approximately 50 to 1000 km above the Earth's surface, plays a pivotal role in global communication, navigation, and satellite-based technologies. Among its various parameters, Total Electron Content (TEC) stands out as a key metric, representing the integrated electron density along a specific path through the ionosphere. This research focuses on analyzing TEC variations during Indonesian earthquakes between 2004 and 2024 using a Recurrent Neural Network (RNN) model. The study investigates the ionospheric response to seismic activity, particularly in Indonesia, a region prone to earthquakes due to its location within the Pacific Ring of Fire and complex tectonic setting involving multiple lithospheric plates. The RNN model, designed to predict TEC variations during earthquakes, utilizes data on solar and geomagnetic activity, including the Kp index, solar wind, and geomagnetic storm activity, obtained from the OMNIWEB data centre, along with TEC data from the IONOLAB data servers for the BAKO station in Cibinong, Indonesia. The earthquakes considered for observation include significant events such as the 2004 Indian Ocean earthquake and tsunami, the 2012 Wharton Basin earthquake, and subsequent seismic events up to 2024. Statistical metrics such as Mean Bias Deviation (MBD), Relative Error (REL_E), Absolute Error (ABS_E), and Root Mean Square Error (RMSE) are used to evaluate the predictive capability of the RNN model and compare it with the International Reference Ionosphere (IRI) 2020 model, a widely accepted empirical model for describing ionospheric parameters. The comparison reveals the performance of the RNN model in capturing TEC variations during seismic events, providing valuable insights for earthquake monitoring and early warning systems. © 2024 COSPAR Earthquake; GPS; IRI-2020; RNN; TEC Earthquakes; Errors; Geomagnetism; Ionosphere; Ionospheric measurement; Mean square error; Content variation; Earth's surface; Earth: atmosphere; Global navigation; Indonesia; International reference ionosphere-2020; International reference ionospheres; Recurrent neural network model; Seismic event; Total electron content; Recurrent neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
225;Hybrid Multivariate Machine Learning Models for Streamflow Forecasting: A Two-Stage Decomposition-Reconstruction Framework;Robust and accurate streamflow forecasting holds significant importance for flood mitigation, drought warning and water resource management. On account of the intricate nonlinear and nonstationary nature of streamflow time series, numerous decomposition-based approaches have been proposed and integrated with other architectures. However, directly decomposing the entire streamflow data set introduces future information into the decomposition and reconstruction processes, while decomposing calibration and validation sets independently can result in undesired boundary effects. Besides, the signal decomposition techniques tend to generate a large number of decomposed modes. Using all these modes directly as input variables results in intricate forecasting models and is prone to overfitting. To address these challenges, we developed a novel two-stage decomposition reconstruction forecasting (TSDRF) framework by coupling sequentially decomposition technique, sample entropy and multivariate machine learning methods in this study. This newly proposed TSDRF framework is assessed at three hydrologic stations from Yellow River, China. Furthermore, the TSDRF framework is also compared with the two-stage decomposition reconstruction hindcasting (TSDRH) framework under different lead times. The findings suggest that TSDRF framework based on variation mode decomposition (VMD) algorithm outperform other models in terms of mitigating boundary effects, minimizing computational costs, and enhancing generalization capabilities across various lead times. © 2024 American Society of Civil Engineers.;"Boundary effects; Daily streamflow forecasting; Decomposition algorithm; Machine learning; Sample entropy; Two-stage decomposition reconstruction forecasting (TSDRF) framework";"China; Yellow River; Entropy; Machine learning; Mode decomposition; Stream flow; Water management; Boundary effects; Daily streamflow forecasting; Decomposition algorithm; Decomposition/reconstruction; Leadtime; Machine learning models; Machine-learning; Sample entropy; Streamflow forecasting; Two-stage decomposition reconstruction forecasting  framework; algorithm; decomposition analysis; flow modeling; forecasting method; machine learning; multivariate analysis; streamflow; Forecasting";"Hybrid Multivariate Machine Learning Models for Streamflow Forecasting: A Two-Stage Decomposition-Reconstruction Framework Robust and accurate streamflow forecasting holds significant importance for flood mitigation, drought warning and water resource management. On account of the intricate nonlinear and nonstationary nature of streamflow time series, numerous decomposition-based approaches have been proposed and integrated with other architectures. However, directly decomposing the entire streamflow data set introduces future information into the decomposition and reconstruction processes, while decomposing calibration and validation sets independently can result in undesired boundary effects. Besides, the signal decomposition techniques tend to generate a large number of decomposed modes. Using all these modes directly as input variables results in intricate forecasting models and is prone to overfitting. To address these challenges, we developed a novel two-stage decomposition reconstruction forecasting (TSDRF) framework by coupling sequentially decomposition technique, sample entropy and multivariate machine learning methods in this study. This newly proposed TSDRF framework is assessed at three hydrologic stations from Yellow River, China. Furthermore, the TSDRF framework is also compared with the two-stage decomposition reconstruction hindcasting (TSDRH) framework under different lead times. The findings suggest that TSDRF framework based on variation mode decomposition (VMD) algorithm outperform other models in terms of mitigating boundary effects, minimizing computational costs, and enhancing generalization capabilities across various lead times. © 2024 American Society of Civil Engineers. Boundary effects; Daily streamflow forecasting; Decomposition algorithm; Machine learning; Sample entropy; Two-stage decomposition reconstruction forecasting (TSDRF) framework China; Yellow River; Entropy; Machine learning; Mode decomposition; Stream flow; Water management; Boundary effects; Daily streamflow forecasting; Decomposition algorithm; Decomposition/reconstruction; Leadtime; Machine learning models; Machine-learning; Sample entropy; Streamflow forecasting; Two-stage decomposition reconstruction forecasting  framework; algorithm; decomposition analysis; flow modeling; forecasting method; machine learning; multivariate analysis; streamflow; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
226;Transcriptional Profiling Analysis Providing Insights into the Harsh Environments Tolerance Mechanisms of Krascheninnikovia arborescens;Krascheninnikovia arborescens, an endemic shrub in China, thrives in desertification-prone environments due to its robust biomass, hairy leaves, and extensive root system. It is vital for ecological restoration and serves as a valuable forage plant. This study explored the molecular mechanisms underlying K. arborescens’ adaptation to desert conditions, focusing on its physiological, biochemical, and transcriptomic responses to drought, salt, and alkali stresses. The results revealed that the three stresses have significant impacts on the photosynthetic, antioxidant, and ion balance systems of the plants, with the alkali stress inducing the most pronounced changes and differential gene expression. The clustering and functional enrichment analyses of differentially expressed genes (DEGs) highlighted the enrichment of the induced genes in pathways related to plant hormone signaling, phenylpropanoid biosynthesis, and transcription factors following stress treatments. In these pathways, the synthesis and signal transduction of abscisic acid (ABA) and ethylene, as well as the flavonoid and lignin synthesis pathways, and transcription factors such as MYB, AP2/ERF, bHLH, NAC, and WRKY responded actively to the stress and played pivotal roles. Through the WGCNA analysis, 10 key modules were identified, with the yellow module demonstrating a high correlation with the ABA and anthocyanin contents, while the turquoise module was enriched in the majority of genes related to hormone and phenylpropanoid pathways. The analysis of hub genes in these modules highlighted the significant roles of the bHLH and MYB transcription factors. These findings could offer new insights into the molecular mechanisms that enable the adaptation of K. arborescens to desert environments, enhancing our understanding of how other desert plants adapt to harsh conditions. These insights are crucial for exploring and utilizing high-quality forage plant germplasm resources and ecological development, with the identified candidate genes serving as valuable targets for further research on stress-resistant genes. © 2024 by the authors.;"abiotic stress; Krascheninnikovia arborescens; phenylpropane metabolism; plant hormones; RNA-Seq; transcription factor; WGCNA";"Adaptation, Physiological; Droughts; Gene Expression Profiling; Gene Expression Regulation, Plant; Plant Proteins; Stress, Physiological; Transcription Factors; Transcriptome; abscisic acid; anthocyanin; ascorbate peroxidase; catalase; ethylene; ethylene responsive factor; flavonoid; lignin; malonaldehyde; phytohormone; plant protein; potassium ion; protein Myb; sodium ion; superoxide dismutase; transcription factor; transcription factor AP 2; transcription factor bHLH; transcription factor NAC; transcription factor WRKY; unclassified drug; plant protein; transcription factor; transcriptome; abiotic stress; acid base balance; alkali stress; Amaranthaceae; antioxidant activity; Article; biochemical analysis; biosynthesis; controlled study; correlational study; desert; desertification; differential gene expression; drought stress; environmental impact; environmental stress; evolutionary adaptation; forage crop; functional enrichment analysis; gene expression level; gene expression profiling; genetic transcription; germplasm; hierarchical clustering; Krascheninnikovia arborescens; nonhuman; photosynthesis; plant gene; plant genetics; plant physiology; plant response; plant stress; plant structures; plant tolerance; salt stress; signal transduction; transcriptome sequencing; transcriptomics; weighted gene co expression network analysis; drought; gene expression regulation; genetics; metabolism; physiological adaptation; physiological stress";"Transcriptional Profiling Analysis Providing Insights into the Harsh Environments Tolerance Mechanisms of Krascheninnikovia arborescens Krascheninnikovia arborescens, an endemic shrub in China, thrives in desertification-prone environments due to its robust biomass, hairy leaves, and extensive root system. It is vital for ecological restoration and serves as a valuable forage plant. This study explored the molecular mechanisms underlying K. arborescens’ adaptation to desert conditions, focusing on its physiological, biochemical, and transcriptomic responses to drought, salt, and alkali stresses. The results revealed that the three stresses have significant impacts on the photosynthetic, antioxidant, and ion balance systems of the plants, with the alkali stress inducing the most pronounced changes and differential gene expression. The clustering and functional enrichment analyses of differentially expressed genes (DEGs) highlighted the enrichment of the induced genes in pathways related to plant hormone signaling, phenylpropanoid biosynthesis, and transcription factors following stress treatments. In these pathways, the synthesis and signal transduction of abscisic acid (ABA) and ethylene, as well as the flavonoid and lignin synthesis pathways, and transcription factors such as MYB, AP2/ERF, bHLH, NAC, and WRKY responded actively to the stress and played pivotal roles. Through the WGCNA analysis, 10 key modules were identified, with the yellow module demonstrating a high correlation with the ABA and anthocyanin contents, while the turquoise module was enriched in the majority of genes related to hormone and phenylpropanoid pathways. The analysis of hub genes in these modules highlighted the significant roles of the bHLH and MYB transcription factors. These findings could offer new insights into the molecular mechanisms that enable the adaptation of K. arborescens to desert environments, enhancing our understanding of how other desert plants adapt to harsh conditions. These insights are crucial for exploring and utilizing high-quality forage plant germplasm resources and ecological development, with the identified candidate genes serving as valuable targets for further research on stress-resistant genes. © 2024 by the authors. abiotic stress; Krascheninnikovia arborescens; phenylpropane metabolism; plant hormones; RNA-Seq; transcription factor; WGCNA Adaptation, Physiological; Droughts; Gene Expression Profiling; Gene Expression Regulation, Plant; Plant Proteins; Stress, Physiological; Transcription Factors; Transcriptome; abscisic acid; anthocyanin; ascorbate peroxidase; catalase; ethylene; ethylene responsive factor; flavonoid; lignin; malonaldehyde; phytohormone; plant protein; potassium ion; protein Myb; sodium ion; superoxide dismutase; transcription factor; transcription factor AP 2; transcription factor bHLH; transcription factor NAC; transcription factor WRKY; unclassified drug; plant protein; transcription factor; transcriptome; abiotic stress; acid base balance; alkali stress; Amaranthaceae; antioxidant activity; Article; biochemical analysis; biosynthesis; controlled study; correlational study; desert; desertification; differential gene expression; drought stress; environmental impact; environmental stress; evolutionary adaptation; forage crop; functional enrichment analysis; gene expression level; gene expression profiling; genetic transcription; germplasm; hierarchical clustering; Krascheninnikovia arborescens; nonhuman; photosynthesis; plant gene; plant genetics; plant physiology; plant response; plant stress; plant structures; plant tolerance; salt stress; signal transduction; transcriptome sequencing; transcriptomics; weighted gene co expression network analysis; drought; gene expression regulation; genetics; metabolism; physiological adaptation; physiological stress";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.4;Climatological;1;Prevention
227;Spatial heterogeneity of ecosystem service bundles and the driving factors in the Beijing-Tianjin-Hebei region;Defining a multi-service spatial pattern and ecosystem packages is the key to ensuring regional ecological security and sustainable development. Ecosystem service bundles can effectively discern the interactions among multiple services and regionalize ecological functional zones. Aiming at a range of environmental and ecological challenges in the Beijing-Tianjin-Hebei region, such as water scarcity, soil erosion, land degradation, poor forest and grassland quality, and shrinking agricultural land, the spatial pattern and driving forces of eight crux ecosystem services, including product supply, soil conservation, water conservation, carbon storage, sand fixation, environment purification, biological diversity and recreation culture, were analyzed by combining multiple model algorithms. Additionally, spatial system clustering was employed to identify ecosystem services bundles and optimize the ecological functional zoning. The results revealed significant spatial heterogeneity of the eight ecosystem services in the Beijing-Tianjin-Hebei region. High product supply concentrated in plain regions, while the rest seven ecosystem services were dominant in in mountainous areas. The region can be classified into four service bundles: ecological conservation service (ECS), ecological restoration service (ERS), product supply service (PSS) and ecological fragile service (EFS). ECS and ERS are dominated by regulating and supporting services, which are primarily driven by climatic factors and NDVI. ERS exhibits relatively low service level and is the crucial area for future ecological restoration. PSS is dominated by food supply, with land use change being the primary driving force. At present, with the rapid urbanization, various ecosystem services provided by EFS is inadequate. In the future, urban ecological construction should be strengthened to enhance regional ecosystem services capabilities. © 2024;"Driving force; Ecosystem services; Ecosystem services bundles; The beijing-tianjin-hebei region";"Anthropogenic; Biotic; Carbon sequestration; Forest ecology; Beijing-tianjin-hebei regions; Driving forces; Ecological restoration; Ecosystem service bundle; Ecosystem services; Restoration services; Service bundles; Spatial heterogeneity; The beijing-tianjin-hebei region; Abiotic";"Spatial heterogeneity of ecosystem service bundles and the driving factors in the Beijing-Tianjin-Hebei region Defining a multi-service spatial pattern and ecosystem packages is the key to ensuring regional ecological security and sustainable development. Ecosystem service bundles can effectively discern the interactions among multiple services and regionalize ecological functional zones. Aiming at a range of environmental and ecological challenges in the Beijing-Tianjin-Hebei region, such as water scarcity, soil erosion, land degradation, poor forest and grassland quality, and shrinking agricultural land, the spatial pattern and driving forces of eight crux ecosystem services, including product supply, soil conservation, water conservation, carbon storage, sand fixation, environment purification, biological diversity and recreation culture, were analyzed by combining multiple model algorithms. Additionally, spatial system clustering was employed to identify ecosystem services bundles and optimize the ecological functional zoning. The results revealed significant spatial heterogeneity of the eight ecosystem services in the Beijing-Tianjin-Hebei region. High product supply concentrated in plain regions, while the rest seven ecosystem services were dominant in in mountainous areas. The region can be classified into four service bundles: ecological conservation service (ECS), ecological restoration service (ERS), product supply service (PSS) and ecological fragile service (EFS). ECS and ERS are dominated by regulating and supporting services, which are primarily driven by climatic factors and NDVI. ERS exhibits relatively low service level and is the crucial area for future ecological restoration. PSS is dominated by food supply, with land use change being the primary driving force. At present, with the rapid urbanization, various ecosystem services provided by EFS is inadequate. In the future, urban ecological construction should be strengthened to enhance regional ecosystem services capabilities. © 2024 Driving force; Ecosystem services; Ecosystem services bundles; The beijing-tianjin-hebei region Anthropogenic; Biotic; Carbon sequestration; Forest ecology; Beijing-tianjin-hebei regions; Driving forces; Ecological restoration; Ecosystem service bundle; Ecosystem services; Restoration services; Service bundles; Spatial heterogeneity; The beijing-tianjin-hebei region; Abiotic";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
228;GeoAI for Natural Disaster Assessment;Climate change has led to a sharp increase in the number and severity of extreme events, such as floods, tornados and wildfires. These events have resulted in adverse effects on human lives and the infrastructure. Swift disaster assessment is crucial for the effective planning of disaster response and relief efforts. AI and big data have provided unprecedented opportunities to enable swift disaster assessment, but two significant hurdles exist: (1) the scarcity of annotated geospatial data to train AI models, and (2) the lack of AI solutions that encode physics knowledge in a geospatial context. My research aims to address both challenges by developing an active-learning-based annotation platform that improves the annotation productivity of geospatial data for geospatial machine learning, and by developing physics-guided machine learning models for accurate natural disaster assessment.  © 2024 Owner/Author.;"active learning; digital elevation model; flood extent mapping; parallelization";"Adversarial machine learning; Contrastive Learning; Floods; Active Learning; Adverse effect; Digital elevation model; Extent mappings; Extreme events; Flood extent mapping; Geo-spatial; Natural disasters; Parallelizations; Sharp increase; Active learning";"GeoAI for Natural Disaster Assessment Climate change has led to a sharp increase in the number and severity of extreme events, such as floods, tornados and wildfires. These events have resulted in adverse effects on human lives and the infrastructure. Swift disaster assessment is crucial for the effective planning of disaster response and relief efforts. AI and big data have provided unprecedented opportunities to enable swift disaster assessment, but two significant hurdles exist: (1) the scarcity of annotated geospatial data to train AI models, and (2) the lack of AI solutions that encode physics knowledge in a geospatial context. My research aims to address both challenges by developing an active-learning-based annotation platform that improves the annotation productivity of geospatial data for geospatial machine learning, and by developing physics-guided machine learning models for accurate natural disaster assessment.  © 2024 Owner/Author. active learning; digital elevation model; flood extent mapping; parallelization Adversarial machine learning; Contrastive Learning; Floods; Active Learning; Adverse effect; Digital elevation model; Extent mappings; Extreme events; Flood extent mapping; Geo-spatial; Natural disasters; Parallelizations; Sharp increase; Active learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
229;Probabilistic Seismic Hazard Assessment of the Southwestern Region of Saudi Arabia;In relation to its rapid infrastructure expansion, exemplified by projects like the Najran Valley Dam or the rehabilitation of agricultural terraces, Saudi Arabia stands out among the Arabian Gulf nations. To mitigate the earthquake-related risks effectively, it is imperative to conduct an exhaustive analysis of its natural hazards. The southwesternmost region of Saudi Arabia is the main subject area of this study for the probabilistic seismic hazard assessment (PSHA), which aims to identify the peak ground acceleration (PGA) and spectral acceleration (SA) values. The investigation encompasses a 10% and 5% probability of occurrence over a 50-year exposure time for both B/C and C NEHRP soils. In order to take into account the earthquake activity that takes place in the vicinity of the Red Sea Rift, which in fact may have an impact on the seismic hazard in this active tectonic region, different seismic source zones were especially designed for this evaluation. Various characteristics such as the uncertainties related to the b-value, the expected maximum magnitude, and different ground motion prediction equations (GMPEs) were integrated using a logic tree scheme. Additionally, regression relationships between the computed ground motion values were established, and a novel design response spectrum was developed and recommended for several cities. Regarding the key findings, it is significant to highlight that the seismic hazard decreases towards the northeast, when moving away from the Red Sea Rift, confirming anticipated trends where proximity to the rift corresponds to increased seismic hazard. Notably, cities such as Farasan Island, Jazan, Al Qunfundhah, Al Lith and Al Birk present the highest observed hazard values among all the cities analyzed. For these cities, the obtained maximum SA values for both 475 and 975 years under B/C site conditions are as follows: 0.268 g and 0.412 g, 0.121 g and 0.167 g, 0.099 g and 0.150 g, 0.083 g and 0.135 g, and 0.066 g and 0.118 g, respectively. These results emphasize the crucial necessity of adequately evaluating and thoroughly updating the seismic hazard inherent to these particular areas to enhance the risk reduction and disaster readiness initiatives. © 2024 by the authors.;"peak ground horizontal acceleration; Red Sea; Saudi Arabia; seismic hazard; spectral acceleration; uniform hazard spectrum";"Acceleration; Aluminum; Earthquake effects; Equations of motion; Motion estimation; Risk assessment; Seismic design; Seismic response; Arabian Gulf; Horizontal acceleration; Peak ground horizontal acceleration; Probabilistic seismic hazard assessment; Red sea; Related risk; Saudi Arabia; Seismic hazards; Spectral acceleration; Uniform hazard spectrums; Hazards";"Probabilistic Seismic Hazard Assessment of the Southwestern Region of Saudi Arabia In relation to its rapid infrastructure expansion, exemplified by projects like the Najran Valley Dam or the rehabilitation of agricultural terraces, Saudi Arabia stands out among the Arabian Gulf nations. To mitigate the earthquake-related risks effectively, it is imperative to conduct an exhaustive analysis of its natural hazards. The southwesternmost region of Saudi Arabia is the main subject area of this study for the probabilistic seismic hazard assessment (PSHA), which aims to identify the peak ground acceleration (PGA) and spectral acceleration (SA) values. The investigation encompasses a 10% and 5% probability of occurrence over a 50-year exposure time for both B/C and C NEHRP soils. In order to take into account the earthquake activity that takes place in the vicinity of the Red Sea Rift, which in fact may have an impact on the seismic hazard in this active tectonic region, different seismic source zones were especially designed for this evaluation. Various characteristics such as the uncertainties related to the b-value, the expected maximum magnitude, and different ground motion prediction equations (GMPEs) were integrated using a logic tree scheme. Additionally, regression relationships between the computed ground motion values were established, and a novel design response spectrum was developed and recommended for several cities. Regarding the key findings, it is significant to highlight that the seismic hazard decreases towards the northeast, when moving away from the Red Sea Rift, confirming anticipated trends where proximity to the rift corresponds to increased seismic hazard. Notably, cities such as Farasan Island, Jazan, Al Qunfundhah, Al Lith and Al Birk present the highest observed hazard values among all the cities analyzed. For these cities, the obtained maximum SA values for both 475 and 975 years under B/C site conditions are as follows: 0.268 g and 0.412 g, 0.121 g and 0.167 g, 0.099 g and 0.150 g, 0.083 g and 0.135 g, and 0.066 g and 0.118 g, respectively. These results emphasize the crucial necessity of adequately evaluating and thoroughly updating the seismic hazard inherent to these particular areas to enhance the risk reduction and disaster readiness initiatives. © 2024 by the authors. peak ground horizontal acceleration; Red Sea; Saudi Arabia; seismic hazard; spectral acceleration; uniform hazard spectrum Acceleration; Aluminum; Earthquake effects; Equations of motion; Motion estimation; Risk assessment; Seismic design; Seismic response; Arabian Gulf; Horizontal acceleration; Peak ground horizontal acceleration; Probabilistic seismic hazard assessment; Red sea; Related risk; Saudi Arabia; Seismic hazards; Spectral acceleration; Uniform hazard spectrums; Hazards";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
230;Integrating ecosystem stress into the assessment of ecosystem health in karst areas and exploring its driving factors;"Accurately evaluating the degree of ecosystem health and comprehending its drivers are of crucial importance for ecosystem management and restoration in fragile ecoregions. Adopting an ecosystem stress perspective, this investigation scientifically diagnosed ecosystem health in Guizhou Province, a typical karst fragile ecological area, by developing a “Vigor-Organization-Resilience-Stress” (VORS) regional ecosystem health assessment framework. The principal driving factors and their spatio-temporal heterogeneity influencing ecosystem health in karst regions were investigated using geographic detectors and geographically and temporally weighted regression. Our findings indicated that: (1) The ecosystem health in Guizhou province presents a spatial distribution pattern of lower in the western region and higher in the eastern region, and the average ecosystem health index (EHI) in 2000, 2010 and 2020 is 0.194, 0.246 and 0.254 respectively, indicating that the overall ecosystem health level in Guizhou province has improved with the passage of time. (2) The spatiotemporal variation of ecosystem stress in Guizhou Province is substantial; the pressure associated with land use is projected to increase, whereas the intensity of stress attributed to rocky desertification and soil erosion is anticipated to diminish over time. (3) The primary determinants influencing the health of ecosystems in karst environments are the social and economic elements. These factors progressively amplify their effect on ecosystem health as time passes. The new assessment framework we have established includes ecosystem integrity and ecological threats faced by the ecosystem, which provides a new perspective for understanding the complex internal characteristics of karst areas and the coupling relationship between human and natural ecosystems. The results of this study can provide scientific reference for land use and ecological management policies in fragile ecological areas. © 2024 The Author(s)";"Driving factors; Ecosystem health assessment; Ecosystem stress; Geographical detector model; Karst region";"China; Guizhou; Anthropogenic; Biotic; Detector modeling; Driving factors; Ecosystem health; Ecosystem health assessment; Ecosystem stress; Geographical detector model; Guizhou Province; Health assessments; Karst areas; Karst regions; ecoregion; ecosystem health; ecosystem management; heterogeneity; karst; land use planning; rocky desertification; soil erosion; spatial distribution; spatial variation; temporal variation; Abiotic";"Integrating ecosystem stress into the assessment of ecosystem health in karst areas and exploring its driving factors Accurately evaluating the degree of ecosystem health and comprehending its drivers are of crucial importance for ecosystem management and restoration in fragile ecoregions. Adopting an ecosystem stress perspective, this investigation scientifically diagnosed ecosystem health in Guizhou Province, a typical karst fragile ecological area, by developing a “Vigor-Organization-Resilience-Stress” (VORS) regional ecosystem health assessment framework. The principal driving factors and their spatio-temporal heterogeneity influencing ecosystem health in karst regions were investigated using geographic detectors and geographically and temporally weighted regression. Our findings indicated that: (1) The ecosystem health in Guizhou province presents a spatial distribution pattern of lower in the western region and higher in the eastern region, and the average ecosystem health index (EHI) in 2000, 2010 and 2020 is 0.194, 0.246 and 0.254 respectively, indicating that the overall ecosystem health level in Guizhou province has improved with the passage of time. (2) The spatiotemporal variation of ecosystem stress in Guizhou Province is substantial; the pressure associated with land use is projected to increase, whereas the intensity of stress attributed to rocky desertification and soil erosion is anticipated to diminish over time. (3) The primary determinants influencing the health of ecosystems in karst environments are the social and economic elements. These factors progressively amplify their effect on ecosystem health as time passes. The new assessment framework we have established includes ecosystem integrity and ecological threats faced by the ecosystem, which provides a new perspective for understanding the complex internal characteristics of karst areas and the coupling relationship between human and natural ecosystems. The results of this study can provide scientific reference for land use and ecological management policies in fragile ecological areas. © 2024 The Author(s) Driving factors; Ecosystem health assessment; Ecosystem stress; Geographical detector model; Karst region China; Guizhou; Anthropogenic; Biotic; Detector modeling; Driving factors; Ecosystem health; Ecosystem health assessment; Ecosystem stress; Geographical detector model; Guizhou Province; Health assessments; Karst areas; Karst regions; ecoregion; ecosystem health; ecosystem management; heterogeneity; karst; land use planning; rocky desertification; soil erosion; spatial distribution; spatial variation; temporal variation; Abiotic";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
231;Analyzing watershed system state through runoff complexity and driver interactions using multiscale entropy and deep learning;Quantifying watershed state is crucial for ecological management and sustainable development. Traditional methods, often based on multiple indicators and subjective weighting, struggle to objectively and comprehensively capture the inherent complexity of watershed ecosystems. Runoff, a key hydrological component, reflects watershed functioning, but most studies focus solely on runoff volume, limiting the ability to link runoff dynamics with surface conditions and broader system processes. This study introduced a novel approach that uses runoff complexity to represent watershed system-level state, bridging the gap between runoff dynamics and ecosystem functioning. The Hydrological Refined Composite Multiscale Entropy (Hydro_RCMFE) method was developed to quantify runoff complexity across various time scales. Combined with the Hydrological Complexity Transformer Model (HydroC_Trans) and Shapley Additive Explanations (SHAP), this method explored interactions between runoff complexity and influencing factors. It was applied to the Yanhe Watershed in China's Loess Plateau, known for severe soil erosion and large-scale ecological restoration. The results revealed significant fluctuations in runoff complexity after 2005. Vegetation cover from the Grain for Green Program enhanced self-organization, buffering climatic variability while introducing instability. Urbanization further amplified runoff complexity, while landscape factors, such as aggregation and hydrological connectivity, had spatially and temporally varied effects, highlighting the need for tailored management strategies. Upstream, efforts should enhance climate resilience, increase vegetation cover—particularly grasslands—and improve landscape aggregation. Midstream and downstream strategies should prioritize optimizing hydrological connectivity, limiting impervious surface expansion, and restoring ecological functions. © 2024 The Authors;"Influencing factors; Refined composite multiscale fuzzy entropy; Runoff complexity; Transformer; Watershed state";"China; Loess Plateau; Sustainable development; Vegetation; Fuzzy entropy; Hydrological connectivity; Influencing factor; Multiscale entropy; Refined composite multiscale fuzzy entropy; Runoff complexity; System state; Transformer; Vegetation cover; Watershed state; climate variation; entropy; grassland; hydrological modeling; machine learning; runoff; soil erosion; vegetation cover; watershed; Runoff";"Analyzing watershed system state through runoff complexity and driver interactions using multiscale entropy and deep learning Quantifying watershed state is crucial for ecological management and sustainable development. Traditional methods, often based on multiple indicators and subjective weighting, struggle to objectively and comprehensively capture the inherent complexity of watershed ecosystems. Runoff, a key hydrological component, reflects watershed functioning, but most studies focus solely on runoff volume, limiting the ability to link runoff dynamics with surface conditions and broader system processes. This study introduced a novel approach that uses runoff complexity to represent watershed system-level state, bridging the gap between runoff dynamics and ecosystem functioning. The Hydrological Refined Composite Multiscale Entropy (Hydro_RCMFE) method was developed to quantify runoff complexity across various time scales. Combined with the Hydrological Complexity Transformer Model (HydroC_Trans) and Shapley Additive Explanations (SHAP), this method explored interactions between runoff complexity and influencing factors. It was applied to the Yanhe Watershed in China's Loess Plateau, known for severe soil erosion and large-scale ecological restoration. The results revealed significant fluctuations in runoff complexity after 2005. Vegetation cover from the Grain for Green Program enhanced self-organization, buffering climatic variability while introducing instability. Urbanization further amplified runoff complexity, while landscape factors, such as aggregation and hydrological connectivity, had spatially and temporally varied effects, highlighting the need for tailored management strategies. Upstream, efforts should enhance climate resilience, increase vegetation cover—particularly grasslands—and improve landscape aggregation. Midstream and downstream strategies should prioritize optimizing hydrological connectivity, limiting impervious surface expansion, and restoring ecological functions. © 2024 The Authors Influencing factors; Refined composite multiscale fuzzy entropy; Runoff complexity; Transformer; Watershed state China; Loess Plateau; Sustainable development; Vegetation; Fuzzy entropy; Hydrological connectivity; Influencing factor; Multiscale entropy; Refined composite multiscale fuzzy entropy; Runoff complexity; System state; Transformer; Vegetation cover; Watershed state; climate variation; entropy; grassland; hydrological modeling; machine learning; runoff; soil erosion; vegetation cover; watershed; Runoff";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
232;Hierarchical and mixed uncertainty quantification for simulation-based structural seismic fragility analysis;Seismic fragility assessments (SFA) encompass significant uncertainties, including ground motions, materials, geometry, boundaries, and modeling, which impart inherent uncertainties to the fragility curve. Quantifying these uncertainties is essential for informed decision-making in earthquake disaster mitigation. This paper presents a hierarchical uncertainty quantification (UQ) framework for simulation-based SFA, which can address mixed aleatory and epistemic uncertainties. The framework comprises two levels: At Level 1, ground motion uncertainties are accounted for through record-to-record variation, while structural parameter uncertainties are captured using well-defined probability distributions. The quantification of uncertainty in seismic fragility is accomplished using mean and quantile fragility curves, representing the average outcome and the dispersion of structural fragility, respectively. Fuzziness is also introduced to enhance the objectivity of failure determination. Level 2 addresses uncertainties in the distribution hyper-parameters of structural variables through evidence theory, facilitating an understanding of their influence on the seismic fragility curve and quantifying uncertainty in mean and quantile fragility curves. This UQ framework builds upon the multivariate seismic fragility analysis method, integrating Kriging regression for seismic demand prediction and logistic regression for generating multivariate seismic fragility functions. The proposed framework is validated numerically on an existing three-span reinforced concrete continuous girder bridge, affirming its practical utility and reliability. © 2024 Elsevier Ltd;"Continuous girder bridge; Hierarchical; Mixed uncertainty quantification; Seismic fragility analysis; Surrogate model";"Decision making; Probability distributions; Reinforced concrete; Seismology; Structural analysis; Uncertainty analysis; Continuous girder bridge; Fragility assessment; Fragility curves; Hierarchical; Mixed uncertainty quantification; Seismic fragility; Seismic fragility analysis; Surrogate modeling; Uncertainty; Uncertainty quantifications; bridge; computer simulation; dynamic analysis; dynamic response; earthquake engineering; ground motion; hierarchical system; model test; model validation; numerical model; quantitative analysis; seismic design; stability analysis; structural analysis; structural response; uncertainty analysis; Kriging";"Hierarchical and mixed uncertainty quantification for simulation-based structural seismic fragility analysis Seismic fragility assessments (SFA) encompass significant uncertainties, including ground motions, materials, geometry, boundaries, and modeling, which impart inherent uncertainties to the fragility curve. Quantifying these uncertainties is essential for informed decision-making in earthquake disaster mitigation. This paper presents a hierarchical uncertainty quantification (UQ) framework for simulation-based SFA, which can address mixed aleatory and epistemic uncertainties. The framework comprises two levels: At Level 1, ground motion uncertainties are accounted for through record-to-record variation, while structural parameter uncertainties are captured using well-defined probability distributions. The quantification of uncertainty in seismic fragility is accomplished using mean and quantile fragility curves, representing the average outcome and the dispersion of structural fragility, respectively. Fuzziness is also introduced to enhance the objectivity of failure determination. Level 2 addresses uncertainties in the distribution hyper-parameters of structural variables through evidence theory, facilitating an understanding of their influence on the seismic fragility curve and quantifying uncertainty in mean and quantile fragility curves. This UQ framework builds upon the multivariate seismic fragility analysis method, integrating Kriging regression for seismic demand prediction and logistic regression for generating multivariate seismic fragility functions. The proposed framework is validated numerically on an existing three-span reinforced concrete continuous girder bridge, affirming its practical utility and reliability. © 2024 Elsevier Ltd Continuous girder bridge; Hierarchical; Mixed uncertainty quantification; Seismic fragility analysis; Surrogate model Decision making; Probability distributions; Reinforced concrete; Seismology; Structural analysis; Uncertainty analysis; Continuous girder bridge; Fragility assessment; Fragility curves; Hierarchical; Mixed uncertainty quantification; Seismic fragility; Seismic fragility analysis; Surrogate modeling; Uncertainty; Uncertainty quantifications; bridge; computer simulation; dynamic analysis; dynamic response; earthquake engineering; ground motion; hierarchical system; model test; model validation; numerical model; quantitative analysis; seismic design; stability analysis; structural analysis; structural response; uncertainty analysis; Kriging";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
233;Surge-NF: Neural Fields inspired peak storm surge surrogate modeling with multi-task learning and positional encoding;Storm surges pose a significant threat to coastal communities, necessitating rapid and precise storm surge prediction methods for long-time risk assessment and emergency management. High-fidelity numerical models such as ADCIRC provide accurate storm surge simulations but are computationally expensive. Surrogate models have emerged as an alternative option to alleviate the computational burden by learning from available numerical datasets. However, existing surrogate models face challenges in capturing the highly non-stationary and non-linear patterns of storm surges, resulting in over-smoothed response surfaces. Moreover, the dry–wet status of nearshore nodes has not been informatively considered in the training process. This study proposes Surge-NF, a novel point-based surrogate model inspired by Neural Fields (NF) from computer graphics. Surge-NF introduces two key innovations. A positional encoding module is proposed to mitigate over-smoothing of high-frequency peak storm surge spatial dependencies. A multi-task learning framework is proposed to simultaneously learn and predict the dry–wet status and peak surge values, leveraging task dependencies to improve prediction accuracy and data efficiency. We evaluate Surge-NF on the NACCS database with comparison to state-of-the-art alternative surrogate models. Surge-NF consistently reduces RMSE/MAE by 50% and achieves 4–5 times computational cost gain over baselines, requiring only 50 training storms to produce accurate predictions. The complementary benefits of the positional encoding and multi-task learning modules are evident from the improved prediction capability with their combined use. Overall, Surge-NF represents a significant advancement in storm surge surrogate modeling, offering its novel and unique ability to capture high-frequency spatial variations and leverage task dependencies. It has the potential to greatly enhance storm surge risk assessment and emergency response management, enabling effective decision-making and mitigation strategies to safeguard coastal communities from the devastating impacts of storm surges. © 2024 Elsevier B.V.;"Deep learning; Multi-task learning; Neural fields; Positional encoding; Storm surge; Surrogate model";"Computer graphics; Decision making; Deep learning; Encoding (symbols); Floods; Learning systems; Risk assessment; Signal encoding; Storms; 'Dry' [; Coastal communities; Deep learning; Encodings; Multitask learning; Neural fields; Positional encoding; Risks assessments; Storm surges; Surrogate modeling; artificial neural network; machine learning; numerical model; storm surge; surrogate method; Risk management";"Surge-NF: Neural Fields inspired peak storm surge surrogate modeling with multi-task learning and positional encoding Storm surges pose a significant threat to coastal communities, necessitating rapid and precise storm surge prediction methods for long-time risk assessment and emergency management. High-fidelity numerical models such as ADCIRC provide accurate storm surge simulations but are computationally expensive. Surrogate models have emerged as an alternative option to alleviate the computational burden by learning from available numerical datasets. However, existing surrogate models face challenges in capturing the highly non-stationary and non-linear patterns of storm surges, resulting in over-smoothed response surfaces. Moreover, the dry–wet status of nearshore nodes has not been informatively considered in the training process. This study proposes Surge-NF, a novel point-based surrogate model inspired by Neural Fields (NF) from computer graphics. Surge-NF introduces two key innovations. A positional encoding module is proposed to mitigate over-smoothing of high-frequency peak storm surge spatial dependencies. A multi-task learning framework is proposed to simultaneously learn and predict the dry–wet status and peak surge values, leveraging task dependencies to improve prediction accuracy and data efficiency. We evaluate Surge-NF on the NACCS database with comparison to state-of-the-art alternative surrogate models. Surge-NF consistently reduces RMSE/MAE by 50% and achieves 4–5 times computational cost gain over baselines, requiring only 50 training storms to produce accurate predictions. The complementary benefits of the positional encoding and multi-task learning modules are evident from the improved prediction capability with their combined use. Overall, Surge-NF represents a significant advancement in storm surge surrogate modeling, offering its novel and unique ability to capture high-frequency spatial variations and leverage task dependencies. It has the potential to greatly enhance storm surge risk assessment and emergency response management, enabling effective decision-making and mitigation strategies to safeguard coastal communities from the devastating impacts of storm surges. © 2024 Elsevier B.V. Deep learning; Multi-task learning; Neural fields; Positional encoding; Storm surge; Surrogate model Computer graphics; Decision making; Deep learning; Encoding (symbols); Floods; Learning systems; Risk assessment; Signal encoding; Storms; 'Dry' [; Coastal communities; Deep learning; Encodings; Multitask learning; Neural fields; Positional encoding; Risks assessments; Storm surges; Surrogate modeling; artificial neural network; machine learning; numerical model; storm surge; surrogate method; Risk management";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;1;Prevention
234;Tracking paddy rice acreage, flooding impacts, and mitigations during El Niño flooding events using Sentinel-1/2 imagery and cloud computing;The frequent occurrence of El Niño events, in the context of climate change, brings heavy precipitation and extreme heat, severely disrupting agricultural production. Previous efforts have focused on monitoring crop planting areas and evaluating affected crops during disasters. Nevertheless, a comprehensive analysis, including crop planting area mapping, crop damage assessment, and mitigation effectiveness throughout the entire course of a disaster, has been seldom addressed. In this study, we built a comprehensive framework to rapidly investigate the areas of early rice, the extent of flooding impacts, and the post-flood mitigations of early rice during the El Niño flooding event in a typical rice production region – Jiangxi Province in 2023. Early rice planting areas were first mapped by integrating 15-day time series gap-filled Sentinel-1/2 datasets using the Google Earth Engine (GEE) platform, based on a random forest classifier built with the 55 optimized training features. Then the flood-affected early rice map was produced by integrating the early rice planting areas and the Sentinel-1 images-based flood map. Finally, the post-flood newly planted rice fields were identified using the random forest algorithm and classification features from the Sentinel-1/2 images composited during four phenology phases of newly planted rice. The results showed the early rice planting area map, the flooding map, and the newly planted early rice map have overall accuracies of over 90 %. The early rice planting areas reached 120 × 104 ha, and an area of 3.60 × 104 ha (3 %) was flooded due to the heavy rain, and 3.43 × 104 ha flooded areas were newly planted, eventually mitigating the flooding impacts on the production of early rice. This study showcases the potential of all the available Sentinel-1/2 data, cloud computing, and well-established mapping algorithms for tracking rice areas, flooding impacts, and mitigations (i.e., after-flooding replanting) during extreme climate events. The established framework is expected to serve as an early warning system for agricultural adaptation to extreme climate events. © 2024 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS);"Agricultural mitigation; Extreme weather; Flood mapping; In-season rice mapping; Post-flood rice replanting";"China; Jiangxi; Analog storage; Atmospheric pressure; Deforestation; Digital storage; Fertilizers; Flood damage; Tropics; Agricultural mitigation; EL Nino; Extreme weather; Flood mapping; Floodings; In-season rice mapping; Planting areas; Post-flood rice replanting; Rice mapping; Sentinel-1; agricultural management; crop damage; early warning system; extreme event; flooding; paddy field; phenology; satellite imagery; Sentinel; time series; Mapping";"Tracking paddy rice acreage, flooding impacts, and mitigations during El Niño flooding events using Sentinel-1/2 imagery and cloud computing The frequent occurrence of El Niño events, in the context of climate change, brings heavy precipitation and extreme heat, severely disrupting agricultural production. Previous efforts have focused on monitoring crop planting areas and evaluating affected crops during disasters. Nevertheless, a comprehensive analysis, including crop planting area mapping, crop damage assessment, and mitigation effectiveness throughout the entire course of a disaster, has been seldom addressed. In this study, we built a comprehensive framework to rapidly investigate the areas of early rice, the extent of flooding impacts, and the post-flood mitigations of early rice during the El Niño flooding event in a typical rice production region – Jiangxi Province in 2023. Early rice planting areas were first mapped by integrating 15-day time series gap-filled Sentinel-1/2 datasets using the Google Earth Engine (GEE) platform, based on a random forest classifier built with the 55 optimized training features. Then the flood-affected early rice map was produced by integrating the early rice planting areas and the Sentinel-1 images-based flood map. Finally, the post-flood newly planted rice fields were identified using the random forest algorithm and classification features from the Sentinel-1/2 images composited during four phenology phases of newly planted rice. The results showed the early rice planting area map, the flooding map, and the newly planted early rice map have overall accuracies of over 90 %. The early rice planting areas reached 120 × 104 ha, and an area of 3.60 × 104 ha (3 %) was flooded due to the heavy rain, and 3.43 × 104 ha flooded areas were newly planted, eventually mitigating the flooding impacts on the production of early rice. This study showcases the potential of all the available Sentinel-1/2 data, cloud computing, and well-established mapping algorithms for tracking rice areas, flooding impacts, and mitigations (i.e., after-flooding replanting) during extreme climate events. The established framework is expected to serve as an early warning system for agricultural adaptation to extreme climate events. © 2024 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS) Agricultural mitigation; Extreme weather; Flood mapping; In-season rice mapping; Post-flood rice replanting China; Jiangxi; Analog storage; Atmospheric pressure; Deforestation; Digital storage; Fertilizers; Flood damage; Tropics; Agricultural mitigation; EL Nino; Extreme weather; Flood mapping; Floodings; In-season rice mapping; Planting areas; Post-flood rice replanting; Rice mapping; Sentinel-1; agricultural management; crop damage; early warning system; extreme event; flooding; paddy field; phenology; satellite imagery; Sentinel; time series; Mapping";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;4;Recovery
235;ADVANCED UAV PATH PLANNING AND CLUSTERING TECHNIQUES FOR 3D RECONSTRUCTION OF POST-EARTHQUAKE ARCHITECTURES;This research provides an exhaustive examination of the pivotal role that three-dimensional (3D) reconstruction techniques play in the post-seismic assessment of architectural structures, with a pronounced emphasis on the deployment of Unmanned Aerial Vehicles (UAVs). Given the profound ramifications of seismic events on urban infrastructures, the study underscores the imperative of conducting timely and meticulous evaluations of the affected edifices. The paper commences by undertaking a 3D modelling of the architectural ensemble, strategically positioning cameras around the target structures to capture previously uncharted points. Subsequently, two sophisticated clustering algorithms, namely spectral clustering and Fuzzy K-means, are employed to cluster these positional points. These clustered points are then judiciously allocated to a fleet of four UAVs for optimal coverage. The research introduces the F-RRT* algorithm, an augmented version of the conventional RRT* algorithm tailored for path planning. This enhancement is spotlighted for its superior convergence rate and its ability to generate more streamlined and efficient trajectories. In summation, this comprehensive investigation furnishes invaluable insights and avant-garde technical methodologies for post-disaster architectural evaluations, championing the advancement of UAV-centric 3D reconstruction endeavours. © School of Engineering, Taylor's University.;"3D Reconstruction; Intelligent system; Path planning; Post-disaster; UAV";NULL;"ADVANCED UAV PATH PLANNING AND CLUSTERING TECHNIQUES FOR 3D RECONSTRUCTION OF POST-EARTHQUAKE ARCHITECTURES This research provides an exhaustive examination of the pivotal role that three-dimensional (3D) reconstruction techniques play in the post-seismic assessment of architectural structures, with a pronounced emphasis on the deployment of Unmanned Aerial Vehicles (UAVs). Given the profound ramifications of seismic events on urban infrastructures, the study underscores the imperative of conducting timely and meticulous evaluations of the affected edifices. The paper commences by undertaking a 3D modelling of the architectural ensemble, strategically positioning cameras around the target structures to capture previously uncharted points. Subsequently, two sophisticated clustering algorithms, namely spectral clustering and Fuzzy K-means, are employed to cluster these positional points. These clustered points are then judiciously allocated to a fleet of four UAVs for optimal coverage. The research introduces the F-RRT* algorithm, an augmented version of the conventional RRT* algorithm tailored for path planning. This enhancement is spotlighted for its superior convergence rate and its ability to generate more streamlined and efficient trajectories. In summation, this comprehensive investigation furnishes invaluable insights and avant-garde technical methodologies for post-disaster architectural evaluations, championing the advancement of UAV-centric 3D reconstruction endeavours. © School of Engineering, Taylor's University. 3D Reconstruction; Intelligent system; Path planning; Post-disaster; UAV NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;3;Response
236;Causal networks and spatiotemporal patterns of different droughts from the perspective of hydrological cycle - a case study of the Nenjiang River Basin, China;Different drought types exhibit propagation phenomena and feedback through the hydrological cycle. Therefore, this study utilized the convergent cross mapping to identify the causality among meteorological drought(MD), agricultural drought(AD), hydrological drought(HD), and ecological drought(ED) in the Nenjiang River Basin from 1980 to 2020, constructing a causal relationship network. The results indicate that different types of droughts are more prone to spread and influence each other at shorter time scales. Although there exist bidirectional causality between MD and other droughts at the monthly scale, no clear causal relationship is observed between AD and ED. Contrasting the direction and strength of causality, presenting the order MD-AD-HD-ED. This finding is consistent with the hydrological cycle pattern. The intensity of drought causality varies significantly across different regions spatially. This study delves into the mechanisms and intensities of mutual influence among different types of droughts by establishing the causal relationship network, offering essential scientific basis for drought monitoring, early warning, and emergency response. © 2024;"Agricultural drought; Causality; Drought propagation; Ecological drought; Hydrological drought; Meteorological drought";"China; Nen Basin; Agriculture; Ecology; Watersheds; Agricultural drought; Causal relationships; Causality; Drought propagation; Ecological drought; Hydrological cycles; Hydrological droughts; Meteorological drought; Nenjiang river basins; Relationship networks; artificial neural network; drought; Granger causality test; hydrological cycle; spatiotemporal analysis; timescale; Drought";"Causal networks and spatiotemporal patterns of different droughts from the perspective of hydrological cycle - a case study of the Nenjiang River Basin, China Different drought types exhibit propagation phenomena and feedback through the hydrological cycle. Therefore, this study utilized the convergent cross mapping to identify the causality among meteorological drought(MD), agricultural drought(AD), hydrological drought(HD), and ecological drought(ED) in the Nenjiang River Basin from 1980 to 2020, constructing a causal relationship network. The results indicate that different types of droughts are more prone to spread and influence each other at shorter time scales. Although there exist bidirectional causality between MD and other droughts at the monthly scale, no clear causal relationship is observed between AD and ED. Contrasting the direction and strength of causality, presenting the order MD-AD-HD-ED. This finding is consistent with the hydrological cycle pattern. The intensity of drought causality varies significantly across different regions spatially. This study delves into the mechanisms and intensities of mutual influence among different types of droughts by establishing the causal relationship network, offering essential scientific basis for drought monitoring, early warning, and emergency response. © 2024 Agricultural drought; Causality; Drought propagation; Ecological drought; Hydrological drought; Meteorological drought China; Nen Basin; Agriculture; Ecology; Watersheds; Agricultural drought; Causal relationships; Causality; Drought propagation; Ecological drought; Hydrological cycles; Hydrological droughts; Meteorological drought; Nenjiang river basins; Relationship networks; artificial neural network; drought; Granger causality test; hydrological cycle; spatiotemporal analysis; timescale; Drought";-1;Não Classificado;NULL;1.4;Climatological;2;Preparation
237;Structural nonlinear seismic time-history response prediction of urban-scale reinforced concrete frames based on deep learning;Efficiently predicting the seismic response of urban building clusters is essential for preemptively identifying potential seismic hazards prior to an earthquake and optimizing resource allocation post-event. However, complete information of buildings at a city scale is generally un-accessible or non-existent. Existing methods struggle to reconcile low information demands, high computational accuracy, and computational efficiency. This paper proposes a fast prediction method for structural seismic time-history responses that combines deep learning methods with easy-getting structural parameters at an urban scale. An end-to-end network with adaptive multilevel fusion output is designed, which incorporates the autoencoder concept for predicting the structural seismic time-history responses based on ground motions records and five easy-getting structural parameters. The models are compared and optimized considering the training hyperparameters and network architecture, resulting in an optimized model with low complexity that provides valuable reference values for structural seismic response. Besides, the proposed model is applied to four actual buildings with different construction time, occupancy types, and floor sizes, demonstrating its good prediction performance and significant computational advantages comparing to the universally used MDOF method. © 2024;"Deep learning; Easy-getting structural parameters; Reinforced concrete frames; Seismic time-history response prediction; Urban-scale seismic response analysis";"Computational efficiency; Deep learning; Earthquakes; Forecasting; Learning systems; Network architecture; Reinforced concrete; Deep learning; Easy-getting structural parameter; Reinforced concrete frames; Response prediction; Seismic response analysis; Seismic time history response; Seismic time-history response prediction; Structural parameter; Urban scale; Urban-scale seismic response analyse; building; ground motion; machine learning; prediction; reinforced concrete; seismic response; structural response; Seismic response";"Structural nonlinear seismic time-history response prediction of urban-scale reinforced concrete frames based on deep learning Efficiently predicting the seismic response of urban building clusters is essential for preemptively identifying potential seismic hazards prior to an earthquake and optimizing resource allocation post-event. However, complete information of buildings at a city scale is generally un-accessible or non-existent. Existing methods struggle to reconcile low information demands, high computational accuracy, and computational efficiency. This paper proposes a fast prediction method for structural seismic time-history responses that combines deep learning methods with easy-getting structural parameters at an urban scale. An end-to-end network with adaptive multilevel fusion output is designed, which incorporates the autoencoder concept for predicting the structural seismic time-history responses based on ground motions records and five easy-getting structural parameters. The models are compared and optimized considering the training hyperparameters and network architecture, resulting in an optimized model with low complexity that provides valuable reference values for structural seismic response. Besides, the proposed model is applied to four actual buildings with different construction time, occupancy types, and floor sizes, demonstrating its good prediction performance and significant computational advantages comparing to the universally used MDOF method. © 2024 Deep learning; Easy-getting structural parameters; Reinforced concrete frames; Seismic time-history response prediction; Urban-scale seismic response analysis Computational efficiency; Deep learning; Earthquakes; Forecasting; Learning systems; Network architecture; Reinforced concrete; Deep learning; Easy-getting structural parameter; Reinforced concrete frames; Response prediction; Seismic response analysis; Seismic time history response; Seismic time-history response prediction; Structural parameter; Urban scale; Urban-scale seismic response analyse; building; ground motion; machine learning; prediction; reinforced concrete; seismic response; structural response; Seismic response";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
238;Deep learning for real-time P-wave detection: A case study in Indonesia's earthquake early warning system;Detecting seismic events in real-time for prompt alerts and responses is a challenging task that requires accurately capturing P-wave arrivals. This task becomes even more challenging in regions like Indonesia, where widely spaced seismic stations exist. The wide station spacing makes associating the seismic signals with specific even more difficult. This paper proposes a novel deep learning-based model with three convolutional layers, enriched with dual attention mechanisms—Squeeze, Excitation, and Transformer Encoder (CNN-SE-T) —to refine feature extraction and improve detection sensitivity. We have integrated several post-processing techniques to further bolster the model's robustness against noise. We conducted comprehensive evaluations of our method using three diverse datasets: local earthquake data from East Java, the publicly available Seismic Waveform Data (STEAD), and a continuous waveform dataset spanning 12 h from multiple Indonesian seismic stations. The performance of the CNN-SE-T P-wave detection model yielded exceptionally high F1 scores of 99.10% for East Java, 92.64% for STEAD, and 80% for the 12-h continuous waveforms across Indonesia's network, demonstrating the model's effectiveness and potential for real-world application in earthquake early warning systems. © 2024 The Authors;"Deep learning; Earthquake early warning system; P-wave detection; Real-time";"Java programming language; Seismic response; Seismic waves; Shear waves; Case-studies; Continuous waveforms; Deep learning; Earthquake early warning systems; Indonesia; P-wave arrival; P-wave detections; Real- time; Seismic event; Seismic station; Earthquakes";"Deep learning for real-time P-wave detection: A case study in Indonesia's earthquake early warning system Detecting seismic events in real-time for prompt alerts and responses is a challenging task that requires accurately capturing P-wave arrivals. This task becomes even more challenging in regions like Indonesia, where widely spaced seismic stations exist. The wide station spacing makes associating the seismic signals with specific even more difficult. This paper proposes a novel deep learning-based model with three convolutional layers, enriched with dual attention mechanisms—Squeeze, Excitation, and Transformer Encoder (CNN-SE-T) —to refine feature extraction and improve detection sensitivity. We have integrated several post-processing techniques to further bolster the model's robustness against noise. We conducted comprehensive evaluations of our method using three diverse datasets: local earthquake data from East Java, the publicly available Seismic Waveform Data (STEAD), and a continuous waveform dataset spanning 12 h from multiple Indonesian seismic stations. The performance of the CNN-SE-T P-wave detection model yielded exceptionally high F1 scores of 99.10% for East Java, 92.64% for STEAD, and 80% for the 12-h continuous waveforms across Indonesia's network, demonstrating the model's effectiveness and potential for real-world application in earthquake early warning systems. © 2024 The Authors Deep learning; Earthquake early warning system; P-wave detection; Real-time Java programming language; Seismic response; Seismic waves; Shear waves; Case-studies; Continuous waveforms; Deep learning; Earthquake early warning systems; Indonesia; P-wave arrival; P-wave detections; Real- time; Seismic event; Seismic station; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
239;Advancements in nano-enhanced steel structures for earthquake resilience: Integrating metallic elements, AI, and sensor technology for engineering disasters mitigation in steel buildings;This study develops Titanium (Ti) and Magnesium (Mg)-based nano-alloys to enhance the earthquake resilience of steel structures using machine learning (SVM) and sensor technology. Embedding Ti and Mg into steel at the nanoscale creates a lightweight, durable, and flexible material capable of withstanding seismic forces. Ti enhances tensile strength and flexibility, while Mg reduces weight, lowering seismic loads on buildings. The performance of these nano-alloys was assessed through shake table tests, cyclic load testing, and dynamic response testing, showing that nano-alloy-enhanced steel structures experienced 60% less displacement and 40% lower acceleration than traditional steel, demonstrating superior energy absorption and stress distribution. Fatigue tests revealed that the nano-alloy could endure 20, 000 loading cycles, outperforming the 8, 000 cycles of conventional steel. Integrated sensor technology, including strain gauges and accelerometers, provided real-time stress and deformation data, confirming the material’s effectiveness in stress distribution and vibration damping. The SVM model optimized alloy composition, achieving 94% prediction accuracy in assessing seismic performance, highlighting the nano-alloys' durability and resilience. This study suggests that Ti and Mg nano-alloys could greatly improve earthquake-resistant construction. Copyright © 2024 Techno-Press, Ltd.;"earthquake-resilient steel structures; Machine Learning (SVM); predictive maintenance and disaster mitigation; seismic energy dissipation; sensor technology; titanium-magnesium nano-alloys";"Cyclic loads; Dynamic response; Earthquake effects; Earthquake engineering; Electric towers; Fatigue testing; Fracture mechanics; Magnesium alloys; Mercury amalgams; Steel structures; Steel testing; Stress analysis; Stress concentration; Tensile testing; Titanium alloys; Disaster mitigation; Earthquake-resilient steel structure; Machine learning (SVM); Machine-learning; Nano-alloys; Predictive maintenance; Predictive maintenance and disaster mitigation; Seismic energy dissipation; Sensor technologies; Titania; Titania-magnesium nano-alloy; Strain gages";"Advancements in nano-enhanced steel structures for earthquake resilience: Integrating metallic elements, AI, and sensor technology for engineering disasters mitigation in steel buildings This study develops Titanium (Ti) and Magnesium (Mg)-based nano-alloys to enhance the earthquake resilience of steel structures using machine learning (SVM) and sensor technology. Embedding Ti and Mg into steel at the nanoscale creates a lightweight, durable, and flexible material capable of withstanding seismic forces. Ti enhances tensile strength and flexibility, while Mg reduces weight, lowering seismic loads on buildings. The performance of these nano-alloys was assessed through shake table tests, cyclic load testing, and dynamic response testing, showing that nano-alloy-enhanced steel structures experienced 60% less displacement and 40% lower acceleration than traditional steel, demonstrating superior energy absorption and stress distribution. Fatigue tests revealed that the nano-alloy could endure 20, 000 loading cycles, outperforming the 8, 000 cycles of conventional steel. Integrated sensor technology, including strain gauges and accelerometers, provided real-time stress and deformation data, confirming the material’s effectiveness in stress distribution and vibration damping. The SVM model optimized alloy composition, achieving 94% prediction accuracy in assessing seismic performance, highlighting the nano-alloys' durability and resilience. This study suggests that Ti and Mg nano-alloys could greatly improve earthquake-resistant construction. Copyright © 2024 Techno-Press, Ltd. earthquake-resilient steel structures; Machine Learning (SVM); predictive maintenance and disaster mitigation; seismic energy dissipation; sensor technology; titanium-magnesium nano-alloys Cyclic loads; Dynamic response; Earthquake effects; Earthquake engineering; Electric towers; Fatigue testing; Fracture mechanics; Magnesium alloys; Mercury amalgams; Steel structures; Steel testing; Stress analysis; Stress concentration; Tensile testing; Titanium alloys; Disaster mitigation; Earthquake-resilient steel structure; Machine learning (SVM); Machine-learning; Nano-alloys; Predictive maintenance; Predictive maintenance and disaster mitigation; Seismic energy dissipation; Sensor technologies; Titania; Titania-magnesium nano-alloy; Strain gages";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
240;A contrastive topic-aware attentive framework with label encodings for post-disaster resource classification;Social media has emerged as a critical platform for disseminating real-time information during disasters. However, extracting actionable resource data, such as needs and availability, from this vast and unstructured content remains a significant challenge, leading to delays in identifying and allocating resources, with severe consequences for affected populations. This study addresses this challenge by investigating the potential of label and topic features, combined with text embeddings, to enhance the performance and efficiency of resource identification from social media data. We propose Crisis Resource Finder (CRFinder), a novel framework that leverages label encoding and topic features to extract richer contextual information, uncover hidden patterns, and reveal the true context of disaster resources. CRFinder incorporates novel techniques such as multi-level text-label attention and contrastive text-topic attention to capture semantic and thematic nuances within the textual data. Additionally, our approach employs topic injection and selective contextualization techniques to enhance thematic relevance and focus on critical information, which is pivotal for targeted relief efforts. Extensive experiments demonstrate the significant improvements achieved by CRFinder over existing state-of-the-art methods, with average weighted F1-score gains of 7.12%, 6.44%, and 7.89% on datasets from the Nepal earthquake, Italy earthquake, and Chennai floods, respectively. By providing timely and accurate insights into resource needs and availabilities, CRFinder has the potential to revolutionize disaster response efforts. © 2024 Elsevier B.V.;"Crisis management; Deep learning; Disaster content classification; Disaster resource request; Need and availability resources";"Content classification; Crisis management; Deep learning; Disaster content classification; Disaster resource request; Label encoding; Need and availability resource; Post disasters; Resource request; Social media; Disasters";"A contrastive topic-aware attentive framework with label encodings for post-disaster resource classification Social media has emerged as a critical platform for disseminating real-time information during disasters. However, extracting actionable resource data, such as needs and availability, from this vast and unstructured content remains a significant challenge, leading to delays in identifying and allocating resources, with severe consequences for affected populations. This study addresses this challenge by investigating the potential of label and topic features, combined with text embeddings, to enhance the performance and efficiency of resource identification from social media data. We propose Crisis Resource Finder (CRFinder), a novel framework that leverages label encoding and topic features to extract richer contextual information, uncover hidden patterns, and reveal the true context of disaster resources. CRFinder incorporates novel techniques such as multi-level text-label attention and contrastive text-topic attention to capture semantic and thematic nuances within the textual data. Additionally, our approach employs topic injection and selective contextualization techniques to enhance thematic relevance and focus on critical information, which is pivotal for targeted relief efforts. Extensive experiments demonstrate the significant improvements achieved by CRFinder over existing state-of-the-art methods, with average weighted F1-score gains of 7.12%, 6.44%, and 7.89% on datasets from the Nepal earthquake, Italy earthquake, and Chennai floods, respectively. By providing timely and accurate insights into resource needs and availabilities, CRFinder has the potential to revolutionize disaster response efforts. © 2024 Elsevier B.V. Crisis management; Deep learning; Disaster content classification; Disaster resource request; Need and availability resources Content classification; Crisis management; Deep learning; Disaster content classification; Disaster resource request; Label encoding; Need and availability resource; Post disasters; Resource request; Social media; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
241;Assessing landscape ecological risk in the Southern Hill and Mountain Belt of China: A 30-year analysis and future projection;"Hills and mountains cover approximately 25 % of Earth's landmass; therefore, investigating landscape ecological risk (LER) in these regions is imperative for ecosystem management. Hilly and mountainous regions susceptible to influences such as desertification and erosion include the Southern Hill and Mountainous Belt (SHMB) in southern China. Over the past 30 years, the SHMB has undergone significant changes, offering an excellent research case. A comprehensive assessment of the ecological status of this region is crucial for the protection and management of the ecosystems in the SHMB. This study assessed the LER of the SHMB, identified influencing factors over the past 30 years, and utilized a patch-generating land-use simulation model (PLUS) to project future land use and LER. Accounting for more than 90 % of the total area, cropland and forestland were the two most common land use types in the SHMB. From 1990 to 2020, impervious surface and water area saw obvious increases, rising by 154 % (from 1209.37 km2 to 3073.59 km2) and 21 % (from 1584.78 km2 to 1924.8 km2), respectively. Grasslands and shrublands decreased by 54 % and 34 %. It is anticipated that there will be little change in land-use types by 2030 compared to 2020. Additionally, the overall LER in the SHMB was low, with low- and medium–low-risk representing around 70 % of the overall risk. The general state of LER improved during the research period but did not converge. Specifically, the LER intensity did not decrease although the total LER showed a declining trend. In the western portion of the research region, the karst area displayed an increasing LER, and the severity of the high-risk area increased. Furthermore, natural factors promote the restoration of LER. For example, the abundance of hydrothermal conditions (i.e., increased temperature and precipitation) promotes the recovery of vegetation growth and reduces the LER. Human activities, such as the construction of stations, highways, and an increasing population, exacerbate LER, contributing to an increase in LER. This study offers helpful guidance for formulating ecological conservation strategies to maintain the ecological balance and sustainability of hilly and mountainous regions. © 2024 The Author(s)";"Driving factors; Geographically weighted regression; Landscape ecological risk; Risk prediction; Southern hill and mountain belt";"China; Ecosystems; Landforms; Risk assessment; Driving factors; Ecological risks; Future projections; Geographically weighted regression; Land use type; Landscape ecological risk; Mountain belts; Mountainous regions; Risk predictions; Southern hill and mountain belt; conservation management; ecological stability; ecosystem management; environmental protection; environmental risk; future prospect; landscape ecology; mountain region; prediction; risk assessment; Land use";"Assessing landscape ecological risk in the Southern Hill and Mountain Belt of China: A 30-year analysis and future projection Hills and mountains cover approximately 25 % of Earth's landmass; therefore, investigating landscape ecological risk (LER) in these regions is imperative for ecosystem management. Hilly and mountainous regions susceptible to influences such as desertification and erosion include the Southern Hill and Mountainous Belt (SHMB) in southern China. Over the past 30 years, the SHMB has undergone significant changes, offering an excellent research case. A comprehensive assessment of the ecological status of this region is crucial for the protection and management of the ecosystems in the SHMB. This study assessed the LER of the SHMB, identified influencing factors over the past 30 years, and utilized a patch-generating land-use simulation model (PLUS) to project future land use and LER. Accounting for more than 90 % of the total area, cropland and forestland were the two most common land use types in the SHMB. From 1990 to 2020, impervious surface and water area saw obvious increases, rising by 154 % (from 1209.37 km2 to 3073.59 km2) and 21 % (from 1584.78 km2 to 1924.8 km2), respectively. Grasslands and shrublands decreased by 54 % and 34 %. It is anticipated that there will be little change in land-use types by 2030 compared to 2020. Additionally, the overall LER in the SHMB was low, with low- and medium–low-risk representing around 70 % of the overall risk. The general state of LER improved during the research period but did not converge. Specifically, the LER intensity did not decrease although the total LER showed a declining trend. In the western portion of the research region, the karst area displayed an increasing LER, and the severity of the high-risk area increased. Furthermore, natural factors promote the restoration of LER. For example, the abundance of hydrothermal conditions (i.e., increased temperature and precipitation) promotes the recovery of vegetation growth and reduces the LER. Human activities, such as the construction of stations, highways, and an increasing population, exacerbate LER, contributing to an increase in LER. This study offers helpful guidance for formulating ecological conservation strategies to maintain the ecological balance and sustainability of hilly and mountainous regions. © 2024 The Author(s) Driving factors; Geographically weighted regression; Landscape ecological risk; Risk prediction; Southern hill and mountain belt China; Ecosystems; Landforms; Risk assessment; Driving factors; Ecological risks; Future projections; Geographically weighted regression; Land use type; Landscape ecological risk; Mountain belts; Mountainous regions; Risk predictions; Southern hill and mountain belt; conservation management; ecological stability; ecosystem management; environmental protection; environmental risk; future prospect; landscape ecology; mountain region; prediction; risk assessment; Land use";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
242;Reliability in mass alert system for dam emergencies: Case study of Vale S/A;In its iron ore operations, Vale, the largest mining company in Brazil and the fourth largest mining company in the world by market value, has 77 dams considered with high potential damage in case of a failure in the stability controls. This classification follows the criteria of the sector regulatory agency in Brazil and determines that, for this class, a mass alert system is necessary for the resident population and workers immediately downstream of the dam within a region preset by legislation, considering a simulated flood area. For Vale, the mass alert system for dam emergencies is part of a critical mitigating control for a business risk analysis considering the hypothetical dam break as an unwanted material event. Therefore, both the probability of failure on demand, spurious failure, and availability of the assets of this protection barrier are important elements for this purpose, as well as the emergency response plan and the evacuation simulation of flood areas, which are part of a risk management system that require continuous verification and continuous improvement. Currently, Vale has 375 sirens, and it is planned to reach 420 sirens by the end of 2024. The purpose of this article is to present how the safety concepts applied to these systems were developed, the good practices applied from this study, and the lessons learned from the dam emergency management arising from the break of the Brumadinho dam in 2019. The reliability analyses of the systems involved and the main contributors to the Safety Critical Unavailability are presented, as well as the results of the reduction in the maintenance time and increase in system availability. © 2024 American Institute of Chemical Engineers.;"dam; mass alert; mining; reliability";"Dams; Floods; Iron ores; Laws and legislation; Risk analysis; Risk assessment; Risk management; Safety engineering; Sirens; Alert systems; Case-studies; Class A; Flood areas; High potential; Market values; Mass alert; Mining companies; Regulatory agencies; Stability control; Mining";"Reliability in mass alert system for dam emergencies: Case study of Vale S/A In its iron ore operations, Vale, the largest mining company in Brazil and the fourth largest mining company in the world by market value, has 77 dams considered with high potential damage in case of a failure in the stability controls. This classification follows the criteria of the sector regulatory agency in Brazil and determines that, for this class, a mass alert system is necessary for the resident population and workers immediately downstream of the dam within a region preset by legislation, considering a simulated flood area. For Vale, the mass alert system for dam emergencies is part of a critical mitigating control for a business risk analysis considering the hypothetical dam break as an unwanted material event. Therefore, both the probability of failure on demand, spurious failure, and availability of the assets of this protection barrier are important elements for this purpose, as well as the emergency response plan and the evacuation simulation of flood areas, which are part of a risk management system that require continuous verification and continuous improvement. Currently, Vale has 375 sirens, and it is planned to reach 420 sirens by the end of 2024. The purpose of this article is to present how the safety concepts applied to these systems were developed, the good practices applied from this study, and the lessons learned from the dam emergency management arising from the break of the Brumadinho dam in 2019. The reliability analyses of the systems involved and the main contributors to the Safety Critical Unavailability are presented, as well as the results of the reduction in the maintenance time and increase in system availability. © 2024 American Institute of Chemical Engineers. dam; mass alert; mining; reliability Dams; Floods; Iron ores; Laws and legislation; Risk analysis; Risk assessment; Risk management; Safety engineering; Sirens; Alert systems; Case-studies; Class A; Flood areas; High potential; Market values; Mass alert; Mining companies; Regulatory agencies; Stability control; Mining";-1;Não Classificado;NULL;1.2;Hydrological;2;Preparation
243;Urban flash flood hazard mapping using machine learning, Bahir Dar, Ethiopia;Increased frequency and magnitude of flooding pose a significant natural hazard to urban areas worldwide. Mapping flood hazard areas are crucial for mitigating potential damage to human life and property. However, conventional hydrodynamic approaches are hindered by their extensive data requirements and computational expenses. As an alternative solution, this paper explores the use of machine learning (ML) techniques to map flood hazards based on readily available geo-environmental variables. We employed various ML classifiers, including decision tree (DT), random forest (RF), XGBoost (XGB), and k-nearest neighbor (kNN), to assess their performance in flood hazard mapping. Model evaluation was conducted using the area under the receiver operating characteristic curve (AUC) and root mean square error (RMSE). Our results demonstrated promising outcomes, with AUC values of 93% (DT), 97% (RF), 98% (XGB), and 91% (kNN) for the validation dataset. RF and XGB have slightly higher performance than DT and kNN and distance to river was the most important factor. The study highlights the potential of ML for urban flood modeling, offering reasonable accuracy and supporting early warning systems. By leveraging available geo-environmental variables, ML techniques provide valuable insights into flood hazard mapping, aiding in effective urban planning and disaster management strategies. © 2024 The Authors.;"Bahir Dar city; flood hazard; machine learning";NULL;"Urban flash flood hazard mapping using machine learning, Bahir Dar, Ethiopia Increased frequency and magnitude of flooding pose a significant natural hazard to urban areas worldwide. Mapping flood hazard areas are crucial for mitigating potential damage to human life and property. However, conventional hydrodynamic approaches are hindered by their extensive data requirements and computational expenses. As an alternative solution, this paper explores the use of machine learning (ML) techniques to map flood hazards based on readily available geo-environmental variables. We employed various ML classifiers, including decision tree (DT), random forest (RF), XGBoost (XGB), and k-nearest neighbor (kNN), to assess their performance in flood hazard mapping. Model evaluation was conducted using the area under the receiver operating characteristic curve (AUC) and root mean square error (RMSE). Our results demonstrated promising outcomes, with AUC values of 93% (DT), 97% (RF), 98% (XGB), and 91% (kNN) for the validation dataset. RF and XGB have slightly higher performance than DT and kNN and distance to river was the most important factor. The study highlights the potential of ML for urban flood modeling, offering reasonable accuracy and supporting early warning systems. By leveraging available geo-environmental variables, ML techniques provide valuable insights into flood hazard mapping, aiding in effective urban planning and disaster management strategies. © 2024 The Authors. Bahir Dar city; flood hazard; machine learning NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
244;Comparative Study on CNN-based Bridge Seismic Damage Identification Using Various Features;Quick and accurate identification of bridge damage after an earthquake is crucial for emergency decision-making and post-disaster rehabilitation. The maturing technology of deep neural networks (DNN) and the integration of health monitoring systems provide a viable solution for seismic damage identification in bridges. However, how to construct damage features that can efficiently characterize the seismic damage of the bridge and are suitable for the use with DNN needs further investigation. This study focuses on seismic damage identification for a continuous rigid bridge using raw acceleration responses, statistical features, frequency features, and time-frequency features as inputs, with damage states as outputs, employing a deep convolutional neural network (CNN) for pattern classification. Results indicate that all four damage features can identify seismic damage, with time-frequency features achieving the highest accuracy but having a complex construction process. Frequency features also demonstrate high accuracy with simpler construction. Raw acceleration response and statistical features perform poorly, with statistical features deemed unsuitable as damage indicators. Overall, frequency features are recommended as CNN inputs for quick and accurate bridge seismic damage identification. © Korean Society of Civil Engineers 2024.;"Continuous rigid bridge; Convolutional neural network; Damage feature; Damage identification; Seismic damage";"Disasters; Earthquake effects; Earthquake engineering; Movable bridges; Rigid structures; Seismic response; Acceleration response; Continuous rigid bridge; Convolutional neural network; Damage features; Damage Identification; Frequency features; Neural-networks; Seismic damage; Seismic damage identification; Statistical features; Deep neural networks";"Comparative Study on CNN-based Bridge Seismic Damage Identification Using Various Features Quick and accurate identification of bridge damage after an earthquake is crucial for emergency decision-making and post-disaster rehabilitation. The maturing technology of deep neural networks (DNN) and the integration of health monitoring systems provide a viable solution for seismic damage identification in bridges. However, how to construct damage features that can efficiently characterize the seismic damage of the bridge and are suitable for the use with DNN needs further investigation. This study focuses on seismic damage identification for a continuous rigid bridge using raw acceleration responses, statistical features, frequency features, and time-frequency features as inputs, with damage states as outputs, employing a deep convolutional neural network (CNN) for pattern classification. Results indicate that all four damage features can identify seismic damage, with time-frequency features achieving the highest accuracy but having a complex construction process. Frequency features also demonstrate high accuracy with simpler construction. Raw acceleration response and statistical features perform poorly, with statistical features deemed unsuitable as damage indicators. Overall, frequency features are recommended as CNN inputs for quick and accurate bridge seismic damage identification. © Korean Society of Civil Engineers 2024. Continuous rigid bridge; Convolutional neural network; Damage feature; Damage identification; Seismic damage Disasters; Earthquake effects; Earthquake engineering; Movable bridges; Rigid structures; Seismic response; Acceleration response; Continuous rigid bridge; Convolutional neural network; Damage features; Damage Identification; Frequency features; Neural-networks; Seismic damage; Seismic damage identification; Statistical features; Deep neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
245;Effect of data drift on the performance of machine-learning models: Seismic damage prediction for aging bridges;Machine-learning models play a crucial role in structural seismic risk assessment and facilitate decision-making by analyzing complex data patterns. However, the dynamic nature of real-world data introduces challenges, particularly data drift, which can significantly affect model performance. This adversely affects machine-learning models intended to aid emergency responders and disaster recovery teams. This study primarily focused on assessing the impact of column corrosion-induced data drift on the performance of machine-learning models for seismic risk assessment of bridges. The machine-learning model performance was evaluated with and without considering the impact of corrosion. The results revealed a significant decrease in prediction accuracy when the data drift effect was not considered. To address this challenge, this study proposes integrating principal component analysis-based anomaly detection to enhance the model performance. The optimized model considering drift demonstrates significant improvements in accuracy across corroded bridges aged 25, 50, and 75 years, with accuracy rates increasing from 90%, 85%, and 81% to 98%, 97%, and 96%, respectively. © 2024 The Author(s). Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd.;"aging bridge; corrosion; data drift; machine learning; principal component analysis; seismic damage";"Earthquakes; Risk assessment; Seismic response; Aging bridge; Data drift; Machine learning models; Machine-learning; Modeling performance; Performance; Principal-component analysis; Seismic damage; Seismic damage prediction; Seismic risk assessment; aging; bridge; corrosion; dynamic analysis; earthquake damage; machine learning; performance assessment; principal component analysis; risk assessment; seismic data; Earthquake engineering";"Effect of data drift on the performance of machine-learning models: Seismic damage prediction for aging bridges Machine-learning models play a crucial role in structural seismic risk assessment and facilitate decision-making by analyzing complex data patterns. However, the dynamic nature of real-world data introduces challenges, particularly data drift, which can significantly affect model performance. This adversely affects machine-learning models intended to aid emergency responders and disaster recovery teams. This study primarily focused on assessing the impact of column corrosion-induced data drift on the performance of machine-learning models for seismic risk assessment of bridges. The machine-learning model performance was evaluated with and without considering the impact of corrosion. The results revealed a significant decrease in prediction accuracy when the data drift effect was not considered. To address this challenge, this study proposes integrating principal component analysis-based anomaly detection to enhance the model performance. The optimized model considering drift demonstrates significant improvements in accuracy across corroded bridges aged 25, 50, and 75 years, with accuracy rates increasing from 90%, 85%, and 81% to 98%, 97%, and 96%, respectively. © 2024 The Author(s). Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd. aging bridge; corrosion; data drift; machine learning; principal component analysis; seismic damage Earthquakes; Risk assessment; Seismic response; Aging bridge; Data drift; Machine learning models; Machine-learning; Modeling performance; Performance; Principal-component analysis; Seismic damage; Seismic damage prediction; Seismic risk assessment; aging; bridge; corrosion; dynamic analysis; earthquake damage; machine learning; performance assessment; principal component analysis; risk assessment; seismic data; Earthquake engineering";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
246;Effects of disturbances on the spatiotemporal patterns and dynamics of coastal wetland vegetation;The coastal wetlands represented by mangroves are vulnerable to disturbances. To clarify the effects of disturbances quantitatively on mangroves, spatiotemporal changes in land use and land cover (LULC) were compared between the inside and outside of mangroves during 1988 and 2020 in Bangladesh by remote sensing. The mangrove region, which had diverse surface greenness due to natural and anthropogenic disturbances, were surveyed. Changes in greenness were estimated by five vegetation indices (VIs). To increase in the accuracy of land use classification, the five VIs were combined under a random forest (RF) classifier. The five VIs were: normalized difference vegetation index (NDVI), green NDVI (GNDVI), soil-adjusted VI (SAVI), green–red VI (GRVI) and enhanced VI (EVI), using data obtained from Landsat TM (30 m resolution) and Sentinel-2A (10 m) via Google Earth Engine. A Moran's I analysis showed a random pattern of training samples of LULC classes for integrated RF classification. The classification accuracy was evaluated using the reference point data with multiple comparisons of VIs and κ coefficient. The area was classified into eight LULC classes. Although the respective VIs depicted unique characteristics, the accuracies of them were less than 89 %. The integrated classifier improved the accuracy of 96–97 % with κ of 0.96 in 1988 and 0.97 in 2020. The VI-integrated RF was the most effective classifier for LULC, particularly for separating mangroves from homestead vegetation. The changes for 32 years showed that the dense mangrove occupied 42 % of surveyed area in 1988 but declined ca 622 km2 in 2020, whereas sparse mangrove and aquaculture increased by 487 km2 and 464 km2, respectively, out of the total area of 8,360 km2. The changes in LULC classes were mostly from dense mangroves to sparse mangroves, sparse mangroves to dense mangroves and from cropland to bare land and aquaculture. Inaccessible mangrove degradation should result from natural disturbances, i.e., tropical cyclones, while areas near forest boundaries were distorted by anthropogenic disturbances such as aquaculture development. The restoration of habitat quality, mangrove afforestation and sustainable aquaculture practices were effective indicators for improving the mangrove ecosystems. Since natural and anthropogenic disturbances induce substantial loss of mangrove vegetation, the VI-integrated RF classifier should be applied for monitoring LULC and for developing the sustainable management strategies. © 2024;"Disturbances; Land use and land cover (LULC) change; Mangrove; Random forest; Satellite imagery; Vegetation indices";"Bangladesh; Conservation; Land use; Reforestation; Remote sensing; Satellite imagery; Storms; Vegetation; Wetlands; Anthropogenic disturbance; Change in land use; Coastal wetlands; Disturbance; Land use and land cover; Land use and land cover change; Mangrove; Natural disturbance; Random forests; Vegetation index; afforestation; aquaculture production; coastal wetland; land cover; land use change; Landsat thematic mapper; mangrove; NDVI; satellite imagery; Aquaculture";"Effects of disturbances on the spatiotemporal patterns and dynamics of coastal wetland vegetation The coastal wetlands represented by mangroves are vulnerable to disturbances. To clarify the effects of disturbances quantitatively on mangroves, spatiotemporal changes in land use and land cover (LULC) were compared between the inside and outside of mangroves during 1988 and 2020 in Bangladesh by remote sensing. The mangrove region, which had diverse surface greenness due to natural and anthropogenic disturbances, were surveyed. Changes in greenness were estimated by five vegetation indices (VIs). To increase in the accuracy of land use classification, the five VIs were combined under a random forest (RF) classifier. The five VIs were: normalized difference vegetation index (NDVI), green NDVI (GNDVI), soil-adjusted VI (SAVI), green–red VI (GRVI) and enhanced VI (EVI), using data obtained from Landsat TM (30 m resolution) and Sentinel-2A (10 m) via Google Earth Engine. A Moran's I analysis showed a random pattern of training samples of LULC classes for integrated RF classification. The classification accuracy was evaluated using the reference point data with multiple comparisons of VIs and κ coefficient. The area was classified into eight LULC classes. Although the respective VIs depicted unique characteristics, the accuracies of them were less than 89 %. The integrated classifier improved the accuracy of 96–97 % with κ of 0.96 in 1988 and 0.97 in 2020. The VI-integrated RF was the most effective classifier for LULC, particularly for separating mangroves from homestead vegetation. The changes for 32 years showed that the dense mangrove occupied 42 % of surveyed area in 1988 but declined ca 622 km2 in 2020, whereas sparse mangrove and aquaculture increased by 487 km2 and 464 km2, respectively, out of the total area of 8,360 km2. The changes in LULC classes were mostly from dense mangroves to sparse mangroves, sparse mangroves to dense mangroves and from cropland to bare land and aquaculture. Inaccessible mangrove degradation should result from natural disturbances, i.e., tropical cyclones, while areas near forest boundaries were distorted by anthropogenic disturbances such as aquaculture development. The restoration of habitat quality, mangrove afforestation and sustainable aquaculture practices were effective indicators for improving the mangrove ecosystems. Since natural and anthropogenic disturbances induce substantial loss of mangrove vegetation, the VI-integrated RF classifier should be applied for monitoring LULC and for developing the sustainable management strategies. © 2024 Disturbances; Land use and land cover (LULC) change; Mangrove; Random forest; Satellite imagery; Vegetation indices Bangladesh; Conservation; Land use; Reforestation; Remote sensing; Satellite imagery; Storms; Vegetation; Wetlands; Anthropogenic disturbance; Change in land use; Coastal wetlands; Disturbance; Land use and land cover; Land use and land cover change; Mangrove; Natural disturbance; Random forests; Vegetation index; afforestation; aquaculture production; coastal wetland; land cover; land use change; Landsat thematic mapper; mangrove; NDVI; satellite imagery; Aquaculture";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
247;Multi-fidelity deep neural network with Monte Carlo dropout technique for uncertainty-aware risk recognition of backward erosion piping in dikes;Backward erosion piping (BEP) is an increasingly critical failure mechanism in dike systems, often triggered by floods resulting from extreme rainfall events, which are exacerbated by the ongoing shifts in climate patterns. This study embarks on a nuanced examination of BEP, a phenomenon that substantially undermines flood control and emergency management endeavors. Addressing the complexities of BEP incidents and the consequential variable impact on dike infrastructure, we have incorporated Monte Carlo dropout techniques within a multi-fidelity deep learning framework to enhance predictive accuracy and provide an uncertainty-aware assessment of BEP risk. Drawing on 16 sets of physical model experiments emulating dike structures from the lower Yangtze River basin in China, we conducted a detailed study of the BEP evolution mechanism under various hydrological scenarios intensified by climate change. These experiments allowed us to observe the initiation and progression of BEP in different conditions. The development of a structural equation model quantifies the effects of several critical factors—including those exacerbated by climatic variability—on BEP dynamics. Seven key factors were identified as influential to BEP risk levels, integrating distinct BEP traits, hydrological attributes, and dike engineering conditions. A Monte Carlo dropout-enhanced multi-fidelity deep neural network (MFDNN) was crafted, synthesizing low-fidelity experimental data with high-fidelity field case studies, to construct an advanced model for BEP risk level identification. Compared against four sophisticated machine learning models, our MFDNN demonstrated superior performance, effectively blending experimental insights with real-world occurrences. The proposed model emerges as a scientifically robust and pragmatic tool for delineating BEP risk levels in dike systems, providing vital guidance for categorizing BEP incidents and forging targeted, climate-informed emergency response strategies. © 2024 Elsevier B.V.;"Backward erosion piping (BEP); Criticality; Data science; Dike; Multi-fidelity deep neural network (MFDNN); Risk level";"Deep neural networks; Emergency services; Flood control; Risk assessment; Risk management; Risk perception; Backward erosion piping; Condition; Criticality; Dike system; Multi fidelities; Multi-fidelity deep neural network; Neural-networks; Piping risk level; Risk levels; Uncertainty";"Multi-fidelity deep neural network with Monte Carlo dropout technique for uncertainty-aware risk recognition of backward erosion piping in dikes Backward erosion piping (BEP) is an increasingly critical failure mechanism in dike systems, often triggered by floods resulting from extreme rainfall events, which are exacerbated by the ongoing shifts in climate patterns. This study embarks on a nuanced examination of BEP, a phenomenon that substantially undermines flood control and emergency management endeavors. Addressing the complexities of BEP incidents and the consequential variable impact on dike infrastructure, we have incorporated Monte Carlo dropout techniques within a multi-fidelity deep learning framework to enhance predictive accuracy and provide an uncertainty-aware assessment of BEP risk. Drawing on 16 sets of physical model experiments emulating dike structures from the lower Yangtze River basin in China, we conducted a detailed study of the BEP evolution mechanism under various hydrological scenarios intensified by climate change. These experiments allowed us to observe the initiation and progression of BEP in different conditions. The development of a structural equation model quantifies the effects of several critical factors—including those exacerbated by climatic variability—on BEP dynamics. Seven key factors were identified as influential to BEP risk levels, integrating distinct BEP traits, hydrological attributes, and dike engineering conditions. A Monte Carlo dropout-enhanced multi-fidelity deep neural network (MFDNN) was crafted, synthesizing low-fidelity experimental data with high-fidelity field case studies, to construct an advanced model for BEP risk level identification. Compared against four sophisticated machine learning models, our MFDNN demonstrated superior performance, effectively blending experimental insights with real-world occurrences. The proposed model emerges as a scientifically robust and pragmatic tool for delineating BEP risk levels in dike systems, providing vital guidance for categorizing BEP incidents and forging targeted, climate-informed emergency response strategies. © 2024 Elsevier B.V. Backward erosion piping (BEP); Criticality; Data science; Dike; Multi-fidelity deep neural network (MFDNN); Risk level Deep neural networks; Emergency services; Flood control; Risk assessment; Risk management; Risk perception; Backward erosion piping; Condition; Criticality; Dike system; Multi fidelities; Multi-fidelity deep neural network; Neural-networks; Piping risk level; Risk levels; Uncertainty";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
248;Diagnosis of ecological security and the spatial heterogeneity of its driving factors in the mining-impacted watershed, based on ecosystem health-risk-services framework;A comprehensive diagnosis of ecological security (ES) and its driving mechanisms in the watershed under mining influence is essential for the conservation and restoration of watershed ecosystems. Few studies have comprehensively evaluated ES by considering the condition of the ecosystem itself, the ecological function it provides, and the risk it faces. Therefore, by innovatively synthesizing ecosystem health (EH), ecological risk (ER), and ecosystem services (ESs), an ES evaluation framework based on EH-ER-ESs was constructed. Based on quantifying the ES of a typical watershed with mines clustered in China's Northern Sand Prevention Belt, a spatial correlation analysis was used to elucidate the spatial differentiation characteristic of ES and verify the necessity of the evaluation framework. The driving mechanism of ES and the spatial heterogeneity of its key driving factors were explored using the Geodetector model and the geographically-weighted regression model. The results show that (1) ES was generally medium-security, and very low-security areas (8.16%) were mainly concentrated in the eastern mining aggregation areas. (2) ES showed high-high and low-low spatial clusters. Overall, ES was positively correlated with EH and ESs, while negatively correlated with ER, but there were individual cases of “high health-low security” and “high services-low security”, and combining EH, ER, and ESs to diagnose ES comprehensively was necessary. (3) Drought index, vegetation cover, distance from mining land, population density, and CONTAG were the key driving factors of ES. The explanatory power of factor interactions was higher than that of single factors. The impact of driving factors showed significant spatial heterogeneity, with the effects of mine agglomeration on ES primarily concentrated in the east. The EH-ER-ESs framework can be used for ES evaluation in other ecosystems, and the findings provide important guidance for conducting integrated watershed management. © 2024;"Driving mechanisms; Ecological security; Geodetector model; Sustainable ecosystems; Watershed";"China; Driving factors; Driving mechanism; Ecological risks; Ecological security; Ecosystem health; Ecosystem services; Geodetector model; Service framework; Spatial heterogeneity; Sustainable ecosystems; ecosystem health; environmental factor; environmental modeling; health risk; heterogeneity; mining; security; spatial analysis; sustainability; watershed; Anthropogenic";"Diagnosis of ecological security and the spatial heterogeneity of its driving factors in the mining-impacted watershed, based on ecosystem health-risk-services framework A comprehensive diagnosis of ecological security (ES) and its driving mechanisms in the watershed under mining influence is essential for the conservation and restoration of watershed ecosystems. Few studies have comprehensively evaluated ES by considering the condition of the ecosystem itself, the ecological function it provides, and the risk it faces. Therefore, by innovatively synthesizing ecosystem health (EH), ecological risk (ER), and ecosystem services (ESs), an ES evaluation framework based on EH-ER-ESs was constructed. Based on quantifying the ES of a typical watershed with mines clustered in China's Northern Sand Prevention Belt, a spatial correlation analysis was used to elucidate the spatial differentiation characteristic of ES and verify the necessity of the evaluation framework. The driving mechanism of ES and the spatial heterogeneity of its key driving factors were explored using the Geodetector model and the geographically-weighted regression model. The results show that (1) ES was generally medium-security, and very low-security areas (8.16%) were mainly concentrated in the eastern mining aggregation areas. (2) ES showed high-high and low-low spatial clusters. Overall, ES was positively correlated with EH and ESs, while negatively correlated with ER, but there were individual cases of “high health-low security” and “high services-low security”, and combining EH, ER, and ESs to diagnose ES comprehensively was necessary. (3) Drought index, vegetation cover, distance from mining land, population density, and CONTAG were the key driving factors of ES. The explanatory power of factor interactions was higher than that of single factors. The impact of driving factors showed significant spatial heterogeneity, with the effects of mine agglomeration on ES primarily concentrated in the east. The EH-ER-ESs framework can be used for ES evaluation in other ecosystems, and the findings provide important guidance for conducting integrated watershed management. © 2024 Driving mechanisms; Ecological security; Geodetector model; Sustainable ecosystems; Watershed China; Driving factors; Driving mechanism; Ecological risks; Ecological security; Ecosystem health; Ecosystem services; Geodetector model; Service framework; Spatial heterogeneity; Sustainable ecosystems; ecosystem health; environmental factor; environmental modeling; health risk; heterogeneity; mining; security; spatial analysis; sustainability; watershed; Anthropogenic";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
249;Dynamic Approach to Update Utility and Choice by Emerging Technologies to Reduce Risk in Urban Road Transportation Systems;"International research attention on evacuation issues has increased significantly following the human and natural disasters at the turn of the century, such as 9/11, Hurricane Katrina, Cyclones Idai and Kenneth, the Black Saturday forest fires and tsunamis in Japan. The main problem concerning when a disaster can occur involves studying the risk reduction. Risk, following all the theoretical and experimental studies, is determined by the product of three components: occurrence, vulnerability and exposure. Vulnerability can be improved over time through major infrastructure actions, but absolute security cannot be achieved. When the event will occur with certainty, only exposure remains to reduce the risk to people before the effect hits them. Exposure can be improved, under fixed conditions of occurrence and vulnerability, by improving evacuation. The main problem in terms of evacuating the population from an area is the available transport system, which must be used to its fullest. So, if the system is well managed, the evacuation improves (shorter times), meaning the exposure is reduced, and therefore, the risk is reduced. A key factor in the analysis of transport systems under emergency conditions is the behavior of the user, and therefore, the study of demand. This work identifies the main research lines that are useful for studying demand under exposure-related risk conditions. The classification of demand models that simulate evacuation conditions in relation to the effect on the transportation system is summarized. The contribution proposes a model for updating choice in relation to emergency conditions and utility. The contribution of emerging ICTs to actualization is formally introduced into the models. Intelligent technologies make it possible to improve user decisions, reducing exposure and therefore risk. The proposed model moves within the two approaches of the literature: it is an inter-period dynamic model with the probability expressed within the discrete choice theory; furthermore, it is a sequential dynamic model with the probability dependent on the previous choices. The contribution presents an example of application of the model, developing a transition matrix considering the case of choice updating under two extreme conditions. © 2024 by the authors.";"dynamic demand models; emerging ICT; evacuation; risk reduction; smart city";NULL;"Dynamic Approach to Update Utility and Choice by Emerging Technologies to Reduce Risk in Urban Road Transportation Systems International research attention on evacuation issues has increased significantly following the human and natural disasters at the turn of the century, such as 9/11, Hurricane Katrina, Cyclones Idai and Kenneth, the Black Saturday forest fires and tsunamis in Japan. The main problem concerning when a disaster can occur involves studying the risk reduction. Risk, following all the theoretical and experimental studies, is determined by the product of three components: occurrence, vulnerability and exposure. Vulnerability can be improved over time through major infrastructure actions, but absolute security cannot be achieved. When the event will occur with certainty, only exposure remains to reduce the risk to people before the effect hits them. Exposure can be improved, under fixed conditions of occurrence and vulnerability, by improving evacuation. The main problem in terms of evacuating the population from an area is the available transport system, which must be used to its fullest. So, if the system is well managed, the evacuation improves (shorter times), meaning the exposure is reduced, and therefore, the risk is reduced. A key factor in the analysis of transport systems under emergency conditions is the behavior of the user, and therefore, the study of demand. This work identifies the main research lines that are useful for studying demand under exposure-related risk conditions. The classification of demand models that simulate evacuation conditions in relation to the effect on the transportation system is summarized. The contribution proposes a model for updating choice in relation to emergency conditions and utility. The contribution of emerging ICTs to actualization is formally introduced into the models. Intelligent technologies make it possible to improve user decisions, reducing exposure and therefore risk. The proposed model moves within the two approaches of the literature: it is an inter-period dynamic model with the probability expressed within the discrete choice theory; furthermore, it is a sequential dynamic model with the probability dependent on the previous choices. The contribution presents an example of application of the model, developing a transition matrix considering the case of choice updating under two extreme conditions. © 2024 by the authors. dynamic demand models; emerging ICT; evacuation; risk reduction; smart city NULL";-1;Não Classificado;NULL;-1;NULL;2;Preparation
250;Coordinated restoration of interdependent critical infrastructures: A novel distributed decision-making mechanism integrating optimization and reinforcement learning;"The proper functioning of any society heavily depends on its critical infrastructures (CIs), such as power grids, road networks, and water and waste-water systems. These infrastructures consist of facilities spread across a community to provide essential services to its residents. Their spatial expansion and functional interdependencies make them highly vulnerable against natural/manmade disasters. Functional interdependencies mean that the functionality of components in one CI relies on the services provided by others. These features, combined with decentralized decision-making structure of CIs and the stochastic nature of post-disaster environments, highly complicate the optimization process for restoring CIs damaged in disasters. Optimizing CI restorations is critical to maximizing the post-disaster resilience of communities. In this paper, we integrate and leverage Reinforcement Learning (RL) and optimization strengths to design a novel distributed modeling and solution approach for advancing the restoration process for interdependent CIs after disasters. The proposed approach (1) bridges the gap between integrative and distinct decision-making, enabling coordinated restoration planning for CIs within a decentralized decision-making context; (2) handles post-disaster uncertainties (e.g., uncertainty in recovery times of disrupted components); (3) generates adaptive solutions that cope with post-disaster dynamics (e.g., varying numbers of recovery teams); and (4) is flexible enough to handle several restoration decisions (e.g., restoration scheduling and resource allocation) simultaneously. To evaluate its performance, we focus on restoring the road and power CIs in Sioux Falls, South Dakota, disrupted by several tornado scenarios. The numerical results show that coordinated policies in the restoration process of interdependent CIs consistently yield higher service for the community. The overperformance of the coordinated restoration policies can be as high as 27.9 %. The impact of coordination is more significant in severe disasters with higher disruptions and in the absence of efficient recovery resources. © 2024";"Disaster management; Interdependent infrastructures; Reinforcement learning; Resource allocation; Restoration scheduling";"Highway administration; Resource allocation; Restoration; Stochastic programming; Decentralized decision-making; Disaster management; Functionals; Interdependent infrastructures; Optimisations; Post disasters; Reinforcement learnings; Resources allocation; Restoration process; Restoration scheduling; decision making; disaster management; infrastructure; optimization; resource allocation; Stochastic systems";"Coordinated restoration of interdependent critical infrastructures: A novel distributed decision-making mechanism integrating optimization and reinforcement learning The proper functioning of any society heavily depends on its critical infrastructures (CIs), such as power grids, road networks, and water and waste-water systems. These infrastructures consist of facilities spread across a community to provide essential services to its residents. Their spatial expansion and functional interdependencies make them highly vulnerable against natural/manmade disasters. Functional interdependencies mean that the functionality of components in one CI relies on the services provided by others. These features, combined with decentralized decision-making structure of CIs and the stochastic nature of post-disaster environments, highly complicate the optimization process for restoring CIs damaged in disasters. Optimizing CI restorations is critical to maximizing the post-disaster resilience of communities. In this paper, we integrate and leverage Reinforcement Learning (RL) and optimization strengths to design a novel distributed modeling and solution approach for advancing the restoration process for interdependent CIs after disasters. The proposed approach (1) bridges the gap between integrative and distinct decision-making, enabling coordinated restoration planning for CIs within a decentralized decision-making context; (2) handles post-disaster uncertainties (e.g., uncertainty in recovery times of disrupted components); (3) generates adaptive solutions that cope with post-disaster dynamics (e.g., varying numbers of recovery teams); and (4) is flexible enough to handle several restoration decisions (e.g., restoration scheduling and resource allocation) simultaneously. To evaluate its performance, we focus on restoring the road and power CIs in Sioux Falls, South Dakota, disrupted by several tornado scenarios. The numerical results show that coordinated policies in the restoration process of interdependent CIs consistently yield higher service for the community. The overperformance of the coordinated restoration policies can be as high as 27.9 %. The impact of coordination is more significant in severe disasters with higher disruptions and in the absence of efficient recovery resources. © 2024 Disaster management; Interdependent infrastructures; Reinforcement learning; Resource allocation; Restoration scheduling Highway administration; Resource allocation; Restoration; Stochastic programming; Decentralized decision-making; Disaster management; Functionals; Interdependent infrastructures; Optimisations; Post disasters; Reinforcement learnings; Resources allocation; Restoration process; Restoration scheduling; decision making; disaster management; infrastructure; optimization; resource allocation; Stochastic systems";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.3;Meteorological;4;Recovery
251;Towards equitable infrastructure asset management: Scour maintenance strategy for aging bridge systems in flood-prone zones using deep reinforcement learning;"Bridges play a critical role in transportation networks; however, they are vulnerable to deterioration, aging, and degradation, especially in the face of climate change and extreme weather events such as floodings. Furthermore, bridges can significantly affect social vulnerability; their damage or destruction can isolate communities, inhibit emergency responses, and disrupt essential services. Maintaining critical bridges in a cost-effective and sustainable manner is crucial to ensure their longevity and protect vulnerable communities. To address the maintenance optimization problem of bridge systems considering the effects of time deterioration, flood degradation, and social vulnerability, this study proposes a deep reinforcement learning algorithm to optimally allocate resources to bridges that are at expected cost of failure due to scour. The algorithm considers the effects of flood degradation with different return periods and is trained using a Markov Decision Process as the environment. The study conducts four flood simulation scenarios using Geographic Information System data. The findings suggest that the deep reinforcement learning algorithm proposes a sequence of repair actions that outperforms the status quo, currently employed by bridge managers. The significance of this study lies in its valuable insights for cities worldwide on how to effectively optimize their limited resources for the maintenance and rehabilitation of critical infrastructure systems to decrease portfolio cost and increase social equity. © 2024 Elsevier Ltd";"Deep reinforcement learning; Equitable infrastructure management; Flood risk management; Geographic information system; Social vulnerability";"Asset management; Information management; Markov processes; Reinforcement learning; Risk management; Bridge systems; Equitable infrastructure management; Flood risk management; Geographic information; Infrastructure asset management; Infrastructure managements; Maintenance strategies; Reinforcement learning algorithms; Reinforcement learnings; Social vulnerability; bridge; flood; GIS; infrastructure; machine learning; maintenance; management practice; scour; vulnerability; Deep reinforcement learning";"Towards equitable infrastructure asset management: Scour maintenance strategy for aging bridge systems in flood-prone zones using deep reinforcement learning Bridges play a critical role in transportation networks; however, they are vulnerable to deterioration, aging, and degradation, especially in the face of climate change and extreme weather events such as floodings. Furthermore, bridges can significantly affect social vulnerability; their damage or destruction can isolate communities, inhibit emergency responses, and disrupt essential services. Maintaining critical bridges in a cost-effective and sustainable manner is crucial to ensure their longevity and protect vulnerable communities. To address the maintenance optimization problem of bridge systems considering the effects of time deterioration, flood degradation, and social vulnerability, this study proposes a deep reinforcement learning algorithm to optimally allocate resources to bridges that are at expected cost of failure due to scour. The algorithm considers the effects of flood degradation with different return periods and is trained using a Markov Decision Process as the environment. The study conducts four flood simulation scenarios using Geographic Information System data. The findings suggest that the deep reinforcement learning algorithm proposes a sequence of repair actions that outperforms the status quo, currently employed by bridge managers. The significance of this study lies in its valuable insights for cities worldwide on how to effectively optimize their limited resources for the maintenance and rehabilitation of critical infrastructure systems to decrease portfolio cost and increase social equity. © 2024 Elsevier Ltd Deep reinforcement learning; Equitable infrastructure management; Flood risk management; Geographic information system; Social vulnerability Asset management; Information management; Markov processes; Reinforcement learning; Risk management; Bridge systems; Equitable infrastructure management; Flood risk management; Geographic information; Infrastructure asset management; Infrastructure managements; Maintenance strategies; Reinforcement learning algorithms; Reinforcement learnings; Social vulnerability; bridge; flood; GIS; infrastructure; machine learning; maintenance; management practice; scour; vulnerability; Deep reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.2;Hydrological;1;Prevention
252;Prediction Equations for Peak-Ground Accelerations and Velocities in Northeast Japan Using the S-net Data;S-net is a seafloor observation network for earthquakes and tsunamis around the Japan Trench, com-prising 150 observatories with seismometers and pressure gauges. The region has been known to experience massive earthquakes, and several magnitude 6 and 7 class earthquakes have occurred after the network was established in 2016. This study constructed ground motion prediction equations (GMPEs) for horizontal peak ground accelerations (PGAs) and peak ground velocities (PGVs) using the S-net data and revealed that the GMPEs can be used to predict the PGAs and PGVs at the land stations where measured S-wave velocities are available. We used a relatively short time window of the S-net records from the viewpoint of earthquake early warning but included S waves. Data from earthquakes of magnitudes between Mw 5.5 and Mw 7.4 were used. The construction of the GMPEs was achieved in two steps. First, regression analysis was conducted for each event data, and mean site residual was obtained over the available records at each S-net site. Second, the data were adjusted by the mean site residuals, and stratified regression analysis, which decouples the source and path factors, was performed. Finally, we applied the GMPEs to predict PGAs and PGVs at the KiK-net sites on land. We determined that the residuals at the KiK-net sites were systematically biased with Vs30 (average S-wave velocity in the upper 30 m). We obtained correction factors for the bias and demonstrated that the PGAs and PGVs at the KiK-net sites could be predicted reasonably well. © Fuji Technology Press Ltd.;"earthquake early warning; Japan Trench; ocean-bottom seismograph; peak ground acceleration; peak ground velocity";"Earthquakes; Geochronology; Gravity waves; Pressure gages; Rayleigh waves; Seismic waves; Ultrasonic waves; Earthquake early warning; Ground motion prediction; Japan trench; Observation networks; Ocean bottom seismographs; Peak ground acceleration; Peak ground velocity; Prediction equations; S-wave velocity; Seafloors; Shear waves";"Prediction Equations for Peak-Ground Accelerations and Velocities in Northeast Japan Using the S-net Data S-net is a seafloor observation network for earthquakes and tsunamis around the Japan Trench, com-prising 150 observatories with seismometers and pressure gauges. The region has been known to experience massive earthquakes, and several magnitude 6 and 7 class earthquakes have occurred after the network was established in 2016. This study constructed ground motion prediction equations (GMPEs) for horizontal peak ground accelerations (PGAs) and peak ground velocities (PGVs) using the S-net data and revealed that the GMPEs can be used to predict the PGAs and PGVs at the land stations where measured S-wave velocities are available. We used a relatively short time window of the S-net records from the viewpoint of earthquake early warning but included S waves. Data from earthquakes of magnitudes between Mw 5.5 and Mw 7.4 were used. The construction of the GMPEs was achieved in two steps. First, regression analysis was conducted for each event data, and mean site residual was obtained over the available records at each S-net site. Second, the data were adjusted by the mean site residuals, and stratified regression analysis, which decouples the source and path factors, was performed. Finally, we applied the GMPEs to predict PGAs and PGVs at the KiK-net sites on land. We determined that the residuals at the KiK-net sites were systematically biased with Vs30 (average S-wave velocity in the upper 30 m). We obtained correction factors for the bias and demonstrated that the PGAs and PGVs at the KiK-net sites could be predicted reasonably well. © Fuji Technology Press Ltd. earthquake early warning; Japan Trench; ocean-bottom seismograph; peak ground acceleration; peak ground velocity Earthquakes; Geochronology; Gravity waves; Pressure gages; Rayleigh waves; Seismic waves; Ultrasonic waves; Earthquake early warning; Ground motion prediction; Japan trench; Observation networks; Ocean bottom seismographs; Peak ground acceleration; Peak ground velocity; Prediction equations; S-wave velocity; Seafloors; Shear waves";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
253;Semi-Supervised Building Extraction with Optical Flow Correction Based on Satellite Video Data in a Tsunami-Induced Disaster Scene;Data and reports indicate an increasing frequency and intensity of natural disasters worldwide. Buildings play a crucial role in disaster responses and damage assessments, aiding in planning rescue efforts and evaluating losses. Despite advances in applying deep learning to building extraction, challenges remain in handling complex natural disaster scenes and reducing reliance on labeled datasets. Recent advances in satellite video are opening a new avenue for efficient and accurate building extraction research. By thoroughly mining the characteristics of disaster video data, this work provides a new semantic segmentation model for accurate and efficient building extraction based on a limited number of training data, which consists of two parts: the prediction module and the automatic correction module. The prediction module, based on a base encoder–decoder structure, initially extracts buildings using a limited amount of training data that are obtained instantly. Then, the automatic correction module takes the output of the prediction module as input, constructs a criterion for identifying pixels with erroneous semantic information, and uses optical flow values to extract the accurate corresponding semantic information on the corrected frame. The experimental results demonstrate that the proposed method outperforms other methods in accuracy and computational complexity in complicated natural disaster scenes. © 2024 by the authors.;"building extraction; disaster; optical flow; satellite video data; semi-supervised";"Network security; Optical flows; Semi-supervised learning; Automatic corrections; Building extraction; Disaster scenes; Natural disasters; Optical-; Satellite video data; Semantics Information; Semi-supervised; Training data; Video data; article; building; controlled study; deep learning; disaster; disaster response; female; human; mining; natural disaster; optic flow; prediction; tsunami; videorecording; Semantic Segmentation";"Semi-Supervised Building Extraction with Optical Flow Correction Based on Satellite Video Data in a Tsunami-Induced Disaster Scene Data and reports indicate an increasing frequency and intensity of natural disasters worldwide. Buildings play a crucial role in disaster responses and damage assessments, aiding in planning rescue efforts and evaluating losses. Despite advances in applying deep learning to building extraction, challenges remain in handling complex natural disaster scenes and reducing reliance on labeled datasets. Recent advances in satellite video are opening a new avenue for efficient and accurate building extraction research. By thoroughly mining the characteristics of disaster video data, this work provides a new semantic segmentation model for accurate and efficient building extraction based on a limited number of training data, which consists of two parts: the prediction module and the automatic correction module. The prediction module, based on a base encoder–decoder structure, initially extracts buildings using a limited amount of training data that are obtained instantly. Then, the automatic correction module takes the output of the prediction module as input, constructs a criterion for identifying pixels with erroneous semantic information, and uses optical flow values to extract the accurate corresponding semantic information on the corrected frame. The experimental results demonstrate that the proposed method outperforms other methods in accuracy and computational complexity in complicated natural disaster scenes. © 2024 by the authors. building extraction; disaster; optical flow; satellite video data; semi-supervised Network security; Optical flows; Semi-supervised learning; Automatic corrections; Building extraction; Disaster scenes; Natural disasters; Optical-; Satellite video data; Semantics Information; Semi-supervised; Training data; Video data; article; building; controlled study; deep learning; disaster; disaster response; female; human; mining; natural disaster; optic flow; prediction; tsunami; videorecording; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
254;Optimizing Fire Scene Analysis: Hybrid Convolutional Neural Network Model Leveraging Multiscale Feature and Attention Mechanisms;The rapid and accurate detection of fire scenes in various environments is crucial for effective disaster management and mitigation. Fire scene classification is a critical aspect of modern fire detection systems that directly affects public safety and property preservation. This research introduced a novel hybrid deep learning model designed to enhance the accuracy and efficiency of fire scene classification across diverse environments. The proposed model integrates advanced convolutional neural networks with multiscale feature extraction, attention mechanisms, and ensemble learning to achieve superior performance in real-time fire detection. By leveraging the strengths of pre-trained networks such as ResNet50, VGG16, and EfficientNet-B3, the model captures detailed features at multiple scales, ensuring robust detection capabilities. Including spatial and channel attention mechanisms further refines the focus on critical areas within the input images, reducing false positives and improving detection precision. Extensive experiments on a comprehensive dataset encompassing wildfires, building fires, vehicle fires, and non-fire scenes demonstrate that the proposed framework outperforms existing cutting-edge techniques. The model also exhibited reduced computational complexity and enhanced inference speed, making it suitable for deployment in real-time applications on various hardware platforms. This study sets a new benchmark for fire detection and offers a powerful tool for early warning systems and emergency response initiatives. © 2024 by the authors.;"attention mechanisms; deep learning; ensemble learning; fire detection; multiscale feature extraction; real-time image processing";NULL;"Optimizing Fire Scene Analysis: Hybrid Convolutional Neural Network Model Leveraging Multiscale Feature and Attention Mechanisms The rapid and accurate detection of fire scenes in various environments is crucial for effective disaster management and mitigation. Fire scene classification is a critical aspect of modern fire detection systems that directly affects public safety and property preservation. This research introduced a novel hybrid deep learning model designed to enhance the accuracy and efficiency of fire scene classification across diverse environments. The proposed model integrates advanced convolutional neural networks with multiscale feature extraction, attention mechanisms, and ensemble learning to achieve superior performance in real-time fire detection. By leveraging the strengths of pre-trained networks such as ResNet50, VGG16, and EfficientNet-B3, the model captures detailed features at multiple scales, ensuring robust detection capabilities. Including spatial and channel attention mechanisms further refines the focus on critical areas within the input images, reducing false positives and improving detection precision. Extensive experiments on a comprehensive dataset encompassing wildfires, building fires, vehicle fires, and non-fire scenes demonstrate that the proposed framework outperforms existing cutting-edge techniques. The model also exhibited reduced computational complexity and enhanced inference speed, making it suitable for deployment in real-time applications on various hardware platforms. This study sets a new benchmark for fire detection and offers a powerful tool for early warning systems and emergency response initiatives. © 2024 by the authors. attention mechanisms; deep learning; ensemble learning; fire detection; multiscale feature extraction; real-time image processing NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
255;Combining Drone LiDAR and Virtual Reality Geovisualizations towards a Cartographic Approach to Visualize Flooding Scenarios;This study aims to create virtual reality (VR) geovisualizations using 3D point clouds obtained from airborne LiDAR technology. These visualizations were used to map the current state of river channels and tributaries in the Thessalian Plain, Greece, following severe flooding in the summer of 2023. The study area examined in this paper is the embankments enclosing the tributaries of the Pineios River in the Thessalian Plain region, specifically between the cities of Karditsa and Trikala in mainland Greece. This area was significantly affected in the summer of 2023 when flooding the region’s rivers destroyed urban elements and crops. The extent of the impact across the entire Thessalian Plain made managing the event highly challenging to the authorities. High-resolution 3D mapping and VR geovisualization of the embarkments encasing the main rivers and the tributaries of the Thessalian Plain essentially provides information for planning the area’s restoration processes and designing prevention and mitigation measures for similar disasters. The proposed methodology consists of four stages. The first and second stages of the methodology present the design of the data acquisition process with airborne LiDAR, aiming at the high-resolution 3D mapping of the sites. The third stage focuses on data processing, cloud point classification, and thematic information creation. The fourth stage is focused on developing the VR application. The VR application will allow users to immerse themselves in the study area, observe, and interact with the existing state of the embankments in high resolution. Additionally, users can interact with the 3D point cloud, where thematic information is displayed describing the classification of the 3D cloud, the altitude, and the RGB color. Additional thematic information in vector form, providing qualitative characteristics, is also illustrated in the virtual space. Furthermore, six different scenarios were visualized in the 3D space using a VR app. Visualizing these 3D scenarios using digital twins of the current antiflood infrastructure provides scenarios of floods at varying water levels. This study aims to explore the efficient visualization of thematic information in 3D virtual space. The goal is to provide an innovative VR tool for managing the impact on anthropogenic infrastructures, livestock, and the ecological capital of various scenarios of a catastrophic flood. © 2024 by the authors.;"flooding; geovisualization; LiDAR; UAS; virtual reality";"Digital storage; Drones; Metadata; Photomapping; 'current; 3D point cloud; Airborne LiDAR; Floodings; Geovisualization; High resolution; LiDAR; Study areas; Thematic information; UAS; Visualization";"Combining Drone LiDAR and Virtual Reality Geovisualizations towards a Cartographic Approach to Visualize Flooding Scenarios This study aims to create virtual reality (VR) geovisualizations using 3D point clouds obtained from airborne LiDAR technology. These visualizations were used to map the current state of river channels and tributaries in the Thessalian Plain, Greece, following severe flooding in the summer of 2023. The study area examined in this paper is the embankments enclosing the tributaries of the Pineios River in the Thessalian Plain region, specifically between the cities of Karditsa and Trikala in mainland Greece. This area was significantly affected in the summer of 2023 when flooding the region’s rivers destroyed urban elements and crops. The extent of the impact across the entire Thessalian Plain made managing the event highly challenging to the authorities. High-resolution 3D mapping and VR geovisualization of the embarkments encasing the main rivers and the tributaries of the Thessalian Plain essentially provides information for planning the area’s restoration processes and designing prevention and mitigation measures for similar disasters. The proposed methodology consists of four stages. The first and second stages of the methodology present the design of the data acquisition process with airborne LiDAR, aiming at the high-resolution 3D mapping of the sites. The third stage focuses on data processing, cloud point classification, and thematic information creation. The fourth stage is focused on developing the VR application. The VR application will allow users to immerse themselves in the study area, observe, and interact with the existing state of the embankments in high resolution. Additionally, users can interact with the 3D point cloud, where thematic information is displayed describing the classification of the 3D cloud, the altitude, and the RGB color. Additional thematic information in vector form, providing qualitative characteristics, is also illustrated in the virtual space. Furthermore, six different scenarios were visualized in the 3D space using a VR app. Visualizing these 3D scenarios using digital twins of the current antiflood infrastructure provides scenarios of floods at varying water levels. This study aims to explore the efficient visualization of thematic information in 3D virtual space. The goal is to provide an innovative VR tool for managing the impact on anthropogenic infrastructures, livestock, and the ecological capital of various scenarios of a catastrophic flood. © 2024 by the authors. flooding; geovisualization; LiDAR; UAS; virtual reality Digital storage; Drones; Metadata; Photomapping; 'current; 3D point cloud; Airborne LiDAR; Floodings; Geovisualization; High resolution; LiDAR; Study areas; Thematic information; UAS; Visualization";-1;Não Classificado;NULL;1.2;Hydrological;4;Recovery
256;Identifying probabilistic weather regimes targeted to a local-scale impact variable;Large-scale atmospheric circulation patterns, so-called weather regimes, modulate the occurrence of extreme events such as heatwaves or extreme precipitation. In their role as mediators between long-range teleconnections and local impacts, weather regimes have demonstrated potential in improving long-Term climate projections as well as sub-seasonal to seasonal forecasts. However, existing methods for identifying weather regimes are not specifically designed to capture the relevant physical processes responsible for variations in the impact variable in question. This paper introduces a novel probabilistic machine learning method, RMM-VAE, for identifying weather regimes targeted to a local-scale impact variable. Based on a variational autoencoder architecture, the method combines non-linear dimensionality reduction with a prediction task and probabilistic clustering in one coherent architecture. The new method is applied to identify circulation patterns over the Mediterranean region targeted to precipitation over Morocco and compared to three existing approaches: Two established linear methods and another machine-learning approach. The RMM-VAE method identifies regimes that are more predictive of the target variable compared to the two linear methods, both in terms of terciles and extremes in precipitation, while also improving the reconstruction of the input space. Further, the regimes identified by the RMM-VAE method are also more robust and persistent compared to the alternative machine learning method. The results demonstrate the potential benefit of the new method for use in various climate applications such as sub-seasonal forecasting, and illustrate the trade-offs involved in targeted clustering.  © The Author(s), 2024.;"extreme events; generative models; Mediterranean precipitation; variational autoencoders; weather regimes";NULL;"Identifying probabilistic weather regimes targeted to a local-scale impact variable Large-scale atmospheric circulation patterns, so-called weather regimes, modulate the occurrence of extreme events such as heatwaves or extreme precipitation. In their role as mediators between long-range teleconnections and local impacts, weather regimes have demonstrated potential in improving long-Term climate projections as well as sub-seasonal to seasonal forecasts. However, existing methods for identifying weather regimes are not specifically designed to capture the relevant physical processes responsible for variations in the impact variable in question. This paper introduces a novel probabilistic machine learning method, RMM-VAE, for identifying weather regimes targeted to a local-scale impact variable. Based on a variational autoencoder architecture, the method combines non-linear dimensionality reduction with a prediction task and probabilistic clustering in one coherent architecture. The new method is applied to identify circulation patterns over the Mediterranean region targeted to precipitation over Morocco and compared to three existing approaches: Two established linear methods and another machine-learning approach. The RMM-VAE method identifies regimes that are more predictive of the target variable compared to the two linear methods, both in terms of terciles and extremes in precipitation, while also improving the reconstruction of the input space. Further, the regimes identified by the RMM-VAE method are also more robust and persistent compared to the alternative machine learning method. The results demonstrate the potential benefit of the new method for use in various climate applications such as sub-seasonal forecasting, and illustrate the trade-offs involved in targeted clustering.  © The Author(s), 2024. extreme events; generative models; Mediterranean precipitation; variational autoencoders; weather regimes NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.3;Meteorological;2;Preparation
257;Spatiotemporal Prediction of Landslide Displacement Using Graph Convolutional Network-Based Models: A Case Study of the Tangjiao 1# Landslide in Chongqing, China;Landslide displacement monitoring can directly reflect the deformation process of a landslide. Predicting landslide displacements using monitored time series data through deep learning is a useful method for landslide early warning. Currently, existing prediction models mainly focus on single-point time series displacement prediction and do not consider the spatial relationship between monitoring points. To fully take into account the temporal and spatial correlation of displacement monitoring data, this paper proposes two models based on the graph convolutional network (GCN) to perform spatiotemporal prediction of the displacement of the Tangjiao 1# landslide. Firstly, the landslide monitoring system is transformed into a fully connected graph (FCG) to depict the spatial relationship among monitoring points on the landslide. Secondly, a temporal graph convolutional network (T-GCN) model and an attention temporal graph convolutional network (A3T-GCN) model of landslide displacement based on the GCN and GRU models are established respectively. Thirdly, the two models are used to predict the displacement of the Tangjiao 1# landslide. The results show that the established spatiotemporal prediction models are effective in predicting the displacement of the Tangjiao 1# landslide, and the proposed A3T-GCN model achieves the highest prediction accuracy. Our conclusion validates the effectiveness of the attention mechanism in predicting landslide displacement. © 2024 by the authors.;"attention mechanism; deep learning; graph convolutional network (GCN); landslide displacement prediction";"Convolutional neural networks; Deep learning; Landslides; Attention mechanisms; Convolutional networks; Deep learning; Displacement monitoring; Displacement prediction; Graph convolutional network; Landslide displacement prediction; Network models; Prediction modelling; Spatio-temporal prediction; Prediction models";"Spatiotemporal Prediction of Landslide Displacement Using Graph Convolutional Network-Based Models: A Case Study of the Tangjiao 1# Landslide in Chongqing, China Landslide displacement monitoring can directly reflect the deformation process of a landslide. Predicting landslide displacements using monitored time series data through deep learning is a useful method for landslide early warning. Currently, existing prediction models mainly focus on single-point time series displacement prediction and do not consider the spatial relationship between monitoring points. To fully take into account the temporal and spatial correlation of displacement monitoring data, this paper proposes two models based on the graph convolutional network (GCN) to perform spatiotemporal prediction of the displacement of the Tangjiao 1# landslide. Firstly, the landslide monitoring system is transformed into a fully connected graph (FCG) to depict the spatial relationship among monitoring points on the landslide. Secondly, a temporal graph convolutional network (T-GCN) model and an attention temporal graph convolutional network (A3T-GCN) model of landslide displacement based on the GCN and GRU models are established respectively. Thirdly, the two models are used to predict the displacement of the Tangjiao 1# landslide. The results show that the established spatiotemporal prediction models are effective in predicting the displacement of the Tangjiao 1# landslide, and the proposed A3T-GCN model achieves the highest prediction accuracy. Our conclusion validates the effectiveness of the attention mechanism in predicting landslide displacement. © 2024 by the authors. attention mechanism; deep learning; graph convolutional network (GCN); landslide displacement prediction Convolutional neural networks; Deep learning; Landslides; Attention mechanisms; Convolutional networks; Deep learning; Displacement monitoring; Displacement prediction; Graph convolutional network; Landslide displacement prediction; Network models; Prediction modelling; Spatio-temporal prediction; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
258;A seismic-risk-based bi-objective stochastic optimization framework for the pre-disaster allocation of earthquake search and rescue units;Accurately predicting earthquakes’ time, location and size is nearly impossible with today’s technology. Severe earthquakes require prompt and effective mobilization of available resources, as the speed of intervention has a direct impact on the number of people rescued alive. This, in turn, calls for a strategic pre-disaster allocation of search and rescue (SAR) units, both teams and equipment, to make the deployment of resources as quick and equitable as possible. In this paper, a seismic risk-based framework is introduced that takes into account distance-based contingencies between cities. This framework is then integrated into a mixed-integer non-linear programming (MINLP) problem for the allocation of SAR units under uncertainty. The two minimization objectives considered are the expected maximum deployment time of different SAR units and the expected mean absolute deviation of the fulfillment rates. We recover the best vulnerability-adjusted routes for each size-location scenario as input to the optimization model using the dynamic programming (DP) approach as part of the broader area of reinforcement learning (RL). The results of the hypothetical example indicate that the comprehensive model is feasible in various risk scenarios and can be used to make allocation-deployment decisions under uncertainty. The results of the sensitivity analysis verify that the model behaves reasonably against changes in selected parameters, namely the number of allowed facilities and weights of individual objectives. Under the assumption that the two objectives are equally important, the model achieves a total deviation of 3.5% from the objectives with an expected maximum dispatch time of 1.1327 hours and an expected mean absolute deviation of 0.01%. © 2024 by the authors.;"dynamic programming; Earthquake response; mixed-integer nonlinear programming (MINLP); SAR units allocation; stochastic optimization";NULL;"A seismic-risk-based bi-objective stochastic optimization framework for the pre-disaster allocation of earthquake search and rescue units Accurately predicting earthquakes’ time, location and size is nearly impossible with today’s technology. Severe earthquakes require prompt and effective mobilization of available resources, as the speed of intervention has a direct impact on the number of people rescued alive. This, in turn, calls for a strategic pre-disaster allocation of search and rescue (SAR) units, both teams and equipment, to make the deployment of resources as quick and equitable as possible. In this paper, a seismic risk-based framework is introduced that takes into account distance-based contingencies between cities. This framework is then integrated into a mixed-integer non-linear programming (MINLP) problem for the allocation of SAR units under uncertainty. The two minimization objectives considered are the expected maximum deployment time of different SAR units and the expected mean absolute deviation of the fulfillment rates. We recover the best vulnerability-adjusted routes for each size-location scenario as input to the optimization model using the dynamic programming (DP) approach as part of the broader area of reinforcement learning (RL). The results of the hypothetical example indicate that the comprehensive model is feasible in various risk scenarios and can be used to make allocation-deployment decisions under uncertainty. The results of the sensitivity analysis verify that the model behaves reasonably against changes in selected parameters, namely the number of allowed facilities and weights of individual objectives. Under the assumption that the two objectives are equally important, the model achieves a total deviation of 3.5% from the objectives with an expected maximum dispatch time of 1.1327 hours and an expected mean absolute deviation of 0.01%. © 2024 by the authors. dynamic programming; Earthquake response; mixed-integer nonlinear programming (MINLP); SAR units allocation; stochastic optimization NULL";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.1;Geological;2;Preparation
259;Stochastic processes driving cyanobacterial temporal succession in response to typhoons in a coastal reservoir;"Typhoons associated with heavy rainfall events, potentially triggering harmful algal blooms (cyanoHABs) dominated by cyanobacteria in coastal reservoirs. These blooms deteriorate water quality and produce toxins, posing a threat to aquatic ecosystems. However, the ecological mechanisms driving cyanobacteria communities in response to typhoons remain unclear. To address this gap, we investigated a coastal reservoir with high-frequency sampling during two typhoon seasons. We employed comprehensive statistical methods under neutral and evolutionary theories to analyze environmental dynamics and cyanobacterial genus succession. Our findings revealed a significant increase in nutrient loads following typhoons, with concentrations of total nitrogen (TN), total phosphorus (TP), and ammonia-nitrogen (NH4-N) rising from 0.4 mg/L to 1.0 mg/L, 0.02 mg/L to 0.63 mg/L, and 0.03 mg/L to 0.26 mg/L, respectively. These changes coincided with fluctuations in other physicochemical parameters under changing hydrometeorological conditions. Despite significant environmental disturbances, the cyanobacterial community exhibited a remarkable recovery within 15–25 days following the typhoons. This recovery progressed through four distinct successional phases, with a notable shift in community composition from Raphidiopsis and Pseudoanabaena to Aphanocapsa, subsequently replaced by Raphidiopsis and Microcystis, before reverting to the pre-typhoon community structure. During the entire successional phase, the availability of TN and the TN/TP ratio played a dominant role, as indicated by PLS-PM analysis (total effects = -0.6; p < 0.05). Pre-typhoon, environmental factors primarily influenced community structure (54 %) based on modified stochasticity ratio. However, following the typhoons, stochastic fluctuations took precedence (71 %-91 %). The rapid recovery of cyanobacterial communities and the shift in driving mechanisms from deterministic to stochastic processes underscore the complex ecological responses to typhoon events. This study provides essential insights for biodiversity preservation and ecosystem restoration, emphasizing the need to consider both stochastic and deterministic processes in ecological management strategies. © 2024 Elsevier Ltd";"Adaptive strategy; Cyanobacterial community; Ecologic processes; Temporal succession; Typhoon";"Cyanobacteria; Cyclonic Storms; Ecosystem; Harmful Algal Bloom; Nitrogen; Phosphorus; Stochastic Processes; Biotic; Blooms (metal); Stochastic systems; ammonia; nitrogen; phosphorus; rain; nitrogen; phosphorus; Adaptive strategy; Coastal reservoirs; Community structures; Cyanobacterial community; Cyanobacterium; Ecologic process; Stochastics; Temporal succession; Total nitrogen; Total phosphorus; adaptation; coastal zone; cyanobacterium; ecological approach; reservoir; stochasticity; succession; typhoon; adaptive environmental management; air pressure; air temperature; alga; aquatic environment; Article; biodiversity; biomass; cell density; coastal waters; community structure; community succession; controlled study; Cryptophyta; cyanobacterium; decomposition; ecosystem restoration; environmental factor; hierarchical clustering; human impact (environment); hurricane; meteorological phenomena; multiple linear regression analysis; nonhuman; nutrient concentration; nutrient content; nutrient dynamics; nutrient supply; pH; phytoplankton; precipitation; signal noise ratio; stochastic model; structural equation modeling; turbidity; water quality; watershed; weather; wind; wind speed; algal bloom; ecosystem; stochastic model; Abiotic";"Stochastic processes driving cyanobacterial temporal succession in response to typhoons in a coastal reservoir Typhoons associated with heavy rainfall events, potentially triggering harmful algal blooms (cyanoHABs) dominated by cyanobacteria in coastal reservoirs. These blooms deteriorate water quality and produce toxins, posing a threat to aquatic ecosystems. However, the ecological mechanisms driving cyanobacteria communities in response to typhoons remain unclear. To address this gap, we investigated a coastal reservoir with high-frequency sampling during two typhoon seasons. We employed comprehensive statistical methods under neutral and evolutionary theories to analyze environmental dynamics and cyanobacterial genus succession. Our findings revealed a significant increase in nutrient loads following typhoons, with concentrations of total nitrogen (TN), total phosphorus (TP), and ammonia-nitrogen (NH4-N) rising from 0.4 mg/L to 1.0 mg/L, 0.02 mg/L to 0.63 mg/L, and 0.03 mg/L to 0.26 mg/L, respectively. These changes coincided with fluctuations in other physicochemical parameters under changing hydrometeorological conditions. Despite significant environmental disturbances, the cyanobacterial community exhibited a remarkable recovery within 15–25 days following the typhoons. This recovery progressed through four distinct successional phases, with a notable shift in community composition from Raphidiopsis and Pseudoanabaena to Aphanocapsa, subsequently replaced by Raphidiopsis and Microcystis, before reverting to the pre-typhoon community structure. During the entire successional phase, the availability of TN and the TN/TP ratio played a dominant role, as indicated by PLS-PM analysis (total effects = -0.6; p < 0.05). Pre-typhoon, environmental factors primarily influenced community structure (54 %) based on modified stochasticity ratio. However, following the typhoons, stochastic fluctuations took precedence (71 %-91 %). The rapid recovery of cyanobacterial communities and the shift in driving mechanisms from deterministic to stochastic processes underscore the complex ecological responses to typhoon events. This study provides essential insights for biodiversity preservation and ecosystem restoration, emphasizing the need to consider both stochastic and deterministic processes in ecological management strategies. © 2024 Elsevier Ltd Adaptive strategy; Cyanobacterial community; Ecologic processes; Temporal succession; Typhoon Cyanobacteria; Cyclonic Storms; Ecosystem; Harmful Algal Bloom; Nitrogen; Phosphorus; Stochastic Processes; Biotic; Blooms (metal); Stochastic systems; ammonia; nitrogen; phosphorus; rain; nitrogen; phosphorus; Adaptive strategy; Coastal reservoirs; Community structures; Cyanobacterial community; Cyanobacterium; Ecologic process; Stochastics; Temporal succession; Total nitrogen; Total phosphorus; adaptation; coastal zone; cyanobacterium; ecological approach; reservoir; stochasticity; succession; typhoon; adaptive environmental management; air pressure; air temperature; alga; aquatic environment; Article; biodiversity; biomass; cell density; coastal waters; community structure; community succession; controlled study; Cryptophyta; cyanobacterium; decomposition; ecosystem restoration; environmental factor; hierarchical clustering; human impact (environment); hurricane; meteorological phenomena; multiple linear regression analysis; nonhuman; nutrient concentration; nutrient content; nutrient dynamics; nutrient supply; pH; phytoplankton; precipitation; signal noise ratio; stochastic model; structural equation modeling; turbidity; water quality; watershed; weather; wind; wind speed; algal bloom; ecosystem; stochastic model; Abiotic";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.3;Meteorological;1;Prevention
260;Serviceability fragility assessment of non-seismically designed railway viaducts in Turkey under near-field and far-field earthquakes;While many railway viaducts still in use in Turkish railway networks are located on active fault zones that produced historical destructive earthquakes, they are not seismically designed in accordance to current standards. This study aims to provide a better understanding about the serviceability fragility of such systems by conducting detailed probability-based seismic assessments under near-field and far-field earthquakes. To achieve this, firstly, 3D finite element models of a range of selected viaducts in Turkey were generated based on their original design drawings. The developed FE models were validated by comparing the analytical modal frequencies with the results of in-situ dynamic tests involving a series of acceleration measurements. Nonlinear time history analyses were then carried out under 25 near-field and 25 far-field real three-component ground motion recordings to obtain the seismic response of each selected viaduct. Subsequently, probabilistic seismic demand models were defined using linear regression analysis to determine relationships between engineering demand parameters (EDPs) and ground motion intensity measures (IMs). Peak ground acceleration (PGA), peak ground velocity (PGV) and spectral acceleration at 1.0 s (Sa (1.0)) were used as the IM parameter, while the maximum lateral displacement of the bridge spans for different service velocities defined in the Eurocode was considered as the EDP at serviceability damage state. Finally, analytical fragility curves for all the selected railway viaducts were developed considering maximum damage probability for the IM levels. The results, in general, demonstrate the seismic vulnerability of the existing viaducts in Turkey. It is shown that while at low speed limits the viaducts exposed to far-field ground motions were more vulnerable than those under near-field ground motions, at high speed limits the viaducts subjected to near-field ground motions were more vulnerable. Also, it is seen that reinforced concrete and masonry viaducts are generally more vulnerable to earthquakes than the steel viaducts. The outcomes of this study should prove useful for the seismic risk assessment, loss estimation and rehabilitation of the railway transportation networks in future studies. © 2024 Institution of Structural Engineers;"Engineering demand parameter; Fragility curve; Intensity measure; Probabilistic seismic assessment; Railway viaduct; Seismic fragility";"Codes (standards); Earthquake effects; Linear regression; Maximum likelihood; Railroad bridges; Railroad transportation; Railroads; Risk analysis; Risk assessment; Risk perception; Seismic design; Steel bridges; Engineering demand parameters; Far-field; Fragility curves; Intensity measure; Near fields; Near-far; Probabilistic seismic assessment; Railway viaducts; Seismic fragility; Speed limit; Reinforced concrete";"Serviceability fragility assessment of non-seismically designed railway viaducts in Turkey under near-field and far-field earthquakes While many railway viaducts still in use in Turkish railway networks are located on active fault zones that produced historical destructive earthquakes, they are not seismically designed in accordance to current standards. This study aims to provide a better understanding about the serviceability fragility of such systems by conducting detailed probability-based seismic assessments under near-field and far-field earthquakes. To achieve this, firstly, 3D finite element models of a range of selected viaducts in Turkey were generated based on their original design drawings. The developed FE models were validated by comparing the analytical modal frequencies with the results of in-situ dynamic tests involving a series of acceleration measurements. Nonlinear time history analyses were then carried out under 25 near-field and 25 far-field real three-component ground motion recordings to obtain the seismic response of each selected viaduct. Subsequently, probabilistic seismic demand models were defined using linear regression analysis to determine relationships between engineering demand parameters (EDPs) and ground motion intensity measures (IMs). Peak ground acceleration (PGA), peak ground velocity (PGV) and spectral acceleration at 1.0 s (Sa (1.0)) were used as the IM parameter, while the maximum lateral displacement of the bridge spans for different service velocities defined in the Eurocode was considered as the EDP at serviceability damage state. Finally, analytical fragility curves for all the selected railway viaducts were developed considering maximum damage probability for the IM levels. The results, in general, demonstrate the seismic vulnerability of the existing viaducts in Turkey. It is shown that while at low speed limits the viaducts exposed to far-field ground motions were more vulnerable than those under near-field ground motions, at high speed limits the viaducts subjected to near-field ground motions were more vulnerable. Also, it is seen that reinforced concrete and masonry viaducts are generally more vulnerable to earthquakes than the steel viaducts. The outcomes of this study should prove useful for the seismic risk assessment, loss estimation and rehabilitation of the railway transportation networks in future studies. © 2024 Institution of Structural Engineers Engineering demand parameter; Fragility curve; Intensity measure; Probabilistic seismic assessment; Railway viaduct; Seismic fragility Codes (standards); Earthquake effects; Linear regression; Maximum likelihood; Railroad bridges; Railroad transportation; Railroads; Risk analysis; Risk assessment; Risk perception; Seismic design; Steel bridges; Engineering demand parameters; Far-field; Fragility curves; Intensity measure; Near fields; Near-far; Probabilistic seismic assessment; Railway viaducts; Seismic fragility; Speed limit; Reinforced concrete";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
261;Crack Detection and Feature Extraction of Heritage Buildings via Point Clouds: A Case Study of Zhonghua Gate Castle in Nanjing;"Zhonghua Gate Castle is on the tentative list for Chinese World Cultural Heritage. Due to long-term sunshine, rain erosion, and man-made damage, its surface appears to have different degrees of cracks and other diseases. This paper centers on Zhonghua Gate Castle; terrestrial laser scanning is used to obtain the exterior wall point cloud data. A crack detection method based on point cloud data curved surface reconstruction is proposed. It involves data preprocessing, crack detection, and the analysis of crack features. This method initially uses data preprocessing techniques to improve data quality. These techniques include removing ground points and super-voxel segmentation. Subsequently, local surface reconstruction was employed to address the issue of missing point cloud data within cracks and the Euclidean clustering algorithm was used for precise crack identification. The article provides a detailed analysis of the geometric characteristics of cracks. They involve the calculation of length, width, and area. The results of the experiment demonstrate that the method could successfully identify cracks and extract geometric features and has millimeter-level accuracy compared to actual crack sizes. © 2024 by the authors.";"crack detection; curved surface reconstruction; Euclidean clustering; heritage building; point clouds";"Clusterings; Curved surface reconstruction; Curved surfaces; Euclidean; Euclidean clustering; Features extraction; Heritage buildings; Point cloud data; Point-clouds; Surfaces reconstruction; Religious buildings";"Crack Detection and Feature Extraction of Heritage Buildings via Point Clouds: A Case Study of Zhonghua Gate Castle in Nanjing Zhonghua Gate Castle is on the tentative list for Chinese World Cultural Heritage. Due to long-term sunshine, rain erosion, and man-made damage, its surface appears to have different degrees of cracks and other diseases. This paper centers on Zhonghua Gate Castle; terrestrial laser scanning is used to obtain the exterior wall point cloud data. A crack detection method based on point cloud data curved surface reconstruction is proposed. It involves data preprocessing, crack detection, and the analysis of crack features. This method initially uses data preprocessing techniques to improve data quality. These techniques include removing ground points and super-voxel segmentation. Subsequently, local surface reconstruction was employed to address the issue of missing point cloud data within cracks and the Euclidean clustering algorithm was used for precise crack identification. The article provides a detailed analysis of the geometric characteristics of cracks. They involve the calculation of length, width, and area. The results of the experiment demonstrate that the method could successfully identify cracks and extract geometric features and has millimeter-level accuracy compared to actual crack sizes. © 2024 by the authors. crack detection; curved surface reconstruction; Euclidean clustering; heritage building; point clouds Clusterings; Curved surface reconstruction; Curved surfaces; Euclidean; Euclidean clustering; Features extraction; Heritage buildings; Point cloud data; Point-clouds; Surfaces reconstruction; Religious buildings";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;1;Prevention
262;Machine learning models for predicting geomagnetic storms across five solar cycles using Dst index and heliospheric variables;This study aims to improve the understanding of geomagnetic storms by utilizing machine learning models and analyzing several heliophysical variables, such as the interplanetary magnetic field, proton density, solar wind speed, and proton temperature. Rather than relying on traditional correlation-based methods, we employ advanced machine learning techniques to examine the complex relationships between these factors and geomagnetic storms. Our analysis covers a large dataset spanning six solar cycles, including the current 25th cycle, to provide comprehensive insights into the dynamics of these storms. Our study highlights the significance of the interplanetary magnetic field as a key predictor of geomagnetic storms, challenging previous beliefs that primarily focused on sunspot activity. By using high-resolution data, we uncover new patterns and provide a more detailed analysis of the factors influencing geomagnetic storms. We emphasize the importance of considering a range of heliophysical variables, such as proton temperature and flow pressure, which offer new insights into the complex dynamics driving these storm events. The application of machine learning models, particularly Random Forest and Gradient Boosting, demonstrated superior predictive accuracy compared to traditional methods. Our results reveal that the Dst-index MIN, scalar B, and alpha/proton ratio are among the most influential factors, accounting for a significant portion of the prediction model's accuracy. These findings underscore the utility of machine learning in identifying critical drivers of geomagnetic activity and enhancing forecast precision. Additionally, our research underscores the need for comprehensive models that can accurately predict geomagnetic storms by integrating various data sources. This machine learning approach not only improves predictive accuracy but also enhances our understanding of the underlying mechanisms of space weather. The insights gained from this study have important implications for both scientific research and practical applications, such as improving early warning systems for geomagnetic storms and mitigating their potential impacts on Earth. © 2024 COSPAR;"Data science; Geomagnetic storms; Machine learning; Space weather; Statistical modeling";"Adaptive boosting; Artificial intelligence; Contrastive Learning; Prediction models; Tropics; Weather forecasting; Dst index; Geomagnetic storm; Interplanetary magnetic fields; Machine learning models; Machine-learning; Predictive accuracy; Proton temperatures; Solar cycle; Space weather; Statistic modeling; Solar wind";"Machine learning models for predicting geomagnetic storms across five solar cycles using Dst index and heliospheric variables This study aims to improve the understanding of geomagnetic storms by utilizing machine learning models and analyzing several heliophysical variables, such as the interplanetary magnetic field, proton density, solar wind speed, and proton temperature. Rather than relying on traditional correlation-based methods, we employ advanced machine learning techniques to examine the complex relationships between these factors and geomagnetic storms. Our analysis covers a large dataset spanning six solar cycles, including the current 25th cycle, to provide comprehensive insights into the dynamics of these storms. Our study highlights the significance of the interplanetary magnetic field as a key predictor of geomagnetic storms, challenging previous beliefs that primarily focused on sunspot activity. By using high-resolution data, we uncover new patterns and provide a more detailed analysis of the factors influencing geomagnetic storms. We emphasize the importance of considering a range of heliophysical variables, such as proton temperature and flow pressure, which offer new insights into the complex dynamics driving these storm events. The application of machine learning models, particularly Random Forest and Gradient Boosting, demonstrated superior predictive accuracy compared to traditional methods. Our results reveal that the Dst-index MIN, scalar B, and alpha/proton ratio are among the most influential factors, accounting for a significant portion of the prediction model's accuracy. These findings underscore the utility of machine learning in identifying critical drivers of geomagnetic activity and enhancing forecast precision. Additionally, our research underscores the need for comprehensive models that can accurately predict geomagnetic storms by integrating various data sources. This machine learning approach not only improves predictive accuracy but also enhances our understanding of the underlying mechanisms of space weather. The insights gained from this study have important implications for both scientific research and practical applications, such as improving early warning systems for geomagnetic storms and mitigating their potential impacts on Earth. © 2024 COSPAR Data science; Geomagnetic storms; Machine learning; Space weather; Statistical modeling Adaptive boosting; Artificial intelligence; Contrastive Learning; Prediction models; Tropics; Weather forecasting; Dst index; Geomagnetic storm; Interplanetary magnetic fields; Machine learning models; Machine-learning; Predictive accuracy; Proton temperatures; Solar cycle; Space weather; Statistic modeling; Solar wind";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
263;Collaborative Control Technology and Prospects of Drone Swarms in Earthquake Emergency Rescue Scenarios;"Currently, in emergency activities such as disaster relief, drones have gradually become a new force in emergency rescue due to their flexibility, intelligence, and safety advantages, and they have great potential in assisting disaster rescue. With the rapid development of drone systems, traditional single-drone disaster rescue assistance can no longer meet the current needs, and multi-drone swarm operations have become the future trend in emergency rescue. Collaborative control of drone swarm formations has become the core of drone swarm applications in earthquake emergency rescue. Traditional multi-drone swarm control systems cannot adapt to complex environments, meet diverse needs, and may even result in mission failure or drone crashes. To address this issue, by combining the physical attributes of drone swarms and environmental factors, we improve control algorithms and propose a GNSS-based collaborative control system for drone swarm formations. Firstly, leveraging the multi-system coupling positioning technology of GNSS, we design a differential model for drone swarms based on pseudorange differential positioning to enhance positioning accuracy and stability; Secondly, a drone decision-making system based on artificial intelligence algorithms and a drone swarm obstacle avoidance system based on image recognition were designed, enabling the drone swarm to make autonomous decisions; At the same time, a packet architecture and early warning system for the drone swarm communication link based on machine learning were adopted to ensure the stability and smoothness of the communication link. The close cooperation and coordination between the various modules of the system ensure the normal operation of the drone swarm. This system not only maximizes the numerical advantage of the drone formation but also avoids the operational chaos and poor task execution caused by an excessive number of drones. It expands new task execution methods, achieving a high degree of autonomy and the mutual cooperation of human-assisted decision-making. It has broad development prospects and application potential in earthquake emergency rescue, express logistics, precision agriculture, and commercial performances. © 2024 Copyright held by the owner/author(s).";"autonomous decision-making algorithm; drone swarm; Earthquake disaster; emergency rescue; formation flight; GNSS differential positioning";"Aircraft accidents; Aircraft communication; Control system stability; Disaster prevention; Disasters; Feedback control; Intelligent systems; Swarm intelligence; Autonomous decision; Autonomous decision-making algorithm; Collaborative control; Decision-making algorithms; Differential positioning; Drone swarm; Earthquake disaster; Emergency rescue; Formation flight; GNSS differential positioning; Global positioning system";"Collaborative Control Technology and Prospects of Drone Swarms in Earthquake Emergency Rescue Scenarios Currently, in emergency activities such as disaster relief, drones have gradually become a new force in emergency rescue due to their flexibility, intelligence, and safety advantages, and they have great potential in assisting disaster rescue. With the rapid development of drone systems, traditional single-drone disaster rescue assistance can no longer meet the current needs, and multi-drone swarm operations have become the future trend in emergency rescue. Collaborative control of drone swarm formations has become the core of drone swarm applications in earthquake emergency rescue. Traditional multi-drone swarm control systems cannot adapt to complex environments, meet diverse needs, and may even result in mission failure or drone crashes. To address this issue, by combining the physical attributes of drone swarms and environmental factors, we improve control algorithms and propose a GNSS-based collaborative control system for drone swarm formations. Firstly, leveraging the multi-system coupling positioning technology of GNSS, we design a differential model for drone swarms based on pseudorange differential positioning to enhance positioning accuracy and stability; Secondly, a drone decision-making system based on artificial intelligence algorithms and a drone swarm obstacle avoidance system based on image recognition were designed, enabling the drone swarm to make autonomous decisions; At the same time, a packet architecture and early warning system for the drone swarm communication link based on machine learning were adopted to ensure the stability and smoothness of the communication link. The close cooperation and coordination between the various modules of the system ensure the normal operation of the drone swarm. This system not only maximizes the numerical advantage of the drone formation but also avoids the operational chaos and poor task execution caused by an excessive number of drones. It expands new task execution methods, achieving a high degree of autonomy and the mutual cooperation of human-assisted decision-making. It has broad development prospects and application potential in earthquake emergency rescue, express logistics, precision agriculture, and commercial performances. © 2024 Copyright held by the owner/author(s). autonomous decision-making algorithm; drone swarm; Earthquake disaster; emergency rescue; formation flight; GNSS differential positioning Aircraft accidents; Aircraft communication; Control system stability; Disaster prevention; Disasters; Feedback control; Intelligent systems; Swarm intelligence; Autonomous decision; Autonomous decision-making algorithm; Collaborative control; Decision-making algorithms; Differential positioning; Drone swarm; Earthquake disaster; Emergency rescue; Formation flight; GNSS differential positioning; Global positioning system";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
264;Damage propagation and failure mechanism of single dome historical Masonry Mosque after February 6, 2023, Kahramanmaraş earthquake doublets (Mw = 7.7 and Mw = 7.6);Many historical masonry mosques and minarets, including Milky Minaret Mosque and Ulu Mosque, which are very close to Ak Minaret Mosque in Malatya, suffered severe damage and collapsed after the Kahramanmaraş earthquake on February 6, 2023. Despite the settled loose soil properties and the presence of a small stream flow near the mosque, Ak Minaret Mosque, a historical masonry mosque with a single dome and square, remains operational even after the aforementioned earthquakes. Additionally, DEMA's strong ground motion station was out of service during both earthquakes. This situation raises questions about the mosque's ability to withstand the seismic load. The basic purpose of this paper is to investigate in detail the seismic performance, damage limits, the reason for unexpectedly less damage due to the earthquakes, the potential failure mode, and possible resisted earthquake loads of the historical Ak Minaret Mosque in Malatya, Türkiye. To achieve this goal, first, dynamic identification was performed on the mosque one year prior to the aforementioned earthquakes. Next, the material properties were determined using both non-destructive and destructive testing methods. Following the dynamic identification, a numerical model was generated by 3-D solid elements, and this 3-D model was calibrated using the dynamic identification tests. The mosque underwent both nonlinear static and nonlinear dynamic analyses. Nine seismic records were selected for the nonlinear dynamic analysis. Five of them were national, including even Kahramanmaraş earthquake records, and three of them were selected from an international database on the basis of fault characteristics and site classification. The analysis results indicate that Ak Minaret Mosque incurred less damage than expected during the Kahramanmaraş earthquakes. This could be due to soil improvement prior to the construction of Ak Minaret Mosque in 1573. Moreover, the effective restoration increased stiffness and maintained the mosque's stability. Finally, the possible resisted PGA by the mosque was around 0.22 g. © 2024 Institution of Structural Engineers;"Ak Minaret Mosque; Failure of single dome mosque; February 6, 2023, earthquakes; Malatya; Nonlinear static and dynamic analysis";"Convergence of numerical methods; Earthquake effects; Earthquake engineering; Fracture mechanics; Masonry construction; Masonry materials; Soil testing; Ak minaret mosque; Damage failures; Dynamic identification; Failure of single dome mosque; February 6, 2023, earthquake; Malatyum; Non-linear dynamic analysis; Non-linear static analysis; Nonlinear statics and dynamics; Static and dynamic analysis; Domes";"Damage propagation and failure mechanism of single dome historical Masonry Mosque after February 6, 2023, Kahramanmaraş earthquake doublets (Mw = 7.7 and Mw = 7.6) Many historical masonry mosques and minarets, including Milky Minaret Mosque and Ulu Mosque, which are very close to Ak Minaret Mosque in Malatya, suffered severe damage and collapsed after the Kahramanmaraş earthquake on February 6, 2023. Despite the settled loose soil properties and the presence of a small stream flow near the mosque, Ak Minaret Mosque, a historical masonry mosque with a single dome and square, remains operational even after the aforementioned earthquakes. Additionally, DEMA's strong ground motion station was out of service during both earthquakes. This situation raises questions about the mosque's ability to withstand the seismic load. The basic purpose of this paper is to investigate in detail the seismic performance, damage limits, the reason for unexpectedly less damage due to the earthquakes, the potential failure mode, and possible resisted earthquake loads of the historical Ak Minaret Mosque in Malatya, Türkiye. To achieve this goal, first, dynamic identification was performed on the mosque one year prior to the aforementioned earthquakes. Next, the material properties were determined using both non-destructive and destructive testing methods. Following the dynamic identification, a numerical model was generated by 3-D solid elements, and this 3-D model was calibrated using the dynamic identification tests. The mosque underwent both nonlinear static and nonlinear dynamic analyses. Nine seismic records were selected for the nonlinear dynamic analysis. Five of them were national, including even Kahramanmaraş earthquake records, and three of them were selected from an international database on the basis of fault characteristics and site classification. The analysis results indicate that Ak Minaret Mosque incurred less damage than expected during the Kahramanmaraş earthquakes. This could be due to soil improvement prior to the construction of Ak Minaret Mosque in 1573. Moreover, the effective restoration increased stiffness and maintained the mosque's stability. Finally, the possible resisted PGA by the mosque was around 0.22 g. © 2024 Institution of Structural Engineers Ak Minaret Mosque; Failure of single dome mosque; February 6, 2023, earthquakes; Malatya; Nonlinear static and dynamic analysis Convergence of numerical methods; Earthquake effects; Earthquake engineering; Fracture mechanics; Masonry construction; Masonry materials; Soil testing; Ak minaret mosque; Damage failures; Dynamic identification; Failure of single dome mosque; February 6, 2023, earthquake; Malatyum; Non-linear dynamic analysis; Non-linear static analysis; Nonlinear statics and dynamics; Static and dynamic analysis; Domes";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
265;Human Adaption to Climate Change: Marine Disaster Risk Reduction in the Era of Intelligence;With the intensification of global warming and sea level rise, extreme weather and climate events occur frequently, increasing the probability and destructive power of marine disasters. The purpose of this paper is to propose the specific application of artificial intelligence (AI) in marine disaster risk reduction. First, this paper uses computer vision to assess the vulnerability of the target and then uses CNN-LSTM to forecast tropical cyclones. Second, this paper proposes a social media communication mechanism based on deep learning and a psychological crisis intervention mechanism based on AIGC. In addition, the rescue response system based on an intelligent unmanned platform is also the focus of this research. Third, this paper also attempts to discuss disaster loss assessment and reconstruction based on machine learning and smart city concepts. After proposing specific application measures, this paper proposes three policy recommendations. The first one is improving legislation to break the technological trap of AI. The second one is promoting scientific and technological innovation to break through key technologies of AI. The third one is strengthening coordination and cooperation to build a disaster reduction system that integrates man and machine. The purpose of this paper is to reduce the risk of marine disasters by applying AI. Furthermore, we hope to provide scientific references for sustainability and human adaptation to climate change. © 2024 by the authors.;"artificial intelligence; climate change; human adaption; marine disaster; risk reduction; sustainability";"adaptation; artificial intelligence; climate change; computer vision; human activity; risk assessment; sustainability";"Human Adaption to Climate Change: Marine Disaster Risk Reduction in the Era of Intelligence With the intensification of global warming and sea level rise, extreme weather and climate events occur frequently, increasing the probability and destructive power of marine disasters. The purpose of this paper is to propose the specific application of artificial intelligence (AI) in marine disaster risk reduction. First, this paper uses computer vision to assess the vulnerability of the target and then uses CNN-LSTM to forecast tropical cyclones. Second, this paper proposes a social media communication mechanism based on deep learning and a psychological crisis intervention mechanism based on AIGC. In addition, the rescue response system based on an intelligent unmanned platform is also the focus of this research. Third, this paper also attempts to discuss disaster loss assessment and reconstruction based on machine learning and smart city concepts. After proposing specific application measures, this paper proposes three policy recommendations. The first one is improving legislation to break the technological trap of AI. The second one is promoting scientific and technological innovation to break through key technologies of AI. The third one is strengthening coordination and cooperation to build a disaster reduction system that integrates man and machine. The purpose of this paper is to reduce the risk of marine disasters by applying AI. Furthermore, we hope to provide scientific references for sustainability and human adaptation to climate change. © 2024 by the authors. artificial intelligence; climate change; human adaption; marine disaster; risk reduction; sustainability adaptation; artificial intelligence; climate change; computer vision; human activity; risk assessment; sustainability";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
266;Social vulnerabilities and wildfire evacuations: A case study of the 2019 Kincade fire;Vulnerable populations (e.g., populations with lower income or disabilities) are disproportionately impacted by natural hazards like wildfires. It is crucial to develop equitable and effective evacuation strategies to meet their unique needs. While existing studies offer valuable insights, we need to improve our understanding of how vulnerabilities affect wildfire evacuation decision-making, as well as how this varies spatially. The goal of this study is to conduct an in-depth analysis of the impacts of social vulnerabilities on aggregated evacuation decisions, including evacuation rates, delay in departure time, and evacuation destination distance by leveraging large-scale GPS data generated by mobile devices. Specifically, we inferred evacuation decisions at the level of the census block group, a geographic unit defined by the U.S. Census, utilizing GPS data. We then employed ordinary least squares and geographically weighted regression models to investigate the impacts of social vulnerabilities on evacuation decisions. We also used Moran's I to test if these impacts were consistent across different block groups. The 2019 Kincade Fire in Sonoma County, California, was used as the case study. The impacts of social vulnerabilities on evacuation rates show significant spatial variations across block groups, whereas their effects on the other two decision types do not. Additionally, unemployment, a factor under-explored in previous studies, was identified as contributing to both an increased delay in departure time and a reduction in destination distance of evacuees at the aggregate level. Furthermore, upon comparing the significant factors across different models, we observed that some of the vulnerabilities contributing to evacuation rates for all residents differed from those affecting the delay in departure time and destination distance, which only applied to evacuees. These new insights can guide emergency managers and transportation planners to enhance equitable wildfire evacuation planning and operations. © 2024 The Author(s);"Bushfire; Equity; Evacuation; GPS data; Social vulnerability; Wildfire";"Decision making; Economic and social effects; Global positioning system; Regression analysis; Block group; Bushfires; Case-studies; Departure time; Equity; Evacuation; GPS data; Low incomes; Social vulnerability; Wildfire; Article; emergency evacuation; emergency transport; geographic distribution; geographically weighted regression; health equity; human; least square analysis; social vulnerability; unemployment; United States; wildfire; Fires";"Social vulnerabilities and wildfire evacuations: A case study of the 2019 Kincade fire Vulnerable populations (e.g., populations with lower income or disabilities) are disproportionately impacted by natural hazards like wildfires. It is crucial to develop equitable and effective evacuation strategies to meet their unique needs. While existing studies offer valuable insights, we need to improve our understanding of how vulnerabilities affect wildfire evacuation decision-making, as well as how this varies spatially. The goal of this study is to conduct an in-depth analysis of the impacts of social vulnerabilities on aggregated evacuation decisions, including evacuation rates, delay in departure time, and evacuation destination distance by leveraging large-scale GPS data generated by mobile devices. Specifically, we inferred evacuation decisions at the level of the census block group, a geographic unit defined by the U.S. Census, utilizing GPS data. We then employed ordinary least squares and geographically weighted regression models to investigate the impacts of social vulnerabilities on evacuation decisions. We also used Moran's I to test if these impacts were consistent across different block groups. The 2019 Kincade Fire in Sonoma County, California, was used as the case study. The impacts of social vulnerabilities on evacuation rates show significant spatial variations across block groups, whereas their effects on the other two decision types do not. Additionally, unemployment, a factor under-explored in previous studies, was identified as contributing to both an increased delay in departure time and a reduction in destination distance of evacuees at the aggregate level. Furthermore, upon comparing the significant factors across different models, we observed that some of the vulnerabilities contributing to evacuation rates for all residents differed from those affecting the delay in departure time and destination distance, which only applied to evacuees. These new insights can guide emergency managers and transportation planners to enhance equitable wildfire evacuation planning and operations. © 2024 The Author(s) Bushfire; Equity; Evacuation; GPS data; Social vulnerability; Wildfire Decision making; Economic and social effects; Global positioning system; Regression analysis; Block group; Bushfires; Case-studies; Departure time; Equity; Evacuation; GPS data; Low incomes; Social vulnerability; Wildfire; Article; emergency evacuation; emergency transport; geographic distribution; geographically weighted regression; health equity; human; least square analysis; social vulnerability; unemployment; United States; wildfire; Fires";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;3;Response
267;Deep learning prediction of rainfall-driven debris flows considering the similar critical thresholds within comparable background conditions;"Machine learning has been widely applied to predict the spatial or temporal likelihood of debris flows by leveraging its powerful capability to fit nonlinear features and uncover underlying patterns or rules in the complex formation mechanisms of debris flows. However, traditional approaches, including some current machine learning-based prediction models, still have limitations when used for debris flow prediction. These include the lack of a specific network structure or model to consider the updating of debris flow critical conditions in relation to geographical background conditions, limiting the universality of prediction models when transferring them to different places. In this study, this article proposes a deep learning network designed to predict the spatiotemporal probability of rainfall-induced debris flows, incorporating the Similarity Mechanism of Debris Flow Critical Conditions (SM-DFCC). The model comprehensively integrates the mining of rainfall-triggering features and couples them with geographical background features to fit the nonlinear relationship with debris flow formation. The model underwent training using data on various historical debris flows triggered by different storms across Liangshan Prefecture from 2020 to 2022. The results indicated that: (i) the method is effective in predicting the spatiotemporal likelihood of debris flows under catchment units, with accuracy scores (ACC) ranging from 0.724 to 0.835; (ii) after optimization using the AVOA algorithm, the predictive performance of the model significantly improved, with an increase of 27.24% in ACC scores for SVC and 8.81% for XGBoost; and (iii) factor importance analysis revealed that rainfall triggering factors have higher cumulative contribution rates when distinguishing between the occurrence and non-occurrence of debris flows. In addition, taking a rainfall storm on 06, September 2020 as a case, this research quantitatively revealed the pattern of debris flow formation, where high-frequency disaster areas exhibit lower rainfall thresholds of debris flows, represented by absolute energy (AE). Despite these findings, the accuracy and reliability of rainfall data still remain the most challenging obstacle in basin/regional-scale debris flow prediction when applying this method. The integration of multiple sources of rainfall data, including station data, satellite rainfall, radar rainfall, etc., is necessary to accurately quantify the impact of rainfall on debris flow formation when applying this method to debris flow monitoring and early warning tasks. Overall, this method shows great potential in providing a scientific reference for the construction of debris flow monitoring and early warning systems in the future. © 2024";"Deep learning; Machine learning; Rainfall features; Rainfall-driven debris flow; Spatiotemporal prediction";"China; Liangshan; Sichuan; Catchments; Debris; Deep learning; Disasters; Forecasting; Learning systems; Reliability analysis; Storms; Background conditions; Debris flows; Deep learning; Flow formations; Flow prediction; Machine-learning; Prediction modelling; Rainfall feature; Rainfall-driven debris flow; Spatio-temporal prediction; climate prediction; debris flow; early warning system; machine learning; optimization; rainstorm; satellite data; spatiotemporal analysis; Rain";"Deep learning prediction of rainfall-driven debris flows considering the similar critical thresholds within comparable background conditions Machine learning has been widely applied to predict the spatial or temporal likelihood of debris flows by leveraging its powerful capability to fit nonlinear features and uncover underlying patterns or rules in the complex formation mechanisms of debris flows. However, traditional approaches, including some current machine learning-based prediction models, still have limitations when used for debris flow prediction. These include the lack of a specific network structure or model to consider the updating of debris flow critical conditions in relation to geographical background conditions, limiting the universality of prediction models when transferring them to different places. In this study, this article proposes a deep learning network designed to predict the spatiotemporal probability of rainfall-induced debris flows, incorporating the Similarity Mechanism of Debris Flow Critical Conditions (SM-DFCC). The model comprehensively integrates the mining of rainfall-triggering features and couples them with geographical background features to fit the nonlinear relationship with debris flow formation. The model underwent training using data on various historical debris flows triggered by different storms across Liangshan Prefecture from 2020 to 2022. The results indicated that: (i) the method is effective in predicting the spatiotemporal likelihood of debris flows under catchment units, with accuracy scores (ACC) ranging from 0.724 to 0.835; (ii) after optimization using the AVOA algorithm, the predictive performance of the model significantly improved, with an increase of 27.24% in ACC scores for SVC and 8.81% for XGBoost; and (iii) factor importance analysis revealed that rainfall triggering factors have higher cumulative contribution rates when distinguishing between the occurrence and non-occurrence of debris flows. In addition, taking a rainfall storm on 06, September 2020 as a case, this research quantitatively revealed the pattern of debris flow formation, where high-frequency disaster areas exhibit lower rainfall thresholds of debris flows, represented by absolute energy (AE). Despite these findings, the accuracy and reliability of rainfall data still remain the most challenging obstacle in basin/regional-scale debris flow prediction when applying this method. The integration of multiple sources of rainfall data, including station data, satellite rainfall, radar rainfall, etc., is necessary to accurately quantify the impact of rainfall on debris flow formation when applying this method to debris flow monitoring and early warning tasks. Overall, this method shows great potential in providing a scientific reference for the construction of debris flow monitoring and early warning systems in the future. © 2024 Deep learning; Machine learning; Rainfall features; Rainfall-driven debris flow; Spatiotemporal prediction China; Liangshan; Sichuan; Catchments; Debris; Deep learning; Disasters; Forecasting; Learning systems; Reliability analysis; Storms; Background conditions; Debris flows; Deep learning; Flow formations; Flow prediction; Machine-learning; Prediction modelling; Rainfall feature; Rainfall-driven debris flow; Spatio-temporal prediction; climate prediction; debris flow; early warning system; machine learning; optimization; rainstorm; satellite data; spatiotemporal analysis; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
268;A Study on the Diagnosis of the Working Conditions of a Traveling Beam Pumping Unit Based on Artificial Intelligence;In order to solve the problem of low accuracy of traditional indicator diagram recognition methods, the research on condition diagnosis of beam pumping units based on artificial intelligence is proposed. Through the application of deep learning convolutional neural network in the field of image recognition, this paper proposes a convolutional neural network model based on LeNet, and realizes the automatic recognition of indicator diagrams. The model built by the research institute takes 15 common downhole working conditions of the pumping unit into consideration while simplifying the model structure, and introduces the Dropout layer and local response normalization layer to prevent the model from over fitting and improve the generalization ability of the model. The experimental results show that the model not only has a fast convergence rate, but also has an average diagnostic accuracy of94.68%It meets the diagnostic accuracy requirements of pumping unit condition detection. Conclusion: This study provides a basis for the construction of pumping unit well condition intelligent monitoring and early warning system, which is of great significance to the construction of intelligent oilfield and the efficient production of oilfield. © 2024 The Authors.;"Condition diagnosis; Convolutional neural network; Indicator diagram; LeNet; pumping unit";"Deep neural networks; Electron beam pumping; Neural network models; Oil well flooding; Oil well pumping; Unit operations (oil wells); Well pumps; Beam pumping unit; Condition; Condition diagnose; Convolutional neural network; Diagnostic accuracy; Indicator diagram; Lenet; Pumping unit; Traveling beams; Unit-based; Convolutional neural networks";"A Study on the Diagnosis of the Working Conditions of a Traveling Beam Pumping Unit Based on Artificial Intelligence In order to solve the problem of low accuracy of traditional indicator diagram recognition methods, the research on condition diagnosis of beam pumping units based on artificial intelligence is proposed. Through the application of deep learning convolutional neural network in the field of image recognition, this paper proposes a convolutional neural network model based on LeNet, and realizes the automatic recognition of indicator diagrams. The model built by the research institute takes 15 common downhole working conditions of the pumping unit into consideration while simplifying the model structure, and introduces the Dropout layer and local response normalization layer to prevent the model from over fitting and improve the generalization ability of the model. The experimental results show that the model not only has a fast convergence rate, but also has an average diagnostic accuracy of94.68%It meets the diagnostic accuracy requirements of pumping unit condition detection. Conclusion: This study provides a basis for the construction of pumping unit well condition intelligent monitoring and early warning system, which is of great significance to the construction of intelligent oilfield and the efficient production of oilfield. © 2024 The Authors. Condition diagnosis; Convolutional neural network; Indicator diagram; LeNet; pumping unit Deep neural networks; Electron beam pumping; Neural network models; Oil well flooding; Oil well pumping; Unit operations (oil wells); Well pumps; Beam pumping unit; Condition; Condition diagnose; Convolutional neural network; Diagnostic accuracy; Indicator diagram; Lenet; Pumping unit; Traveling beams; Unit-based; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
269;Smart hotspot detection using geospatial artificial intelligence: A machine learning approach to reduce flood risk;This study employs Geospatial Artificial Intelligence (GeoAI) and the Random Forest Machine Learning (ML) algorithm to enhance flood hazard assessments in Portugal. It utilizes NASA's LP DAAC (2023) Digital Elevation Model (DEM) and slope data from EPIC WEBGIS PORTUGAL DATA, offering detailed topographical insights for environmental planning. Additionally, it incorporates data on proximity to water bodies from the Portuguese Environment Agency and the European Environment Agency, and soil characteristics from EPIC WEBGIS PORTUGAL DATA, facilitating a thorough examination of flood risks. This approach prioritizes long-term land features over short-term weather patterns, providing a comprehensive understanding of flood vulnerability. The study processes data at a 1 km x 1 km resolution, adapting TIFF maps for compatibility with the Random Forest model. The produced flood hazard maps identify potential flood hotspots at both national and city levels, crucial for urban planning. These maps aid in assessing the vulnerability of key infrastructure and assets, such as transport networks and buildings. The research highlights the importance of integrating additional data on assets and socioeconomic factors to enhance urban resilience. It sets the stage for future research aimed at improving predictive accuracy and underscores the necessity of extensive geospatial analytics in managing infrastructure risks. © 2024 The Author(s);"Disaster risk reduction; Flood hazard mapping; Geospatial artificial intelligence; Geospatial data analysis; Machine learning; Urban resilience";"Portugal; Adversarial machine learning; Banks (bodies of water); Environmental Protection Agency; Mapping; Network security; Predictive analytics; Risk assessment; Disaster risk reductions; Flood hazard mapping; Flood hazards; Geo-spatial; Geospatial artificial intelligence; Geospatial data analyze; Hazards mappings; Machine-learning; Urban resilience; artificial intelligence; detection method; digital elevation model; disaster management; flood control; GIS; mapping method; risk assessment; spatial analysis; urban area; Floods";"Smart hotspot detection using geospatial artificial intelligence: A machine learning approach to reduce flood risk This study employs Geospatial Artificial Intelligence (GeoAI) and the Random Forest Machine Learning (ML) algorithm to enhance flood hazard assessments in Portugal. It utilizes NASA's LP DAAC (2023) Digital Elevation Model (DEM) and slope data from EPIC WEBGIS PORTUGAL DATA, offering detailed topographical insights for environmental planning. Additionally, it incorporates data on proximity to water bodies from the Portuguese Environment Agency and the European Environment Agency, and soil characteristics from EPIC WEBGIS PORTUGAL DATA, facilitating a thorough examination of flood risks. This approach prioritizes long-term land features over short-term weather patterns, providing a comprehensive understanding of flood vulnerability. The study processes data at a 1 km x 1 km resolution, adapting TIFF maps for compatibility with the Random Forest model. The produced flood hazard maps identify potential flood hotspots at both national and city levels, crucial for urban planning. These maps aid in assessing the vulnerability of key infrastructure and assets, such as transport networks and buildings. The research highlights the importance of integrating additional data on assets and socioeconomic factors to enhance urban resilience. It sets the stage for future research aimed at improving predictive accuracy and underscores the necessity of extensive geospatial analytics in managing infrastructure risks. © 2024 The Author(s) Disaster risk reduction; Flood hazard mapping; Geospatial artificial intelligence; Geospatial data analysis; Machine learning; Urban resilience Portugal; Adversarial machine learning; Banks (bodies of water); Environmental Protection Agency; Mapping; Network security; Predictive analytics; Risk assessment; Disaster risk reductions; Flood hazard mapping; Flood hazards; Geo-spatial; Geospatial artificial intelligence; Geospatial data analyze; Hazards mappings; Machine-learning; Urban resilience; artificial intelligence; detection method; digital elevation model; disaster management; flood control; GIS; mapping method; risk assessment; spatial analysis; urban area; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
270;Improving deep learning-based streamflow forecasting under trend varying conditions through evaluation of new wavelet preprocessing technique;Accurate machine learning streamflow prediction often requires coupling data-driven models with preprocessing techniques. This study aims to improve the performance of deep learning (DL) models, including long short-term memory, recurrent neural network (RNN), and gated recurrent unit (GRU) by incorporating maximal overlap discrete wavelet entropy transform (MODWET) techniques for streamflow forecasting. The merit of MODWET over maximal overlap discrete wavelet transform (MODWT) is that MODWET utilizes Entropy to determine the optimal decomposition level and suitable wavelet function, which was an unaddressed problem in wavelet-based decomposition models. Suitable decomposition level prevents providing unnecessary information or missing essential information. In this study we show that a unique decomposition level and wavelet filter is not suitable for any dataset. The research focuses on monthly streamflow data from three case studies in the CAMEL dataset in the USA. The accuracy of the models is evaluated using statistical measures such as Nash–Sutcliffe efficiency (NSE), root-mean-squared error, percent bias, and correlation coefficient (r). To determine the optimal model, a Taylor diagram is utilized. The results demonstrate the effectiveness of coupling MODWET with DL models in flood forecasting. Furthermore, genetic programming (GP) and partial correlation index (PCI) are employed for predictor selection. Hybrid models, namely MODWET-GP-GRU (NSE of 0.83), MODWET-GP-RNN (NSE of 0.95), and MODWET-PCI-GRU (NSE of 0.95), outperform simple DL models in terms of NSE and Taylor diagram evaluation. This study emphasizes the potential of hybrid models that combine DL algorithms with the recently proposed MODWET technique for streamflow prediction. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.;"Decomposition level; Deep learning models; MODWET; Streamflow forecasting; Wavelet function";"United States; Discrete wavelet transforms; Entropy; Flood control; Function evaluation; Genetic algorithms; Genetic programming; Learning systems; Recurrent neural networks; Signal reconstruction; Stream flow; Wavelet decomposition; Decomposition level; Deep learning model; Discrete wavelets; Learning models; Maximal overlap discrete wavelet entropy transform; Pre-processing techniques; Streamflow forecasting; Streamflow prediction; Wavelet entropies; Wavelet function; algorithm; forecasting method; machine learning; streamflow; trend analysis; wavelet analysis; Mean square error";"Improving deep learning-based streamflow forecasting under trend varying conditions through evaluation of new wavelet preprocessing technique Accurate machine learning streamflow prediction often requires coupling data-driven models with preprocessing techniques. This study aims to improve the performance of deep learning (DL) models, including long short-term memory, recurrent neural network (RNN), and gated recurrent unit (GRU) by incorporating maximal overlap discrete wavelet entropy transform (MODWET) techniques for streamflow forecasting. The merit of MODWET over maximal overlap discrete wavelet transform (MODWT) is that MODWET utilizes Entropy to determine the optimal decomposition level and suitable wavelet function, which was an unaddressed problem in wavelet-based decomposition models. Suitable decomposition level prevents providing unnecessary information or missing essential information. In this study we show that a unique decomposition level and wavelet filter is not suitable for any dataset. The research focuses on monthly streamflow data from three case studies in the CAMEL dataset in the USA. The accuracy of the models is evaluated using statistical measures such as Nash–Sutcliffe efficiency (NSE), root-mean-squared error, percent bias, and correlation coefficient (r). To determine the optimal model, a Taylor diagram is utilized. The results demonstrate the effectiveness of coupling MODWET with DL models in flood forecasting. Furthermore, genetic programming (GP) and partial correlation index (PCI) are employed for predictor selection. Hybrid models, namely MODWET-GP-GRU (NSE of 0.83), MODWET-GP-RNN (NSE of 0.95), and MODWET-PCI-GRU (NSE of 0.95), outperform simple DL models in terms of NSE and Taylor diagram evaluation. This study emphasizes the potential of hybrid models that combine DL algorithms with the recently proposed MODWET technique for streamflow prediction. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024. Decomposition level; Deep learning models; MODWET; Streamflow forecasting; Wavelet function United States; Discrete wavelet transforms; Entropy; Flood control; Function evaluation; Genetic algorithms; Genetic programming; Learning systems; Recurrent neural networks; Signal reconstruction; Stream flow; Wavelet decomposition; Decomposition level; Deep learning model; Discrete wavelets; Learning models; Maximal overlap discrete wavelet entropy transform; Pre-processing techniques; Streamflow forecasting; Streamflow prediction; Wavelet entropies; Wavelet function; algorithm; forecasting method; machine learning; streamflow; trend analysis; wavelet analysis; Mean square error";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
271;IoT and AI: a panacea for climate change-resilient smart agriculture;Abstract: The application of Internet of Things (IoT) and Artificial Intelligence (AI) for disaster preparedness and sustainable agriculture has been a topic of great interest lately. In the last few years, extreme weather swings due to climate change caused by global warming have caught the farming community off guard, especially in the developing world. One of the key objectives of smart agriculture is optimal use of freshwater, which has become an increasingly scarce resource around the world. Reference Evapotranspiration (ETo), an estimation of total flux of water evaporating from a reference surface is an important parameter for irrigation management. IoT & AI-based location-specific estimation of ETo for crop water requirements augments the decision-making process. In this work, we utilize the Hargeaves and Samani (H–S) model and six regression algorithms for the estimation of ETo. We create a location-specific dataset with locally sensed IoT data from a flood warning system and remotely sensed meteorological data, spanning over 5 years. We train and test Linear Regression (LR), Multilayer Perceptron (MLP), Radial Basis Function (RBF), Support Vector Regression (SVR), Bagging and Random Forest (RF) algorithms on the locally curated dataset with 20 basic, extracted, and derived attributes. We gradually reduce number of attributes in the dataset from 20 to 3 and compare performance of the six algorithms using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Relative Absolute Error (RAE), Root Relative Squared Error (RRSE), Coefficient of Determination R2, Kendall Tau and Spearman Rho metrics. SVR shows superior performance with an MAE of 0.03 and an RMSE of 0.05, followed closely by MLP with an MAE of 0.04 and RMSE of 0.06 with a dataset of 12 attributes. The performance of Bagging and RF algorithms remains relatively unchanged with feature reduction whereas RBF shows slight improvement in performance when number of attributes is reduced to 3. Finally, we develop a novel ensemble hybrid model using the Stacked Generalization technique, which outperforms all individual models in prediction accuracy when using reduced-feature datasets. This work clearly delineates the performances of a diverse set of ML algorithms for feature-rich and feature-scarce scenarios and demonstrates the efficacy of our hybrid ensemble ML algorithm for estimating ETo under limited availability of data in resource-constrained environments. © The Author(s) 2024.;NULL;"Fertilizers; Forward error correction; Linear regression; Time difference of arrival; Base function; Disaster preparedness; Mean absolute error; Multilayers perceptrons; Performance; Radial basis; Random forest algorithm; Root mean squared errors; Smart agricultures; Support vector regressions; Support vector regression";"IoT and AI: a panacea for climate change-resilient smart agriculture Abstract: The application of Internet of Things (IoT) and Artificial Intelligence (AI) for disaster preparedness and sustainable agriculture has been a topic of great interest lately. In the last few years, extreme weather swings due to climate change caused by global warming have caught the farming community off guard, especially in the developing world. One of the key objectives of smart agriculture is optimal use of freshwater, which has become an increasingly scarce resource around the world. Reference Evapotranspiration (ETo), an estimation of total flux of water evaporating from a reference surface is an important parameter for irrigation management. IoT & AI-based location-specific estimation of ETo for crop water requirements augments the decision-making process. In this work, we utilize the Hargeaves and Samani (H–S) model and six regression algorithms for the estimation of ETo. We create a location-specific dataset with locally sensed IoT data from a flood warning system and remotely sensed meteorological data, spanning over 5 years. We train and test Linear Regression (LR), Multilayer Perceptron (MLP), Radial Basis Function (RBF), Support Vector Regression (SVR), Bagging and Random Forest (RF) algorithms on the locally curated dataset with 20 basic, extracted, and derived attributes. We gradually reduce number of attributes in the dataset from 20 to 3 and compare performance of the six algorithms using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Relative Absolute Error (RAE), Root Relative Squared Error (RRSE), Coefficient of Determination R2, Kendall Tau and Spearman Rho metrics. SVR shows superior performance with an MAE of 0.03 and an RMSE of 0.05, followed closely by MLP with an MAE of 0.04 and RMSE of 0.06 with a dataset of 12 attributes. The performance of Bagging and RF algorithms remains relatively unchanged with feature reduction whereas RBF shows slight improvement in performance when number of attributes is reduced to 3. Finally, we develop a novel ensemble hybrid model using the Stacked Generalization technique, which outperforms all individual models in prediction accuracy when using reduced-feature datasets. This work clearly delineates the performances of a diverse set of ML algorithms for feature-rich and feature-scarce scenarios and demonstrates the efficacy of our hybrid ensemble ML algorithm for estimating ETo under limited availability of data in resource-constrained environments. © The Author(s) 2024. NULL Fertilizers; Forward error correction; Linear regression; Time difference of arrival; Base function; Disaster preparedness; Mean absolute error; Multilayers perceptrons; Performance; Radial basis; Random forest algorithm; Root mean squared errors; Smart agricultures; Support vector regressions; Support vector regression";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
272;Effects of grass-shrub vegetation and litter on overland flow resistance coefficients;Vegetation communities can drastically influence the hydrodynamic characteristics of the overland flow, change the overland flow resistance mechanism, and regulate soil erosion. To investigate the effect of grass-shrub vegetation and litter combinations on overland flow resistance, simulated rainfall experiments were carried out. The experiments involved a 15° gradient slope, five rainfall intensities (I = 60-120 mm h−1), five grass-shrub coverages (Cgs), and six litter volumes (Cl). The results showed that the mean values of form resistance under Cgs = 15%-75% ranged from 0.195 to 1.775, and the mean values of form resistance under Cl = 5-25 g m−2 ranged from 0.609 to 2.160. The form resistance of grass-shrub slopes with the addition of litter was 1.82-12.47 times higher than that under a single grass-shrub cover. The order of magnitude of the factors influencing the slope form resistance coefficients of the grass-shrub + litter was Cl > Cgs > Cl × Cgs > I. With varying vegetation kinds and coverage rates, I had diverse effects on form resistance. The resistance superposition principle does not hold under combined vegetation cover conditions. The degree of difference ( Δ f form ) in form resistance was inversely proportional to I and directly proportional to Cl. Through dimensional and multiple nonlinear regression analyses, a general model for calculating form resistance was established (Adj. R2 = 0.99, NSE = 0.97). Theoretically, the results of this study can be used to evaluate soil and water conservation during vegetation community succession and restoration. © 2024 Author(s).;NULL;"Vegetation; Flow changes; Flow resistance; Flow-resistance coefficients; Form resistances; Hydrodynamic characteristics; Mean values; Overland flow; Resistance mechanisms; Soil erosion; Vegetation community; Abiotic";"Effects of grass-shrub vegetation and litter on overland flow resistance coefficients Vegetation communities can drastically influence the hydrodynamic characteristics of the overland flow, change the overland flow resistance mechanism, and regulate soil erosion. To investigate the effect of grass-shrub vegetation and litter combinations on overland flow resistance, simulated rainfall experiments were carried out. The experiments involved a 15° gradient slope, five rainfall intensities (I = 60-120 mm h−1), five grass-shrub coverages (Cgs), and six litter volumes (Cl). The results showed that the mean values of form resistance under Cgs = 15%-75% ranged from 0.195 to 1.775, and the mean values of form resistance under Cl = 5-25 g m−2 ranged from 0.609 to 2.160. The form resistance of grass-shrub slopes with the addition of litter was 1.82-12.47 times higher than that under a single grass-shrub cover. The order of magnitude of the factors influencing the slope form resistance coefficients of the grass-shrub + litter was Cl > Cgs > Cl × Cgs > I. With varying vegetation kinds and coverage rates, I had diverse effects on form resistance. The resistance superposition principle does not hold under combined vegetation cover conditions. The degree of difference ( Δ f form ) in form resistance was inversely proportional to I and directly proportional to Cl. Through dimensional and multiple nonlinear regression analyses, a general model for calculating form resistance was established (Adj. R2 = 0.99, NSE = 0.97). Theoretically, the results of this study can be used to evaluate soil and water conservation during vegetation community succession and restoration. © 2024 Author(s). NULL Vegetation; Flow changes; Flow resistance; Flow-resistance coefficients; Form resistances; Hydrodynamic characteristics; Mean values; Overland flow; Resistance mechanisms; Soil erosion; Vegetation community; Abiotic";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
273;Generating a Landslide Susceptibility Map Using Integrated Meta-Heuristic Optimization and Machine Learning Models;A landslide susceptibility assessment is one of the critical steps in planning for landslide disaster prevention. Advanced machine learning methods can be used as data-driven approaches for landslide susceptibility zonation with several landslide conditioning factors. Despite there being a number of studies on landslide susceptibility assessment, the literature is limited in several contexts, such as parameter optimization, an examination of the factors in detail, and study area. This study addresses these lacks in the literature and aims to develop a landslide susceptibility map of Kentucky, US. Four machine learning methods, namely artificial neural network (ANN), k-nearest neighbor (KNN), support vector machine (SVM), and stochastic gradient boosting (SGB), were used to train the dataset comprising sixteen landslide conditioning factors after pre-processing the data in terms of data encoding, data scaling, and dimension reduction. The hyperparameters of the machine learning methods were optimized using a state-of-the-art artificial bee colony (ABC) algorithm. The permutation importance and Shapley additive explanations (SHAP) methods were employed to reduce the dimension of the dataset and examine the contributions of each landslide conditioning factor to the output variable, respectively. The findings show that the ABC-SGB hybrid model achieved the highest prediction performance. The SHAP summary plot developed using the ABC-SGB model shows that intense precipitation, distance to faults, and slope were the most significant factors affecting landslide susceptibility. The SHAP analysis further underlines that increases in intense precipitation, distance to faults, and slope are associated with an increase in the probability of landslide incidents. The findings attained in this study can be used by decision makers to develop the most effective resource allocation plan for preventing landslides and minimizing related damages. © 2024 by the author.;"artificial bee colony algorithm; landslide susceptibility; landslides; machine learning; model interpretability";"Kentucky; United States; algorithm; artificial neural network; bee; data interpretation; heuristics; landslide; machine learning; optimization; resource allocation; support vector machine";"Generating a Landslide Susceptibility Map Using Integrated Meta-Heuristic Optimization and Machine Learning Models A landslide susceptibility assessment is one of the critical steps in planning for landslide disaster prevention. Advanced machine learning methods can be used as data-driven approaches for landslide susceptibility zonation with several landslide conditioning factors. Despite there being a number of studies on landslide susceptibility assessment, the literature is limited in several contexts, such as parameter optimization, an examination of the factors in detail, and study area. This study addresses these lacks in the literature and aims to develop a landslide susceptibility map of Kentucky, US. Four machine learning methods, namely artificial neural network (ANN), k-nearest neighbor (KNN), support vector machine (SVM), and stochastic gradient boosting (SGB), were used to train the dataset comprising sixteen landslide conditioning factors after pre-processing the data in terms of data encoding, data scaling, and dimension reduction. The hyperparameters of the machine learning methods were optimized using a state-of-the-art artificial bee colony (ABC) algorithm. The permutation importance and Shapley additive explanations (SHAP) methods were employed to reduce the dimension of the dataset and examine the contributions of each landslide conditioning factor to the output variable, respectively. The findings show that the ABC-SGB hybrid model achieved the highest prediction performance. The SHAP summary plot developed using the ABC-SGB model shows that intense precipitation, distance to faults, and slope were the most significant factors affecting landslide susceptibility. The SHAP analysis further underlines that increases in intense precipitation, distance to faults, and slope are associated with an increase in the probability of landslide incidents. The findings attained in this study can be used by decision makers to develop the most effective resource allocation plan for preventing landslides and minimizing related damages. © 2024 by the author. artificial bee colony algorithm; landslide susceptibility; landslides; machine learning; model interpretability Kentucky; United States; algorithm; artificial neural network; bee; data interpretation; heuristics; landslide; machine learning; optimization; resource allocation; support vector machine";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
274;LandslideNet: A landslide semantic segmentation network based on single-temporal optical remote sensing images;Swiftly and accurately acquiring the spatial distribution, location, and magnitude of landslides while documenting them in a landslide cataloging database can furnish crucial information for precise disaster mitigation measures and secondary hazard prevention. The extraction of landslides using existing semantic segmentation algorithms may give rise to issues such as false detection and missed detection due to the diverse shape and texture features of landslides in remote sensing images, the abundance of spectral features, and the complexity of the environment. In this article, we proposed LandslideNet, a novel model specifically designed for accurate segmentation of landslides in single-temporal high spatial resolution optical remote sensing images. By constructing a landslide image dataset and employing the LandslideNet model, we successfully identify and segment landslides with high precision. Quantitative experimental results demonstrate that our LandslideNet achieves superior performance compared to widely used semantic segmentation models including U-Net, PSPNet, Deeplabv3+, HRNetv2, Segformer and GELAN-c with F1-score, mIoU, FWIoU, mPA and OA reaching 72.53 %, 78.41 %, 99.86 %, 83.33 % and 99.93 % respectively. Moreover, our model exhibits lower complexity while demonstrating improved capability in detecting landslides with complex shapes and different sizes. © 2024 COSPAR;"Deep learning; Google earth; Landslide extraction; Remote sensing; YOLOv8";"Complex networks; Deep learning; Disaster prevention; Landslides; Remote sensing; Semantic Segmentation; Semantics; Textures; Deep learning; Disaster mitigation; Google earths; Landslide extraction; Network-based; Optical remote sensing; Remote sensing images; Remote-sensing; Semantic segmentation; YOLOv8; Extraction";"LandslideNet: A landslide semantic segmentation network based on single-temporal optical remote sensing images Swiftly and accurately acquiring the spatial distribution, location, and magnitude of landslides while documenting them in a landslide cataloging database can furnish crucial information for precise disaster mitigation measures and secondary hazard prevention. The extraction of landslides using existing semantic segmentation algorithms may give rise to issues such as false detection and missed detection due to the diverse shape and texture features of landslides in remote sensing images, the abundance of spectral features, and the complexity of the environment. In this article, we proposed LandslideNet, a novel model specifically designed for accurate segmentation of landslides in single-temporal high spatial resolution optical remote sensing images. By constructing a landslide image dataset and employing the LandslideNet model, we successfully identify and segment landslides with high precision. Quantitative experimental results demonstrate that our LandslideNet achieves superior performance compared to widely used semantic segmentation models including U-Net, PSPNet, Deeplabv3+, HRNetv2, Segformer and GELAN-c with F1-score, mIoU, FWIoU, mPA and OA reaching 72.53 %, 78.41 %, 99.86 %, 83.33 % and 99.93 % respectively. Moreover, our model exhibits lower complexity while demonstrating improved capability in detecting landslides with complex shapes and different sizes. © 2024 COSPAR Deep learning; Google earth; Landslide extraction; Remote sensing; YOLOv8 Complex networks; Deep learning; Disaster prevention; Landslides; Remote sensing; Semantic Segmentation; Semantics; Textures; Deep learning; Disaster mitigation; Google earths; Landslide extraction; Network-based; Optical remote sensing; Remote sensing images; Remote-sensing; Semantic segmentation; YOLOv8; Extraction";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
275;Enhancing Flood Management and Early Warning Systems in Mountain Rivers: An ANN-Based Approach;The study focuses on developing an HEC-RAS 1D model to simulate flood hydrographs at desired locations, considering the inflows from two different rivers Rishiganga and Dhauliganga. The objective is to analyze the impact of inflow hydrographs from the aforementioned rivers, utilizing three sets of flood data: high flow (2400 m3/sec), low flow (15 m3/sec), and 1.5 times the high flow (3600 m3/sec) peaks. An Artificial Neural Network (ANN) model, employing the Feed-forward Backpropagation method, is utilized to train the neurons using the maximum flow hydrograph. The trained ANN model is subsequently tested with two unseen inflow hydrographs of low and 1.5 x high flow to evaluate its robustness. The findings reveal that the ANN model performs very well in flood estimation, providing efficient predictions based on the testing with the maximum flow and validating with the minimum and 1.5 times higher inflow hydrographs. Its primary advantage lies in saving time, enabling timely actions when flood warnings are issued. This research significantly contributes to flood management and shall enhance the lead time in disseminating early warning to local authorities and communities. Work shall help in providing timely and accurate flood predictions for proactive measures during any flood event. The study's significance is rooted in its potential to enhance flood preparedness and response in areas affected by the Rishiganga and Dhauliganga rivers, ultimately ensuring the safety and well-being of local communities and infrastructures. © 2024 IDRiM Society. All rights reserved.;"ANN model; barrage; flood hydrograph; flood management; HEC-RAS; hydraulic model";NULL;"Enhancing Flood Management and Early Warning Systems in Mountain Rivers: An ANN-Based Approach The study focuses on developing an HEC-RAS 1D model to simulate flood hydrographs at desired locations, considering the inflows from two different rivers Rishiganga and Dhauliganga. The objective is to analyze the impact of inflow hydrographs from the aforementioned rivers, utilizing three sets of flood data: high flow (2400 m3/sec), low flow (15 m3/sec), and 1.5 times the high flow (3600 m3/sec) peaks. An Artificial Neural Network (ANN) model, employing the Feed-forward Backpropagation method, is utilized to train the neurons using the maximum flow hydrograph. The trained ANN model is subsequently tested with two unseen inflow hydrographs of low and 1.5 x high flow to evaluate its robustness. The findings reveal that the ANN model performs very well in flood estimation, providing efficient predictions based on the testing with the maximum flow and validating with the minimum and 1.5 times higher inflow hydrographs. Its primary advantage lies in saving time, enabling timely actions when flood warnings are issued. This research significantly contributes to flood management and shall enhance the lead time in disseminating early warning to local authorities and communities. Work shall help in providing timely and accurate flood predictions for proactive measures during any flood event. The study's significance is rooted in its potential to enhance flood preparedness and response in areas affected by the Rishiganga and Dhauliganga rivers, ultimately ensuring the safety and well-being of local communities and infrastructures. © 2024 IDRiM Society. All rights reserved. ANN model; barrage; flood hydrograph; flood management; HEC-RAS; hydraulic model NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
276;Time history seismic response prediction of multiple homogeneous building structures using only one deep learning-based Structure Temporal Fusion Network;Structural response prediction under earthquakes is crucial for evaluating the structural performance and subsequent functional restoration. Deep learning provides the potential to rapidly obtain the responses by skipping the time-consuming nonlinear finite element analysis. However, a single deep learning network may only predict the time history responses of one specific structure, resulting in redundancy and resource waste when building multiple networks for modeling different structures. Thus, this study proposes a Structure Temporal Fusion Network (STFN) that can predict responses of various homogeneous structures using a single network. The key concept is that the seismic waves and the structural characteristics, such as story numbers, are fused together to predict diverse time history responses. Two numeric experiments are conducted, including predicting responses of ideal single-degree-of-freedom (SDOF) structures and regular multistory reinforced concrete frames. Furthermore, a series of ablation analyses are carried out to validate the network architecture. The results indicate that STFN can predict nonlinear time history responses of different structures with mean square errors in the magnitude of (Formula presented.) and (Formula presented.) for two experiments, respectively. The solutions also highlight the importance of fusing static characteristics for the modeling of various structures with only one network. The STFN presents a promising solution for time history response prediction across multiple structures in regions. © 2024 John Wiley & Sons Ltd.;"deep learning; multiple structures; seismic response prediction; structure characteristic; time history response";"Deep learning; Degrees of freedom (mechanics); Earthquakes; Forecasting; Mean square error; Network architecture; Reinforced concrete; Seismic waves; Structural analysis; Time series analysis; Building structure; Deep learning; Different structure; Multiple structures; Response prediction; Seismic response prediction; Structural performance; Structural response prediction; Structure characteristic; Time history response; building; earthquake engineering; machine learning; prediction; seismic response; structural analysis; structural response; Seismic response";"Time history seismic response prediction of multiple homogeneous building structures using only one deep learning-based Structure Temporal Fusion Network Structural response prediction under earthquakes is crucial for evaluating the structural performance and subsequent functional restoration. Deep learning provides the potential to rapidly obtain the responses by skipping the time-consuming nonlinear finite element analysis. However, a single deep learning network may only predict the time history responses of one specific structure, resulting in redundancy and resource waste when building multiple networks for modeling different structures. Thus, this study proposes a Structure Temporal Fusion Network (STFN) that can predict responses of various homogeneous structures using a single network. The key concept is that the seismic waves and the structural characteristics, such as story numbers, are fused together to predict diverse time history responses. Two numeric experiments are conducted, including predicting responses of ideal single-degree-of-freedom (SDOF) structures and regular multistory reinforced concrete frames. Furthermore, a series of ablation analyses are carried out to validate the network architecture. The results indicate that STFN can predict nonlinear time history responses of different structures with mean square errors in the magnitude of (Formula presented.) and (Formula presented.) for two experiments, respectively. The solutions also highlight the importance of fusing static characteristics for the modeling of various structures with only one network. The STFN presents a promising solution for time history response prediction across multiple structures in regions. © 2024 John Wiley & Sons Ltd. deep learning; multiple structures; seismic response prediction; structure characteristic; time history response Deep learning; Degrees of freedom (mechanics); Earthquakes; Forecasting; Mean square error; Network architecture; Reinforced concrete; Seismic waves; Structural analysis; Time series analysis; Building structure; Deep learning; Different structure; Multiple structures; Response prediction; Seismic response prediction; Structural performance; Structural response prediction; Structure characteristic; Time history response; building; earthquake engineering; machine learning; prediction; seismic response; structural analysis; structural response; Seismic response";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
277;Effectiveness of Generative AI for Post-Earthquake Damage Assessment;After an earthquake, rapid assessment of building damage is crucial for emergency response, reconstruction planning, and public safety. This study evaluates the performance of various Generative Artificial Intelligence (GAI) models in analyzing post-earthquake images to classify structural damage according to the EMS-98 scale, ranging from minor damage to total destruction. Correct classification rates for masonry buildings varied from 28.6% to 64.3%, with mean damage grade errors between 0.50 and 0.79, while for reinforced concrete buildings, rates ranged from 37.5% to 75.0%, with errors between 0.50 and 0.88. Fine-tuning these models could substantially improve accuracy. The practical implications are significant: integrating accurate GAI models into disaster response protocols can drastically reduce the time and resources required for damage assessment compared to traditional methods. This acceleration enables emergency services to make faster, data-driven decisions, optimize resource allocation, and potentially save lives. Furthermore, the widespread adoption of GAI models can enhance resilience planning by providing valuable data for future infrastructure improvements. The results of this work demonstrate the promise of GAI models for rapid, automated, and precise damage evaluation, underscoring their potential as invaluable tools for engineers, policymakers, and emergency responders in post-earthquake scenarios. © 2024 by the author.;"damage classification; EMS-98 scale; generative artificial intelligence; post-earthquake damage assessment";"Concrete buildings; Disasters; Earthquake effects; Earthquake engineering; Building damage; Damage assessments; Damage classification; Earthquake damages; Emergency response; EMS-98 scale; Generative artificial intelligence; Intelligence models; Post-earthquake damage assessment; Rapid assessment; Reinforced concrete";"Effectiveness of Generative AI for Post-Earthquake Damage Assessment After an earthquake, rapid assessment of building damage is crucial for emergency response, reconstruction planning, and public safety. This study evaluates the performance of various Generative Artificial Intelligence (GAI) models in analyzing post-earthquake images to classify structural damage according to the EMS-98 scale, ranging from minor damage to total destruction. Correct classification rates for masonry buildings varied from 28.6% to 64.3%, with mean damage grade errors between 0.50 and 0.79, while for reinforced concrete buildings, rates ranged from 37.5% to 75.0%, with errors between 0.50 and 0.88. Fine-tuning these models could substantially improve accuracy. The practical implications are significant: integrating accurate GAI models into disaster response protocols can drastically reduce the time and resources required for damage assessment compared to traditional methods. This acceleration enables emergency services to make faster, data-driven decisions, optimize resource allocation, and potentially save lives. Furthermore, the widespread adoption of GAI models can enhance resilience planning by providing valuable data for future infrastructure improvements. The results of this work demonstrate the promise of GAI models for rapid, automated, and precise damage evaluation, underscoring their potential as invaluable tools for engineers, policymakers, and emergency responders in post-earthquake scenarios. © 2024 by the author. damage classification; EMS-98 scale; generative artificial intelligence; post-earthquake damage assessment Concrete buildings; Disasters; Earthquake effects; Earthquake engineering; Building damage; Damage assessments; Damage classification; Earthquake damages; Emergency response; EMS-98 scale; Generative artificial intelligence; Intelligence models; Post-earthquake damage assessment; Rapid assessment; Reinforced concrete";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
278;FloodGNN-GRU: A spatio-temporal graph neural network for flood prediction;"Classical approaches for flood prediction apply numerical methods for the solution of partial differential equations that capture the physics of inundation processes (e.g., the 2D Shallow Water equations). However, traditional inundation models are still unable to satisfy the requirements of many relevant applications, including early-warning systems, high-resolution (or large spatial domain) simulations, and robust inference over distributions of inputs (e.g., rainfall events). Machine learning (ML) approaches are a promising alternative to physics-based models due to their ability to efficiently capture correlations between relevant inputs and outputs in a data-driven fashion. In particular, once trained, ML models can be tested/deployed much more efficiently than classical approaches. Yet, few ML-based solutions for spatio-temporal flood prediction have been developed, and their reliability/accuracy is poorly understood. In this paper, we propose FloodGNN-GRU, a spatio-temporal flood prediction model that combines a graph neural network (GNN) and a gated recurrent unit (GRU) architecture. Compared to existing approaches, FloodGNN-GRU (i) employs a graph-based model (GNN); (ii) operates on both spatial and temporal dimensions; and (iii) processes the water flow velocities as vector features, instead of scalar features. We evaluate FloodGNN-GRU using a LISFLOOD-FP simulation of Hurricane Harvey (2017) in Houston, Texas. Our results, based on several metrics, show that FloodGNN-GRU outperforms several data-driven alternatives in terms of accuracy. Moreover, our approach can be trained 100x faster and tested 1000x faster than the time required to run a comparable simulation. These findings illustrate the potential of ML-based methods to efficiently emulate physics-based inundation models, especially for short-term predictions.  © The Author(s), 2024.";"deep learning; flood prediction; graph neural networks; machine learning";NULL;"FloodGNN-GRU: A spatio-temporal graph neural network for flood prediction Classical approaches for flood prediction apply numerical methods for the solution of partial differential equations that capture the physics of inundation processes (e.g., the 2D Shallow Water equations). However, traditional inundation models are still unable to satisfy the requirements of many relevant applications, including early-warning systems, high-resolution (or large spatial domain) simulations, and robust inference over distributions of inputs (e.g., rainfall events). Machine learning (ML) approaches are a promising alternative to physics-based models due to their ability to efficiently capture correlations between relevant inputs and outputs in a data-driven fashion. In particular, once trained, ML models can be tested/deployed much more efficiently than classical approaches. Yet, few ML-based solutions for spatio-temporal flood prediction have been developed, and their reliability/accuracy is poorly understood. In this paper, we propose FloodGNN-GRU, a spatio-temporal flood prediction model that combines a graph neural network (GNN) and a gated recurrent unit (GRU) architecture. Compared to existing approaches, FloodGNN-GRU (i) employs a graph-based model (GNN); (ii) operates on both spatial and temporal dimensions; and (iii) processes the water flow velocities as vector features, instead of scalar features. We evaluate FloodGNN-GRU using a LISFLOOD-FP simulation of Hurricane Harvey (2017) in Houston, Texas. Our results, based on several metrics, show that FloodGNN-GRU outperforms several data-driven alternatives in terms of accuracy. Moreover, our approach can be trained 100x faster and tested 1000x faster than the time required to run a comparable simulation. These findings illustrate the potential of ML-based methods to efficiently emulate physics-based inundation models, especially for short-term predictions.  © The Author(s), 2024. deep learning; flood prediction; graph neural networks; machine learning NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
279;Data-driven modelling of coastal storm erosion for real-time forecasting at a wave-dominated embayed beach;Emergency managers have an increasing need for tools to enhance preparedness to extreme coastal storms and support disaster risk reduction measures. With the emergence of Early Warning Systems (EWSs) for coastal storm hazards, a fundamental challenge is the accurate prediction of sandy beach erosion at lead times of days to weeks corresponding to an approaching storm event. This work presents a data-driven modelling approach to predict storm-driven beach erosion (shoreline change) using a large dataset of 276 individual storm events at Narrabeen-Collaroy Beach, SE Australia. Correlation analysis between individual storm characteristics and shoreline response at three locations along the embayment with varying exposure to the prevailing waves indicates that cumulative storm wave energy is the dominant driver of storm erosion at this site. This is followed by the pre-storm beach width, storm wave direction and to a minimal extent, storm wave period and water levels. A multi-linear regression model of storm erosion is developed and found to accurately predict shoreline change due to individual storm events (RMSE = 3.7 m–6.4 m). This work highlights the value of high-frequency shoreline data for storm erosion forecasting and provides a framework for real-time forecasting applications. © 2024 The Authors;"Argus; Early warning system; Multi-linear regression; Narrabeen; Remote sensing; Shoreline change";"Australia; Narrabeen Beach; New South Wales; Erosion; Linear regression; Prediction models; Risk assessment; Weather forecasting; Argi; Coastal storm; Early Warning System; Multilinear regression; Narrabeen; Remote-sensing; Shoreline change; Storm erosion; Storm events; Storm waves; beach erosion; early warning system; modeling; multiple regression; remote sensing; shoreline change; storm; Beaches";"Data-driven modelling of coastal storm erosion for real-time forecasting at a wave-dominated embayed beach Emergency managers have an increasing need for tools to enhance preparedness to extreme coastal storms and support disaster risk reduction measures. With the emergence of Early Warning Systems (EWSs) for coastal storm hazards, a fundamental challenge is the accurate prediction of sandy beach erosion at lead times of days to weeks corresponding to an approaching storm event. This work presents a data-driven modelling approach to predict storm-driven beach erosion (shoreline change) using a large dataset of 276 individual storm events at Narrabeen-Collaroy Beach, SE Australia. Correlation analysis between individual storm characteristics and shoreline response at three locations along the embayment with varying exposure to the prevailing waves indicates that cumulative storm wave energy is the dominant driver of storm erosion at this site. This is followed by the pre-storm beach width, storm wave direction and to a minimal extent, storm wave period and water levels. A multi-linear regression model of storm erosion is developed and found to accurately predict shoreline change due to individual storm events (RMSE = 3.7 m–6.4 m). This work highlights the value of high-frequency shoreline data for storm erosion forecasting and provides a framework for real-time forecasting applications. © 2024 The Authors Argus; Early warning system; Multi-linear regression; Narrabeen; Remote sensing; Shoreline change Australia; Narrabeen Beach; New South Wales; Erosion; Linear regression; Prediction models; Risk assessment; Weather forecasting; Argi; Coastal storm; Early Warning System; Multilinear regression; Narrabeen; Remote-sensing; Shoreline change; Storm erosion; Storm events; Storm waves; beach erosion; early warning system; modeling; multiple regression; remote sensing; shoreline change; storm; Beaches";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
280;The empirical study of tweet classification system for disaster response using shallow and deep learning models;"Disaster-based tweets during an emergency consist of a variety of information on people who have been hurt or killed, people who are lost or discovered, infrastructure and utilities destroyed; this information can assist governmental and humanitarian organizations in prioritizing their aid and rescue efforts. It is crucial to build a model that can categorize these tweets into distinct types due to their massive volume so as to better organize rescue and relief effort and save lives. In this study, Twitter data of 2013 Queensland flood and 2015 Nepal earthquake has been classified as disaster or non-disaster by employing three classes of models. The first model is performed using the lexical feature based on Term Frequency-Inverse Document Frequency (TF-IDF). The classification was performed using five classification algorithms such as DT, LR, SVM, RF, while Ensemble Voting was used to produce the outcome of the models. The second model uses shallow classifiers in conjunction with several features, including lexical (TF-IDF), hashtag, POS, and GloVe embedding. The third set of the model utilized deep learning algorithms including LSTM, LSTM, and GRU, using BERT (Bidirectional Encoder Representations from Transformers) for constructing semantic word embedding to learn the context. The key performance evaluation metrics such as accuracy, F1 score, recall, and precision were employed to measure and compare the three sets of models for disaster response classification on two publicly available Twitter datasets. By performing a comprehensive empirical evaluation of the tweet classification technique across different disaster kinds, the predictive performance shows that the best accuracy was achieved with DT algorithm which attained the highest performance accuracy followed by Bi-LSTM models for disaster response classification by attaining the best accuracy of 96.46% and 96.40% on the Queensland flood dataset; DT algorithm also attained 78.3% accuracy on the Nepal earthquake dataset based on the majority-voting ensemble respectively. Thus, this research contributes by investigating the integration of deep and shallow learning models effectively in a tweet classification system designed for disaster response. Examining the ways that these two methods work seamlessly offers insights into how to best utilize their complimentary advantages to increase the robustness and accuracy of locating suitable data in disaster crisis. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.";"Deep learning; Disaster response; Machine learning; Natural language processing; Tweet classification";"Earthquakes; Embeddings; Emergency services; Floods; Inverse problems; Long short-term memory; Natural language processing systems; Semantics; Social networking (online); Support vector machines; Text processing; Classification system; Deep learning; Disaster-response; Language processing; Learning models; Machine-learning; Natural language processing; Natural languages; Queensland; Tweet classification; Classification (of information)";"The empirical study of tweet classification system for disaster response using shallow and deep learning models Disaster-based tweets during an emergency consist of a variety of information on people who have been hurt or killed, people who are lost or discovered, infrastructure and utilities destroyed; this information can assist governmental and humanitarian organizations in prioritizing their aid and rescue efforts. It is crucial to build a model that can categorize these tweets into distinct types due to their massive volume so as to better organize rescue and relief effort and save lives. In this study, Twitter data of 2013 Queensland flood and 2015 Nepal earthquake has been classified as disaster or non-disaster by employing three classes of models. The first model is performed using the lexical feature based on Term Frequency-Inverse Document Frequency (TF-IDF). The classification was performed using five classification algorithms such as DT, LR, SVM, RF, while Ensemble Voting was used to produce the outcome of the models. The second model uses shallow classifiers in conjunction with several features, including lexical (TF-IDF), hashtag, POS, and GloVe embedding. The third set of the model utilized deep learning algorithms including LSTM, LSTM, and GRU, using BERT (Bidirectional Encoder Representations from Transformers) for constructing semantic word embedding to learn the context. The key performance evaluation metrics such as accuracy, F1 score, recall, and precision were employed to measure and compare the three sets of models for disaster response classification on two publicly available Twitter datasets. By performing a comprehensive empirical evaluation of the tweet classification technique across different disaster kinds, the predictive performance shows that the best accuracy was achieved with DT algorithm which attained the highest performance accuracy followed by Bi-LSTM models for disaster response classification by attaining the best accuracy of 96.46% and 96.40% on the Queensland flood dataset; DT algorithm also attained 78.3% accuracy on the Nepal earthquake dataset based on the majority-voting ensemble respectively. Thus, this research contributes by investigating the integration of deep and shallow learning models effectively in a tweet classification system designed for disaster response. Examining the ways that these two methods work seamlessly offers insights into how to best utilize their complimentary advantages to increase the robustness and accuracy of locating suitable data in disaster crisis. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024. Deep learning; Disaster response; Machine learning; Natural language processing; Tweet classification Earthquakes; Embeddings; Emergency services; Floods; Inverse problems; Long short-term memory; Natural language processing systems; Semantics; Social networking (online); Support vector machines; Text processing; Classification system; Deep learning; Disaster-response; Language processing; Learning models; Machine-learning; Natural language processing; Natural languages; Queensland; Tweet classification; Classification (of information)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
281;Use of the SNOWED Dataset for Sentinel-2 Remote Sensing of Water Bodies: The Case of the Po River;The paper demonstrates the effectiveness of the SNOWED dataset, specifically designed for identifying water bodies in Sentinel-2 images, in developing a remote sensing system based on deep neural networks. For this purpose, a system is implemented for monitoring the Po River, Italy’s most important watercourse. By leveraging the SNOWED dataset, a simple U-Net neural model is trained to segment satellite images and distinguish, in general, water and land regions. After verifying its performance in segmenting the SNOWED validation set, the trained neural network is employed to measure the area of water regions along the Po River, a task that involves segmenting a large number of images that are quite different from those in SNOWED. It is clearly shown that SNOWED-based water area measurements describe the river status, in terms of flood or drought periods, with a surprisingly good accordance with water level measurements provided by 23 in situ gauge stations (official measurements managed by the Interregional Agency for the Po). Consequently, the sensing system is used to take measurements at 100 “virtual” gauge stations along the Po River, over the 10-year period (2015–2024) covered by the Sentinel-2 satellites of the Copernicus Programme. In this way, an overall space-time monitoring of the Po River is obtained, with a spatial resolution unattainable, in a cost-effective way, by local physical sensors. Altogether, the obtained results demonstrate not only the usefulness of the SNOWED dataset for deep learning-based satellite sensing, but also the ability of such sensing systems to effectively complement traditional in situ sensing stations, providing precious tools for environmental monitoring, especially of locations difficult to reach, and permitting the reconstruction of historical data related to floods and draughts. Although physical monitoring stations are designed for rapid monitoring and prevention of flood or other disasters, the developed tool for remote sensing of water bodies could help decision makers to define long-term policies to reduce specific risks in areas not covered by physical monitoring or to define medium- to long-term strategies such as dam construction or infrastructure design. © 2024 by the authors.;"deep learning-based measurements; remote sensing; river monitoring; satellite monitoring; Sentinel-2 monitoring of environment; water bodies segmentation; water depth sensors";"Gages; Photomapping; Risk management; Satellite imagery; Tropics; Body segmentations; Deep learning-based measurement; Depth sensors; Remote-sensing; River monitoring; Satellite monitoring; Sentinel-2 monitoring of environment; Water body segmentation; Water depth; Water depth sensor; Waterbodies; article; controlled study; deep learning; deep neural network; drought; environmental monitoring; environmental surveillance; flooding; Italy; nerve cell network; prevention; remote sensing; river; sensor; water; water depth; Level measurement";"Use of the SNOWED Dataset for Sentinel-2 Remote Sensing of Water Bodies: The Case of the Po River The paper demonstrates the effectiveness of the SNOWED dataset, specifically designed for identifying water bodies in Sentinel-2 images, in developing a remote sensing system based on deep neural networks. For this purpose, a system is implemented for monitoring the Po River, Italy’s most important watercourse. By leveraging the SNOWED dataset, a simple U-Net neural model is trained to segment satellite images and distinguish, in general, water and land regions. After verifying its performance in segmenting the SNOWED validation set, the trained neural network is employed to measure the area of water regions along the Po River, a task that involves segmenting a large number of images that are quite different from those in SNOWED. It is clearly shown that SNOWED-based water area measurements describe the river status, in terms of flood or drought periods, with a surprisingly good accordance with water level measurements provided by 23 in situ gauge stations (official measurements managed by the Interregional Agency for the Po). Consequently, the sensing system is used to take measurements at 100 “virtual” gauge stations along the Po River, over the 10-year period (2015–2024) covered by the Sentinel-2 satellites of the Copernicus Programme. In this way, an overall space-time monitoring of the Po River is obtained, with a spatial resolution unattainable, in a cost-effective way, by local physical sensors. Altogether, the obtained results demonstrate not only the usefulness of the SNOWED dataset for deep learning-based satellite sensing, but also the ability of such sensing systems to effectively complement traditional in situ sensing stations, providing precious tools for environmental monitoring, especially of locations difficult to reach, and permitting the reconstruction of historical data related to floods and draughts. Although physical monitoring stations are designed for rapid monitoring and prevention of flood or other disasters, the developed tool for remote sensing of water bodies could help decision makers to define long-term policies to reduce specific risks in areas not covered by physical monitoring or to define medium- to long-term strategies such as dam construction or infrastructure design. © 2024 by the authors. deep learning-based measurements; remote sensing; river monitoring; satellite monitoring; Sentinel-2 monitoring of environment; water bodies segmentation; water depth sensors Gages; Photomapping; Risk management; Satellite imagery; Tropics; Body segmentations; Deep learning-based measurement; Depth sensors; Remote-sensing; River monitoring; Satellite monitoring; Sentinel-2 monitoring of environment; Water body segmentation; Water depth; Water depth sensor; Waterbodies; article; controlled study; deep learning; deep neural network; drought; environmental monitoring; environmental surveillance; flooding; Italy; nerve cell network; prevention; remote sensing; river; sensor; water; water depth; Level measurement";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
282;Intelligent Forecasting of Flooding Intensity Using Machine Learning;This innovative study addresses critical flood prediction needs in Bor County, South Sudan, utilizing machine learning to develop an intelligent forecasting model. The research integrates diverse analytical techniques, including land use analysis and rainfall calculations, with a decade of weather data to understand complex hydrological dynamics. This research employs machine learning classifiers such as Support Vector Machines, Decision Trees, and Neural Networks. Findings reveal promising results, with the Linear SVM classifier achieving 87.5% prediction accuracy for raw data and 100% accuracy for high-velocity flooding events. The Naive Bayes classifier matched this performance, while Artificial Neural Networks showed a slight advantage in runoff estimation. The study's novelty lies in its holistic approach, combining machine learning with advanced visualization tools and geographic information systems. This creates a dynamic, real-time forecasting system bridging sophisticated analysis and practical flood management strategies. Focusing on model interpretability and multi-scale forecasting enhances its value to policymakers and disaster management authorities. This research significantly advances the application of AI to flood prediction and disaster management in offering future studies on humanitarian challenges. By enhancing early warning capabilities, this system substantially reduces flood-related losses and transforms disaster preparedness in vulnerable regions worldwide, potentially saving lives and mitigating economic impacts. © 2024 by the authors.;"Classification Learners; Confusion Matrix; Flood Intensity; Rainfall Data; Support Vector Machines";NULL;"Intelligent Forecasting of Flooding Intensity Using Machine Learning This innovative study addresses critical flood prediction needs in Bor County, South Sudan, utilizing machine learning to develop an intelligent forecasting model. The research integrates diverse analytical techniques, including land use analysis and rainfall calculations, with a decade of weather data to understand complex hydrological dynamics. This research employs machine learning classifiers such as Support Vector Machines, Decision Trees, and Neural Networks. Findings reveal promising results, with the Linear SVM classifier achieving 87.5% prediction accuracy for raw data and 100% accuracy for high-velocity flooding events. The Naive Bayes classifier matched this performance, while Artificial Neural Networks showed a slight advantage in runoff estimation. The study's novelty lies in its holistic approach, combining machine learning with advanced visualization tools and geographic information systems. This creates a dynamic, real-time forecasting system bridging sophisticated analysis and practical flood management strategies. Focusing on model interpretability and multi-scale forecasting enhances its value to policymakers and disaster management authorities. This research significantly advances the application of AI to flood prediction and disaster management in offering future studies on humanitarian challenges. By enhancing early warning capabilities, this system substantially reduces flood-related losses and transforms disaster preparedness in vulnerable regions worldwide, potentially saving lives and mitigating economic impacts. © 2024 by the authors. Classification Learners; Confusion Matrix; Flood Intensity; Rainfall Data; Support Vector Machines NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
283;Landslide Assessment Classification Using Deep Neural Networks Based on Climate and Geospatial Data;This study presents a method for classifying landslide triggers and sizes using climate and geospatial data. The landslide data were sourced from the Global Landslide Catalog (GLC), which identifies rainfall-triggered landslide events globally, regardless of size, impact, or location. Compiled from 2007 to 2018 at NASA Goddard Space Flight Center, the GLC includes various mass movements triggered by rainfall and other events. Climatic data for the 10 years preceding each landslide event, including variables such as rainfall amounts, humidity, pressure, and temperature, were integrated with the landslide data. This dataset was then used to classify landslide triggers and sizes using deep neural networks (DNNs) optimized through genetic algorithm (GA)-driven hyperparameter tuning. The optimized DNN models achieved accuracies of 0.67 and 0.82, respectively, in multiclass classification tasks. This research demonstrates the effectiveness of GA to enhance landslide disaster risk management. © 2024 by the authors.;"classification; climate change adaptation; climate impact; climate variability; deep neural networks; disaster risk reduction; extreme weather; genetic algorithm; landslides; ML";"adaptation; classification; climate change; climate effect; climate variation; extreme event; genetic algorithm; landslide; machine learning; natural disaster; remote sensing; spatial data; weather";"Landslide Assessment Classification Using Deep Neural Networks Based on Climate and Geospatial Data This study presents a method for classifying landslide triggers and sizes using climate and geospatial data. The landslide data were sourced from the Global Landslide Catalog (GLC), which identifies rainfall-triggered landslide events globally, regardless of size, impact, or location. Compiled from 2007 to 2018 at NASA Goddard Space Flight Center, the GLC includes various mass movements triggered by rainfall and other events. Climatic data for the 10 years preceding each landslide event, including variables such as rainfall amounts, humidity, pressure, and temperature, were integrated with the landslide data. This dataset was then used to classify landslide triggers and sizes using deep neural networks (DNNs) optimized through genetic algorithm (GA)-driven hyperparameter tuning. The optimized DNN models achieved accuracies of 0.67 and 0.82, respectively, in multiclass classification tasks. This research demonstrates the effectiveness of GA to enhance landslide disaster risk management. © 2024 by the authors. classification; climate change adaptation; climate impact; climate variability; deep neural networks; disaster risk reduction; extreme weather; genetic algorithm; landslides; ML adaptation; classification; climate change; climate effect; climate variation; extreme event; genetic algorithm; landslide; machine learning; natural disaster; remote sensing; spatial data; weather";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
284;Fostering urban resilience and accessibility in cities: A dynamic knowledge graph approach;This paper explores the utilisation of knowledge graphs and an agent-based implementation to enhance urban resilience and accessibility in city planning. We expand The World Avatar (TWA) dynamic knowledge graph to support decision-making in disaster response and urban planning. By employing a set of connected agents and integrating diverse data sources — including flood data, geospatial building information, land plots, and open-source data — through sets of ontologies, we demonstrate disaster response in a coastal town in the UK and various aspects relevant to city planning for a mid-sized town in Germany using TWA. In King's Lynn, our agent-based approach facilitates holistic disaster response by calculating optimal routes, avoiding flooded segments dynamically, assessing infrastructure accessibility before and during a flood using isochrones, identifying inaccessible population areas, guiding infrastructure restoration, and conducting critical path analysis. In Pirmasens, for city planning purposes, the knowledge graph-driven isochrone generation provides evidence-based insights into current amenity coverage and enables scenario planning for future amenities while adhering to land regulations. The implementation of agents and knowledge graphs achieves interoperability and enhances urban resilience and accessibility by enabling cross-domain correlation analysis that extends various areas including geospatial buildings, population demographics, accessibility coverage, and land use regulations. © 2024;"15-minute city; Accessibility; Disaster resilience; Isochrone analysis; Knowledge graph";"Germany; United Kingdom; Decision making; Floods; Land use; Regression analysis; 15-minute city; Accessibility; Disaster resiliences; Disaster-response; Geo-spatial; Isochrone analyse; Isochrones; Knowledge graphs; Urban accessibilities; Urban resilience; accessibility; decision support system; disaster management; urban planning; Knowledge graph";"Fostering urban resilience and accessibility in cities: A dynamic knowledge graph approach This paper explores the utilisation of knowledge graphs and an agent-based implementation to enhance urban resilience and accessibility in city planning. We expand The World Avatar (TWA) dynamic knowledge graph to support decision-making in disaster response and urban planning. By employing a set of connected agents and integrating diverse data sources — including flood data, geospatial building information, land plots, and open-source data — through sets of ontologies, we demonstrate disaster response in a coastal town in the UK and various aspects relevant to city planning for a mid-sized town in Germany using TWA. In King's Lynn, our agent-based approach facilitates holistic disaster response by calculating optimal routes, avoiding flooded segments dynamically, assessing infrastructure accessibility before and during a flood using isochrones, identifying inaccessible population areas, guiding infrastructure restoration, and conducting critical path analysis. In Pirmasens, for city planning purposes, the knowledge graph-driven isochrone generation provides evidence-based insights into current amenity coverage and enables scenario planning for future amenities while adhering to land regulations. The implementation of agents and knowledge graphs achieves interoperability and enhances urban resilience and accessibility by enabling cross-domain correlation analysis that extends various areas including geospatial buildings, population demographics, accessibility coverage, and land use regulations. © 2024 15-minute city; Accessibility; Disaster resilience; Isochrone analysis; Knowledge graph Germany; United Kingdom; Decision making; Floods; Land use; Regression analysis; 15-minute city; Accessibility; Disaster resiliences; Disaster-response; Geo-spatial; Isochrone analyse; Isochrones; Knowledge graphs; Urban accessibilities; Urban resilience; accessibility; decision support system; disaster management; urban planning; Knowledge graph";-1;Não Classificado;NULL;1.2;Hydrological;3;Response
285;Runoff prediction based on the IGWOLSTM model: Achieving accurate flood forecasting and emergency management;With the acceleration of global climate change and urbanization, the frequency and impact of flood disasters are increasing year by year, making flood emergency management increasingly crucial for safeguarding people's lives, property, and societal stability. To enhance the accuracy of river flow prediction, this study employs an Improved Gray Wolf Optimization Algorithm (IGWO) to optimize parameters of the Long Short-Term Memory Network (LSTM) model. Experimental results demonstrate that the proposed algorithm significantly improves the accuracy of river flow prediction, achieving higher precision and better generalization compared to traditional machine learning algorithms. This method provides more reliable data support for flood warning systems, aiding in the accurate prediction of flood occurrence timing and intensity, thereby providing scientific basis for flood prevention and mitigation efforts. Moreover, this approach supports hydro-logical research, enhancing understanding of river water cycle processes and ecosystem changes. © 2024 International Association for Hydro-environment Engineering and Research, Asia Pacific Division;"Emergency management; Flood warning systems; Hydro-logical research; Improved Gray Wolf Optimization Algorithm; Long Short-Term Memory Network; Runoff prediction";"Flood damage; Information management; Prediction models; Rivers; Runoff; Emergency management; Flood warning system; Gray wolves; Hydro-logical research; Improved gray wolf optimization algorithm; Long short-term memory network; Memory network; Optimization algorithms; Runoff prediction; Short term memory; accuracy assessment; algorithm; flood forecasting; hazard management; prediction; river flow; runoff; warning system; Risk management";"Runoff prediction based on the IGWOLSTM model: Achieving accurate flood forecasting and emergency management With the acceleration of global climate change and urbanization, the frequency and impact of flood disasters are increasing year by year, making flood emergency management increasingly crucial for safeguarding people's lives, property, and societal stability. To enhance the accuracy of river flow prediction, this study employs an Improved Gray Wolf Optimization Algorithm (IGWO) to optimize parameters of the Long Short-Term Memory Network (LSTM) model. Experimental results demonstrate that the proposed algorithm significantly improves the accuracy of river flow prediction, achieving higher precision and better generalization compared to traditional machine learning algorithms. This method provides more reliable data support for flood warning systems, aiding in the accurate prediction of flood occurrence timing and intensity, thereby providing scientific basis for flood prevention and mitigation efforts. Moreover, this approach supports hydro-logical research, enhancing understanding of river water cycle processes and ecosystem changes. © 2024 International Association for Hydro-environment Engineering and Research, Asia Pacific Division Emergency management; Flood warning systems; Hydro-logical research; Improved Gray Wolf Optimization Algorithm; Long Short-Term Memory Network; Runoff prediction Flood damage; Information management; Prediction models; Rivers; Runoff; Emergency management; Flood warning system; Gray wolves; Hydro-logical research; Improved gray wolf optimization algorithm; Long short-term memory network; Memory network; Optimization algorithms; Runoff prediction; Short term memory; accuracy assessment; algorithm; flood forecasting; hazard management; prediction; river flow; runoff; warning system; Risk management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
286;Computer vision tools for early post-disaster assessment: Enhancing generalizability;Remote sensing data, particularly satellite imagery, have made early, post-hazard aerial damage assessment possible due to its fast availability and extensive coverage. Despite breakthroughs in using deep computer vision with satellite image inputs, achieving high generalizability across diverse hazards and locations remains the main obstacle to effectively deploying early assessment tools in real-world scenarios, as the same hazard can manifest differently across various landscapes and urban textures. This challenge may be overlooked when working with curated datasets with minimal (test-to-train) shifts in urban textures and hazard damage features (e.g., due to undersized study regions), leaving models ill-prepared for real-world scenarios. The primary objective of this study was to understand non-trivial generalizability challenges by taking the 2023 Turkiye earthquake and the Maui wildfire incidents as “training” and “unseen test” events that simulated a demanding scenario. Subsequently, strategies such as augmenting image channels with damage proxy maps, data fusion, deep ensemble learning, and Test Time Augmentation were exclusively designed and implemented to address those challenges. These measures significantly improved the damage detection, with or without severity classification, with F1 scores increased from 0.71 to 0.82 and 0.40 to 0.87, respectively. Furthermore, and through data fusion, the proposed framework accommodates estimating socioeconomic loss metrics at the individual building level, supporting both response and recovery phases. This research has the potential to enhance the effectiveness of rapid aerial damage assessment models, ultimately aiding in more efficient and targeted disaster response and recovery efforts. Data, models, and codes are available at https://github.com/TRG-AI4Good/Lahaina_Generalizability. © 2024 The Authors;"Aerial damage assessment; Data fusion; Disaster recovery; Disaster response; Ensemble learning; Natural hazards";"Computer vision; Damage detection; Data fusion; Deep learning; Emergency services; Hazards; Recovery; Remote sensing; Satellite imagery; Textures; Aerial damage assessment; Damage assessments; Disaster recovery; Disaster-response; Ensemble learning; Natural hazard; Post disasters; Real-world scenario; Urban texture; Vision tools; Antennas";"Computer vision tools for early post-disaster assessment: Enhancing generalizability Remote sensing data, particularly satellite imagery, have made early, post-hazard aerial damage assessment possible due to its fast availability and extensive coverage. Despite breakthroughs in using deep computer vision with satellite image inputs, achieving high generalizability across diverse hazards and locations remains the main obstacle to effectively deploying early assessment tools in real-world scenarios, as the same hazard can manifest differently across various landscapes and urban textures. This challenge may be overlooked when working with curated datasets with minimal (test-to-train) shifts in urban textures and hazard damage features (e.g., due to undersized study regions), leaving models ill-prepared for real-world scenarios. The primary objective of this study was to understand non-trivial generalizability challenges by taking the 2023 Turkiye earthquake and the Maui wildfire incidents as “training” and “unseen test” events that simulated a demanding scenario. Subsequently, strategies such as augmenting image channels with damage proxy maps, data fusion, deep ensemble learning, and Test Time Augmentation were exclusively designed and implemented to address those challenges. These measures significantly improved the damage detection, with or without severity classification, with F1 scores increased from 0.71 to 0.82 and 0.40 to 0.87, respectively. Furthermore, and through data fusion, the proposed framework accommodates estimating socioeconomic loss metrics at the individual building level, supporting both response and recovery phases. This research has the potential to enhance the effectiveness of rapid aerial damage assessment models, ultimately aiding in more efficient and targeted disaster response and recovery efforts. Data, models, and codes are available at https://github.com/TRG-AI4Good/Lahaina_Generalizability. © 2024 The Authors Aerial damage assessment; Data fusion; Disaster recovery; Disaster response; Ensemble learning; Natural hazards Computer vision; Damage detection; Data fusion; Deep learning; Emergency services; Hazards; Recovery; Remote sensing; Satellite imagery; Textures; Aerial damage assessment; Damage assessments; Disaster recovery; Disaster-response; Ensemble learning; Natural hazard; Post disasters; Real-world scenario; Urban texture; Vision tools; Antennas";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
287;Forecasting road network functionality states during extreme rainfall events to facilitate real-time emergency response planning;Rapid prediction of Road Network Functionality (RNF) during extreme rainfall-induced flooding is crucial for supporting proactive and real-time emergency planning, such as rescue, evacuation planning, and emergency supply distribution. Unlike normal operational conditions, extreme rainfall events introduce complex non-stationary, non-Euclidean characteristics to RNF due to intricate meteorological and hydrological processes, as well as the role of a community's road network in emergency response planning. Conventional physics-based flood simulations and flow-based road network analyses typically lack the computational efficiency required for real-time RNF predictions, hindering timely risk mitigation decisions. This study leverages the accuracy of physics-based simulations and the efficacy of deep-learning technologies to develop a deep learning-based surrogate model for Rain-to-RNF (R2R) predictions. This model couples Long Short-Term Memory (LSTM) networks with Spatial-Temporal Graph Convolutional Networks (ST-GCNs) to uniquely capture the spatiotemporal dynamics of RNF under extreme rainfall events. The predictive accuracy, stability, and versatility of the R2R surrogate model are demonstrated in four flood-prone communities in Zhejiang Province. Its implementation during Typhoon Fitow (2013) over a 30-hour intense rainfall showcases its promising predictive capacity and unparalleled computational efficiency. This research advances disaster management, enhancing the resilience and responsiveness of community infrastructure during extreme weather events. © 2024 Elsevier Ltd;"Deep learning; Extreme rainfall; Long Short-Term Memory (LSTM) network; Rapid prediction; Real-time forecast; Road network functionality; Spatial-Temporal graph convolutional network (ST-GCN)";"Emergency services; Highway administration; Long short-term memory; Motor transportation; Risk assessment; Risk management; Weather forecasting; Convolutional networks; Deep learning; Extreme rainfall; Long short-term memory  network; Memory network; Network functionality; Rapid prediction; Real-time forecasts; Road network; Road network functionality; Short term memory; Spatial temporals; Spatial-temporal graph convolutional network; Temporal graphs; Rain";"Forecasting road network functionality states during extreme rainfall events to facilitate real-time emergency response planning Rapid prediction of Road Network Functionality (RNF) during extreme rainfall-induced flooding is crucial for supporting proactive and real-time emergency planning, such as rescue, evacuation planning, and emergency supply distribution. Unlike normal operational conditions, extreme rainfall events introduce complex non-stationary, non-Euclidean characteristics to RNF due to intricate meteorological and hydrological processes, as well as the role of a community's road network in emergency response planning. Conventional physics-based flood simulations and flow-based road network analyses typically lack the computational efficiency required for real-time RNF predictions, hindering timely risk mitigation decisions. This study leverages the accuracy of physics-based simulations and the efficacy of deep-learning technologies to develop a deep learning-based surrogate model for Rain-to-RNF (R2R) predictions. This model couples Long Short-Term Memory (LSTM) networks with Spatial-Temporal Graph Convolutional Networks (ST-GCNs) to uniquely capture the spatiotemporal dynamics of RNF under extreme rainfall events. The predictive accuracy, stability, and versatility of the R2R surrogate model are demonstrated in four flood-prone communities in Zhejiang Province. Its implementation during Typhoon Fitow (2013) over a 30-hour intense rainfall showcases its promising predictive capacity and unparalleled computational efficiency. This research advances disaster management, enhancing the resilience and responsiveness of community infrastructure during extreme weather events. © 2024 Elsevier Ltd Deep learning; Extreme rainfall; Long Short-Term Memory (LSTM) network; Rapid prediction; Real-time forecast; Road network functionality; Spatial-Temporal graph convolutional network (ST-GCN) Emergency services; Highway administration; Long short-term memory; Motor transportation; Risk assessment; Risk management; Weather forecasting; Convolutional networks; Deep learning; Extreme rainfall; Long short-term memory  network; Memory network; Network functionality; Rapid prediction; Real-time forecasts; Road network; Road network functionality; Short term memory; Spatial temporals; Spatial-temporal graph convolutional network; Temporal graphs; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
288;Wave runup and total water level observations from time series imagery at several sites with varying nearshore morphologies;Coastal imaging systems have been developed to measure wave runup and total water level (TWL) at the shoreline, which is a key metric for assessing coastal flooding and erosion. However, extracting quantitative measurements from coastal images has typically been done through the laborious task of hand-digitization of wave runup timestacks. Timestacks are images created by sampling a cross-shore array of pixels from an image through time as waves propagate towards and run up a beach. We utilize over 7000 hand-digitized timestacks from six diverse locations to train and validate machine learning models to automate the process of TWL extraction. Using these data, we evaluate two deep learning model architectures for the task of runup detection. One is based on a fully convolutional architecture trained from scratch, and the other is a transformer-based architecture trained using transfer learning. The deep learning models provide a probability of each pixel being either wet or dry. When contoured at the 50% level (equal chance of being wet or dry), the deep learning models more accurately identified TWL maxima than minima at all sites. This resulted in accurate predictions of 2% exceedance runup, but under predictions of significant swash and over predictions of wave setup. Improved agreement with the complete TWL time series was obtained through post-processing by utilizing the wet/dry probability of each pixel to weight the contouring toward lower dryness probabilities for runup minima (maxima agreed well with observations without tuning). Overall, a transformer-based model using transfer learning provided the best agreement with wave runup statistics, including a) the 2% exceedance runup, b) significant swash, and c) wave setup at the shoreline. For a random subset of images, the model was found to be within the uncertainty range of hand-digitization. The relative success of the transfer learning model suggests that fine-tuning a large model has advantages compared to training a smaller model from scratch. Models provide per-pixel probabilistic estimates in less than 10 s per timestack on a single computational unit, versus the more than 5 min required for hand-digitization. The model is therefore well-suited for near real-time applications, allowing for the development of early warning systems for difficult to forecast events. Real-time wave runup and total water level observations can also be incorporated into coastal hazards forecasts for data assimilation and continual model validation and improvement. © 2024;"Coastal imaging; Image segmentation; Machine learning; Nearshore processes; Swash zone; Total water level; Wave runup";"Distribution functions; Health risks; Image enhancement; Image sampling; Light transmission; Maximum likelihood; Photointerpretation; Risk assessment; Time series; Coastal imaging; Digitisation; Images segmentations; Learning models; Machine-learning; Nearshore process; Swash zone; Total water level; Transfer learning; Wave runup; data assimilation; early warning system; image processing; imaging method; machine learning; nearshore dynamics; pixel; time series analysis; water level; wave runup; Image segmentation";"Wave runup and total water level observations from time series imagery at several sites with varying nearshore morphologies Coastal imaging systems have been developed to measure wave runup and total water level (TWL) at the shoreline, which is a key metric for assessing coastal flooding and erosion. However, extracting quantitative measurements from coastal images has typically been done through the laborious task of hand-digitization of wave runup timestacks. Timestacks are images created by sampling a cross-shore array of pixels from an image through time as waves propagate towards and run up a beach. We utilize over 7000 hand-digitized timestacks from six diverse locations to train and validate machine learning models to automate the process of TWL extraction. Using these data, we evaluate two deep learning model architectures for the task of runup detection. One is based on a fully convolutional architecture trained from scratch, and the other is a transformer-based architecture trained using transfer learning. The deep learning models provide a probability of each pixel being either wet or dry. When contoured at the 50% level (equal chance of being wet or dry), the deep learning models more accurately identified TWL maxima than minima at all sites. This resulted in accurate predictions of 2% exceedance runup, but under predictions of significant swash and over predictions of wave setup. Improved agreement with the complete TWL time series was obtained through post-processing by utilizing the wet/dry probability of each pixel to weight the contouring toward lower dryness probabilities for runup minima (maxima agreed well with observations without tuning). Overall, a transformer-based model using transfer learning provided the best agreement with wave runup statistics, including a) the 2% exceedance runup, b) significant swash, and c) wave setup at the shoreline. For a random subset of images, the model was found to be within the uncertainty range of hand-digitization. The relative success of the transfer learning model suggests that fine-tuning a large model has advantages compared to training a smaller model from scratch. Models provide per-pixel probabilistic estimates in less than 10 s per timestack on a single computational unit, versus the more than 5 min required for hand-digitization. The model is therefore well-suited for near real-time applications, allowing for the development of early warning systems for difficult to forecast events. Real-time wave runup and total water level observations can also be incorporated into coastal hazards forecasts for data assimilation and continual model validation and improvement. © 2024 Coastal imaging; Image segmentation; Machine learning; Nearshore processes; Swash zone; Total water level; Wave runup Distribution functions; Health risks; Image enhancement; Image sampling; Light transmission; Maximum likelihood; Photointerpretation; Risk assessment; Time series; Coastal imaging; Digitisation; Images segmentations; Learning models; Machine-learning; Nearshore process; Swash zone; Total water level; Transfer learning; Wave runup; data assimilation; early warning system; image processing; imaging method; machine learning; nearshore dynamics; pixel; time series analysis; water level; wave runup; Image segmentation";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;2;Preparation
289;Identifying climate-related failures in railway infrastructure using machine learning;Climate change impacts pose challenges to a dependable operation of railway infrastructure assets, thus necessitating understanding and mitigating its effects. This study proposes a machine learning framework to distinguish between climatic and non-climatic failures in railway infrastructure. The maintenance data of turnout assets from Sweden's railway were collected and integrated with asset design, geographical and meteorological parameters. Various machine learning algorithms were employed to classify failures across multiple time horizons. The Random Forest model demonstrated a high accuracy of 0.827 and stable F1-scores across all time horizons. The study identified minimum-temperature and quantity of snow and rain prior to the event as the most influential factors. The 24-hour time horizon prior to failure emerged as the most effective time window for the classification. The practical implications and applications include enhancement of maintenance and renewal process, supporting more effective resource allocation, and implementing climate adaptation measures towards resilience railway infrastructure management. © 2024 The Author(s);"Climate Change; Climate-related Failure Classification; Environmental Impact; Railway Infrastructure; Switches and Crossing";"Sweden; Decision trees; Machine learning; Railroads; Random forests; Resource allocation; Climate change impact; Climate-related failure classification; Design parameters; Failure classification; Infrastructure assets; Learning frameworks; Machine-learning; Railway infrastructure; Switches and crossings; Time horizons; algorithm; climate change; environmental impact; machine learning; railway transport; temperature anomaly; Adversarial machine learning";"Identifying climate-related failures in railway infrastructure using machine learning Climate change impacts pose challenges to a dependable operation of railway infrastructure assets, thus necessitating understanding and mitigating its effects. This study proposes a machine learning framework to distinguish between climatic and non-climatic failures in railway infrastructure. The maintenance data of turnout assets from Sweden's railway were collected and integrated with asset design, geographical and meteorological parameters. Various machine learning algorithms were employed to classify failures across multiple time horizons. The Random Forest model demonstrated a high accuracy of 0.827 and stable F1-scores across all time horizons. The study identified minimum-temperature and quantity of snow and rain prior to the event as the most influential factors. The 24-hour time horizon prior to failure emerged as the most effective time window for the classification. The practical implications and applications include enhancement of maintenance and renewal process, supporting more effective resource allocation, and implementing climate adaptation measures towards resilience railway infrastructure management. © 2024 The Author(s) Climate Change; Climate-related Failure Classification; Environmental Impact; Railway Infrastructure; Switches and Crossing Sweden; Decision trees; Machine learning; Railroads; Random forests; Resource allocation; Climate change impact; Climate-related failure classification; Design parameters; Failure classification; Infrastructure assets; Learning frameworks; Machine-learning; Railway infrastructure; Switches and crossings; Time horizons; algorithm; climate change; environmental impact; machine learning; railway transport; temperature anomaly; Adversarial machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
290;Characterizing Shifts in Major Land Use Types and the Response of Water Yield in a Catchment with Widespread Peaty Wetlands;Landscape patterns have changed substantially in many countries over the last few decades which have profound impacts on hydrological processes. Wetlands have great potential in increasing watershed water retention capacity, so it is considered an important nature-based solution to improve water management. Yet, the impacts of wetland area alongside climate change on water yield are not adequately reported, especially in headwater catchments with widespread wetland distributions. In this study, we carried out analysis in the Zoige Plateau where owns the largest peaty wetlands in China. After reclassification of the land use land cover (LULC) types by applying a machine learning algorithm on Landsat imageries, we found an overall increase in wetland area during 1991–2022, mostly converted from grassland. Wetland experienced large dynamic changes in the region, distinguishable in two phases, i.e., dramatic degradation before 2009 because of overgrazing, and recovery afterwards after the policy implementation for wetland restoration. Furthermore, the impacts of climate factors and LULC types on water yield were quantified using Structural Equation Model and Multiple Linear Regression methods. Results showed that precipitation was evidently the dominant factor with a contributing factor of over 0.7, followed by wetland area (~ 0.2). In comparison, forests and grassland change and drought conditions played a much weaker role. The analysis indicates that in the context of wetting and warming tendency in the region, increasing the wetland areas can play a critical role in modulating hydrological processes for sustainable water supply to the downstream areas. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"Climate impacts; Landscape patterns; Water yield; Wetland; Zoige plateau";"China; Qinghai-Xizang Plateau; Zoige Plateau; Catchments; Climate change; Multiple linear regression; Runoff; Climate impacts; Hydrological process; Land use type; Land use/land cover; Land-cover types; Landscape pattern; Water retention capacity; Water yield; Wetland area; Zoige plateau; climate effect; headwater; land cover; land use; Landsat; machine learning; peatland; water management; water retention; water supply; water yield; watershed; wetland; Wetlands";"Characterizing Shifts in Major Land Use Types and the Response of Water Yield in a Catchment with Widespread Peaty Wetlands Landscape patterns have changed substantially in many countries over the last few decades which have profound impacts on hydrological processes. Wetlands have great potential in increasing watershed water retention capacity, so it is considered an important nature-based solution to improve water management. Yet, the impacts of wetland area alongside climate change on water yield are not adequately reported, especially in headwater catchments with widespread wetland distributions. In this study, we carried out analysis in the Zoige Plateau where owns the largest peaty wetlands in China. After reclassification of the land use land cover (LULC) types by applying a machine learning algorithm on Landsat imageries, we found an overall increase in wetland area during 1991–2022, mostly converted from grassland. Wetland experienced large dynamic changes in the region, distinguishable in two phases, i.e., dramatic degradation before 2009 because of overgrazing, and recovery afterwards after the policy implementation for wetland restoration. Furthermore, the impacts of climate factors and LULC types on water yield were quantified using Structural Equation Model and Multiple Linear Regression methods. Results showed that precipitation was evidently the dominant factor with a contributing factor of over 0.7, followed by wetland area (~ 0.2). In comparison, forests and grassland change and drought conditions played a much weaker role. The analysis indicates that in the context of wetting and warming tendency in the region, increasing the wetland areas can play a critical role in modulating hydrological processes for sustainable water supply to the downstream areas. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. Climate impacts; Landscape patterns; Water yield; Wetland; Zoige plateau China; Qinghai-Xizang Plateau; Zoige Plateau; Catchments; Climate change; Multiple linear regression; Runoff; Climate impacts; Hydrological process; Land use type; Land use/land cover; Land-cover types; Landscape pattern; Water retention capacity; Water yield; Wetland area; Zoige plateau; climate effect; headwater; land cover; land use; Landsat; machine learning; peatland; water management; water retention; water supply; water yield; watershed; wetland; Wetlands";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
291;Enhancing Disaster Response and Resilience Through Near-Time GIS for Flood Monitoring and Analysis in Niger River Basin, Nigeria;This study develops an integrated framework leveraging Google Earth Engine for near real-time flood mapping and impact analysis in the flood-prone Niger River Basin of Nigeria. Multi-temporal optical, radar and terrain data quantified changing flood hazards and exposure across 150,000 km2 between peak floods in 2022. Results indicate over 50% rise in inundation, with 15,000 hectares of vegetation and 143,000 residents enduring impacts. Attributing factors include elevated antecedent rainfall versus historical medians coupled with accelerating catchment modifications expanding runoff. Floodplain zones face recurrent impacts, necessitating adaptation. Accurate flood delineation was achieved by applying water indices like NDWI on Landsat and Sentinel-2 data using land cover and land use. Exposure analytics overlay flood extents on land use, infrastructure and demographic layers to estimate affected populations and livelihoods. Google Earth Engine enabled rapid data processing using cloud parallelization, while random forest integration powered machine learning semantic segmentation for robust feature extraction. Going forward, assimilating real-time data from radar and hydrological sensors would enable predictive flood risk models using machine learning algorithms on this cloud GIS platform tailored for resilience applications globally. In a changing climate, such scalable geospatial technologies provide evidence-based decision support capabilities to emergency planners targeting proactive adaptation investments for vulnerable communities based on quantified flood risk analytics. © Author(s) 2024.;"Disaster Response; Flood Mapping; Flood Monitoring; Flood Resilience; Geospatial Analysis; Google Earth Engine; NDWI";"Deforestation; Network security; Population statistics; Predictive analytics; Risk analysis; Risk assessment; Risk perception; Tropics; Disaster-response; Flood mapping; Flood monitoring; Flood resilience; Geo-spatial analysis; Google earth engine; Google earths; NDWI; Niger; River basins; Runoff";"Enhancing Disaster Response and Resilience Through Near-Time GIS for Flood Monitoring and Analysis in Niger River Basin, Nigeria This study develops an integrated framework leveraging Google Earth Engine for near real-time flood mapping and impact analysis in the flood-prone Niger River Basin of Nigeria. Multi-temporal optical, radar and terrain data quantified changing flood hazards and exposure across 150,000 km2 between peak floods in 2022. Results indicate over 50% rise in inundation, with 15,000 hectares of vegetation and 143,000 residents enduring impacts. Attributing factors include elevated antecedent rainfall versus historical medians coupled with accelerating catchment modifications expanding runoff. Floodplain zones face recurrent impacts, necessitating adaptation. Accurate flood delineation was achieved by applying water indices like NDWI on Landsat and Sentinel-2 data using land cover and land use. Exposure analytics overlay flood extents on land use, infrastructure and demographic layers to estimate affected populations and livelihoods. Google Earth Engine enabled rapid data processing using cloud parallelization, while random forest integration powered machine learning semantic segmentation for robust feature extraction. Going forward, assimilating real-time data from radar and hydrological sensors would enable predictive flood risk models using machine learning algorithms on this cloud GIS platform tailored for resilience applications globally. In a changing climate, such scalable geospatial technologies provide evidence-based decision support capabilities to emergency planners targeting proactive adaptation investments for vulnerable communities based on quantified flood risk analytics. © Author(s) 2024. Disaster Response; Flood Mapping; Flood Monitoring; Flood Resilience; Geospatial Analysis; Google Earth Engine; NDWI Deforestation; Network security; Population statistics; Predictive analytics; Risk analysis; Risk assessment; Risk perception; Tropics; Disaster-response; Flood mapping; Flood monitoring; Flood resilience; Geo-spatial analysis; Google earth engine; Google earths; NDWI; Niger; River basins; Runoff";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
292;SeismoNet: A proximal policy optimization-based earthquake early warning system using dilated convolution layers and online data augmentation;In seismic safety, Earthquake Early Warning (EEW) systems are indispensable for mitigating earthquake hazards. These systems strive to quickly evaluate earthquake magnitudes to determine which events warrant immediate alerts. Despite their importance, conventional approaches to rapid earthquake classification face significant hurdles, particularly with the skewed data distribution where instances requiring alerts are far less common. This study proposes an EEW method, called SeismoNet, that leverages a 7-second snapshot of three-dimensional seismic waveforms from the China Earthquake Network Center (CENC). SeismoNet incorporates a trio of dilated convolution layers to distill essential features for effective classification, addressing the critical challenge of data imbalance. Our novel integration of the Off-policy Proximal Policy Optimization (Off-policy PPO) algorithm enriches the model by rewarding accurate classifications, refining its sensitivity to less frequent yet critical seismic events. The training approach is conceptualized as a sequence of decision-making steps, with each seismic sample treated as a unique scenario. The model, acting as a decision-making agent, is rewarded or penalized based on its ability to differentiate between less and more common events. To further enhance classification accuracy, we employ a generative adversarial network (GAN) for dynamic data augmentation and a regularization technique to prevent common pitfalls like mode collapse and training instability. The efficacy of SeismoNet is underscored by its superior performance metrics, including an F-measure of 88.14% and a geometric mean of 90.66%, outperforming existing deep-learning solutions, traditional algorithms, and transfer learning methods. Our exhaustive evaluations, including ablation studies that remove key components like dilated convolutions and PPO, affirm the vital contribution of these elements to SeismoNet's exceptional capability in EEW. SeismoNet can successfully issue timely warnings for significant seismic events, allowing for essential preventative measures and significantly reducing the potential impact on these communities. © 2024 Elsevier Ltd;"Earthquake early warning; Generative adversarial network; Imbalanced classification; Magnitude classification; Proximal Policy Optimization";"Classification (of information); Convolution; Decision making; Deep learning; Earthquakes; Learning systems; Data augmentation; Earthquake early warning; Earthquake early warning systems; Imbalanced classification; Layer data; Magnitude classification; Online data; Policy optimization; Proximal policy optimization; Seismic event; Generative adversarial networks";"SeismoNet: A proximal policy optimization-based earthquake early warning system using dilated convolution layers and online data augmentation In seismic safety, Earthquake Early Warning (EEW) systems are indispensable for mitigating earthquake hazards. These systems strive to quickly evaluate earthquake magnitudes to determine which events warrant immediate alerts. Despite their importance, conventional approaches to rapid earthquake classification face significant hurdles, particularly with the skewed data distribution where instances requiring alerts are far less common. This study proposes an EEW method, called SeismoNet, that leverages a 7-second snapshot of three-dimensional seismic waveforms from the China Earthquake Network Center (CENC). SeismoNet incorporates a trio of dilated convolution layers to distill essential features for effective classification, addressing the critical challenge of data imbalance. Our novel integration of the Off-policy Proximal Policy Optimization (Off-policy PPO) algorithm enriches the model by rewarding accurate classifications, refining its sensitivity to less frequent yet critical seismic events. The training approach is conceptualized as a sequence of decision-making steps, with each seismic sample treated as a unique scenario. The model, acting as a decision-making agent, is rewarded or penalized based on its ability to differentiate between less and more common events. To further enhance classification accuracy, we employ a generative adversarial network (GAN) for dynamic data augmentation and a regularization technique to prevent common pitfalls like mode collapse and training instability. The efficacy of SeismoNet is underscored by its superior performance metrics, including an F-measure of 88.14% and a geometric mean of 90.66%, outperforming existing deep-learning solutions, traditional algorithms, and transfer learning methods. Our exhaustive evaluations, including ablation studies that remove key components like dilated convolutions and PPO, affirm the vital contribution of these elements to SeismoNet's exceptional capability in EEW. SeismoNet can successfully issue timely warnings for significant seismic events, allowing for essential preventative measures and significantly reducing the potential impact on these communities. © 2024 Elsevier Ltd Earthquake early warning; Generative adversarial network; Imbalanced classification; Magnitude classification; Proximal Policy Optimization Classification (of information); Convolution; Decision making; Deep learning; Earthquakes; Learning systems; Data augmentation; Earthquake early warning; Earthquake early warning systems; Imbalanced classification; Layer data; Magnitude classification; Online data; Policy optimization; Proximal policy optimization; Seismic event; Generative adversarial networks";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.1;Geological;2;Preparation
293;Early warning for maximum tsunami heights and arrival time based on an artificial neural network;Tsunamis can cause extensive damages and loss of lives in coastal communities. Early warning for tsunami can help save lives and mitigate damages from tsunamis. This study aimed to develop an early warning for tsunamis using an artificial neural network (ANN) that can predict maximum tsunami heights and arrival time. Imwon Port, located on the eastern coast of Korea was selected as the target area. A weighted logic tree approach that assigns weights to fault parameters of earthquake based on their importance was proposed to establish tsunami scenarios and generate tsunami big data. Nine offshore observations in the East Sea were used as standard observations for predicting maximum tsunami height and arrival time at Imwon Port. ANN was developed to predict maximum tsunami heights and arrival time. The Kriging method was adopted to investigate the spatial distribution of the maximum tsunami height in the port, and the root mean square error, and coefficient of determination were used to evaluate the model's performance. The estimates of maximum tsunami heights and arrival times generated by the proposed model agreed with the results of the numerical model. Furthermore, the ANN can generate these estimation quickly, enhancing the effectiveness of early tsunami warnings. © 2024 Elsevier B.V.;"Arrival time; Artificial neural network; Early warning; Maximum tsunami height; Tsunamis";"Imwon; Kangwon; South Korea; Forecasting; Mean square error; Offshore oil well production; Tsunamis; Arrival time; Coastal communities; Early warning; Fault parameters; Kriging methods; Logic tree; Loss of life; Maximum tsunami height; Offshores; Time based; arrival time; artificial neural network; coastal zone; early warning system; estimation method; hazard assessment; kriging; mitigation; model test; natural hazard; performance assessment; tsunami; vulnerability; wave generation; wave height; Neural networks";"Early warning for maximum tsunami heights and arrival time based on an artificial neural network Tsunamis can cause extensive damages and loss of lives in coastal communities. Early warning for tsunami can help save lives and mitigate damages from tsunamis. This study aimed to develop an early warning for tsunamis using an artificial neural network (ANN) that can predict maximum tsunami heights and arrival time. Imwon Port, located on the eastern coast of Korea was selected as the target area. A weighted logic tree approach that assigns weights to fault parameters of earthquake based on their importance was proposed to establish tsunami scenarios and generate tsunami big data. Nine offshore observations in the East Sea were used as standard observations for predicting maximum tsunami height and arrival time at Imwon Port. ANN was developed to predict maximum tsunami heights and arrival time. The Kriging method was adopted to investigate the spatial distribution of the maximum tsunami height in the port, and the root mean square error, and coefficient of determination were used to evaluate the model's performance. The estimates of maximum tsunami heights and arrival times generated by the proposed model agreed with the results of the numerical model. Furthermore, the ANN can generate these estimation quickly, enhancing the effectiveness of early tsunami warnings. © 2024 Elsevier B.V. Arrival time; Artificial neural network; Early warning; Maximum tsunami height; Tsunamis Imwon; Kangwon; South Korea; Forecasting; Mean square error; Offshore oil well production; Tsunamis; Arrival time; Coastal communities; Early warning; Fault parameters; Kriging methods; Logic tree; Loss of life; Maximum tsunami height; Offshores; Time based; arrival time; artificial neural network; coastal zone; early warning system; estimation method; hazard assessment; kriging; mitigation; model test; natural hazard; performance assessment; tsunami; vulnerability; wave generation; wave height; Neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
294;STMP and PVPA as Templating Analogs of Noncollagenous Proteins Induce Intrafibrillar Mineralization of Type I Collagen via PCCP Process;The phosphorylated noncollagenous proteins (NCPs) play a vital role in manipulating biomineralization, while the mechanism of phosphorylation of NCPs in intrafibrillar mineralization of collagen fibril has not been completely deciphered. Poly(vinylphosphonic acid) (PVPA) and sodium trimetaphosphate (STMP) as templating analogs of NCPs induce hierarchical mineralization in cooperation with indispensable sequestration analogs such as polyacrylic acid (PAA) via polymer-induced liquid-like precursor (PILP) process. Herein, STMP-Ca and PVPA-Ca complexes are proposed to achieve rapid intrafibrillar mineralization through polyelectrolyte-Ca complexes pre-precursor (PCCP) process. This strategy is further verified effectively for remineralization of demineralized dentin matrix both in vitro and in vivo. Although STMP micromolecule fails to stabilize amorphous calcium phosphate (ACP) precursor, STMP-Ca complexes facilely permeate into intrafibrillar interstices and trigger phase transition of ACP to hydroxyapatite within collagen. In contrast, PVPA-stabilized ACP precursors lack liquid-like characteristic and crystallize outside collagen due to rigid conformation of PVPA macromolecule, while PVPA-Ca complexes infiltrate into partial intrafibrillar intervals under electrostatic attraction and osmotic pressure as evidenced by intuitionistic 3D stochastic optical reconstruction microscopy (3D-STORM). The study not only extends the variety and size range of polyelectrolyte for PCCP process but also sheds light on the role of phosphorylation for NCPs in biomineralization. © 2024 Wiley-VCH GmbH.;"intrafibrillar mineralization; PCCP process; phosphorylation; poly(vinylphosphonic acid); sodium trimetaphosphate";"Acrylic Resins; Animals; Calcium Phosphates; Collagen Type I; Dentin; Humans; Organophosphonates; Phosphorylation; Polyphosphates; Polyvinyls; Vinyl Compounds; Biomineralization; Calcium phosphate; Collagen; Hydroxyapatite; Mineralogy; Osmosis; Phosphorylation; Sodium compounds; Stochastic systems; calcium phosphate; collagen; collagen sponge; collagen type 1; epoxy resin; hydroxyapatite; poly(vinylphosphonic acid); polyelectrolyte; polyelectrolyte Ca complexes pre precursor; polymer; sodium trimetaphosphate; unclassified drug; acrylic acid resin; amorphous calcium phosphate; calcium phosphate; carbopol 940; phosphonic acid derivative; polyphosphate; polyvinyl derivative; trimetaphosphoric acid; vinyl derivative; vinylphosphonic acid; Amorphous calcium phosphate; Ca complexes; Intrafibrillar mineralization; Mineralisation; Non-collagenous proteins; Poly(vinylphosphonic acid); Poly(vinylphosphonic acids); Polyelectrolyte-ca complex pre-precursor process; Sodium trimetaphosphates; Templating; animal experiment; animal model; animal tissue; Article; cell infiltration; circular dichroism; collagen fibril; controlled study; crystallization; dialysis; electrochemical analysis; electron diffraction; energy dispersive X ray spectroscopy; fibril; Fourier transform infrared spectroscopy; freeze drying; hierarchical clustering; hydrogen bond; image reconstruction; infrared spectroscopy; intuitionistic 3D stochastic optical reconstruction microscopy; macromolecule; male; mineralization; nonhuman; osmolarity; osmotic pressure; phase transition; positive end expiratory pressure ventilation; protein phosphorylation; Raman spectrometry; scanning electron microscopy; spectroscopy; static electricity; stochastic model; surface property; transmission electron microscopy; viscosity; X ray diffraction; Young modulus; zeta potential; animal; chemistry; dentin; human; metabolism; phosphorylation; Polyelectrolytes";"STMP and PVPA as Templating Analogs of Noncollagenous Proteins Induce Intrafibrillar Mineralization of Type I Collagen via PCCP Process The phosphorylated noncollagenous proteins (NCPs) play a vital role in manipulating biomineralization, while the mechanism of phosphorylation of NCPs in intrafibrillar mineralization of collagen fibril has not been completely deciphered. Poly(vinylphosphonic acid) (PVPA) and sodium trimetaphosphate (STMP) as templating analogs of NCPs induce hierarchical mineralization in cooperation with indispensable sequestration analogs such as polyacrylic acid (PAA) via polymer-induced liquid-like precursor (PILP) process. Herein, STMP-Ca and PVPA-Ca complexes are proposed to achieve rapid intrafibrillar mineralization through polyelectrolyte-Ca complexes pre-precursor (PCCP) process. This strategy is further verified effectively for remineralization of demineralized dentin matrix both in vitro and in vivo. Although STMP micromolecule fails to stabilize amorphous calcium phosphate (ACP) precursor, STMP-Ca complexes facilely permeate into intrafibrillar interstices and trigger phase transition of ACP to hydroxyapatite within collagen. In contrast, PVPA-stabilized ACP precursors lack liquid-like characteristic and crystallize outside collagen due to rigid conformation of PVPA macromolecule, while PVPA-Ca complexes infiltrate into partial intrafibrillar intervals under electrostatic attraction and osmotic pressure as evidenced by intuitionistic 3D stochastic optical reconstruction microscopy (3D-STORM). The study not only extends the variety and size range of polyelectrolyte for PCCP process but also sheds light on the role of phosphorylation for NCPs in biomineralization. © 2024 Wiley-VCH GmbH. intrafibrillar mineralization; PCCP process; phosphorylation; poly(vinylphosphonic acid); sodium trimetaphosphate Acrylic Resins; Animals; Calcium Phosphates; Collagen Type I; Dentin; Humans; Organophosphonates; Phosphorylation; Polyphosphates; Polyvinyls; Vinyl Compounds; Biomineralization; Calcium phosphate; Collagen; Hydroxyapatite; Mineralogy; Osmosis; Phosphorylation; Sodium compounds; Stochastic systems; calcium phosphate; collagen; collagen sponge; collagen type 1; epoxy resin; hydroxyapatite; poly(vinylphosphonic acid); polyelectrolyte; polyelectrolyte Ca complexes pre precursor; polymer; sodium trimetaphosphate; unclassified drug; acrylic acid resin; amorphous calcium phosphate; calcium phosphate; carbopol 940; phosphonic acid derivative; polyphosphate; polyvinyl derivative; trimetaphosphoric acid; vinyl derivative; vinylphosphonic acid; Amorphous calcium phosphate; Ca complexes; Intrafibrillar mineralization; Mineralisation; Non-collagenous proteins; Poly(vinylphosphonic acid); Poly(vinylphosphonic acids); Polyelectrolyte-ca complex pre-precursor process; Sodium trimetaphosphates; Templating; animal experiment; animal model; animal tissue; Article; cell infiltration; circular dichroism; collagen fibril; controlled study; crystallization; dialysis; electrochemical analysis; electron diffraction; energy dispersive X ray spectroscopy; fibril; Fourier transform infrared spectroscopy; freeze drying; hierarchical clustering; hydrogen bond; image reconstruction; infrared spectroscopy; intuitionistic 3D stochastic optical reconstruction microscopy; macromolecule; male; mineralization; nonhuman; osmolarity; osmotic pressure; phase transition; positive end expiratory pressure ventilation; protein phosphorylation; Raman spectrometry; scanning electron microscopy; spectroscopy; static electricity; stochastic model; surface property; transmission electron microscopy; viscosity; X ray diffraction; Young modulus; zeta potential; animal; chemistry; dentin; human; metabolism; phosphorylation; Polyelectrolytes";-1;Não Classificado;NULL;-1;NULL;-1;NULL
295;Multi-UAV Assisted Air–Ground Collaborative MEC System: DRL-Based Joint Task Offloading and Resource Allocation and 3D UAV Trajectory Optimization;"In disaster-stricken areas that were severely damaged by earthquakes, typhoons, floods, mudslides, and the like, employing unmanned aerial vehicles (UAVs) as airborne base stations for mobile edge computing (MEC) constitutes an effective solution. Concerning this, we investigate a 3D air–ground collaborative MEC scenario facilitated by multi-UAV for multiple ground devices (GDs). Specifically, we first design a 3D multi-UAV-assisted air–ground cooperative MEC system, and construct system communication, computation, and UAV flight energy consumption models. Subsequently, a cooperative resource optimization (CRO) problem is proposed by jointly optimizing task offloading, UAV flight trajectories, and edge computing resource allocation to minimize the total energy consumption of the system. Further, the CRO problem is decoupled into two sub-problems. Among them, the MATD3 deep reinforcement learning algorithm is utilized to jointly optimize the offloading decisions of GDs and the flight trajectories of UAVs; subsequently, the optimal resource allocation scheme at the edge is demonstrated through the derivation of KKT conditions. Finally, the simulation results show that the algorithm has good convergence compared with other algorithms and can effectively reduce the system energy consumption. © 2024 by the authors.";"computation resource allocation; convex optimization; deep reinforcement learning; mobile-edge computing (MEC); task offloading; trajectory optimization; unmanned aerial vehicles (UAVs)";"Aircraft communication; Antenna grounds; Computation offloading; Deep reinforcement learning; Flight simulators; Mobile edge computing; Reinforcement learning; Resource allocation; Unmanned aerial vehicles (UAV); VTOL/STOL aircraft; Aerial vehicle; Air grounds; Computation resource allocations; Convex optimisation; Edge computing; Mobile-edge computing; Reinforcement learnings; Task offloading; Trajectory optimization; Unmanned aerial vehicle; Convex optimization";"Multi-UAV Assisted Air–Ground Collaborative MEC System: DRL-Based Joint Task Offloading and Resource Allocation and 3D UAV Trajectory Optimization In disaster-stricken areas that were severely damaged by earthquakes, typhoons, floods, mudslides, and the like, employing unmanned aerial vehicles (UAVs) as airborne base stations for mobile edge computing (MEC) constitutes an effective solution. Concerning this, we investigate a 3D air–ground collaborative MEC scenario facilitated by multi-UAV for multiple ground devices (GDs). Specifically, we first design a 3D multi-UAV-assisted air–ground cooperative MEC system, and construct system communication, computation, and UAV flight energy consumption models. Subsequently, a cooperative resource optimization (CRO) problem is proposed by jointly optimizing task offloading, UAV flight trajectories, and edge computing resource allocation to minimize the total energy consumption of the system. Further, the CRO problem is decoupled into two sub-problems. Among them, the MATD3 deep reinforcement learning algorithm is utilized to jointly optimize the offloading decisions of GDs and the flight trajectories of UAVs; subsequently, the optimal resource allocation scheme at the edge is demonstrated through the derivation of KKT conditions. Finally, the simulation results show that the algorithm has good convergence compared with other algorithms and can effectively reduce the system energy consumption. © 2024 by the authors. computation resource allocation; convex optimization; deep reinforcement learning; mobile-edge computing (MEC); task offloading; trajectory optimization; unmanned aerial vehicles (UAVs) Aircraft communication; Antenna grounds; Computation offloading; Deep reinforcement learning; Flight simulators; Mobile edge computing; Reinforcement learning; Resource allocation; Unmanned aerial vehicles (UAV); VTOL/STOL aircraft; Aerial vehicle; Air grounds; Computation resource allocations; Convex optimisation; Edge computing; Mobile-edge computing; Reinforcement learnings; Task offloading; Trajectory optimization; Unmanned aerial vehicle; Convex optimization";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;3;Response
296;Futuristic flood risks assessment, in the Upper Vellar Basin, integrating AHP and bivariate analysis;Flood susceptibility maps provide invaluable information for assessing and managing flood-prone areas, aiding in proactive planning, risk reduction strategies, and safeguarding vulnerable communities. The current research concentrates on advancing sustainable development practices by undertaking a comprehensive assessment of flood susceptibility in the Upper Vellar basin, with a projection for 2050. Employing an integrative methodology, this study utilizes an Analytical Hierarchy Process (AHP) and Bivariate Analysis. Nine critical parameters were used: elevation, distance from the river, distance from the road, drainage density, predicted LULC, projected precipitation, slope, soil type, and Topographic Wetness Index (TWI). The Modules of Land Use Change Evaluation (MOLUSE) plugin, which uses Cellular Automata-Artificial Neural Network (CA-ANN), was employed to predict the LULC map for the year 2050. Furthermore, bias-corrected Coupled Model Intercomparison Project 6 (CMIP 6) EC EARTH 3 Model (GCM) RCP 4.5 and 8.5 projected precipitation data were employed. The resulting flood susceptibility zones are classified into three categories: low, moderate, and high, with proportions of 32.64%, 55.52%, and 11.84% for RCP 4.5, and 34.63%, 53.46%, and 11.91% for RCP 8.5, respectively, concerning the total area. In both scenarios, nearly 38% of the settlement area is at high flood risk. This study provides essential insights for policymakers and stakeholders, facilitating the formulation of sustainable strategies to address projected changes in land use, precipitation patterns, and flood susceptibility in the Upper Vellar region up to 2050. © 2024 COSPAR;"Climate Change Scenarios; Coupled Model Intercomparison Project; Future Flood susceptibility map; Land use landcover prediction; Multi-Criteria Decision Analysis; Sustainable Management";"Hierarchical systems; Risk analysis; Risk assessment; Risk management; Risk perception; River basin projects; Analytical Hierarchy Process; Climate change scenarios; Coupled Model Intercomparison Project; Future flood susceptibility map; Land cover; Land use landcover prediction; Multi-criteria decision analysis; Process analysis; Susceptibility maps; Sustainable management; Analytic hierarchy process";"Futuristic flood risks assessment, in the Upper Vellar Basin, integrating AHP and bivariate analysis Flood susceptibility maps provide invaluable information for assessing and managing flood-prone areas, aiding in proactive planning, risk reduction strategies, and safeguarding vulnerable communities. The current research concentrates on advancing sustainable development practices by undertaking a comprehensive assessment of flood susceptibility in the Upper Vellar basin, with a projection for 2050. Employing an integrative methodology, this study utilizes an Analytical Hierarchy Process (AHP) and Bivariate Analysis. Nine critical parameters were used: elevation, distance from the river, distance from the road, drainage density, predicted LULC, projected precipitation, slope, soil type, and Topographic Wetness Index (TWI). The Modules of Land Use Change Evaluation (MOLUSE) plugin, which uses Cellular Automata-Artificial Neural Network (CA-ANN), was employed to predict the LULC map for the year 2050. Furthermore, bias-corrected Coupled Model Intercomparison Project 6 (CMIP 6) EC EARTH 3 Model (GCM) RCP 4.5 and 8.5 projected precipitation data were employed. The resulting flood susceptibility zones are classified into three categories: low, moderate, and high, with proportions of 32.64%, 55.52%, and 11.84% for RCP 4.5, and 34.63%, 53.46%, and 11.91% for RCP 8.5, respectively, concerning the total area. In both scenarios, nearly 38% of the settlement area is at high flood risk. This study provides essential insights for policymakers and stakeholders, facilitating the formulation of sustainable strategies to address projected changes in land use, precipitation patterns, and flood susceptibility in the Upper Vellar region up to 2050. © 2024 COSPAR Climate Change Scenarios; Coupled Model Intercomparison Project; Future Flood susceptibility map; Land use landcover prediction; Multi-Criteria Decision Analysis; Sustainable Management Hierarchical systems; Risk analysis; Risk assessment; Risk management; Risk perception; River basin projects; Analytical Hierarchy Process; Climate change scenarios; Coupled Model Intercomparison Project; Future flood susceptibility map; Land cover; Land use landcover prediction; Multi-criteria decision analysis; Process analysis; Susceptibility maps; Sustainable management; Analytic hierarchy process";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
297;Integrating Object-Based Classification and Multivariate Analysis to Evaluate Flood Vulnerability in Unplanned Settlements of Malang City, Indonesia;Urban flooding poses a significant challenge in Malang City, particularly within unplanned settlements where rapid urbanization and inadequate infrastructure intensify flood risks. This study evaluates flood vulnerability by integrating object-based classification with multivariate analysis, focusing on the effects of land cover types, slopes, elevations, rainfall intensity, and NDWI on flood occurrences. Using advanced remote sensing with high and medium resolution imagery, coupled with Multiple Linear Regression Analysis, the research quantifies the influence of these factors on flood potential. The findings reveal that built-up areas significantly increase flood risk, as evidenced by a high positive regression coefficient (β = 0.580), while open spaces and vegetation, such as fields and trees, strongly mitigate flood risk (β = -0.653 for rivers, β = -0.401 for fields). The model explains 84.7% of the variability in flood vulnerability (R2= 0.847), indicating the substantial impact of environmental and urban factors. These results emphasize the need for urban planners to integrate flood risk management into city planning, particularly through the preservation of green spaces and the enforcement of stricter zoning regulations. Such strategies are crucial for enhancing urban resilience to flooding and advancing sustainable development goals, particularly SDG 11 (Sustainable Cities and Communities) and SDG 13 (Climate Action). Copyright: ©2024 The authors.;"disaster mitigation; environmental risk assessment; flood risk mapping; land use planning; remote sensing applications; sustainable urban development; unplanned urban areas; urban flooding";NULL;"Integrating Object-Based Classification and Multivariate Analysis to Evaluate Flood Vulnerability in Unplanned Settlements of Malang City, Indonesia Urban flooding poses a significant challenge in Malang City, particularly within unplanned settlements where rapid urbanization and inadequate infrastructure intensify flood risks. This study evaluates flood vulnerability by integrating object-based classification with multivariate analysis, focusing on the effects of land cover types, slopes, elevations, rainfall intensity, and NDWI on flood occurrences. Using advanced remote sensing with high and medium resolution imagery, coupled with Multiple Linear Regression Analysis, the research quantifies the influence of these factors on flood potential. The findings reveal that built-up areas significantly increase flood risk, as evidenced by a high positive regression coefficient (β = 0.580), while open spaces and vegetation, such as fields and trees, strongly mitigate flood risk (β = -0.653 for rivers, β = -0.401 for fields). The model explains 84.7% of the variability in flood vulnerability (R2= 0.847), indicating the substantial impact of environmental and urban factors. These results emphasize the need for urban planners to integrate flood risk management into city planning, particularly through the preservation of green spaces and the enforcement of stricter zoning regulations. Such strategies are crucial for enhancing urban resilience to flooding and advancing sustainable development goals, particularly SDG 11 (Sustainable Cities and Communities) and SDG 13 (Climate Action). Copyright: ©2024 The authors. disaster mitigation; environmental risk assessment; flood risk mapping; land use planning; remote sensing applications; sustainable urban development; unplanned urban areas; urban flooding NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
298;A novel multi-step ahead prediction method for landslide displacement based on autoregressive integrated moving average and intelligent algorithm;Accurate landslide displacement prediction is crucial for prevention and early warning. In this paper, we proposed a novel hybrid multi-step-ahead prediction model (ARIMA-IM) that combines autoregressive integrated moving average (ARIMA) and intelligent models (IMs) for landslide displacement prediction. This model integrates the linear prediction strengths of ARIMA, the signal decomposition capabilities of variational mode decomposition (VMD) and empirical wavelet transform (EWT), and the nonlinear change-capturing ability of IMs. The proposed model cannot only effectively capture abrupt and long-term trends in landslide displacement but also enable high-precision multi-step-ahead prediction. To validate the effectiveness of the proposed model, we predicted the displacement of the Bazimen landslide in the Three Gorges Reservoir by using four IMs, namely long short-term memory (LSTM), bidirectional LSTM (BiLSTM), gate recurrent unit (GRU), and artificial neural network (ANN), in both continuous and jump strategies for multi-step-ahead prediction. Results showed that ARIMA-IM, especially the hybrid model combining ARIMA and deep learning, achieved high accuracy in 1–5-step-ahead prediction. Continuous multi-step-ahead prediction exhibited higher accuracy and provided more prediction information for decision-making. Compared to other models, ARIMA-IM not only achieved multi-step-ahead prediction but also exhibited comparable or better prediction accuracy, which is of high practical significance for landslide disaster early warning. © 2024 Elsevier Ltd;"Autoregressive integrated moving average; Intelligent model; Landslide displacement; Multi-step-ahead prediction";"Variational mode decomposition; Auto-regressive; Autoregressive integrated moving average; Displacement prediction; Early warning; High-accuracy; Intelligent models; Landslide displacement; Moving averages; Multi-step-ahead predictions; Short term memory; Prediction models";"A novel multi-step ahead prediction method for landslide displacement based on autoregressive integrated moving average and intelligent algorithm Accurate landslide displacement prediction is crucial for prevention and early warning. In this paper, we proposed a novel hybrid multi-step-ahead prediction model (ARIMA-IM) that combines autoregressive integrated moving average (ARIMA) and intelligent models (IMs) for landslide displacement prediction. This model integrates the linear prediction strengths of ARIMA, the signal decomposition capabilities of variational mode decomposition (VMD) and empirical wavelet transform (EWT), and the nonlinear change-capturing ability of IMs. The proposed model cannot only effectively capture abrupt and long-term trends in landslide displacement but also enable high-precision multi-step-ahead prediction. To validate the effectiveness of the proposed model, we predicted the displacement of the Bazimen landslide in the Three Gorges Reservoir by using four IMs, namely long short-term memory (LSTM), bidirectional LSTM (BiLSTM), gate recurrent unit (GRU), and artificial neural network (ANN), in both continuous and jump strategies for multi-step-ahead prediction. Results showed that ARIMA-IM, especially the hybrid model combining ARIMA and deep learning, achieved high accuracy in 1–5-step-ahead prediction. Continuous multi-step-ahead prediction exhibited higher accuracy and provided more prediction information for decision-making. Compared to other models, ARIMA-IM not only achieved multi-step-ahead prediction but also exhibited comparable or better prediction accuracy, which is of high practical significance for landslide disaster early warning. © 2024 Elsevier Ltd Autoregressive integrated moving average; Intelligent model; Landslide displacement; Multi-step-ahead prediction Variational mode decomposition; Auto-regressive; Autoregressive integrated moving average; Displacement prediction; Early warning; High-accuracy; Intelligent models; Landslide displacement; Moving averages; Multi-step-ahead predictions; Short term memory; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
299;Fusion of diverse data sources for flood extent mapping and risk assessment in Sindh: A comparative study of inundation mapping approaches;Accurate and timely mapping of flood extent is crucial for effective disaster response and management. However, existing methods often face challenges in integrating diverse data sources and leveraging advanced techniques for precise inundation mapping. This research aimed to address this gap by proposing and evaluating four different methods for improving flood extent mapping in Sindh, Pakistan, by integrating various data sources, including precipitation, Synthetic Aperture Radar (SAR), buildings, population, and optical data. The first strategy involves categorizing post-flood SAR data immediately using non-parametric (Support Vector Machine (SVM) and Random Forest (RF)), parametric (Maximum Likelihood Estimation (MLE)), and cluster (ISO) algorithms for Vertical transmit and Vertical received (VV) and Vertical transmit and Horizontal received (VH) polarisations. The second method relies on Otsu's method for automatic thresholding on post-flood event VH and VV data. The third approach utilizes both pre- and post-flood event data by creating a stack of Pre-event VH, Pre-event VV, Post-event VH, and Post-event VV data, which is then classified using the SVM, RF, MLE, and ISO algorithms. Lastly, the fourth method employs the stack of Principal Component Analysis (PCA) bands, consisting of PCA components 1, 2, and 3, to categorize data for flooding using the SVM, RF, MLE, and ISO algorithms. The rationale behind evaluating these diverse methods was to identify the most effective approach for accurately mapping flood extent by exploiting the strengths of different algorithms and data processing techniques. The accuracy and precision of these approaches were rigorously evaluated using Landsat-9 Normalized Difference Water Index (NDWI) as a reference dataset, along with error matrix and compound value (Cv) metrics. The findings show that a PCA-based technique with SVM is marginally superior (based on the overall accuracy and Kappa coefficient with a value of 92.20 % and kappa coefficient 0.821) than the other approaches tested in the study. By reducing the dimensionality of the dataset while preserving relevant information, PCA offers a computationally efficient approach to flood extent mapping. Among all the approaches, the collective performance of the algorithms was computed using the Cv metric. The SVM algorithm with a Cv of 1.25 demonstrated the best performance, followed by the RF algorithm with Cv of 1.5, and the MLE algorithm with Cv of 2.5. The worst performing algorithm among all is found to be ISO with Cv of 4. It comes into view that first and second approach underperformed in VH polarisation compared to the VV polarisation and provide a more accurate representation of flooded area in VV polarisation. The flood extent with highest accuracy, together with the world's gridded population, latest Microsoft's Global ML Building Footprints, and OpenStreetMap building data are used in conjunction to estimate the number of people and buildings at risk within the study area. © 2024 COSPAR;"Flood; GEE; GPWv4; IMERG; Machine learning; OpenStreetMap";"Clustering algorithms; Data integration; Forestry; Mapping; Maximum likelihood estimation; Population statistics; Principal component analysis; Risk assessment; Support vector machines; Synthetic aperture radar; Data-source; Extent mappings; GEE; GPWv4; IMERG; Machine-learning; Maximum-likelihood estimation; Openstreetmap; Principal-component analysis; Support vectors machine; Floods";"Fusion of diverse data sources for flood extent mapping and risk assessment in Sindh: A comparative study of inundation mapping approaches Accurate and timely mapping of flood extent is crucial for effective disaster response and management. However, existing methods often face challenges in integrating diverse data sources and leveraging advanced techniques for precise inundation mapping. This research aimed to address this gap by proposing and evaluating four different methods for improving flood extent mapping in Sindh, Pakistan, by integrating various data sources, including precipitation, Synthetic Aperture Radar (SAR), buildings, population, and optical data. The first strategy involves categorizing post-flood SAR data immediately using non-parametric (Support Vector Machine (SVM) and Random Forest (RF)), parametric (Maximum Likelihood Estimation (MLE)), and cluster (ISO) algorithms for Vertical transmit and Vertical received (VV) and Vertical transmit and Horizontal received (VH) polarisations. The second method relies on Otsu's method for automatic thresholding on post-flood event VH and VV data. The third approach utilizes both pre- and post-flood event data by creating a stack of Pre-event VH, Pre-event VV, Post-event VH, and Post-event VV data, which is then classified using the SVM, RF, MLE, and ISO algorithms. Lastly, the fourth method employs the stack of Principal Component Analysis (PCA) bands, consisting of PCA components 1, 2, and 3, to categorize data for flooding using the SVM, RF, MLE, and ISO algorithms. The rationale behind evaluating these diverse methods was to identify the most effective approach for accurately mapping flood extent by exploiting the strengths of different algorithms and data processing techniques. The accuracy and precision of these approaches were rigorously evaluated using Landsat-9 Normalized Difference Water Index (NDWI) as a reference dataset, along with error matrix and compound value (Cv) metrics. The findings show that a PCA-based technique with SVM is marginally superior (based on the overall accuracy and Kappa coefficient with a value of 92.20 % and kappa coefficient 0.821) than the other approaches tested in the study. By reducing the dimensionality of the dataset while preserving relevant information, PCA offers a computationally efficient approach to flood extent mapping. Among all the approaches, the collective performance of the algorithms was computed using the Cv metric. The SVM algorithm with a Cv of 1.25 demonstrated the best performance, followed by the RF algorithm with Cv of 1.5, and the MLE algorithm with Cv of 2.5. The worst performing algorithm among all is found to be ISO with Cv of 4. It comes into view that first and second approach underperformed in VH polarisation compared to the VV polarisation and provide a more accurate representation of flooded area in VV polarisation. The flood extent with highest accuracy, together with the world's gridded population, latest Microsoft's Global ML Building Footprints, and OpenStreetMap building data are used in conjunction to estimate the number of people and buildings at risk within the study area. © 2024 COSPAR Flood; GEE; GPWv4; IMERG; Machine learning; OpenStreetMap Clustering algorithms; Data integration; Forestry; Mapping; Maximum likelihood estimation; Population statistics; Principal component analysis; Risk assessment; Support vector machines; Synthetic aperture radar; Data-source; Extent mappings; GEE; GPWv4; IMERG; Machine-learning; Maximum-likelihood estimation; Openstreetmap; Principal-component analysis; Support vectors machine; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
300;Exploring the feasibility of Support Vector Machine for short-term hydrological forecasting in South Tyrol: challenges and prospects;Short-term hydrological forecasting is crucial for suitable multipurpose water resource management involving water uses, hydrological security, and renewable production. In the Alpine Regions such as South Tyrol, characterized by several small watersheds, quick information is essential to feed the decision processes in critical cases such as flood events. Predicting water availability ahead is equally crucial for optimizing resource utilization, such as irrigation or snow-making. The increasing data availability and computational power led to data-driven models becoming a serious alternative to physically based hydrological models, especially in complex conditions such as the Alpine Region and for short predictive horizons. This paper proposes a data-driven pipeline to use the local ground station data to infer information in a Support Vector Regression model, which can forecast streamflow in the main closure points of the area at hourly resolution with 48 h of lead time. The main steps of the pipeline are analysed and discussed, with promising results that depend on available information, watershed complexity, and human interactions in the catchment. The presented pipeline, as it stands, offers an accessible tool for integrating these models into decision-making processes to guarantee real-time streamflow information at several points of the hydrological network. Discussion enhances the potentialities, open challenges, and prospects of short-term streamflow forecasting to accommodate broader studies. © The Author(s) 2024.;"Alpine region; Data-driven pipeline; Hydrological modelling; Short-term streamflow forecasting; Water resource management";"Catchments; Complex networks; Decision making; Floods; Forecasting; Information management; Resource allocation; Stream flow; Support vector machines; Water management; Watersheds; Alpine regions; Data driven; Data-driven pipeline; Hydrological forecasting; Hydrological models; Short-term streamflow forecasting; South Tyrol; Streamflow forecasting; Support vectors machine; Water resources management; Pipelines";"Exploring the feasibility of Support Vector Machine for short-term hydrological forecasting in South Tyrol: challenges and prospects Short-term hydrological forecasting is crucial for suitable multipurpose water resource management involving water uses, hydrological security, and renewable production. In the Alpine Regions such as South Tyrol, characterized by several small watersheds, quick information is essential to feed the decision processes in critical cases such as flood events. Predicting water availability ahead is equally crucial for optimizing resource utilization, such as irrigation or snow-making. The increasing data availability and computational power led to data-driven models becoming a serious alternative to physically based hydrological models, especially in complex conditions such as the Alpine Region and for short predictive horizons. This paper proposes a data-driven pipeline to use the local ground station data to infer information in a Support Vector Regression model, which can forecast streamflow in the main closure points of the area at hourly resolution with 48 h of lead time. The main steps of the pipeline are analysed and discussed, with promising results that depend on available information, watershed complexity, and human interactions in the catchment. The presented pipeline, as it stands, offers an accessible tool for integrating these models into decision-making processes to guarantee real-time streamflow information at several points of the hydrological network. Discussion enhances the potentialities, open challenges, and prospects of short-term streamflow forecasting to accommodate broader studies. © The Author(s) 2024. Alpine region; Data-driven pipeline; Hydrological modelling; Short-term streamflow forecasting; Water resource management Catchments; Complex networks; Decision making; Floods; Forecasting; Information management; Resource allocation; Stream flow; Support vector machines; Water management; Watersheds; Alpine regions; Data driven; Data-driven pipeline; Hydrological forecasting; Hydrological models; Short-term streamflow forecasting; South Tyrol; Streamflow forecasting; Support vectors machine; Water resources management; Pipelines";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
301;Cumulative absolute velocity prediction for earthquake early warning with deep learning;A rapid and accurate estimate of earthquake damage is a key component in a successful earthquake early warning (EEW) system. The cumulative absolute velocity (CAV) is an important and widely used parameter to measure ground motion intensity, but it cannot be correctly estimated via the traditional approach with the limited information available in typical EEW systems. Therefore, current EEW systems cannot effectively use CAV to predict earthquake damage. Herein, a CAV prediction model (DLcav) based on convolutional neural networks was proposed for EEW systems. DLcav is an end-to-end solution to continuously predict CAV using arriving seismic waves of increasing length and supplemented with additional auxiliary information. The effectiveness of DLcav to predict CAV was tested based on Japanese ground motion records, and the generalization ability of DLcav was assessed using the ground motion records from Chile. The results demonstrate that DLcav can rapidly predict CAV with good accuracy, which will help better estimate earthquake damage in EEW systems. © 2023 Computer-Aided Civil and Infrastructure Engineering.;NULL;"Japan; Convolutional neural networks; Deep learning; Earthquake effects; 'current; Cumulative absolute velocities; Earthquake damages; Earthquake early warning; Earthquake early warning systems; Ground motion intensities; Ground-motion; Limited information; Traditional approaches; Velocity prediction; early warning system; earthquake prediction; estimation method; ground motion; machine learning; prediction; seismic velocity; Forecasting";"Cumulative absolute velocity prediction for earthquake early warning with deep learning A rapid and accurate estimate of earthquake damage is a key component in a successful earthquake early warning (EEW) system. The cumulative absolute velocity (CAV) is an important and widely used parameter to measure ground motion intensity, but it cannot be correctly estimated via the traditional approach with the limited information available in typical EEW systems. Therefore, current EEW systems cannot effectively use CAV to predict earthquake damage. Herein, a CAV prediction model (DLcav) based on convolutional neural networks was proposed for EEW systems. DLcav is an end-to-end solution to continuously predict CAV using arriving seismic waves of increasing length and supplemented with additional auxiliary information. The effectiveness of DLcav to predict CAV was tested based on Japanese ground motion records, and the generalization ability of DLcav was assessed using the ground motion records from Chile. The results demonstrate that DLcav can rapidly predict CAV with good accuracy, which will help better estimate earthquake damage in EEW systems. © 2023 Computer-Aided Civil and Infrastructure Engineering. NULL Japan; Convolutional neural networks; Deep learning; Earthquake effects; 'current; Cumulative absolute velocities; Earthquake damages; Earthquake early warning; Earthquake early warning systems; Ground motion intensities; Ground-motion; Limited information; Traditional approaches; Velocity prediction; early warning system; earthquake prediction; estimation method; ground motion; machine learning; prediction; seismic velocity; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
302;Enhanced prediction of highway flood inundation through Bayesian generalized linear geostatistical models;Transportation infrastructure facilitates the mobility of goods and humans. Following flooding, blocked road access would prevent vulnerable communities from accessing essential services and disaster relief resources. To reduce the impact of damaged transportation infrastructure on community lifelines, efficient infrastructure restoration is desired. Conventionally, damage identification is often performed via field inspections. However, due to the blocked road access and safety concerns, a limited amount of damage inspection data can be collected immediately following flooding. Aimed at providing a quick prediction of highway inundation status, this research proposes a novel approach that integrates geospatial correlation to address the data sparsity issue. At the core of this approach is a Bayesian generalized linear geostatistical model (BGLGM) that measures (1) correlations between highway inundation status and the associated geospatial variables (e.g., road elevation and flood depth), (2) spatially correlated residuals that cannot be explained by the geospatial variables, and (3) parametric uncertainties. To verify and validate the proposed approach, a case study on highway flood inundation in Harris County, Texas, following Hurricane Harvey was conducted. A sensitivity analysis of the model performance to the availability of damage inspection data was conducted. The results show that the proposed approach is capable of providing accurate highway inundation prediction using limited damage inspection data, which validates the concept of integrating geospatial correlation for more accurate highway inundation representation and prediction. In addition to supporting rapid damage inspection, the validation opens up possibilities for integrating geospatial correlation into machine learning and deep learning models to enhance model performance. The region-specific geospatial correlation also has the potential to recalibrate pre-trained models, improving their generalizability to other regions. © 2024 Elsevier Ltd;"Bayesian approach; Damage identification; Data sparsity; Geospatial correlation; Transportation infrastructure";"Bayesian networks; Damage detection; Deep learning; Disaster prevention; Disasters; Flood control; Floods; Forecasting; Roads and streets; Bayesian; Bayesian approaches; Damage Identification; Damage inspection; Data sparsity; Geo-spatial; Geospatial correlation; Geostatistical models; Inspection datum; Transportation infrastructures; Sensitivity analysis";"Enhanced prediction of highway flood inundation through Bayesian generalized linear geostatistical models Transportation infrastructure facilitates the mobility of goods and humans. Following flooding, blocked road access would prevent vulnerable communities from accessing essential services and disaster relief resources. To reduce the impact of damaged transportation infrastructure on community lifelines, efficient infrastructure restoration is desired. Conventionally, damage identification is often performed via field inspections. However, due to the blocked road access and safety concerns, a limited amount of damage inspection data can be collected immediately following flooding. Aimed at providing a quick prediction of highway inundation status, this research proposes a novel approach that integrates geospatial correlation to address the data sparsity issue. At the core of this approach is a Bayesian generalized linear geostatistical model (BGLGM) that measures (1) correlations between highway inundation status and the associated geospatial variables (e.g., road elevation and flood depth), (2) spatially correlated residuals that cannot be explained by the geospatial variables, and (3) parametric uncertainties. To verify and validate the proposed approach, a case study on highway flood inundation in Harris County, Texas, following Hurricane Harvey was conducted. A sensitivity analysis of the model performance to the availability of damage inspection data was conducted. The results show that the proposed approach is capable of providing accurate highway inundation prediction using limited damage inspection data, which validates the concept of integrating geospatial correlation for more accurate highway inundation representation and prediction. In addition to supporting rapid damage inspection, the validation opens up possibilities for integrating geospatial correlation into machine learning and deep learning models to enhance model performance. The region-specific geospatial correlation also has the potential to recalibrate pre-trained models, improving their generalizability to other regions. © 2024 Elsevier Ltd Bayesian approach; Damage identification; Data sparsity; Geospatial correlation; Transportation infrastructure Bayesian networks; Damage detection; Deep learning; Disaster prevention; Disasters; Flood control; Floods; Forecasting; Roads and streets; Bayesian; Bayesian approaches; Damage Identification; Damage inspection; Data sparsity; Geo-spatial; Geospatial correlation; Geostatistical models; Inspection datum; Transportation infrastructures; Sensitivity analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
303;Machine Learning-Driven Dynamic Maps Supporting Wildfire Risk Management;Recent decades have seen an increase in wildfires activity, posing risks to human settlements, and forcing exploration of new technologies for wildfire risk management. Utilizing Machine Learning in Time Series classification, this study produces decision support maps for Civil Protection system in Italy, which is responsible for coordinating national firefighting air fleet. Trained on past events data, the model gives daily indication on wildfire occurrence and aerial support requests for each administrative unit utilizing time series of Forest Fire Danger Rating indexes from RISICO model. Despite its recent implementation, it performed properly in 2023, showcasing model's potential for decision support. © 2024 Elsevier B.V.. All rights reserved.;"machine learning; risk management; time series classification; Wildfire";"Adversarial machine learning; Risk management; Civil protection; Decision supports; Dynamic maps; Forcings; Human settlements; Machine-learning; Risks management; Time series classifications; Wildfire; Wildfire risks; Premixed flames";"Machine Learning-Driven Dynamic Maps Supporting Wildfire Risk Management Recent decades have seen an increase in wildfires activity, posing risks to human settlements, and forcing exploration of new technologies for wildfire risk management. Utilizing Machine Learning in Time Series classification, this study produces decision support maps for Civil Protection system in Italy, which is responsible for coordinating national firefighting air fleet. Trained on past events data, the model gives daily indication on wildfire occurrence and aerial support requests for each administrative unit utilizing time series of Forest Fire Danger Rating indexes from RISICO model. Despite its recent implementation, it performed properly in 2023, showcasing model's potential for decision support. © 2024 Elsevier B.V.. All rights reserved. machine learning; risk management; time series classification; Wildfire Adversarial machine learning; Risk management; Civil protection; Decision supports; Dynamic maps; Forcings; Human settlements; Machine-learning; Risks management; Time series classifications; Wildfire; Wildfire risks; Premixed flames";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
304;A study of rainfall thresholds for landslides in Badung Regency using satellite-derived rainfall grid datasets;Integrating field rainfall data with satellite data improves data accuracy and overcomes rainfall data limitations for rain thresholds. Integration can involve field rainfall data, satellite rainfall data, or a different satellite dataset. Merging these rainfall data sources provides more spatial coverage of satellite data. To determine how well rainfall thresholds predict rainfall-triggered landslides, the threshold model must be validated. This study will evaluate satellite rainfall data before and after integration in developing a rainfall threshold model for landslide prediction in Badung Regency. To do so, the study used a cumulative rainfall threshold over 3, 7, 15, and 30 days and two rainfall satellite products (integrated merged multi-satellite retrievals (IMERG) and precipitation estimation from remotely sensed information using artificial neural networks (PERSIANN)). Median, first, and third quartiles were used to set thresholds. The area under the curve (AUC) was calculated to validate rainfall threshold outcomes using receiver operating characteristic (ROC) curves. Analysis showed that integrating satellite rainfall data into the rainfall threshold model for landslide prediction yields better results than other methods. An AUC value of 0.903 (90.3%) for the 30-day cumulative rainfall thresholds supports this claim. This model could be a good input for a landslide early warning system in Badung Regency. © 2024, Intelektual Pustaka Media Utama. All rights reserved.;"Badung; Integration; Landslide; Rainfall; Threshold";NULL;"A study of rainfall thresholds for landslides in Badung Regency using satellite-derived rainfall grid datasets Integrating field rainfall data with satellite data improves data accuracy and overcomes rainfall data limitations for rain thresholds. Integration can involve field rainfall data, satellite rainfall data, or a different satellite dataset. Merging these rainfall data sources provides more spatial coverage of satellite data. To determine how well rainfall thresholds predict rainfall-triggered landslides, the threshold model must be validated. This study will evaluate satellite rainfall data before and after integration in developing a rainfall threshold model for landslide prediction in Badung Regency. To do so, the study used a cumulative rainfall threshold over 3, 7, 15, and 30 days and two rainfall satellite products (integrated merged multi-satellite retrievals (IMERG) and precipitation estimation from remotely sensed information using artificial neural networks (PERSIANN)). Median, first, and third quartiles were used to set thresholds. The area under the curve (AUC) was calculated to validate rainfall threshold outcomes using receiver operating characteristic (ROC) curves. Analysis showed that integrating satellite rainfall data into the rainfall threshold model for landslide prediction yields better results than other methods. An AUC value of 0.903 (90.3%) for the 30-day cumulative rainfall thresholds supports this claim. This model could be a good input for a landslide early warning system in Badung Regency. © 2024, Intelektual Pustaka Media Utama. All rights reserved. Badung; Integration; Landslide; Rainfall; Threshold NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
305;Optimising post-disaster waste collection by a deep learning-enhanced differential evolution approach;In the aftermath of natural disasters, efficient waste collection becomes a crucial challenge, owing to the dynamic and unpredictable nature of waste generation, coupled with resource constraints. This paper presents an innovative hybrid methodology that synergizes Long Short-Term Memory (LSTM) machine learning with Differential Evolution (DE) optimisation to augment waste collection efforts post-disaster. The approach leverages real-time data to forecast waste generation with high accuracy, facilitating the development of adaptable waste collection strategies. Our approach is designed to dynamically update collection plans in response to evolving scenarios, ensuring timely and effective decision-making. Field tests conducted in an earthquake-prone city have demonstrated the superior performance of this method in managing waste collection under fluctuating conditions. Moreover, an in-depth sensitivity analysis helps in identifying key areas for improvement. Significantly outperforming traditional models, this method offers substantial time savings and equips disaster response teams with a robust tool for addressing the challenges of waste collection. © 2024 The Authors;"Differential evolution; Long short-term memory; Metaheuristic; Post-disaster waste management; Real-time data analysis";"Brain; Decision making; Disasters; Evolutionary algorithms; Optimization; Sensitivity analysis; Waste management; Differential Evolution; Disaster wastes; Metaheuristic; Natural disasters; Post disasters; Post-disaster waste management; Real time data analysis; Resource Constraint; Waste collection; Waste generation; Long short-term memory";"Optimising post-disaster waste collection by a deep learning-enhanced differential evolution approach In the aftermath of natural disasters, efficient waste collection becomes a crucial challenge, owing to the dynamic and unpredictable nature of waste generation, coupled with resource constraints. This paper presents an innovative hybrid methodology that synergizes Long Short-Term Memory (LSTM) machine learning with Differential Evolution (DE) optimisation to augment waste collection efforts post-disaster. The approach leverages real-time data to forecast waste generation with high accuracy, facilitating the development of adaptable waste collection strategies. Our approach is designed to dynamically update collection plans in response to evolving scenarios, ensuring timely and effective decision-making. Field tests conducted in an earthquake-prone city have demonstrated the superior performance of this method in managing waste collection under fluctuating conditions. Moreover, an in-depth sensitivity analysis helps in identifying key areas for improvement. Significantly outperforming traditional models, this method offers substantial time savings and equips disaster response teams with a robust tool for addressing the challenges of waste collection. © 2024 The Authors Differential evolution; Long short-term memory; Metaheuristic; Post-disaster waste management; Real-time data analysis Brain; Decision making; Disasters; Evolutionary algorithms; Optimization; Sensitivity analysis; Waste management; Differential Evolution; Disaster wastes; Metaheuristic; Natural disasters; Post disasters; Post-disaster waste management; Real time data analysis; Resource Constraint; Waste collection; Waste generation; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
306;Utilizing social media for emergency response: a tweet classification system using attention-based BiLSTM and CNN for resource management;During disasters and emergencies, microblogging platforms like Twitter are crucial sources of real-time information. With so much verbal content present during such situations, it is challenging to extract pertinent situational information. There have been various attempts to identify tweets that are relevant to disasters, but relatively few have concentrated on finding tweets with precise information. Techniques employing the underlying linguistic qualities had been applied in earlier studies which are not suitable for microblogs. We concentrate on one specific application that is crucial for the efficient administration of disaster-related recovery operations: recognizing tweets that provide information about the requirements and availability of vital resources. We focus on a supervised approach using deep learning techniques to differentiate resource tweets from others. The proposed system is a hybrid model employing CNN and BiLSTM to effectively learn fine-grained features from the tweet text. The attention mechanism is also incorporated into the model to get an importance-weighted feature vector. Once the resource tweets have been found, emergency responders can use them to schedule resource allocation so that recovery actions can be carried out efficiently. A supervised model is trained on tweets collected during earthquakes that struck Nepal and Italy in 2015 and 2016, respectively. To verify the appropriateness of the system for practical deployment, we performed in-domain and cross-domain experiments. Our system surpassed several state-of-the-art approaches. According to experimental findings, text categorization in a chaotic environment, such as a disaster event, will gain by considering local key information and global term dependency. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.;"Attention; BiLSTM; CNN; Disaster tweet classification; Resource tweets; RoBERTa";"Bismuth compounds; Deep learning; Emergency services; Learning systems; Resource allocation; Social networking (online); Text processing; Attention; BiLSTM; Classification system; Disaster tweet classification; Emergency response; Micro-blogging platforms; Resource management; Resource tweet; RoBERTa; Social media; Classification (of information)";"Utilizing social media for emergency response: a tweet classification system using attention-based BiLSTM and CNN for resource management During disasters and emergencies, microblogging platforms like Twitter are crucial sources of real-time information. With so much verbal content present during such situations, it is challenging to extract pertinent situational information. There have been various attempts to identify tweets that are relevant to disasters, but relatively few have concentrated on finding tweets with precise information. Techniques employing the underlying linguistic qualities had been applied in earlier studies which are not suitable for microblogs. We concentrate on one specific application that is crucial for the efficient administration of disaster-related recovery operations: recognizing tweets that provide information about the requirements and availability of vital resources. We focus on a supervised approach using deep learning techniques to differentiate resource tweets from others. The proposed system is a hybrid model employing CNN and BiLSTM to effectively learn fine-grained features from the tweet text. The attention mechanism is also incorporated into the model to get an importance-weighted feature vector. Once the resource tweets have been found, emergency responders can use them to schedule resource allocation so that recovery actions can be carried out efficiently. A supervised model is trained on tweets collected during earthquakes that struck Nepal and Italy in 2015 and 2016, respectively. To verify the appropriateness of the system for practical deployment, we performed in-domain and cross-domain experiments. Our system surpassed several state-of-the-art approaches. According to experimental findings, text categorization in a chaotic environment, such as a disaster event, will gain by considering local key information and global term dependency. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023. Attention; BiLSTM; CNN; Disaster tweet classification; Resource tweets; RoBERTa Bismuth compounds; Deep learning; Emergency services; Learning systems; Resource allocation; Social networking (online); Text processing; Attention; BiLSTM; Classification system; Disaster tweet classification; Emergency response; Micro-blogging platforms; Resource management; Resource tweet; RoBERTa; Social media; Classification (of information)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
307;A Cost-Effective Earthquake Disaster Assessment Model for Power Systems Based on Nighttime Light Information;"The power system is one of the most important urban lifeline engineering systems. Identifying the damage to the power system is an important task in earthquake disaster assessments. Considering the importance of timeliness and accessibility, a hyperparameter optimization model is proposed to address the assessment of disaster losses in power systems on earthquakes. The power system vulnerability on earthquakes, PSVE, is assessed by the hyperparameter optimization model based on nighttime light information. Through the utilization of the computational resources provided by Google Earth Engine, the accuracy of the baseline model has been significantly improved to 87.9%; meanwhile, the cost-effectiveness in the evaluation process is maintained. The PSVE-based damage evaluation has the potential to aid in assessing earthquake damage to cities’ energy supply, power infrastructure, and lighting. Furthermore, the PSVE-based damage evaluation can provide valuable guidance for prioritizing and efficiently allocating resources for rapid repair and reconstruction efforts. © 2024 by the authors.";"earthquake disaster assessment; GEE; machine learning; nighttime light information; power system vulnerability";NULL;"A Cost-Effective Earthquake Disaster Assessment Model for Power Systems Based on Nighttime Light Information The power system is one of the most important urban lifeline engineering systems. Identifying the damage to the power system is an important task in earthquake disaster assessments. Considering the importance of timeliness and accessibility, a hyperparameter optimization model is proposed to address the assessment of disaster losses in power systems on earthquakes. The power system vulnerability on earthquakes, PSVE, is assessed by the hyperparameter optimization model based on nighttime light information. Through the utilization of the computational resources provided by Google Earth Engine, the accuracy of the baseline model has been significantly improved to 87.9%; meanwhile, the cost-effectiveness in the evaluation process is maintained. The PSVE-based damage evaluation has the potential to aid in assessing earthquake damage to cities’ energy supply, power infrastructure, and lighting. Furthermore, the PSVE-based damage evaluation can provide valuable guidance for prioritizing and efficiently allocating resources for rapid repair and reconstruction efforts. © 2024 by the authors. earthquake disaster assessment; GEE; machine learning; nighttime light information; power system vulnerability NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
308;Development of IoT Slope Monitoring System and its Applications for Kratu-Patong Road Landslide in Phuket, Thailand;This paper presents an on-going development of Internet-of-Things (IoT) slope monitoring for landslide early warning system in Thailand. The current system employs a variety of sensors, namely MEMs-based tensiometers, piezometers, soil moisture sensor, tiltmeter, in-placed inclinometer and tipping bucket raingauge, all connected to Arduino-based microcontroller which relied on Narrowband, NB-IoT, protocol for data transmission to the cloud server. A specially designed application platform was developed to convert the sensor readings to engineering unit and ultimately geotechnical parameters, such as factor of safety, which enable engineers to readily understand the situation and make an informed-decision based on such parameters. A weighted approach was proposed in calculating the overall landslide hazard level based various kinds of sensor readings. A case history of Kratu-Patong Road Landslide in Phuket, Southern Thailand, taking place in Year 2022 was presented to demonstrate how the developed IoT system was used real-time together with geotechnical analysis to aid in traffic management during the critical time. The warning event primarily stemmed from spikes in slope movement, spurred by heightened traffic intensity. Rapid slope movement during the incident was characterized by a tilting magnitude of-2 to 1.2 degrees and a velocity ranging from-1.7 to 1.8 degrees per hour. Notably, the calculation of the warning index based on tilting magnitude provides a continuous warning message, in contrast to an intermittent message based on tilting velocity. The tensiometer effectively detected the decrease in suction caused by slope movement, while the piezometer only registered changes in pore-water pressure when the groundwater table rose above the measurement point. Finally, an Artificial Neural Network (ANN) model was used to predict the pore-water pressure at different depths based on 5 rainfall parameters, namely, 5-min, 1-hour, 1-day, 3-day and 7-day antecedent rainfalls. The model demonstrated satisfactory predictive accuracy (R² = 0.644, RMSE = 3.637 kPa), offering promising potential for integration with the IoT platform in the future. © 2024, Southeast Asian Geotechnical Society. All rights reserved.;"and Pore-water pressure; IoT slope monitoring system; Landslide early warning; Slope stability";"Phuket; Groundwater; Internet of things; Moisture control; Neural networks; Pore pressure; Rain; Roads and streets; Safety factor; Slope protection; Soil moisture; And pore-water pressure; Early warning; Internet-of-thing slope monitoring system; ITS applications; Landslide early warning; Pore-water pressures; Sensor readings; Slope monitoring system; System applications; Thailand; data transmission; early warning system; landslide; porewater; slope stability; soil moisture; tensiometer; traffic management; Landslides";"Development of IoT Slope Monitoring System and its Applications for Kratu-Patong Road Landslide in Phuket, Thailand This paper presents an on-going development of Internet-of-Things (IoT) slope monitoring for landslide early warning system in Thailand. The current system employs a variety of sensors, namely MEMs-based tensiometers, piezometers, soil moisture sensor, tiltmeter, in-placed inclinometer and tipping bucket raingauge, all connected to Arduino-based microcontroller which relied on Narrowband, NB-IoT, protocol for data transmission to the cloud server. A specially designed application platform was developed to convert the sensor readings to engineering unit and ultimately geotechnical parameters, such as factor of safety, which enable engineers to readily understand the situation and make an informed-decision based on such parameters. A weighted approach was proposed in calculating the overall landslide hazard level based various kinds of sensor readings. A case history of Kratu-Patong Road Landslide in Phuket, Southern Thailand, taking place in Year 2022 was presented to demonstrate how the developed IoT system was used real-time together with geotechnical analysis to aid in traffic management during the critical time. The warning event primarily stemmed from spikes in slope movement, spurred by heightened traffic intensity. Rapid slope movement during the incident was characterized by a tilting magnitude of-2 to 1.2 degrees and a velocity ranging from-1.7 to 1.8 degrees per hour. Notably, the calculation of the warning index based on tilting magnitude provides a continuous warning message, in contrast to an intermittent message based on tilting velocity. The tensiometer effectively detected the decrease in suction caused by slope movement, while the piezometer only registered changes in pore-water pressure when the groundwater table rose above the measurement point. Finally, an Artificial Neural Network (ANN) model was used to predict the pore-water pressure at different depths based on 5 rainfall parameters, namely, 5-min, 1-hour, 1-day, 3-day and 7-day antecedent rainfalls. The model demonstrated satisfactory predictive accuracy (R² = 0.644, RMSE = 3.637 kPa), offering promising potential for integration with the IoT platform in the future. © 2024, Southeast Asian Geotechnical Society. All rights reserved. and Pore-water pressure; IoT slope monitoring system; Landslide early warning; Slope stability Phuket; Groundwater; Internet of things; Moisture control; Neural networks; Pore pressure; Rain; Roads and streets; Safety factor; Slope protection; Soil moisture; And pore-water pressure; Early warning; Internet-of-thing slope monitoring system; ITS applications; Landslide early warning; Pore-water pressures; Sensor readings; Slope monitoring system; System applications; Thailand; data transmission; early warning system; landslide; porewater; slope stability; soil moisture; tensiometer; traffic management; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
309;Spatiotemporal Flood Hazard Map Prediction Using Machine Learning for a Flood Early Warning Case Study: Chiang Mai Province, Thailand;"Floods cause disastrous damage to the environment, economy, and humanity. Flood losses can be reduced if adequate management is implemented in the pre-disaster period. Flood hazard maps comprise disaster risk information displayed on geo-location maps and the potential flood events that occur in an area. This paper proposes a spatiotemporal flood hazard map framework to generate a flood hazard map using spatiotemporal data. The framework has three processes: (1) temporal prediction, which uses the LSTM technique to predict water levels and rainfall for the next time; (2) spatial interpolation, which uses the IDW technique to estimate values; and (3) map generation, which uses the CNN technique to predict flood events and generate flood hazard maps. The study area is Chiang Mai Province, Thailand. The generated hazard map covers 20,107 km2. There are 14 water-level telemetry stations and 16 rain gauge stations. The proposed model accurately predicts water level and rainfall, as demonstrated by the evaluation results (RMSE, MAE, and R2). The generated map has a (Formula presented.) mean accuracy and a (Formula presented.) mean F1-score when compared to the actual flood event. The framework enhances the accuracy and responsiveness of flood hazard maps to reduce potential losses before floods occur. © 2024 by the authors.";"CNN; flood hazard map; IDW; LSTM; spatiotemporal prediction";"Chiang Mai [Northern Region]; Northern Region [Thailand]; Thailand; early warning system; flood control; hazard assessment; machine learning; raingauge; spatiotemporal analysis; telemetry; temporal analysis; water level";"Spatiotemporal Flood Hazard Map Prediction Using Machine Learning for a Flood Early Warning Case Study: Chiang Mai Province, Thailand Floods cause disastrous damage to the environment, economy, and humanity. Flood losses can be reduced if adequate management is implemented in the pre-disaster period. Flood hazard maps comprise disaster risk information displayed on geo-location maps and the potential flood events that occur in an area. This paper proposes a spatiotemporal flood hazard map framework to generate a flood hazard map using spatiotemporal data. The framework has three processes: (1) temporal prediction, which uses the LSTM technique to predict water levels and rainfall for the next time; (2) spatial interpolation, which uses the IDW technique to estimate values; and (3) map generation, which uses the CNN technique to predict flood events and generate flood hazard maps. The study area is Chiang Mai Province, Thailand. The generated hazard map covers 20,107 km2. There are 14 water-level telemetry stations and 16 rain gauge stations. The proposed model accurately predicts water level and rainfall, as demonstrated by the evaluation results (RMSE, MAE, and R2). The generated map has a (Formula presented.) mean accuracy and a (Formula presented.) mean F1-score when compared to the actual flood event. The framework enhances the accuracy and responsiveness of flood hazard maps to reduce potential losses before floods occur. © 2024 by the authors. CNN; flood hazard map; IDW; LSTM; spatiotemporal prediction Chiang Mai [Northern Region]; Northern Region [Thailand]; Thailand; early warning system; flood control; hazard assessment; machine learning; raingauge; spatiotemporal analysis; telemetry; temporal analysis; water level";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
310;An Optimized Smoke Segmentation Method for Forest and Grassland Fire Based on the UNet Framework;Smoke, a byproduct of forest and grassland combustion, holds the key to precise and rapid identification—an essential breakthrough in early wildfire detection, critical for forest and grassland fire monitoring and early warning. To address the scarcity of middle–high-resolution satellite datasets for forest and grassland fire smoke, and the associated challenges in identifying smoke, the CAF_SmokeSEG dataset was constructed for smoke segmentation. The dataset was created based on GF-6 WFV smoke images of forest and grassland fire globally from 2019 to 2022. Then, an optimized segmentation algorithm, GFUNet, was proposed based on the UNet framework. Through comprehensive analysis, including method comparison, module ablation, band combination, and data transferability experiments, this study revealed that GF-6 WFV data effectively represent information related to forest and grassland fire smoke. The CAF_SmokeSEG dataset was found to be valuable for pixel-level smoke segmentation tasks. GFUNet exhibited robust smoke feature learning capability and segmentation stability. It demonstrated clear smoke area delineation, significantly outperforming UNet and other optimized methods, with an F1-Score and Jaccard coefficient of 85.50% and 75.76%, respectively. Additionally, augmenting the common spectral bands with additional bands improved the smoke segmentation accuracy, particularly shorter-wavelength bands like the coastal blue band, outperforming longer-wavelength bands such as the red-edge band. GFUNet was trained on the combination of red, green, blue, and NIR bands from common multispectral sensors. The method showed promising transferability and enabled the segmentation of smoke areas in GF-1 WFV and HJ-2A/B CCD images with comparable spatial resolution and similar bands. The integration of high spatiotemporal multispectral data like GF-6 WFV with the advanced information extraction capabilities of deep learning algorithms effectively meets the practical needs for pixel-level identification of smoke areas in forest and grassland fire scenarios. It shows promise in improving and optimizing existing forest and grassland fire monitoring systems, providing valuable decision-making support for fire monitoring and early warning systems. © 2024 by the authors.;"deep learning; forest and grassland fire; GF-6 WFV; smoke segmentation";NULL;"An Optimized Smoke Segmentation Method for Forest and Grassland Fire Based on the UNet Framework Smoke, a byproduct of forest and grassland combustion, holds the key to precise and rapid identification—an essential breakthrough in early wildfire detection, critical for forest and grassland fire monitoring and early warning. To address the scarcity of middle–high-resolution satellite datasets for forest and grassland fire smoke, and the associated challenges in identifying smoke, the CAF_SmokeSEG dataset was constructed for smoke segmentation. The dataset was created based on GF-6 WFV smoke images of forest and grassland fire globally from 2019 to 2022. Then, an optimized segmentation algorithm, GFUNet, was proposed based on the UNet framework. Through comprehensive analysis, including method comparison, module ablation, band combination, and data transferability experiments, this study revealed that GF-6 WFV data effectively represent information related to forest and grassland fire smoke. The CAF_SmokeSEG dataset was found to be valuable for pixel-level smoke segmentation tasks. GFUNet exhibited robust smoke feature learning capability and segmentation stability. It demonstrated clear smoke area delineation, significantly outperforming UNet and other optimized methods, with an F1-Score and Jaccard coefficient of 85.50% and 75.76%, respectively. Additionally, augmenting the common spectral bands with additional bands improved the smoke segmentation accuracy, particularly shorter-wavelength bands like the coastal blue band, outperforming longer-wavelength bands such as the red-edge band. GFUNet was trained on the combination of red, green, blue, and NIR bands from common multispectral sensors. The method showed promising transferability and enabled the segmentation of smoke areas in GF-1 WFV and HJ-2A/B CCD images with comparable spatial resolution and similar bands. The integration of high spatiotemporal multispectral data like GF-6 WFV with the advanced information extraction capabilities of deep learning algorithms effectively meets the practical needs for pixel-level identification of smoke areas in forest and grassland fire scenarios. It shows promise in improving and optimizing existing forest and grassland fire monitoring systems, providing valuable decision-making support for fire monitoring and early warning systems. © 2024 by the authors. deep learning; forest and grassland fire; GF-6 WFV; smoke segmentation NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
311;Optimization-driven artificial intelligence-enhanced municipal waste classification system for disaster waste management;This research addresses the critical challenge of disaster waste management, a growing concern exacerbated by the increasing frequency and intensity of natural disasters like flooding. Traditional waste systems often struggle with the volume and heterogeneity of disaster waste, highlighting the need for innovative solutions. In this study, we present a novel disaster waste classification model integrating advanced artificial intelligence (AI) and optimization techniques to streamline waste categorization in post-disaster environments. Our approach leverages a dual ensemble deep learning framework. The first ensemble combines various image-segmentation methods, while the second integrates outputs from diverse convolutional neural network architectures. A modified artificial multiple intelligence system serves as a decision fusion strategy, enhancing accuracy at both ensemble points. We rigorously evaluated our model using three datasets: the “TrashNet” dataset for benchmarking against existing methods, as well as two meticulously curated, real-world datasets collected from flood-affected areas in Thailand. The results demonstrate that our method outperforms existing algorithms like VGG19, YoloV5, and InceptionV3 in general solid waste classification, achieving an average improvement of 11.18%. Regarding disaster waste specifically, our model achieves 96.48% and 96.49% accuracy on the curated datasets, consistently outperforming ResNet-101, DenseNet-121, and InceptionV3 by an average of 3.47%. These findings demonstrate the potential of our AI-enhanced model to revolutionize disaster waste management practices. Thus, we advocate integrating such technologies into municipal waste management policies to enhance resilience and optimize disaster responses. Future research will explore scaling the model to diverse disaster types and incorporating real-time data for adaptable waste management strategies. © 2024 The Authors;"Artificial intelligence-enhanced system; Disaster waste classification; Environmental sustainability; Municipal waste management; Optimization-driven";"Deep learning; Disasters; Floods; Image segmentation; Network architecture; Sustainable development; Artificial intelligence-enhanced system; Classification system; Disaster waste classification; Disaster wastes; Environmental sustainability; Municipal waste; Municipal waste management; Optimisations; Optimization-driven; Waste classification; Waste management";"Optimization-driven artificial intelligence-enhanced municipal waste classification system for disaster waste management This research addresses the critical challenge of disaster waste management, a growing concern exacerbated by the increasing frequency and intensity of natural disasters like flooding. Traditional waste systems often struggle with the volume and heterogeneity of disaster waste, highlighting the need for innovative solutions. In this study, we present a novel disaster waste classification model integrating advanced artificial intelligence (AI) and optimization techniques to streamline waste categorization in post-disaster environments. Our approach leverages a dual ensemble deep learning framework. The first ensemble combines various image-segmentation methods, while the second integrates outputs from diverse convolutional neural network architectures. A modified artificial multiple intelligence system serves as a decision fusion strategy, enhancing accuracy at both ensemble points. We rigorously evaluated our model using three datasets: the “TrashNet” dataset for benchmarking against existing methods, as well as two meticulously curated, real-world datasets collected from flood-affected areas in Thailand. The results demonstrate that our method outperforms existing algorithms like VGG19, YoloV5, and InceptionV3 in general solid waste classification, achieving an average improvement of 11.18%. Regarding disaster waste specifically, our model achieves 96.48% and 96.49% accuracy on the curated datasets, consistently outperforming ResNet-101, DenseNet-121, and InceptionV3 by an average of 3.47%. These findings demonstrate the potential of our AI-enhanced model to revolutionize disaster waste management practices. Thus, we advocate integrating such technologies into municipal waste management policies to enhance resilience and optimize disaster responses. Future research will explore scaling the model to diverse disaster types and incorporating real-time data for adaptable waste management strategies. © 2024 The Authors Artificial intelligence-enhanced system; Disaster waste classification; Environmental sustainability; Municipal waste management; Optimization-driven Deep learning; Disasters; Floods; Image segmentation; Network architecture; Sustainable development; Artificial intelligence-enhanced system; Classification system; Disaster waste classification; Disaster wastes; Environmental sustainability; Municipal waste; Municipal waste management; Optimisations; Optimization-driven; Waste classification; Waste management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
312;UsIL-6: An unbalanced learning strategy for identifying IL-6 inducing peptides by undersampling technique;Background and objective: Interleukin-6 (IL-6) is the critical factor of early warning, monitoring, and prognosis in the inflammatory storm of COVID-19 cases. IL-6 inducing peptides, which can induce cytokine IL-6 production, are very important for the development of diagnosis and immunotherapy. Although the existing methods have some success in predicting IL-6 inducing peptides, there is still room for improvement in the performance of these models in practical application. Methods: In this study, we proposed UsIL-6, a high-performance bioinformatics tool for identifying IL-6 inducing peptides. First, we extracted five groups of physicochemical properties and sequence structural information from IL-6 inducing peptide sequences, and obtained a 636-dimensional feature vector, we also employed NearMiss3 undersampling method and normalization method StandardScaler to process the data. Then, a 40-dimensional optimal feature vector was obtained by Boruta feature selection method. Finally, we combined this feature vector with extreme randomization tree classifier to build the final model UsIL-6. Results: The AUC value of UsIL-6 on the independent test dataset was 0.87, and the BACC value was 0.808, which indicated that UsIL-6 had better performance than the existing methods in IL-6 inducing peptide recognition. Conclusions: The performance comparison on independent test dataset confirmed that UsIL-6 could achieve the highest performance, best robustness, and most excellent generalization ability. We hope that UsIL-6 will become a valuable method to identify, annotate and characterize new IL-6 inducing peptides. © 2024;"Feature selection; IL-6 inducing peptides; Machine learning; Undersampling strategy";"Algorithms; Computational Biology; COVID-19; Humans; Interleukin-6; Machine Learning; Peptides; SARS-CoV-2; COVID-19; Feature Selection; Learning systems; Peptides; Statistical tests; cytokine; interleukin 6; peptide; IL6 protein, human; interleukin 6; peptide; Features selection; Features vector; Inducing peptides; Interleukin-6 inducing peptide; Interleukin6 (IL6); Learning strategy; Machine-learning; Performance; Under-sampling; Undersampling strategy; adult; amino acid sequence; area under the curve; Article; bioinformatics; comparative study; controlled study; feature extraction; feature selection; machine learning; middle aged; nonhuman; performance indicator; physical chemistry; algorithm; bioinformatics; chemistry; coronavirus disease 2019; human; machine learning; procedures; Severe acute respiratory syndrome coronavirus 2; Physicochemical properties";"UsIL-6: An unbalanced learning strategy for identifying IL-6 inducing peptides by undersampling technique Background and objective: Interleukin-6 (IL-6) is the critical factor of early warning, monitoring, and prognosis in the inflammatory storm of COVID-19 cases. IL-6 inducing peptides, which can induce cytokine IL-6 production, are very important for the development of diagnosis and immunotherapy. Although the existing methods have some success in predicting IL-6 inducing peptides, there is still room for improvement in the performance of these models in practical application. Methods: In this study, we proposed UsIL-6, a high-performance bioinformatics tool for identifying IL-6 inducing peptides. First, we extracted five groups of physicochemical properties and sequence structural information from IL-6 inducing peptide sequences, and obtained a 636-dimensional feature vector, we also employed NearMiss3 undersampling method and normalization method StandardScaler to process the data. Then, a 40-dimensional optimal feature vector was obtained by Boruta feature selection method. Finally, we combined this feature vector with extreme randomization tree classifier to build the final model UsIL-6. Results: The AUC value of UsIL-6 on the independent test dataset was 0.87, and the BACC value was 0.808, which indicated that UsIL-6 had better performance than the existing methods in IL-6 inducing peptide recognition. Conclusions: The performance comparison on independent test dataset confirmed that UsIL-6 could achieve the highest performance, best robustness, and most excellent generalization ability. We hope that UsIL-6 will become a valuable method to identify, annotate and characterize new IL-6 inducing peptides. © 2024 Feature selection; IL-6 inducing peptides; Machine learning; Undersampling strategy Algorithms; Computational Biology; COVID-19; Humans; Interleukin-6; Machine Learning; Peptides; SARS-CoV-2; COVID-19; Feature Selection; Learning systems; Peptides; Statistical tests; cytokine; interleukin 6; peptide; IL6 protein, human; interleukin 6; peptide; Features selection; Features vector; Inducing peptides; Interleukin-6 inducing peptide; Interleukin6 (IL6); Learning strategy; Machine-learning; Performance; Under-sampling; Undersampling strategy; adult; amino acid sequence; area under the curve; Article; bioinformatics; comparative study; controlled study; feature extraction; feature selection; machine learning; middle aged; nonhuman; performance indicator; physical chemistry; algorithm; bioinformatics; chemistry; coronavirus disease 2019; human; machine learning; procedures; Severe acute respiratory syndrome coronavirus 2; Physicochemical properties";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;-1;NULL
313;ConvEQ: Convolutional neural network for earthquake phase classification using short time frequency transform;We present ConvEQ as a tool for discriminating seismic phases, leveraging artificial intelligence technique (Convolutional Neural Network) for short-time Frequency Transform of the seismic signal. Timely detection of the vertical (P) wave from an earthquake can generate a warning several tens of precious seconds before the more destructive waves strike. We propose a train-for-each-station approach for an Internet-of-Things-based Smart Earthquake Early Warning System, where lightweight neural networks trained for the seismic data belonging to each station are implemented on edge devices directly interfaced with seismometers. The approach has the potential to get the most from the sparse seismic network for Pakistan and other third-world countries. We train networks for multi-station and single-station data and achieve 96% and 99% accuracy, respectively, proving that train-for-each-station maximizes accuracy. The total processing time (including preprocessing and inference) is about 30ms for each event, thus suitable for real-time deployment. We further compare the performance of ConvEQ on simulated real-time data with several state-of-the-art contemporary algorithms. Our proposed approach demonstrates a robust response on diverse metrics. The ConvEQZ classifies the vertical seismic signal component with high accuracy and the ConvEQX can classify any seismic data component, inculcating robustness against connectivity issues. © 2024 Elsevier Ltd;"Artificial intelligence; Classification; Convolutional neural networks; Earthquake early warning systems; Seismology";"Classification (of information); Convolution; Convolutional neural networks; Geophysical prospecting; Seismic response; Seismic waves; A-train; Artificial intelligence techniques; Convolutional neural network; Earthquake early warning systems; P waves; Phase classification; Seismic datas; Seismic phase; Seismic signals; Short time frequency transforms; artificial intelligence; artificial neural network; early warning system; earthquake event; frequency analysis; seismicity; seismograph; Earthquakes";"ConvEQ: Convolutional neural network for earthquake phase classification using short time frequency transform We present ConvEQ as a tool for discriminating seismic phases, leveraging artificial intelligence technique (Convolutional Neural Network) for short-time Frequency Transform of the seismic signal. Timely detection of the vertical (P) wave from an earthquake can generate a warning several tens of precious seconds before the more destructive waves strike. We propose a train-for-each-station approach for an Internet-of-Things-based Smart Earthquake Early Warning System, where lightweight neural networks trained for the seismic data belonging to each station are implemented on edge devices directly interfaced with seismometers. The approach has the potential to get the most from the sparse seismic network for Pakistan and other third-world countries. We train networks for multi-station and single-station data and achieve 96% and 99% accuracy, respectively, proving that train-for-each-station maximizes accuracy. The total processing time (including preprocessing and inference) is about 30ms for each event, thus suitable for real-time deployment. We further compare the performance of ConvEQ on simulated real-time data with several state-of-the-art contemporary algorithms. Our proposed approach demonstrates a robust response on diverse metrics. The ConvEQZ classifies the vertical seismic signal component with high accuracy and the ConvEQX can classify any seismic data component, inculcating robustness against connectivity issues. © 2024 Elsevier Ltd Artificial intelligence; Classification; Convolutional neural networks; Earthquake early warning systems; Seismology Classification (of information); Convolution; Convolutional neural networks; Geophysical prospecting; Seismic response; Seismic waves; A-train; Artificial intelligence techniques; Convolutional neural network; Earthquake early warning systems; P waves; Phase classification; Seismic datas; Seismic phase; Seismic signals; Short time frequency transforms; artificial intelligence; artificial neural network; early warning system; earthquake event; frequency analysis; seismicity; seismograph; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
314;Reconstruction of missing wind data based on limited wind pressure measurements and machine learning;In structural health monitoring (SHM), wind field monitoring sometimes suffers from data loss owing to monitoring device failure, which inevitably creates barriers to subsequent data analysis and data mining. To this end, a novel strategy for reconstructing missing wind field data based on machine learning (ML) utilizing limited wind pressure measurements is proposed in this paper. Several ML algorithms, including decision tree, random forest, gradient boosting regression tree, support vector regression, Gaussian process regression, and backpropagation neural network, are employed to characterize potential relationships between wind pressure information (including time series and statistical parameters of wind pressures) and wind field information (e.g., wind direction and wind speed). Moreover, the effect of input information (including the type of input variables as well as the number and location of pressure transducers providing input data) on reconstruction performance and efficiency is investigated. Field measured records from an SHM system in a 600-m-high supertall building during typhoons are utilized to validate the feasibility and robustness of the proposed strategy. The results show that the presented strategy can effectively reconstruct missing wind field information in the SHM of the skyscraper during typhoons. Compared with the time series of wind pressures, selecting statistical parameters of wind pressures as input variables can effectively improve the performance and efficiency of reconstruction models. Choosing appropriate input information (e.g., using multiple input variables, adopting data from a larger number of pressure transducers, and utilizing data from pressure transducers closer to an anemometer) is beneficial for enhancing the performance of reconstruction models. © 2024 Author(s).;NULL;"Data mining; Decision trees; Efficiency; Machine learning; Neural networks; Office buildings; Pressure measurement; Pressure transducers; Regression analysis; Storms; Structural dynamics; Structural health monitoring; Tall buildings; Time series; Wind effects; Wind speed; Field informations; Field monitoring; Input variables; Machine-learning; Performance; Statistical parameters; Times series; Wind data; Wind field; Wind pressures; Pressure sensors";"Reconstruction of missing wind data based on limited wind pressure measurements and machine learning In structural health monitoring (SHM), wind field monitoring sometimes suffers from data loss owing to monitoring device failure, which inevitably creates barriers to subsequent data analysis and data mining. To this end, a novel strategy for reconstructing missing wind field data based on machine learning (ML) utilizing limited wind pressure measurements is proposed in this paper. Several ML algorithms, including decision tree, random forest, gradient boosting regression tree, support vector regression, Gaussian process regression, and backpropagation neural network, are employed to characterize potential relationships between wind pressure information (including time series and statistical parameters of wind pressures) and wind field information (e.g., wind direction and wind speed). Moreover, the effect of input information (including the type of input variables as well as the number and location of pressure transducers providing input data) on reconstruction performance and efficiency is investigated. Field measured records from an SHM system in a 600-m-high supertall building during typhoons are utilized to validate the feasibility and robustness of the proposed strategy. The results show that the presented strategy can effectively reconstruct missing wind field information in the SHM of the skyscraper during typhoons. Compared with the time series of wind pressures, selecting statistical parameters of wind pressures as input variables can effectively improve the performance and efficiency of reconstruction models. Choosing appropriate input information (e.g., using multiple input variables, adopting data from a larger number of pressure transducers, and utilizing data from pressure transducers closer to an anemometer) is beneficial for enhancing the performance of reconstruction models. © 2024 Author(s). NULL Data mining; Decision trees; Efficiency; Machine learning; Neural networks; Office buildings; Pressure measurement; Pressure transducers; Regression analysis; Storms; Structural dynamics; Structural health monitoring; Tall buildings; Time series; Wind effects; Wind speed; Field informations; Field monitoring; Input variables; Machine-learning; Performance; Statistical parameters; Times series; Wind data; Wind field; Wind pressures; Pressure sensors";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
315;Meteorological Drought Prediction Based on Evaluating the Efficacy of Several Prediction Models;The prediction of drought is critically important for early warning and mitigation of its impacts. Selecting the most appropriate prediction model provides opportunities for reducing the drought’s adverse effects on different sectors. So, in this study, the capability of several prediction models to predict drought (based on the Reconnaissance Drought Index (RDI) in 1 and 3-month time scales) was compared. The models included stationary time series models (ST), cyclostationary time series (CST) models, autoregressive fractionally integrated moving average (ARFIMA), periodic autoregressive fractionally integrated moving average (PARFIMA), first-order Markov chain (FOMC), and second-order Markov chain (SOMC). For choosing the best-fitted model, the correlation coefficient (R2) and absolute values of T-Statistics (AVTS) between predicted (using each model) and observed RDI in 1 and 3-month time scales at 15 stations during the period 2017–2021 in Iran were used. For this purpose, the 1967 to 2016 data series was used. Then the best prediction model (with the highest performance level) was used to predict 1 and 3-month RDI in the investigated stations from 2022 to 2031. For this purpose, 1967 to 2021 data series was used. The results showed that CST models with the highest R2 values (significantly at the 5% level in both time scales in all stations) and the lowest AVTS values (significantly at the 1% level in both time scales in all stations) for the best-fitted models in 1 and 3-month time scales had the best performance in predicting monthly and seasonal RDI. To predict monthly and seasonal RDI in all stations, the PARIMA (24, 0, 0), 12 and PARIMA (20, 0, 0), 4 models were used as the best models, respectively. The predictions indicated that normal (No) and moderately (Mod) dry classes would be more frequent in both time scales. This study demonstrates that CST models can be useful tools for drought prediction and management. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"Drought; Iran; Markov chain; Prediction; RDI index; Time series";"Iran; Forecasting; Markov processes; Time measurement; Time series; Auto-regressive; Cyclostationary; Iran; Moving averages; Prediction modelling; Reconnaissance drought index index; Reconnaissance drought indices; Time-scales; Times series; Times series models; drought; efficiency measurement; Markov chain; regression analysis; time series analysis; waste disposal; Drought";"Meteorological Drought Prediction Based on Evaluating the Efficacy of Several Prediction Models The prediction of drought is critically important for early warning and mitigation of its impacts. Selecting the most appropriate prediction model provides opportunities for reducing the drought’s adverse effects on different sectors. So, in this study, the capability of several prediction models to predict drought (based on the Reconnaissance Drought Index (RDI) in 1 and 3-month time scales) was compared. The models included stationary time series models (ST), cyclostationary time series (CST) models, autoregressive fractionally integrated moving average (ARFIMA), periodic autoregressive fractionally integrated moving average (PARFIMA), first-order Markov chain (FOMC), and second-order Markov chain (SOMC). For choosing the best-fitted model, the correlation coefficient (R2) and absolute values of T-Statistics (AVTS) between predicted (using each model) and observed RDI in 1 and 3-month time scales at 15 stations during the period 2017–2021 in Iran were used. For this purpose, the 1967 to 2016 data series was used. Then the best prediction model (with the highest performance level) was used to predict 1 and 3-month RDI in the investigated stations from 2022 to 2031. For this purpose, 1967 to 2021 data series was used. The results showed that CST models with the highest R2 values (significantly at the 5% level in both time scales in all stations) and the lowest AVTS values (significantly at the 1% level in both time scales in all stations) for the best-fitted models in 1 and 3-month time scales had the best performance in predicting monthly and seasonal RDI. To predict monthly and seasonal RDI in all stations, the PARIMA (24, 0, 0), 12 and PARIMA (20, 0, 0), 4 models were used as the best models, respectively. The predictions indicated that normal (No) and moderately (Mod) dry classes would be more frequent in both time scales. This study demonstrates that CST models can be useful tools for drought prediction and management. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. Drought; Iran; Markov chain; Prediction; RDI index; Time series Iran; Forecasting; Markov processes; Time measurement; Time series; Auto-regressive; Cyclostationary; Iran; Moving averages; Prediction modelling; Reconnaissance drought index index; Reconnaissance drought indices; Time-scales; Times series; Times series models; drought; efficiency measurement; Markov chain; regression analysis; time series analysis; waste disposal; Drought";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
316;XGBoost-based prediction of on-site acceleration response spectra with multi-feature inputs from P-wave arrivals;On-site earthquake early warning systems use the information from the initial P-wave to quickly estimate the degree of local damage. Existing on-site earthquake early warning models focus on ground motion information and generally ignore the degree of structural response, with many machine learning models characterized by poor interpretability. Therefore, an on-site prediction model based on XGBoost with a continuous time window and multi-feature inputs is proposed. The model uses the response spectrum value, which is closely related to the structural response, as the predicted intensity measure and uses strong motion data from Japanese K-NET stations for training. After using permutation importance to eliminate irrelevant input feature parameters, 11 parameters (Pa, Pd Pav, Pad, IAV, IV2, Ia, TP, Fmax, TW, T) are determined as the final model input parameters. The results based on a test dataset show that 3 s after P-wave arrival, the mean square error of the model prediction is 15.10*10−4 g. As the input time window is extended, the mean square error of the model prediction gradually decreases, and the overestimation of small values is mitigated. Additionally, 10 s after P-wave arrival, the mean square error decreases by 7.66*10−4 g compared with that at 1 s. After using the PDP and SHAP methods to explain the model, T, Ia, and Fmax were determined as the feature parameters with the greatest impact on the model results. Generalization tests base on large seismic events not included in the training and test datasets indicated that the model has good generalization capabilities. © 2024 Elsevier Ltd;"Earthquake early warning; Interpretable machine learning; Response spectrum; XGBoost";"Continuous time systems; Errors; Forecasting; Large datasets; Machine learning; Seismic waves; Statistical tests; Earthquake early warning; Feature input; Interpretable machine learning; Machine-learning; Means square errors; Multifeatures; P-wave arrival; Response spectra; Structural response; Xgboost; acceleration; early warning system; machine learning; P-wave; prediction; response analysis; software; strong motion; structural response; Mean square error";"XGBoost-based prediction of on-site acceleration response spectra with multi-feature inputs from P-wave arrivals On-site earthquake early warning systems use the information from the initial P-wave to quickly estimate the degree of local damage. Existing on-site earthquake early warning models focus on ground motion information and generally ignore the degree of structural response, with many machine learning models characterized by poor interpretability. Therefore, an on-site prediction model based on XGBoost with a continuous time window and multi-feature inputs is proposed. The model uses the response spectrum value, which is closely related to the structural response, as the predicted intensity measure and uses strong motion data from Japanese K-NET stations for training. After using permutation importance to eliminate irrelevant input feature parameters, 11 parameters (Pa, Pd Pav, Pad, IAV, IV2, Ia, TP, Fmax, TW, T) are determined as the final model input parameters. The results based on a test dataset show that 3 s after P-wave arrival, the mean square error of the model prediction is 15.10*10−4 g. As the input time window is extended, the mean square error of the model prediction gradually decreases, and the overestimation of small values is mitigated. Additionally, 10 s after P-wave arrival, the mean square error decreases by 7.66*10−4 g compared with that at 1 s. After using the PDP and SHAP methods to explain the model, T, Ia, and Fmax were determined as the feature parameters with the greatest impact on the model results. Generalization tests base on large seismic events not included in the training and test datasets indicated that the model has good generalization capabilities. © 2024 Elsevier Ltd Earthquake early warning; Interpretable machine learning; Response spectrum; XGBoost Continuous time systems; Errors; Forecasting; Large datasets; Machine learning; Seismic waves; Statistical tests; Earthquake early warning; Feature input; Interpretable machine learning; Machine-learning; Means square errors; Multifeatures; P-wave arrival; Response spectra; Structural response; Xgboost; acceleration; early warning system; machine learning; P-wave; prediction; response analysis; software; strong motion; structural response; Mean square error";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
317;Optimizing landslide susceptibility mapping using machine learning and geospatial techniques;Landslides present a substantial risk to human lives, the environment, and infrastructure. Consequently, it is crucial to highlight the regions prone to future landslides by examining the correlation between past landslides and various geo-environmental factors. This study aims to investigate the optimal data selection and machine learning model, or ensemble technique, for evaluating the vulnerability of areas to landslides and determining the most accurate approach. To attain our objectives, we considered two different scenarios for selecting landslide-free random points (a slope threshold and a buffer-based approach) and performed a comparative analysis of five machine learning models for landslide susceptibility mapping, namely: Support Vector Machine (SVM), Logistic Regression (LR), Linear Discriminant Analysis (LDA), Random Forest (RF), and Extreme Gradient Boosting (XGBoost). The study area for this research is an area in Polk County in Western North Carolina that has experienced fatal landslides, leading to casualties and significant damage to infrastructure, properties, and road networks. The model construction process involves the utilization of a dataset comprising 1215 historical landslide occurrences and 1215 non-landslide points. We integrated a total of fourteen geospatial data layers, consisting of topographic variables, soil data, geological data, and land cover attributes. We use various metrics to assess the models' performance, including accuracy, F1-score, Kappa score, and AUC-ROC. In addition, we used the seeded-cell area index (SCAI) to evaluate map consistency. The ensemble of the five models using Weighted Average produces outstanding results, with an AUC-ROC of 99.4% for the slope threshold scenario and 91.8% for the buffer-based scenario. Our findings emphasize the significant impact of non-landslide random sampling on model performance in landslide susceptibility mapping. Furthermore, by optimally identifying landslide-prone regions and hotspots that need urgent risk management and land use planning, our study demonstrates the effectiveness of machine learning models in analyzing landslide susceptibility and providing valuable insights for informed decision-making and disaster risk reduction initiatives. © 2024;"Data driven; Landslide susceptibility; Machine learning; Natural disaster; Remote sensing";"North Carolina; Polk County [North Carolina]; United States; land cover; land use planning; landslide; machine learning; mapping method; natural disaster; optimization; remote sensing; vulnerability";"Optimizing landslide susceptibility mapping using machine learning and geospatial techniques Landslides present a substantial risk to human lives, the environment, and infrastructure. Consequently, it is crucial to highlight the regions prone to future landslides by examining the correlation between past landslides and various geo-environmental factors. This study aims to investigate the optimal data selection and machine learning model, or ensemble technique, for evaluating the vulnerability of areas to landslides and determining the most accurate approach. To attain our objectives, we considered two different scenarios for selecting landslide-free random points (a slope threshold and a buffer-based approach) and performed a comparative analysis of five machine learning models for landslide susceptibility mapping, namely: Support Vector Machine (SVM), Logistic Regression (LR), Linear Discriminant Analysis (LDA), Random Forest (RF), and Extreme Gradient Boosting (XGBoost). The study area for this research is an area in Polk County in Western North Carolina that has experienced fatal landslides, leading to casualties and significant damage to infrastructure, properties, and road networks. The model construction process involves the utilization of a dataset comprising 1215 historical landslide occurrences and 1215 non-landslide points. We integrated a total of fourteen geospatial data layers, consisting of topographic variables, soil data, geological data, and land cover attributes. We use various metrics to assess the models' performance, including accuracy, F1-score, Kappa score, and AUC-ROC. In addition, we used the seeded-cell area index (SCAI) to evaluate map consistency. The ensemble of the five models using Weighted Average produces outstanding results, with an AUC-ROC of 99.4% for the slope threshold scenario and 91.8% for the buffer-based scenario. Our findings emphasize the significant impact of non-landslide random sampling on model performance in landslide susceptibility mapping. Furthermore, by optimally identifying landslide-prone regions and hotspots that need urgent risk management and land use planning, our study demonstrates the effectiveness of machine learning models in analyzing landslide susceptibility and providing valuable insights for informed decision-making and disaster risk reduction initiatives. © 2024 Data driven; Landslide susceptibility; Machine learning; Natural disaster; Remote sensing North Carolina; Polk County [North Carolina]; United States; land cover; land use planning; landslide; machine learning; mapping method; natural disaster; optimization; remote sensing; vulnerability";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
318;Enhancing Flooding Depth Forecasting Accuracy in an Urban Area Using a Novel Trend Forecasting Method;Accurate flood runoff and water level predictions are crucial research topics due to their significance for early warning systems, particularly in improving peak flood level forecasts and reducing time lags. This study proposes a novel method, Trend Forecasting Method (TFM), to improve model accuracy and overcome the time lag problem due to data scarcity. The proposed method includes the following steps: (1) select appropriate input factors causing flood events, (2) determine the most suitable AI method as the basis for forecasting models, (3) a forecasting model using a multi-step-ahead approach and a forecasting model with variation in flood depth as input are developed as compared to the selected model in Step 2, and (4) according to the rising limb and falling limb of a flood hydrograph, the maximum and minimum values predicted by the models above are respectively selected as the final outputs. The application to demonstrate the advantages of the proposed method was conducted in the Annan District of Tainan City, Taiwan. Of all the models tested, the Gated Recurrent Unit (GRU) demonstrated superior accuracy in forecasting flood depths, followed by Long Short-Term Memory (LSTM) and Bidirectional LSTM, with the Back Propagation Neural Network falling behind. With a Nash–Sutcliffe efficiency coefficient (NSE) of 0.56 for the next hour’s forecast, the GRU model’s structure appears particularly fitting for flood depth forecast. However, all four models showed time lag issues. TFM substantially enhanced the GRU model’s forecast accuracy, mitigating the time lag. TFM achieved an NSE of 0.82 for forecasting 10-, 20-, 30-, 40-, 50-, and 60-min lead time. The observed flood depths had a 68% probability of consistent rise or fall, validating TFM’s underlying hypothesis. Furthermore, including an autoregressive model in TFM reduced the time lag problem. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"BiLSTM; BPNN; Flooding depth; GRU; LSTM";"Tainan; Taiwan; Backpropagation; Flood control; Long short-term memory; Water levels; Weather forecasting; BiLSTM; BPNN; Efficiency coefficient; Flooding depth; Floodings; Forecasting methods; Forecasting models; Gated recurrent unit; Time lag; Trend forecasting; accuracy assessment; artificial neural network; back propagation; depth; flood forecasting; trend analysis; urban area; Floods";"Enhancing Flooding Depth Forecasting Accuracy in an Urban Area Using a Novel Trend Forecasting Method Accurate flood runoff and water level predictions are crucial research topics due to their significance for early warning systems, particularly in improving peak flood level forecasts and reducing time lags. This study proposes a novel method, Trend Forecasting Method (TFM), to improve model accuracy and overcome the time lag problem due to data scarcity. The proposed method includes the following steps: (1) select appropriate input factors causing flood events, (2) determine the most suitable AI method as the basis for forecasting models, (3) a forecasting model using a multi-step-ahead approach and a forecasting model with variation in flood depth as input are developed as compared to the selected model in Step 2, and (4) according to the rising limb and falling limb of a flood hydrograph, the maximum and minimum values predicted by the models above are respectively selected as the final outputs. The application to demonstrate the advantages of the proposed method was conducted in the Annan District of Tainan City, Taiwan. Of all the models tested, the Gated Recurrent Unit (GRU) demonstrated superior accuracy in forecasting flood depths, followed by Long Short-Term Memory (LSTM) and Bidirectional LSTM, with the Back Propagation Neural Network falling behind. With a Nash–Sutcliffe efficiency coefficient (NSE) of 0.56 for the next hour’s forecast, the GRU model’s structure appears particularly fitting for flood depth forecast. However, all four models showed time lag issues. TFM substantially enhanced the GRU model’s forecast accuracy, mitigating the time lag. TFM achieved an NSE of 0.82 for forecasting 10-, 20-, 30-, 40-, 50-, and 60-min lead time. The observed flood depths had a 68% probability of consistent rise or fall, validating TFM’s underlying hypothesis. Furthermore, including an autoregressive model in TFM reduced the time lag problem. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. BiLSTM; BPNN; Flooding depth; GRU; LSTM Tainan; Taiwan; Backpropagation; Flood control; Long short-term memory; Water levels; Weather forecasting; BiLSTM; BPNN; Efficiency coefficient; Flooding depth; Floodings; Forecasting methods; Forecasting models; Gated recurrent unit; Time lag; Trend forecasting; accuracy assessment; artificial neural network; back propagation; depth; flood forecasting; trend analysis; urban area; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
319;Natural-Language-Processing-Enabled Quantitative Risk Analysis of Aerial Wildfire Operations;"Aerial wildfire operations are high risk and account for a large number of firefighter deaths. The increasing intensity of wildfires is driving a surge in aerial operations, as well as interest to improve system safety and performance. In this work, wildfire aviation mishaps documented using the Aviation Safety Communiqué (SAFECOM) system are analyzed using a previously developed framework for hazard extraction and analysis of trends. Hazards and specific failure modes are extracted from the narrative data in SAFECOM forms using natural language processing techniques. Metrics for each hazard (including the frequency, rate, and severity) are calculated. We examine whether these metrics change over time and whether they are related to metadata, such as region and aircraft type. The results of the hazard analysis are presented in a risk matrix, identifying the highest and lowest risk hazards based on the rate of occurrence and average severity. The analysis of all SAFECOM reports indicated that the jumper operations hazards were classified as high risk; whereas the hydraulic fluid malfunctions, bucket or tank failures, retardant loading and jettison failures, prescribed burn operations, cargo letdown failures, and severe weather were classified as serious risk. However, when applied to a specific operational scenario, risk levels change across hazards. © 2023 by the American Institute of Aeronautics and Astronautics, Inc.";"Aircraft Hazards; Aviation; Aviation Safety Reporting System; Disaster Response; Machine Learning; Natural Language Processing; Risk Analysis; Safety; Safety Management System";"Aircraft; Aircraft accidents; Antennas; Aviation; Fires; Hazards; Learning algorithms; Machine learning; Natural language processing systems; Risk assessment; Risk perception; Aircraft hazards; Aviation safety; Aviation safety reporting system; Disaster-response; Language processing; Machine-learning; Natural language processing; Natural languages; Reporting systems; Safety management systems; Risk analysis";"Natural-Language-Processing-Enabled Quantitative Risk Analysis of Aerial Wildfire Operations Aerial wildfire operations are high risk and account for a large number of firefighter deaths. The increasing intensity of wildfires is driving a surge in aerial operations, as well as interest to improve system safety and performance. In this work, wildfire aviation mishaps documented using the Aviation Safety Communiqué (SAFECOM) system are analyzed using a previously developed framework for hazard extraction and analysis of trends. Hazards and specific failure modes are extracted from the narrative data in SAFECOM forms using natural language processing techniques. Metrics for each hazard (including the frequency, rate, and severity) are calculated. We examine whether these metrics change over time and whether they are related to metadata, such as region and aircraft type. The results of the hazard analysis are presented in a risk matrix, identifying the highest and lowest risk hazards based on the rate of occurrence and average severity. The analysis of all SAFECOM reports indicated that the jumper operations hazards were classified as high risk; whereas the hydraulic fluid malfunctions, bucket or tank failures, retardant loading and jettison failures, prescribed burn operations, cargo letdown failures, and severe weather were classified as serious risk. However, when applied to a specific operational scenario, risk levels change across hazards. © 2023 by the American Institute of Aeronautics and Astronautics, Inc. Aircraft Hazards; Aviation; Aviation Safety Reporting System; Disaster Response; Machine Learning; Natural Language Processing; Risk Analysis; Safety; Safety Management System Aircraft; Aircraft accidents; Antennas; Aviation; Fires; Hazards; Learning algorithms; Machine learning; Natural language processing systems; Risk assessment; Risk perception; Aircraft hazards; Aviation safety; Aviation safety reporting system; Disaster-response; Language processing; Machine-learning; Natural language processing; Natural languages; Reporting systems; Safety management systems; Risk analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
320;Posthurricane damage assessment using satellite imagery and geolocation features;Gaining timely and reliable situation awareness after hazard events such as a hurricane is crucial to emergency managers and first responders. One effective way to achieve that goal is through damage assessment. Recently, disaster researchers have been utilizing imagery captured through satellites or drones to quantify the number of flooded/damaged buildings. In this paper, we propose a mixed-data approach, which leverages publicly available satellite imagery and geolocation features of the affected area to identify damaged buildings after a hurricane. The method demonstrated significant improvement from performing a similar task using only imagery features, based on a case study of Hurricane Harvey affecting Greater Houston area in 2017. This result opens door to a wide range of possibilities to unify the advancement in computer vision algorithms such as convolutional neural networks and traditional methods in damage assessment, for example, using flood depth or bare-earth topology. In this work, a creative choice of the geolocation features was made to provide extra information to the imagery features, but it is up to the users to decide which other features can be included to model the physical behavior of the events, depending on their domain knowledge and the type of disaster. The data set curated in this work is made openly available (DOI: 10.17603/ds2-3cca-f398). © 2023 Society for Risk Analysis.;"disaster damage assessment; emergency management; first response; hurricane; image classification; mixed data";"Houston; Texas; United States; Damage detection; Emergency services; Floods; Hurricanes; Image enhancement; Risk management; Satellite imagery; Affected area; Damage assessments; Disaster damage assessment; Emergency management; First responders; First response; Geolocations; Images classification; Mixed data; Situation awareness; computer vision; disaster management; flood; flood control; natural disaster; article; computer vision; convolutional neural network; disaster; emergency management; human; human experiment; physical model; satellite imagery; Image classification";"Posthurricane damage assessment using satellite imagery and geolocation features Gaining timely and reliable situation awareness after hazard events such as a hurricane is crucial to emergency managers and first responders. One effective way to achieve that goal is through damage assessment. Recently, disaster researchers have been utilizing imagery captured through satellites or drones to quantify the number of flooded/damaged buildings. In this paper, we propose a mixed-data approach, which leverages publicly available satellite imagery and geolocation features of the affected area to identify damaged buildings after a hurricane. The method demonstrated significant improvement from performing a similar task using only imagery features, based on a case study of Hurricane Harvey affecting Greater Houston area in 2017. This result opens door to a wide range of possibilities to unify the advancement in computer vision algorithms such as convolutional neural networks and traditional methods in damage assessment, for example, using flood depth or bare-earth topology. In this work, a creative choice of the geolocation features was made to provide extra information to the imagery features, but it is up to the users to decide which other features can be included to model the physical behavior of the events, depending on their domain knowledge and the type of disaster. The data set curated in this work is made openly available (DOI: 10.17603/ds2-3cca-f398). © 2023 Society for Risk Analysis. disaster damage assessment; emergency management; first response; hurricane; image classification; mixed data Houston; Texas; United States; Damage detection; Emergency services; Floods; Hurricanes; Image enhancement; Risk management; Satellite imagery; Affected area; Damage assessments; Disaster damage assessment; Emergency management; First responders; First response; Geolocations; Images classification; Mixed data; Situation awareness; computer vision; disaster management; flood; flood control; natural disaster; article; computer vision; convolutional neural network; disaster; emergency management; human; human experiment; physical model; satellite imagery; Image classification";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;3;Response
321;Intensity Prediction Model Based on Machine Learning for Regional Earthquake Early Warning;Seismic intensity plays a crucial role in influencing the decision-making process of users utilizing earthquake early warning (EEW) systems upon receiving warning information. Improving intensity warnings' speed and accuracy is vital. We present a straightforward and dependable model for predicting intensity, which is based only on location and magnitude information. We use the catalog of intensity data from the Japan Meteorological Agency (JMA) released as a dataset, totaling 944877 intensity instances. To address the issue of imbalanced dataset distribution, we employ the synthetic minority over-sampling technique (SMOTE) as a means to improve this situation. Considering the distribution of high-intensity data and the importance of features in the model, we construct and jointly apply intensity prediction models for magnitude below 5.7 and above 5.7, respectively. Further, we analyze the robustness of the model by adding random errors for magnitude and location information. We test the transfer capability of the proposed model with four earthquake events in China. Further, we use 466 seismic events (20 542 intensity instances) without published intensity data from the Kyoshin network (K-NET) as the application dataset. We simulate the phenomenon of underestimation of large earthquakes and overestimation of small earthquakes, which is used to analyze the possible application of the proposed model to EEWs. The findings indicate that the model achieves an accuracy of 97.77% when subjected to a magnitude error of 0.3 and a location error of 0.2°. Finally, we analyze the timeliness of the proposed model with a magnitude 7.4 event in 2022.  © 2023 IEEE.;"Earthquake early warning (EEW); earthquake engineering; earthquake intensity; machine learning; seismic signal processing";"Artificial intelligence; Decision making; Earthquake engineering; Forecasting; Learning algorithms; Learning systems; Location; Random errors; Signal processing; Adaptation models; Earthquake early warning; Earthquake intensity; Intensity data; Intensity prediction; Machine learning algorithms; Machine-learning; Prediction modelling; Predictive models; Seismic signal processing; Earthquakes";"Intensity Prediction Model Based on Machine Learning for Regional Earthquake Early Warning Seismic intensity plays a crucial role in influencing the decision-making process of users utilizing earthquake early warning (EEW) systems upon receiving warning information. Improving intensity warnings' speed and accuracy is vital. We present a straightforward and dependable model for predicting intensity, which is based only on location and magnitude information. We use the catalog of intensity data from the Japan Meteorological Agency (JMA) released as a dataset, totaling 944877 intensity instances. To address the issue of imbalanced dataset distribution, we employ the synthetic minority over-sampling technique (SMOTE) as a means to improve this situation. Considering the distribution of high-intensity data and the importance of features in the model, we construct and jointly apply intensity prediction models for magnitude below 5.7 and above 5.7, respectively. Further, we analyze the robustness of the model by adding random errors for magnitude and location information. We test the transfer capability of the proposed model with four earthquake events in China. Further, we use 466 seismic events (20 542 intensity instances) without published intensity data from the Kyoshin network (K-NET) as the application dataset. We simulate the phenomenon of underestimation of large earthquakes and overestimation of small earthquakes, which is used to analyze the possible application of the proposed model to EEWs. The findings indicate that the model achieves an accuracy of 97.77% when subjected to a magnitude error of 0.3 and a location error of 0.2°. Finally, we analyze the timeliness of the proposed model with a magnitude 7.4 event in 2022.  © 2023 IEEE. Earthquake early warning (EEW); earthquake engineering; earthquake intensity; machine learning; seismic signal processing Artificial intelligence; Decision making; Earthquake engineering; Forecasting; Learning algorithms; Learning systems; Location; Random errors; Signal processing; Adaptation models; Earthquake early warning; Earthquake intensity; Intensity data; Intensity prediction; Machine learning algorithms; Machine-learning; Prediction modelling; Predictive models; Seismic signal processing; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
322;Evaluation of safe-to-fail flood solution alternatives and integration of safe-to-fail concept in AEC education to develop resilient coastal cities;"Purpose: Safe-to-fail (SF) is an emerging resilient design approach that has the potential to minimize the severity of flood damages. The purpose of this study is to explore the SF design strategies to reduce flood disaster damages in US coastal cities. Therefore, this study addresses two research questions: identifying the most suitable SF criteria and flood solution alternatives for coastal cities from industry professionals’ perspective; and investigating the controlling factors that influence the AEC students’ interest to learn about SF concepts through the curricula. Design/methodology/approach: This study used the analytical hierarchy process to evaluate the SF criteria and flood solutions where data were collected through surveying 29 Department of Transportation professionals from different states. In addition, the study adopted a quantitative methodology by surveying 55 versed participants who reside in a coastal area and have coastal flood experiences. The data analysis included ordinal probit regression and descriptive analysis. Findings: The results suggest that robustness is the highest weighted criterion for implementing SF design in coastal cities. The results demonstrated that ecosystem restoration is the highest-ranked SF flood solution followed by green infrastructure. Moreover, the results highlighted that age, duration spent in the program and prior knowledge of SF are significantly related to AEC students’ interest to learn this concept. Originality/value: SF design anticipates failures while designing infrastructures thus minimizing failure consequences due to flood disasters. The findings can facilitate the implementation of the SF design concept during the construction of new infrastructures in coastal cities as well as educate the future workforces to contribute to developing resilient built environments. © 2022, Emerald Publishing Limited.";"Disaster education; Disaster management; Disaster resilience; Flood mitigation; Resilient infrastructure; Safe-to-fail";"United States; analytical hierarchy process; design; disaster management; disaster relief; education; flood control; mitigation; surveying";"Evaluation of safe-to-fail flood solution alternatives and integration of safe-to-fail concept in AEC education to develop resilient coastal cities Purpose: Safe-to-fail (SF) is an emerging resilient design approach that has the potential to minimize the severity of flood damages. The purpose of this study is to explore the SF design strategies to reduce flood disaster damages in US coastal cities. Therefore, this study addresses two research questions: identifying the most suitable SF criteria and flood solution alternatives for coastal cities from industry professionals’ perspective; and investigating the controlling factors that influence the AEC students’ interest to learn about SF concepts through the curricula. Design/methodology/approach: This study used the analytical hierarchy process to evaluate the SF criteria and flood solutions where data were collected through surveying 29 Department of Transportation professionals from different states. In addition, the study adopted a quantitative methodology by surveying 55 versed participants who reside in a coastal area and have coastal flood experiences. The data analysis included ordinal probit regression and descriptive analysis. Findings: The results suggest that robustness is the highest weighted criterion for implementing SF design in coastal cities. The results demonstrated that ecosystem restoration is the highest-ranked SF flood solution followed by green infrastructure. Moreover, the results highlighted that age, duration spent in the program and prior knowledge of SF are significantly related to AEC students’ interest to learn this concept. Originality/value: SF design anticipates failures while designing infrastructures thus minimizing failure consequences due to flood disasters. The findings can facilitate the implementation of the SF design concept during the construction of new infrastructures in coastal cities as well as educate the future workforces to contribute to developing resilient built environments. © 2022, Emerald Publishing Limited. Disaster education; Disaster management; Disaster resilience; Flood mitigation; Resilient infrastructure; Safe-to-fail United States; analytical hierarchy process; design; disaster management; disaster relief; education; flood control; mitigation; surveying";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
323;Building a Vision Transformer-Based Damage Severity Classifier with Ground-Level Imagery of Homes Affected by California Wildfires;The increase in both the frequency and magnitude of natural disasters, coupled with recent advancements in artificial intelligence, has introduced prospects for investigating the potential of new technologies to facilitate disaster response processes. Preliminary Damage Assessment (PDA), a labor-intensive procedure necessitating manual examination of residential structures to ascertain post-disaster damage severity, stands to significantly benefit from the integration of computer vision-based classification algorithms, promising efficiency gains and heightened accuracy. Our paper proposes a Vision Transformer (ViT)-based model for classifying damage severity, achieving an accuracy rate of 95%. Notably, our model, trained on a repository of over 18,000 ground-level images of homes with damage severity annotated by damage assessment professionals during the 2020–2022 California wildfires, represents a novel application of ViT technology within this domain. Furthermore, we have open sourced this dataset—the first of its kind and scale—to be used by the research community. Additionally, we have developed a publicly accessible web application prototype built on this classification algorithm, which we have demonstrated to disaster management practitioners and received feedback on. Hence, our contribution to the literature encompasses the provision of a novel imagery dataset, an applied framework from field professionals, and a damage severity classification model with high accuracy. © 2024 by the authors.;"computer vision; damage assessment; damage classification; wildfire damage";NULL;"Building a Vision Transformer-Based Damage Severity Classifier with Ground-Level Imagery of Homes Affected by California Wildfires The increase in both the frequency and magnitude of natural disasters, coupled with recent advancements in artificial intelligence, has introduced prospects for investigating the potential of new technologies to facilitate disaster response processes. Preliminary Damage Assessment (PDA), a labor-intensive procedure necessitating manual examination of residential structures to ascertain post-disaster damage severity, stands to significantly benefit from the integration of computer vision-based classification algorithms, promising efficiency gains and heightened accuracy. Our paper proposes a Vision Transformer (ViT)-based model for classifying damage severity, achieving an accuracy rate of 95%. Notably, our model, trained on a repository of over 18,000 ground-level images of homes with damage severity annotated by damage assessment professionals during the 2020–2022 California wildfires, represents a novel application of ViT technology within this domain. Furthermore, we have open sourced this dataset—the first of its kind and scale—to be used by the research community. Additionally, we have developed a publicly accessible web application prototype built on this classification algorithm, which we have demonstrated to disaster management practitioners and received feedback on. Hence, our contribution to the literature encompasses the provision of a novel imagery dataset, an applied framework from field professionals, and a damage severity classification model with high accuracy. © 2024 by the authors. computer vision; damage assessment; damage classification; wildfire damage NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;3;Response
324;Multivariate Hydrological Modeling Based on Long Short-Term Memory Networks for Water Level Forecasting †;In the Department of Chocó, flooding poses a recurrent and significant challenge due to heavy rainfall and the dense network of rivers characterizing the region. However, the lack of adequate infrastructure to prevent and predict floods exacerbates this situation. The absence of early warning systems, the scarcity of meteorological and hydrological monitoring stations, and deficiencies in urban planning contribute to the vulnerability of communities to these phenomena. It is imperative to invest in flood prediction and prevention infrastructure, including advanced monitoring systems, the development of hydrological prediction models, and the construction of hydraulic infrastructure, to reduce risk and protect vulnerable communities in Chocó. Additionally, raising public awareness of the associated risks and encouraging the adoption of mitigation and preparedness measures throughout the population are essential. This study introduces a novel approach for the multivariate prediction of hydrological variables, specifically focusing on water level forecasts for two hydrological stations along the Atrato River in Colombia. The model, utilizing a specialized type of recurrent neural network (RNN) called the long short-term memory (LSTM) network, integrates data from hydrological variables, such as the flow, precipitation, and level. With a model architecture featuring four inputs and two outputs, where flow and precipitation serve as inputs and the level serves as the output for each station, the LSTM model is adept at capturing the complex dynamics and cross-correlations among these variables. Validation involves comparing the LSTM model’s performance with linear and nonlinear Autoregressive with Exogenous Input (NARX) models, considering factors such as the estimation error and computational time. Furthermore, this study explores different scenarios for water level prediction, aiming to utilize the proposed approach as an effective flood early warning system. © 2024 by the authors.;"data analysis; flood risk assessment; nonlinear autoregressive model; prediction; recurrent neural network; time-series analysis";"Brain; Flood control; Floods; Forecasting; Multivariant analysis; Rain; Risk assessment; Time series analysis; Water levels; Early Warning System; Flood risk assessments; Hydrological models; Hydrological variables; Memory modeling; Memory network; Model-based OPC; Nonlinear autoregressive model; Time-series analysis; Water level forecasting; Long short-term memory";"Multivariate Hydrological Modeling Based on Long Short-Term Memory Networks for Water Level Forecasting † In the Department of Chocó, flooding poses a recurrent and significant challenge due to heavy rainfall and the dense network of rivers characterizing the region. However, the lack of adequate infrastructure to prevent and predict floods exacerbates this situation. The absence of early warning systems, the scarcity of meteorological and hydrological monitoring stations, and deficiencies in urban planning contribute to the vulnerability of communities to these phenomena. It is imperative to invest in flood prediction and prevention infrastructure, including advanced monitoring systems, the development of hydrological prediction models, and the construction of hydraulic infrastructure, to reduce risk and protect vulnerable communities in Chocó. Additionally, raising public awareness of the associated risks and encouraging the adoption of mitigation and preparedness measures throughout the population are essential. This study introduces a novel approach for the multivariate prediction of hydrological variables, specifically focusing on water level forecasts for two hydrological stations along the Atrato River in Colombia. The model, utilizing a specialized type of recurrent neural network (RNN) called the long short-term memory (LSTM) network, integrates data from hydrological variables, such as the flow, precipitation, and level. With a model architecture featuring four inputs and two outputs, where flow and precipitation serve as inputs and the level serves as the output for each station, the LSTM model is adept at capturing the complex dynamics and cross-correlations among these variables. Validation involves comparing the LSTM model’s performance with linear and nonlinear Autoregressive with Exogenous Input (NARX) models, considering factors such as the estimation error and computational time. Furthermore, this study explores different scenarios for water level prediction, aiming to utilize the proposed approach as an effective flood early warning system. © 2024 by the authors. data analysis; flood risk assessment; nonlinear autoregressive model; prediction; recurrent neural network; time-series analysis Brain; Flood control; Floods; Forecasting; Multivariant analysis; Rain; Risk assessment; Time series analysis; Water levels; Early Warning System; Flood risk assessments; Hydrological models; Hydrological variables; Memory modeling; Memory network; Model-based OPC; Nonlinear autoregressive model; Time-series analysis; Water level forecasting; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
325;A Novel Coupled Model for Monthly Rainfall Prediction Based on ESMD-EWT-SVD-LSTM;"Precise predicting of rainfall is paramount for effective water resource management, ecological conservation, and the prevention of droughts and floods. Influenced by numerous variables, the process of rainfall is complex and the rainfall series exhibit high degrees of nonlinearity, making it challenging for traditional statistical prediction models to accurately capture the characteristics of rainfall series. Therefore, this paper proposes a new coupled model for predicting monthly rainfall based on Extreme-Point Symmetric Mode Decomposition (ESMD), Empirical Wavelet Transform (EWT), Singular Value Decomposition (SVD) and Long Short-Term Memory Neural Network (LSTM). By training and evaluating the ESMD-EWT-SVD-LSTM model on Kaifeng City’s monthly rainfall data from 2009 to 2020 and comparing its predictions with those of the ESMD-SVD-LSTM, SVD-LSTM, LSTM models, the analysis reveals that: the quadratic decomposition of ESMD-EWT and SVD denoising can further reduce the complexity of rainfall data, obtain more predictable feature IMFs, and enhance the precision in LSTM predicting; in comparison with alternative models, the ESMD-EWT-SVD-LSTM coupled model shows the highest accuracy in predicting results, with MAE of 4.96, RMSE of 6.13, and SI of 0.12, indicating that the ESMD-EWT-SVD-LSTM model has strong nonlinear process learning ability and accuracy in regional monthly rainfall prediction. This study can offer dependable scientific grounding and technical assistance for regional rainfall predicting, water resources planning, and disaster mitigation. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.";"Empirical wavelet transform; Extreme-point symmetric mode decomposition; Long short-term memory neural network; Rainfall prediction; Singular value decomposition";"China; Henan; Kaifeng; Brain; Complex networks; Forecasting; Long short-term memory; Rain; Singular value decomposition; Water management; Wavelet decomposition; Empirical wavelet transform; Extreme points; Extreme-point symmetric mode decomposition; Long short-term memory neural network; Mode decomposition; Monthly rainfalls; Neural-networks; Rainfall prediction; Symmetric mode; Wavelets transform; climate prediction; complexity; disaster management; drought; flood frequency; nonlinearity; precision; rainfall; temporal variation; water management; water planning; water resource; wavelet analysis; weather forecasting; Mode decomposition";"A Novel Coupled Model for Monthly Rainfall Prediction Based on ESMD-EWT-SVD-LSTM Precise predicting of rainfall is paramount for effective water resource management, ecological conservation, and the prevention of droughts and floods. Influenced by numerous variables, the process of rainfall is complex and the rainfall series exhibit high degrees of nonlinearity, making it challenging for traditional statistical prediction models to accurately capture the characteristics of rainfall series. Therefore, this paper proposes a new coupled model for predicting monthly rainfall based on Extreme-Point Symmetric Mode Decomposition (ESMD), Empirical Wavelet Transform (EWT), Singular Value Decomposition (SVD) and Long Short-Term Memory Neural Network (LSTM). By training and evaluating the ESMD-EWT-SVD-LSTM model on Kaifeng City’s monthly rainfall data from 2009 to 2020 and comparing its predictions with those of the ESMD-SVD-LSTM, SVD-LSTM, LSTM models, the analysis reveals that: the quadratic decomposition of ESMD-EWT and SVD denoising can further reduce the complexity of rainfall data, obtain more predictable feature IMFs, and enhance the precision in LSTM predicting; in comparison with alternative models, the ESMD-EWT-SVD-LSTM coupled model shows the highest accuracy in predicting results, with MAE of 4.96, RMSE of 6.13, and SI of 0.12, indicating that the ESMD-EWT-SVD-LSTM model has strong nonlinear process learning ability and accuracy in regional monthly rainfall prediction. This study can offer dependable scientific grounding and technical assistance for regional rainfall predicting, water resources planning, and disaster mitigation. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. Empirical wavelet transform; Extreme-point symmetric mode decomposition; Long short-term memory neural network; Rainfall prediction; Singular value decomposition China; Henan; Kaifeng; Brain; Complex networks; Forecasting; Long short-term memory; Rain; Singular value decomposition; Water management; Wavelet decomposition; Empirical wavelet transform; Extreme points; Extreme-point symmetric mode decomposition; Long short-term memory neural network; Mode decomposition; Monthly rainfalls; Neural-networks; Rainfall prediction; Symmetric mode; Wavelets transform; climate prediction; complexity; disaster management; drought; flood frequency; nonlinearity; precision; rainfall; temporal variation; water management; water planning; water resource; wavelet analysis; weather forecasting; Mode decomposition";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
326;Enhanced Lightweight YOLOX for Small Object Wildfire Detection in UAV Imagery;Target detection technology based on unmanned aerial vehicle (UAV)-derived aerial imagery has been widely applied in the field of forest fire patrol and rescue. However, due to the specificity of UAV platforms, there are still significant issues to be resolved such as severe omission, low detection accuracy, and poor early warning effectiveness. In light of these issues, this paper proposes an improved YOLOX network for the rapid detection of forest fires in images captured by UAVs. Firstly, to enhance the network’s feature-extraction capability in complex fire environments, a multi-level-feature-extraction structure, CSP-ML, is designed to improve the algorithm’s detection accuracy for small-target fire areas. Additionally, a CBAM attention mechanism is embedded in the neck network to reduce interference caused by background noise and irrelevant information. Secondly, an adaptive-feature-extraction module is introduced in the YOLOX network’s feature fusion part to prevent the loss of important feature information during the fusion process, thus enhancing the network’s feature-learning capability. Lastly, the CIoU loss function is used to replace the original loss function, to address issues such as excessive optimization of negative samples and poor gradient-descent direction, thereby strengthening the network’s effective recognition of positive samples. Experimental results show that the improved YOLOX network has better detection performance, with mAP@50 and mAP@50_95 increasing by 6.4% and 2.17%, respectively, compared to the traditional YOLOX network. In multi-target flame and small-target flame scenarios, the improved YOLO model achieved a mAP of 96.3%, outperforming deep learning algorithms such as FasterRCNN, SSD, and YOLOv5 by 33.5%, 7.7%, and 7%, respectively. It has a lower omission rate and higher detection accuracy, and it is capable of handling small-target detection tasks in complex fire environments. This can provide support for UAV patrol and rescue applications from a high-altitude perspective. © 2024 by the authors.;"CSP-ML; small-target detection; UAV; wildfire detection; YOLOX";"Aerial photography; Aircraft detection; Antennas; Complex networks; Deep learning; Deforestation; Extraction; Fire hazards; Fires; Gradient methods; Learning algorithms; Military applications; Military vehicles; Object detection; Unmanned aerial vehicles (UAV); Aerial vehicle; CSP-ML; Detection accuracy; Features extraction; Fire environment; Forest fires; Small target detection; Unmanned aerial vehicle; Wildfire detection; YOLOX; adult; algorithm; altitude; article; background noise; controlled study; deep learning; diagnosis; diagnostic test accuracy study; feature extraction; flame; human; imagery; learning algorithm; unmanned aerial vehicle; wildfire; Feature extraction";"Enhanced Lightweight YOLOX for Small Object Wildfire Detection in UAV Imagery Target detection technology based on unmanned aerial vehicle (UAV)-derived aerial imagery has been widely applied in the field of forest fire patrol and rescue. However, due to the specificity of UAV platforms, there are still significant issues to be resolved such as severe omission, low detection accuracy, and poor early warning effectiveness. In light of these issues, this paper proposes an improved YOLOX network for the rapid detection of forest fires in images captured by UAVs. Firstly, to enhance the network’s feature-extraction capability in complex fire environments, a multi-level-feature-extraction structure, CSP-ML, is designed to improve the algorithm’s detection accuracy for small-target fire areas. Additionally, a CBAM attention mechanism is embedded in the neck network to reduce interference caused by background noise and irrelevant information. Secondly, an adaptive-feature-extraction module is introduced in the YOLOX network’s feature fusion part to prevent the loss of important feature information during the fusion process, thus enhancing the network’s feature-learning capability. Lastly, the CIoU loss function is used to replace the original loss function, to address issues such as excessive optimization of negative samples and poor gradient-descent direction, thereby strengthening the network’s effective recognition of positive samples. Experimental results show that the improved YOLOX network has better detection performance, with mAP@50 and mAP@50_95 increasing by 6.4% and 2.17%, respectively, compared to the traditional YOLOX network. In multi-target flame and small-target flame scenarios, the improved YOLO model achieved a mAP of 96.3%, outperforming deep learning algorithms such as FasterRCNN, SSD, and YOLOv5 by 33.5%, 7.7%, and 7%, respectively. It has a lower omission rate and higher detection accuracy, and it is capable of handling small-target detection tasks in complex fire environments. This can provide support for UAV patrol and rescue applications from a high-altitude perspective. © 2024 by the authors. CSP-ML; small-target detection; UAV; wildfire detection; YOLOX Aerial photography; Aircraft detection; Antennas; Complex networks; Deep learning; Deforestation; Extraction; Fire hazards; Fires; Gradient methods; Learning algorithms; Military applications; Military vehicles; Object detection; Unmanned aerial vehicles (UAV); Aerial vehicle; CSP-ML; Detection accuracy; Features extraction; Fire environment; Forest fires; Small target detection; Unmanned aerial vehicle; Wildfire detection; YOLOX; adult; algorithm; altitude; article; background noise; controlled study; deep learning; diagnosis; diagnostic test accuracy study; feature extraction; flame; human; imagery; learning algorithm; unmanned aerial vehicle; wildfire; Feature extraction";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
327;A dual Kriging-XGBoost model for reconstructing building seismic responses using strong motion data;Structural response reconstruction (SRR) and prediction modeling is an area of growing interest in earthquake engineering research. Within the post-earthquake environment, SRR models are useful for estimating seismic demands in uninstrumented buildings using the response measurements from those that are equipped with strong motion sensors. This paper introduces a dual model that uses kriging combined with the extreme gradient boosting (XGBoost) algorithm to reconstruct seismic response demands in buildings during real earthquakes. The model is constructed with data that consists of responses from 207 buildings and 35 earthquakes. Kriging is first used to predict peak ground accelerations (PGAs) at the location of the uninstrumented buildings, using the measured PGAs from instrumented building sites with similar features (e.g., event magnitude, source-to-site distance, and location). The XGBoost algorithm is then used, with PGA as one of several features, to reconstruct the maximum (over the building height) peak (over the response history) story drift ratio and peak floor acceleration in the uninstrumented buildings. The residuals are then examined to assess overall model performance. © The Author(s), under exclusive licence to Springer Nature B.V. 2023.;"Earthquake; Gradient boosting models; Gradient descent; Machine learning; Modeling; Structural response reconstruction";"Acceleration; Adaptive boosting; Buildings; Earthquake engineering; Earthquakes; Gradient methods; Machine learning; Seismic response; Gradient boosting; Gradient boosting model; Gradient-descent; Kriging; Machine-learning; Modeling; Peak ground acceleration; Response reconstruction; Structural response; Structural response reconstruction; algorithm; architectural design; earthquake engineering; kriging; machine learning; numerical model; peak acceleration; prediction; reconstruction; seismic response; strong motion; Kriging";"A dual Kriging-XGBoost model for reconstructing building seismic responses using strong motion data Structural response reconstruction (SRR) and prediction modeling is an area of growing interest in earthquake engineering research. Within the post-earthquake environment, SRR models are useful for estimating seismic demands in uninstrumented buildings using the response measurements from those that are equipped with strong motion sensors. This paper introduces a dual model that uses kriging combined with the extreme gradient boosting (XGBoost) algorithm to reconstruct seismic response demands in buildings during real earthquakes. The model is constructed with data that consists of responses from 207 buildings and 35 earthquakes. Kriging is first used to predict peak ground accelerations (PGAs) at the location of the uninstrumented buildings, using the measured PGAs from instrumented building sites with similar features (e.g., event magnitude, source-to-site distance, and location). The XGBoost algorithm is then used, with PGA as one of several features, to reconstruct the maximum (over the building height) peak (over the response history) story drift ratio and peak floor acceleration in the uninstrumented buildings. The residuals are then examined to assess overall model performance. © The Author(s), under exclusive licence to Springer Nature B.V. 2023. Earthquake; Gradient boosting models; Gradient descent; Machine learning; Modeling; Structural response reconstruction Acceleration; Adaptive boosting; Buildings; Earthquake engineering; Earthquakes; Gradient methods; Machine learning; Seismic response; Gradient boosting; Gradient boosting model; Gradient-descent; Kriging; Machine-learning; Modeling; Peak ground acceleration; Response reconstruction; Structural response; Structural response reconstruction; algorithm; architectural design; earthquake engineering; kriging; machine learning; numerical model; peak acceleration; prediction; reconstruction; seismic response; strong motion; Kriging";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
328;Enhancing wildfire mapping accuracy using mono-temporal Sentinel-2 data: A novel approach through qualitative and quantitative feature selection with explainable AI;Accurate wildfire severity mapping (WSM) is crucial in environmental damage assessment and recovery strategies. Machine learning (ML) and remote sensing technologies are extensively integrated and employed as powerful tools for WSM. However, the intricate nature of ML algorithms often leads to ‘black box’ systems, obscuring the decision-making process and significantly limiting stakeholders' ability to comprehend the basis of predictions. This opacity hinders efforts to enhance performance and risks exacerbating overfitting. This present study proposes an innovative WSM approach that incorporates qualitative and quantitative feature selection techniques within the Explainable AI (XAI) framework. The methodology aims to enhance the precision of WSM and provide insights into the factors contributing to model decisions, thereby increasing the interpretability of predictions and streamlining models to improve performance. To achieve this objective, we employed the SHapley Additive exPlanations (SHAP)-Forward Stepwise Selection (FSS) method to demonstrate its efficacy in elucidating the qualitative and quantitative impacts of predictors on ML algorithm performance, accuracy, and interpretability designed for WSM. Utilizing post-fire imagery from Sentinel-2 (S2), we analyzed ten bands to generate 225 unique spectral indices utilizing five different calculations: normalized, algebraic sum, difference, ratio, and product forms. Combined with the original S2 bands, this resulted in 235 potential predictors for ML classifications. A random forest model was subsequently developed using these predictors and optimized through extensive hyperparameter tuning, achieving an overall accuracy (OA) of 0.917 and a Kappa statistic of 0.896. The most influential predictors were identified using SHAP values, with an FSS process narrowing them down to the 12 most critical for effective WSM, as evidenced by stabilized OA and Kappa values (0.904 and 0.881, respectively). Further validation using a ninefold spatial cross-validation technique demonstrated the method's consistent performance across different data partitions, with OA values ranging from 0.705 to 0.894 and Kappa values from 0.607 to 0.867. By providing a more accurate and comprehensible XAI-based method for WSM, this research contributes to the broader field of environmental monitoring and disaster response, underscoring the potential of integrated qualitative and quantitative analysis to enhance ML models' capabilities. © 2024 The Author(s);"Forward stepwise selection; Machine learning; Sentinel-2; SHAP; Wildfire severity mapping";"accuracy assessment; algorithm; environmental monitoring; machine learning; mapping method; precision agriculture; qualitative analysis; quantitative analysis; remote sensing; satellite data; satellite imagery; Sentinel; stakeholder; wildfire";"Enhancing wildfire mapping accuracy using mono-temporal Sentinel-2 data: A novel approach through qualitative and quantitative feature selection with explainable AI Accurate wildfire severity mapping (WSM) is crucial in environmental damage assessment and recovery strategies. Machine learning (ML) and remote sensing technologies are extensively integrated and employed as powerful tools for WSM. However, the intricate nature of ML algorithms often leads to ‘black box’ systems, obscuring the decision-making process and significantly limiting stakeholders' ability to comprehend the basis of predictions. This opacity hinders efforts to enhance performance and risks exacerbating overfitting. This present study proposes an innovative WSM approach that incorporates qualitative and quantitative feature selection techniques within the Explainable AI (XAI) framework. The methodology aims to enhance the precision of WSM and provide insights into the factors contributing to model decisions, thereby increasing the interpretability of predictions and streamlining models to improve performance. To achieve this objective, we employed the SHapley Additive exPlanations (SHAP)-Forward Stepwise Selection (FSS) method to demonstrate its efficacy in elucidating the qualitative and quantitative impacts of predictors on ML algorithm performance, accuracy, and interpretability designed for WSM. Utilizing post-fire imagery from Sentinel-2 (S2), we analyzed ten bands to generate 225 unique spectral indices utilizing five different calculations: normalized, algebraic sum, difference, ratio, and product forms. Combined with the original S2 bands, this resulted in 235 potential predictors for ML classifications. A random forest model was subsequently developed using these predictors and optimized through extensive hyperparameter tuning, achieving an overall accuracy (OA) of 0.917 and a Kappa statistic of 0.896. The most influential predictors were identified using SHAP values, with an FSS process narrowing them down to the 12 most critical for effective WSM, as evidenced by stabilized OA and Kappa values (0.904 and 0.881, respectively). Further validation using a ninefold spatial cross-validation technique demonstrated the method's consistent performance across different data partitions, with OA values ranging from 0.705 to 0.894 and Kappa values from 0.607 to 0.867. By providing a more accurate and comprehensible XAI-based method for WSM, this research contributes to the broader field of environmental monitoring and disaster response, underscoring the potential of integrated qualitative and quantitative analysis to enhance ML models' capabilities. © 2024 The Author(s) Forward stepwise selection; Machine learning; Sentinel-2; SHAP; Wildfire severity mapping accuracy assessment; algorithm; environmental monitoring; machine learning; mapping method; precision agriculture; qualitative analysis; quantitative analysis; remote sensing; satellite data; satellite imagery; Sentinel; stakeholder; wildfire";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;4;Recovery
329;Forecasting Convective Storms Trajectory and Intensity by Neural Networks;Convective storms represent a dangerous atmospheric phenomenon, particularly for the heavy and concentrated precipitation they can trigger. Given their high velocity and variability, their prediction is challenging, though it is crucial to issue reliable alarms. The paper presents a neural network approach to forecast the convective cell trajectory and intensity, using, as an example, a region in northern Italy that is frequently hit by convective storms in spring and summer. The predictor input is constituted by radar-derived information about the center of gravity of the cell, its reflectivity (a proxy for the intensity of the precipitation), and the area affected by the storm. The essential characteristic of the proposed approach is that the neural network directly forecasts the evolution of the convective cell position and of the other features for the following hour at a 5-min temporal resolution without a relevant loss of accuracy in comparison to predictors trained for each specific variable at a particular time step. Besides its accuracy ((Formula presented.) of the position is about 0.80 one hour in advance), this machine learning approach has clear advantages over the classical numerical weather predictors since it runs at orders of magnitude more rapidly, thus allowing for the implementation of a real-time early-warning system. © 2024 by the authors.;"convective cells; multi-output neural network; radar reflectivity; storm positioning; weather forecasting";NULL;"Forecasting Convective Storms Trajectory and Intensity by Neural Networks Convective storms represent a dangerous atmospheric phenomenon, particularly for the heavy and concentrated precipitation they can trigger. Given their high velocity and variability, their prediction is challenging, though it is crucial to issue reliable alarms. The paper presents a neural network approach to forecast the convective cell trajectory and intensity, using, as an example, a region in northern Italy that is frequently hit by convective storms in spring and summer. The predictor input is constituted by radar-derived information about the center of gravity of the cell, its reflectivity (a proxy for the intensity of the precipitation), and the area affected by the storm. The essential characteristic of the proposed approach is that the neural network directly forecasts the evolution of the convective cell position and of the other features for the following hour at a 5-min temporal resolution without a relevant loss of accuracy in comparison to predictors trained for each specific variable at a particular time step. Besides its accuracy ((Formula presented.) of the position is about 0.80 one hour in advance), this machine learning approach has clear advantages over the classical numerical weather predictors since it runs at orders of magnitude more rapidly, thus allowing for the implementation of a real-time early-warning system. © 2024 by the authors. convective cells; multi-output neural network; radar reflectivity; storm positioning; weather forecasting NULL";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;2;Preparation
330;Threshold and real-time initiation mechanism of urban flood emergency response under combined disaster scenarios;Scientific and reasonable emergency response initiation mechanisms can provide important support for decision making regarding the emergency management of urban floods. However, there is a lack of a unified paradigm on how to calculate the threshold for emergency response initiation and reasonably initiate emergency response. Therefore, this study proposes a loss-driven urban flood emergency response initiation framework from the perspective of combined disasters. A discrimination mechanism of the emergency response initiation level was established based on the optimal threshold and loss function. And the rainfall event that occurred in Zhengzhou, China, on July 20, 2021, was taken as an example to realize real-time emergency response discrimination and initiation driven by forecast data. Results showed that the initiation time of the Level I emergency response using the proposed method was 9.5 h earlier than the time of the government release, thereby significantly increasing the preparation time for flood management personnel. In addition, the results of the optimal threshold selection indicated that the Natural Breakpoint method was the optimal method for loss threshold partitioning, with the comprehensive evaluation index (CEI) being 3.56–9.53 % higher than those of the K-means, Equal Interval, and Quantile method. These results constitute a reference for urban emergency management and related research. © 2024 Elsevier Ltd;"Combined disaster; Emergency response; Real-time initiation; Threshold; Urban flood";"China; Henan; Zhengzhou; Civil defense; Emergency services; Flood control; Floods; Human resource management; K-means clustering; Risk management; Combined disaster; Disaster scenario; Emergency management; Emergency response; Initiation mechanism; Optimal threshold; Real- time; Real-time initiation; Threshold; Urban floods; decision making; disaster management; flooding; precipitation intensity; real time; threshold; Decision making";"Threshold and real-time initiation mechanism of urban flood emergency response under combined disaster scenarios Scientific and reasonable emergency response initiation mechanisms can provide important support for decision making regarding the emergency management of urban floods. However, there is a lack of a unified paradigm on how to calculate the threshold for emergency response initiation and reasonably initiate emergency response. Therefore, this study proposes a loss-driven urban flood emergency response initiation framework from the perspective of combined disasters. A discrimination mechanism of the emergency response initiation level was established based on the optimal threshold and loss function. And the rainfall event that occurred in Zhengzhou, China, on July 20, 2021, was taken as an example to realize real-time emergency response discrimination and initiation driven by forecast data. Results showed that the initiation time of the Level I emergency response using the proposed method was 9.5 h earlier than the time of the government release, thereby significantly increasing the preparation time for flood management personnel. In addition, the results of the optimal threshold selection indicated that the Natural Breakpoint method was the optimal method for loss threshold partitioning, with the comprehensive evaluation index (CEI) being 3.56–9.53 % higher than those of the K-means, Equal Interval, and Quantile method. These results constitute a reference for urban emergency management and related research. © 2024 Elsevier Ltd Combined disaster; Emergency response; Real-time initiation; Threshold; Urban flood China; Henan; Zhengzhou; Civil defense; Emergency services; Flood control; Floods; Human resource management; K-means clustering; Risk management; Combined disaster; Disaster scenario; Emergency management; Emergency response; Initiation mechanism; Optimal threshold; Real- time; Real-time initiation; Threshold; Urban floods; decision making; disaster management; flooding; precipitation intensity; real time; threshold; Decision making";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.2;Hydrological;2;Preparation
331;Analyzing Gas Station Visits during Hurricane Ida: Implications for Future Fuel Supply;Fuel demand surges and supply shortages during hurricanes are problematic. However, fuel consumption and refueling behavior are not well discussed in disaster scenarios compared with the extant literature on these topics during normal conditions. This study used an emerging data source to report when fuel demand (indicated by gas station visits) deviated from a normal variation range during Hurricane Ida (a Category 4 Atlantic hurricane in 2021), how long the deviation lasted, and which area(s) experienced greater deviations. Gas station visits are likely to surge within 2 days before storm landfall, while evacuation destinations and intermediate trip connectors had longer surges. Using univariate linear regression, this study also statistically explored what factors might affect the fuel demand deviation at an aggregate level. The testing results revealed that zones of higher vehicle ownership, more daily commuters driving alone, lower residential stability, more mobile homes, and less storm impacts tended to have more gas station visits (i.e., greater fuel demand). Then, this study analyzed data at a more disaggregate level to identify the characteristics of gas stations that are more likely to have more visitors. Gas stations near interstates/highways attracted more visits, especially within 2 days before storm landfall. Gas stations farther away from interstates/highways attracted more visits after storm landfall. Overall, this study contributes to our knowledge about fuel consumption and refueling location preferences during hurricanes. These findings could help public agencies and private companies in their fuel supply planning (e.g., fuel distribution) and response (e.g., providing power generators to focal locations). © National Academy of Sciences: Transportation Research Board 2023.;"business continuity; disaster recovery; disaster response; disaster response; emergency evacuation; emergency management; emergency response; logistics and supply chains; planning and preparedness; recovery; sustainability and resilience";"Emergency services; Gases; Hurricanes; Risk management; Storms; Business continuity; Disaster recovery; Disaster-response; Emergency evacuation; Emergency management; Emergency response; Logistic and supply chain; Planning and preparedness; Sustainability and resilience; Supply chains";"Analyzing Gas Station Visits during Hurricane Ida: Implications for Future Fuel Supply Fuel demand surges and supply shortages during hurricanes are problematic. However, fuel consumption and refueling behavior are not well discussed in disaster scenarios compared with the extant literature on these topics during normal conditions. This study used an emerging data source to report when fuel demand (indicated by gas station visits) deviated from a normal variation range during Hurricane Ida (a Category 4 Atlantic hurricane in 2021), how long the deviation lasted, and which area(s) experienced greater deviations. Gas station visits are likely to surge within 2 days before storm landfall, while evacuation destinations and intermediate trip connectors had longer surges. Using univariate linear regression, this study also statistically explored what factors might affect the fuel demand deviation at an aggregate level. The testing results revealed that zones of higher vehicle ownership, more daily commuters driving alone, lower residential stability, more mobile homes, and less storm impacts tended to have more gas station visits (i.e., greater fuel demand). Then, this study analyzed data at a more disaggregate level to identify the characteristics of gas stations that are more likely to have more visitors. Gas stations near interstates/highways attracted more visits, especially within 2 days before storm landfall. Gas stations farther away from interstates/highways attracted more visits after storm landfall. Overall, this study contributes to our knowledge about fuel consumption and refueling location preferences during hurricanes. These findings could help public agencies and private companies in their fuel supply planning (e.g., fuel distribution) and response (e.g., providing power generators to focal locations). © National Academy of Sciences: Transportation Research Board 2023. business continuity; disaster recovery; disaster response; disaster response; emergency evacuation; emergency management; emergency response; logistics and supply chains; planning and preparedness; recovery; sustainability and resilience Emergency services; Gases; Hurricanes; Risk management; Storms; Business continuity; Disaster recovery; Disaster-response; Emergency evacuation; Emergency management; Emergency response; Logistic and supply chain; Planning and preparedness; Sustainability and resilience; Supply chains";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
332;BDHE-Net: A Novel Building Damage Heterogeneity Enhancement Network for Accurate and Efficient Post-Earthquake Assessment Using Aerial and Remote Sensing Data;Accurate and efficient post-earthquake building damage assessment methods enable key building damage information to be obtained more quickly after an earthquake, providing strong support for rescue and reconstruction efforts. Although many methods have been proposed, most have limited effect on accurately extracting severely damaged and collapsed buildings, and they cannot meet the needs of emergency response and rescue operations. Therefore, in this paper, we develop a novel building damage heterogeneity enhancement network for pixel-level building damage classification of post-earthquake unmanned aerial vehicle (UAV) and remote sensing data. The proposed BDHE-Net includes the following three modules: a data augmentation module (DAM), a building damage attention module (BDAM), and a multilevel feature adaptive fusion module (MFAF), which are used to alleviate the weight deviation of intact and slightly damaged categories during model training, pay attention to the heterogeneous characteristics of damaged buildings, and enhance the extraction of house integrity contour information at different resolutions of the image. In addition, a combined loss function is used to focus more attention on the small number of severely damaged and collapsed classes. The proposed model was tested on remote sensing and UAV images acquired from the Afghanistan and Baoxing earthquakes, and the combined loss function and the role of the three modules were studied. The results show that compared with the state-of-the-art methods, the proposed BDHE-Net achieves the best results, with an F1 score improvement of 6.19–8.22%. By integrating the DBA, BDAM, and MFAF modules and combining the loss functions, the model’s classification accuracy for severely damaged and collapsed categories can be improved. © 2024 by the authors.;"building damage assessment; building damage heterogeneity; deep learning; UAV and remote sensing data";NULL;"BDHE-Net: A Novel Building Damage Heterogeneity Enhancement Network for Accurate and Efficient Post-Earthquake Assessment Using Aerial and Remote Sensing Data Accurate and efficient post-earthquake building damage assessment methods enable key building damage information to be obtained more quickly after an earthquake, providing strong support for rescue and reconstruction efforts. Although many methods have been proposed, most have limited effect on accurately extracting severely damaged and collapsed buildings, and they cannot meet the needs of emergency response and rescue operations. Therefore, in this paper, we develop a novel building damage heterogeneity enhancement network for pixel-level building damage classification of post-earthquake unmanned aerial vehicle (UAV) and remote sensing data. The proposed BDHE-Net includes the following three modules: a data augmentation module (DAM), a building damage attention module (BDAM), and a multilevel feature adaptive fusion module (MFAF), which are used to alleviate the weight deviation of intact and slightly damaged categories during model training, pay attention to the heterogeneous characteristics of damaged buildings, and enhance the extraction of house integrity contour information at different resolutions of the image. In addition, a combined loss function is used to focus more attention on the small number of severely damaged and collapsed classes. The proposed model was tested on remote sensing and UAV images acquired from the Afghanistan and Baoxing earthquakes, and the combined loss function and the role of the three modules were studied. The results show that compared with the state-of-the-art methods, the proposed BDHE-Net achieves the best results, with an F1 score improvement of 6.19–8.22%. By integrating the DBA, BDAM, and MFAF modules and combining the loss functions, the model’s classification accuracy for severely damaged and collapsed categories can be improved. © 2024 by the authors. building damage assessment; building damage heterogeneity; deep learning; UAV and remote sensing data NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
333;UAV-Taken Wind Turbine Image Dehazing With a Double-Patch Lightweight Neural Network;Unmanned aerial vehicles (UAVs) offer a solution for remote inspection of wind turbines (WTs). However, in stormy weather conditions, the visual quality of UAV-taken images is affected by contaminated suspended atmospheric particles. To address this problem, a double-patch lightweight convolutional dehazing neural network (DPLDN) is proposed to reconstruct hazy images and enhance the image quality. Unlike other learning-based methods that measure transmission map and atmospheric light separately, the proposed DPLDN uses a transformed atmospheric scattering model to jointly transmission map and atmospheric light, employs depth-separable convolution instead of conventional convolution, and splits the image into double patches. In addition, a super-resolution reconstruction method is proposed to transform the processed low-resolution images into higher-quality images. Extensive experiments show that our proposed method has better dehazing performance compared to other state-of-the-art image dehazing techniques. Meanwhile, the applicability of the method in WT blade image segmentation is experimentally verified.  © 2014 IEEE.;"Convolutional neural networks (CNNs); image dehazing; image enhancement; unmanned aerial vehicles (UAVs)";"Antennas; Convolution; Demulsification; Feature extraction; Generative adversarial networks; Image enhancement; Image reconstruction; Image segmentation; Light transmission; Neural networks; Turbomachine blades; Unmanned aerial vehicles (UAV); Aerial vehicle; Atmospheric modeling; Convolutional neural network; Dehazing; Double patches; Features extraction; Image dehazing; Images reconstruction; Neural-networks; Unmanned aerial vehicle; Wind turbines";"UAV-Taken Wind Turbine Image Dehazing With a Double-Patch Lightweight Neural Network Unmanned aerial vehicles (UAVs) offer a solution for remote inspection of wind turbines (WTs). However, in stormy weather conditions, the visual quality of UAV-taken images is affected by contaminated suspended atmospheric particles. To address this problem, a double-patch lightweight convolutional dehazing neural network (DPLDN) is proposed to reconstruct hazy images and enhance the image quality. Unlike other learning-based methods that measure transmission map and atmospheric light separately, the proposed DPLDN uses a transformed atmospheric scattering model to jointly transmission map and atmospheric light, employs depth-separable convolution instead of conventional convolution, and splits the image into double patches. In addition, a super-resolution reconstruction method is proposed to transform the processed low-resolution images into higher-quality images. Extensive experiments show that our proposed method has better dehazing performance compared to other state-of-the-art image dehazing techniques. Meanwhile, the applicability of the method in WT blade image segmentation is experimentally verified.  © 2014 IEEE. Convolutional neural networks (CNNs); image dehazing; image enhancement; unmanned aerial vehicles (UAVs) Antennas; Convolution; Demulsification; Feature extraction; Generative adversarial networks; Image enhancement; Image reconstruction; Image segmentation; Light transmission; Neural networks; Turbomachine blades; Unmanned aerial vehicles (UAV); Aerial vehicle; Atmospheric modeling; Convolutional neural network; Dehazing; Double patches; Features extraction; Image dehazing; Images reconstruction; Neural-networks; Unmanned aerial vehicle; Wind turbines";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
334;Soft computing machine learning applications for assessing regional-scale landslide susceptibility in the Nepal Himalaya;Purpose: This research is aimed at preparing landslide susceptibility using spatial analysis and soft computing machine learning techniques based on convolutional neural networks (CNNs), artificial neural networks (ANNs) and logistic regression (LR) models. Design/methodology/approach: Using the Geographical Information System (GIS), a spatial database including topographic, hydrologic, geological and landuse data is created for the study area. The data are randomly divided between a training set (70%), a validation (10%) and a test set (20%). Findings: The validation findings demonstrate that the CNN model (has an 89% success rate and an 84% prediction rate). The ANN model (with an 84% success rate and an 81% prediction rate) predicts landslides better than the LR model (with a success rate of 82% and a prediction rate of 79%). In comparison, the CNN proves to be more accurate than the logistic regression and is utilized for final susceptibility. Research limitations/implications: Land cover data and geological data are limited in largescale, making it challenging to develop accurate and comprehensive susceptibility maps. Practical implications: It helps to identify areas with a higher likelihood of experiencing landslides. This information is crucial for assessing the risk posed to human lives, infrastructure and properties in these areas. It allows authorities and stakeholders to prioritize risk management efforts and allocate resources more effectively. Social implications: The social implications of a landslide susceptibility map are profound, as it provides vital information for disaster preparedness, risk mitigation and landuse planning. Communities can utilize these maps to identify vulnerable areas, implement zoning regulations and develop evacuation plans, ultimately safeguarding lives and property. Additionally, access to such information promotes public awareness and education about landslide risks, fostering a proactive approach to disaster management. However, reliance solely on these maps may also create a false sense of security, necessitating continuous updates and integration with other risk assessment measures to ensure effective disaster resilience strategies are in place. Originality/value: Landslide susceptibility mapping provides a proactive approach to identifying areas at higher risk of landslides before any significant events occur. Researchers continually explore new data sources, modeling techniques and validation approaches, leading to a better understanding of landslide dynamics and susceptibility factors. © 2024, Emerald Publishing Limited.;"Artificial neural network; Convolution neural network; Landslide susceptibility; Logistic regression; Machine learning; Validation";"Convolution; Decision making; Disaster prevention; Disasters; Forecasting; Geology; Information use; Neural networks; Risk assessment; Risk management; Social aspects; Soft computing; Computing machines; Convolution neural network; Convolutional neural network; Landslide susceptibility; Logistic Regression modeling; Logistics regressions; Machine-learning; Prediction-rates; Soft-Computing; Validation; Machine learning";"Soft computing machine learning applications for assessing regional-scale landslide susceptibility in the Nepal Himalaya Purpose: This research is aimed at preparing landslide susceptibility using spatial analysis and soft computing machine learning techniques based on convolutional neural networks (CNNs), artificial neural networks (ANNs) and logistic regression (LR) models. Design/methodology/approach: Using the Geographical Information System (GIS), a spatial database including topographic, hydrologic, geological and landuse data is created for the study area. The data are randomly divided between a training set (70%), a validation (10%) and a test set (20%). Findings: The validation findings demonstrate that the CNN model (has an 89% success rate and an 84% prediction rate). The ANN model (with an 84% success rate and an 81% prediction rate) predicts landslides better than the LR model (with a success rate of 82% and a prediction rate of 79%). In comparison, the CNN proves to be more accurate than the logistic regression and is utilized for final susceptibility. Research limitations/implications: Land cover data and geological data are limited in largescale, making it challenging to develop accurate and comprehensive susceptibility maps. Practical implications: It helps to identify areas with a higher likelihood of experiencing landslides. This information is crucial for assessing the risk posed to human lives, infrastructure and properties in these areas. It allows authorities and stakeholders to prioritize risk management efforts and allocate resources more effectively. Social implications: The social implications of a landslide susceptibility map are profound, as it provides vital information for disaster preparedness, risk mitigation and landuse planning. Communities can utilize these maps to identify vulnerable areas, implement zoning regulations and develop evacuation plans, ultimately safeguarding lives and property. Additionally, access to such information promotes public awareness and education about landslide risks, fostering a proactive approach to disaster management. However, reliance solely on these maps may also create a false sense of security, necessitating continuous updates and integration with other risk assessment measures to ensure effective disaster resilience strategies are in place. Originality/value: Landslide susceptibility mapping provides a proactive approach to identifying areas at higher risk of landslides before any significant events occur. Researchers continually explore new data sources, modeling techniques and validation approaches, leading to a better understanding of landslide dynamics and susceptibility factors. © 2024, Emerald Publishing Limited. Artificial neural network; Convolution neural network; Landslide susceptibility; Logistic regression; Machine learning; Validation Convolution; Decision making; Disaster prevention; Disasters; Forecasting; Geology; Information use; Neural networks; Risk assessment; Risk management; Social aspects; Soft computing; Computing machines; Convolution neural network; Convolutional neural network; Landslide susceptibility; Logistic Regression modeling; Logistics regressions; Machine-learning; Prediction-rates; Soft-Computing; Validation; Machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
335;Landslide Susceptibility Zoning: Integrating Multiple Intelligent Models with SHAP Analysis;In this study, we aim to delineate landslide susceptibility zones within Dien Bien province, Vietnam, leveraging the capabilities of various machine learning models including Light Gradient Boosting Machine (LGBM), K-Nearest Neighbors (KNN), and Gradient Boosting (GB). Harnessing a dataset comprising 665 data points and encompassing 14 influential factors such as slope, aspect, curvature, elevation, geological composition, Normalized Difference Vegetation Index (NDVI), and proximity to geological features like faults, rivers, and roads, a comprehensive database for landslide modeling was constructed. The analysis entailed rigorous evaluation and comparison of model accuracy employing established statistical metrics, notably Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC). The findings underscore the efficacy of the Light Gradient Boosting Machine model, exhibiting superior performance with an AUC score of 0.85, surpassing both the Gradient Boosting model (AUC = 0.81) and the K-Nearest Neighbors model (AUC = 0.79). Notably, the Light Gradient Boosting Machine model emerges as a promising tool for precise landslide prediction within the study area, offering significant potential for the creation of a robust landslide susceptibility map. The resulting spatial forecast map for Dien Bien province holds considerable utility for informing land use planning initiatives aimed at mitigating the impact of landslide disasters in the region. Moreover, the application of SHAP (Shapley Additive explanation) values to quantify the contribution of each factor to landslide susceptibility prediction, offering novel insights into model interpretation and feature importance. The resulting spatial forecast map holds significant implications for land use planning and disaster mitigation efforts in Dien Bien province, showcasing the potential of advanced machine learning techniques in enhancing landslide risk management strategies. © 2024, University of Transport Technology. All rights reserved.;"Dien Bien; GB; GIS; KNN; Landslide; LGBM; Viet Nam";NULL;"Landslide Susceptibility Zoning: Integrating Multiple Intelligent Models with SHAP Analysis In this study, we aim to delineate landslide susceptibility zones within Dien Bien province, Vietnam, leveraging the capabilities of various machine learning models including Light Gradient Boosting Machine (LGBM), K-Nearest Neighbors (KNN), and Gradient Boosting (GB). Harnessing a dataset comprising 665 data points and encompassing 14 influential factors such as slope, aspect, curvature, elevation, geological composition, Normalized Difference Vegetation Index (NDVI), and proximity to geological features like faults, rivers, and roads, a comprehensive database for landslide modeling was constructed. The analysis entailed rigorous evaluation and comparison of model accuracy employing established statistical metrics, notably Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC). The findings underscore the efficacy of the Light Gradient Boosting Machine model, exhibiting superior performance with an AUC score of 0.85, surpassing both the Gradient Boosting model (AUC = 0.81) and the K-Nearest Neighbors model (AUC = 0.79). Notably, the Light Gradient Boosting Machine model emerges as a promising tool for precise landslide prediction within the study area, offering significant potential for the creation of a robust landslide susceptibility map. The resulting spatial forecast map for Dien Bien province holds considerable utility for informing land use planning initiatives aimed at mitigating the impact of landslide disasters in the region. Moreover, the application of SHAP (Shapley Additive explanation) values to quantify the contribution of each factor to landslide susceptibility prediction, offering novel insights into model interpretation and feature importance. The resulting spatial forecast map holds significant implications for land use planning and disaster mitigation efforts in Dien Bien province, showcasing the potential of advanced machine learning techniques in enhancing landslide risk management strategies. © 2024, University of Transport Technology. All rights reserved. Dien Bien; GB; GIS; KNN; Landslide; LGBM; Viet Nam NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
336;Hydraulic Partial Factors in Ultimate Limit State of Bridges against Foundation Scour Based on Inverse Reliability Analysis;Owing to undesired hydraulic effects, bridge collapse accidents have become increasingly frequent during the flooding season. Considering hydraulic effects in bridge design is paramount to controlling and mitigating hydrological damage. Despite the efforts in previous studies on bridges under floods, a proper partial factor of hydraulic effects considering foundation scour is still lacking for bridge design against floods. To bridge the gap, this study proposed an inverse reliability analysis framework to generate a proper partial factor of hydraulic effects and a corresponding partial factor of resistance. First, failure modes of typical short- to medium-span bridges were summarized to construct limit state functions. Then, an inverse reliability analysis framework considering hydraulic effects was proposed. The optimum partial factors of hydraulic effects and corresponding partial factors of resistance were calculated for various design specifications. Finally, a classification early warning system of foundation scour was built to forecast hydraulic failure accurately. This paper reveals the necessity of considering the hydraulic effects of floods in bridge design. The proposed framework and warning system can effectively enhance bridge safety against floods. © 2024 American Society of Civil Engineers.;"Early warning system; Failure modes; Hydraulic effects; Partial factor; Reliability";"Disasters; Factor analysis; Failure (mechanical); Reliability analysis; Scour; Analysis frameworks; Bridge collapse; Bridge design; Early Warning System; Flooding season; Foundation scours; Hydraulics effects; Inverse reliability analysis; Limit state; Partial factor; Floods";"Hydraulic Partial Factors in Ultimate Limit State of Bridges against Foundation Scour Based on Inverse Reliability Analysis Owing to undesired hydraulic effects, bridge collapse accidents have become increasingly frequent during the flooding season. Considering hydraulic effects in bridge design is paramount to controlling and mitigating hydrological damage. Despite the efforts in previous studies on bridges under floods, a proper partial factor of hydraulic effects considering foundation scour is still lacking for bridge design against floods. To bridge the gap, this study proposed an inverse reliability analysis framework to generate a proper partial factor of hydraulic effects and a corresponding partial factor of resistance. First, failure modes of typical short- to medium-span bridges were summarized to construct limit state functions. Then, an inverse reliability analysis framework considering hydraulic effects was proposed. The optimum partial factors of hydraulic effects and corresponding partial factors of resistance were calculated for various design specifications. Finally, a classification early warning system of foundation scour was built to forecast hydraulic failure accurately. This paper reveals the necessity of considering the hydraulic effects of floods in bridge design. The proposed framework and warning system can effectively enhance bridge safety against floods. © 2024 American Society of Civil Engineers. Early warning system; Failure modes; Hydraulic effects; Partial factor; Reliability Disasters; Factor analysis; Failure (mechanical); Reliability analysis; Scour; Analysis frameworks; Bridge collapse; Bridge design; Early Warning System; Flooding season; Foundation scours; Hydraulics effects; Inverse reliability analysis; Limit state; Partial factor; Floods";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
337;Machine Learning for Tsunami Waves Forecasting Using Regression Trees;After a seismic event, tsunami early warning systems (TEWSs) try to accurately forecast the maximum height of incident waves at specific target points in front of the coast, so that early warnings can be launched on locations where the impact of tsunami waves can be destructive to deliver aids in these locations in the immediate post-event management. The uncertainty on the forecast can be quantified with ensembles of alternative scenarios. Similarly, in probabilistic tsunami hazard analysis (PTHA) a large number of simulations is required to cover the natural variability of the source process in each location. To improve the accuracy and computational efficiency of tsunami forecasting methods, scientists have recently started to exploit machine learning techniques to process pre-computed simulation data. However, the approaches proposed in literature, mainly based on neural networks, suffer of high training time and limited model explainability. To overtake these issues, this paper describes a machine learning approach based on regression trees to model and forecast tsunami evolutions. The algorithm takes as input a set of simulations forming an ensemble that describes potential benefit regional impact of tsunami source scenarios in a given source area, and it provides predictive models to forecast the tsunami waves for other potential tsunami sources in the same area. The experimental evaluation, performed on the 2003 M6.8 Zemmouri-Boumerdes earthquake and tsunami simulation data, shows that regression trees achieve high forecasting accuracy. Moreover, they provide domain experts with fully-explainable and interpretable models, which are a valuable support for environmental scientists because they describe underlying rules and patterns behind the models and allow for an explicit inspection of their functioning. This can enable a full and trustable exploration of source uncertainty in tsunami early-warning and urgent computing scenarios, with large ensembles of computationally light tsunami simulations. © 2024 The Author(s);"Machine learning; Regression tree; Tsunami forecasting";"Computational efficiency; Forecasting; Location; Regression analysis; Trees (mathematics); Tsunamis; Early warning; Machine-learning; Regression trees; Seismic event; Simulation data; Tsunami forecasting; Tsunami simulation; Tsunami waves; Uncertainty; Wave forecasting; Machine learning";"Machine Learning for Tsunami Waves Forecasting Using Regression Trees After a seismic event, tsunami early warning systems (TEWSs) try to accurately forecast the maximum height of incident waves at specific target points in front of the coast, so that early warnings can be launched on locations where the impact of tsunami waves can be destructive to deliver aids in these locations in the immediate post-event management. The uncertainty on the forecast can be quantified with ensembles of alternative scenarios. Similarly, in probabilistic tsunami hazard analysis (PTHA) a large number of simulations is required to cover the natural variability of the source process in each location. To improve the accuracy and computational efficiency of tsunami forecasting methods, scientists have recently started to exploit machine learning techniques to process pre-computed simulation data. However, the approaches proposed in literature, mainly based on neural networks, suffer of high training time and limited model explainability. To overtake these issues, this paper describes a machine learning approach based on regression trees to model and forecast tsunami evolutions. The algorithm takes as input a set of simulations forming an ensemble that describes potential benefit regional impact of tsunami source scenarios in a given source area, and it provides predictive models to forecast the tsunami waves for other potential tsunami sources in the same area. The experimental evaluation, performed on the 2003 M6.8 Zemmouri-Boumerdes earthquake and tsunami simulation data, shows that regression trees achieve high forecasting accuracy. Moreover, they provide domain experts with fully-explainable and interpretable models, which are a valuable support for environmental scientists because they describe underlying rules and patterns behind the models and allow for an explicit inspection of their functioning. This can enable a full and trustable exploration of source uncertainty in tsunami early-warning and urgent computing scenarios, with large ensembles of computationally light tsunami simulations. © 2024 The Author(s) Machine learning; Regression tree; Tsunami forecasting Computational efficiency; Forecasting; Location; Regression analysis; Trees (mathematics); Tsunamis; Early warning; Machine-learning; Regression trees; Seismic event; Simulation data; Tsunami forecasting; Tsunami simulation; Tsunami waves; Uncertainty; Wave forecasting; Machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
338;Environmental Surveillance through Machine Learning-Empowered Utilization of Optical Networks;We present the use of interconnected optical mesh networks for early earthquake detection and localization, exploiting the existing terrestrial fiber infrastructure. Employing a waveplate model, we integrate real ground displacement data from seven earthquakes with magnitudes ranging from four to six to simulate the strains within fiber cables and collect a large set of light polarization evolution data. These simulations help to enhance a machine learning model that is trained and validated to detect primary wave arrivals that precede earthquakes’ destructive surface waves. The validation results show that the model achieves over 95% accuracy. The machine learning model is then tested against an M4.3 earthquake, exploiting three interconnected mesh networks as a smart sensing grid. Each network is equipped with a sensing fiber placed to correspond with three distinct seismic stations. The objective is to confirm earthquake detection across the interconnected networks, localize the epicenter coordinates via a triangulation method and calculate the fiber-to-epicenter distance. This setup allows early warning generation for municipalities close to the epicenter location, progressing to those further away. The model testing shows a 98% accuracy in detecting primary waves and a one second detection time, affording nearby areas 21 s to take countermeasures, which extends to 57 s in more distant areas. © 2024 by the authors.;"early warnings; earthquakes; machine learning; optical networks; polarization; sensing; waveplate model";"Earthquakes; Fiber optic networks; Fibers; Machine learning; Mesh generation; MESH networking; Surface waves; Detection and localization; Early warning; Earthquake detection; Environmental surveillance; Machine learning models; Machine-learning; Optical mesh network; Sensing; Terrestrial fibers; Waveplate model; article; controlled study; diagnosis; earthquake; environmental monitoring; environmental surveillance; laboratory software; machine learning; optics; polarization; simulation; triangulation; Polarization";"Environmental Surveillance through Machine Learning-Empowered Utilization of Optical Networks We present the use of interconnected optical mesh networks for early earthquake detection and localization, exploiting the existing terrestrial fiber infrastructure. Employing a waveplate model, we integrate real ground displacement data from seven earthquakes with magnitudes ranging from four to six to simulate the strains within fiber cables and collect a large set of light polarization evolution data. These simulations help to enhance a machine learning model that is trained and validated to detect primary wave arrivals that precede earthquakes’ destructive surface waves. The validation results show that the model achieves over 95% accuracy. The machine learning model is then tested against an M4.3 earthquake, exploiting three interconnected mesh networks as a smart sensing grid. Each network is equipped with a sensing fiber placed to correspond with three distinct seismic stations. The objective is to confirm earthquake detection across the interconnected networks, localize the epicenter coordinates via a triangulation method and calculate the fiber-to-epicenter distance. This setup allows early warning generation for municipalities close to the epicenter location, progressing to those further away. The model testing shows a 98% accuracy in detecting primary waves and a one second detection time, affording nearby areas 21 s to take countermeasures, which extends to 57 s in more distant areas. © 2024 by the authors. early warnings; earthquakes; machine learning; optical networks; polarization; sensing; waveplate model Earthquakes; Fiber optic networks; Fibers; Machine learning; Mesh generation; MESH networking; Surface waves; Detection and localization; Early warning; Earthquake detection; Environmental surveillance; Machine learning models; Machine-learning; Optical mesh network; Sensing; Terrestrial fibers; Waveplate model; article; controlled study; diagnosis; earthquake; environmental monitoring; environmental surveillance; laboratory software; machine learning; optics; polarization; simulation; triangulation; Polarization";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
339;Reinforcement Learning-Based Multidimensional Perception and Energy Awareness Optimized Link State Routing for Flying Ad-Hoc Networks;One of the uncrewed aerial vehicles (UAV) in a Flying Ad-hoc network (FANET) can link directly to the infrastructure. At the same time, the other UAVs in the system may have a multi-hop connection in which each node works as a relay and a data collection node. We may not have the support of traditional infrastructure-based networks when natural disasters such as floods or earthquakes strike. This is fatal because trapped people are challenging to find by search and rescue personnel. In such cases, an airborne network of small drones is valuable for giving quick and adequate coverage of the affected area and instant insights to rescue workers. At the same time, such networks face various challenges, and ongoing research and development show promise in making such technology more dependable and effective. This paper presents Multidimensional Perception and Energy Awareness Optimized Link State Routing (MPEAOLSR) for Flying Ad-hoc networks, which is based on Reinforcement Learning (RL). The protocol is a mobile wireless LAN-specific version of the traditional link state algorithm. The protocol largely relies on the idea of multipoint relays (MPRs). During the flooding process, MPRs are chosen to forward broadcast messages. This technique considerably minimizes message overhead associated to a standard flooding system in which each node retransmits each message after receiving the first copy. In RL-MPEAOLSR, only nodes designated as MPRs generate link state information. Furthermore, the RL-MPEAOLSR node can opt to report just links between itself and its MPR selectors, decreasing the number of control messages flooding the network. Because MPRs function well in large and dense networks, the MPEAOLSR protocol is suited for them. The proposed approach outperforms the conventional Energy Awareness Optimised Link State Routing for Flying Ad-Hoc Networks, according to the results. The technique far outperforms current methods in terms of modern bandwidth consumption of 1478.04kpbs, network density of 95.64%, packet delivery ratio of 95.85%, packet loss ratio of 31.94%, delay in transmission time of 6.981 s and accuracy 6.981%. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.;"Decentralized; FANET; Multidimensional Perception and Energy Awareness; Optimized Link State Routing; Reinforcement learning; Routing; UAV";"Antennas; Disasters; Floods; Internet protocols; Mobile ad hoc networks; Packet networks; Routing protocols; Unmanned aerial vehicles (UAV); Ad-hoc networks; Decentralised; Energy-awareness; Flying ad-hoc network; Multidimensional perception and energy awareness; Multipoint relays; Optimized Link State Routing; Reinforcement learnings; Routings; Uncrewed aerial vehicles; Reinforcement learning";"Reinforcement Learning-Based Multidimensional Perception and Energy Awareness Optimized Link State Routing for Flying Ad-Hoc Networks One of the uncrewed aerial vehicles (UAV) in a Flying Ad-hoc network (FANET) can link directly to the infrastructure. At the same time, the other UAVs in the system may have a multi-hop connection in which each node works as a relay and a data collection node. We may not have the support of traditional infrastructure-based networks when natural disasters such as floods or earthquakes strike. This is fatal because trapped people are challenging to find by search and rescue personnel. In such cases, an airborne network of small drones is valuable for giving quick and adequate coverage of the affected area and instant insights to rescue workers. At the same time, such networks face various challenges, and ongoing research and development show promise in making such technology more dependable and effective. This paper presents Multidimensional Perception and Energy Awareness Optimized Link State Routing (MPEAOLSR) for Flying Ad-hoc networks, which is based on Reinforcement Learning (RL). The protocol is a mobile wireless LAN-specific version of the traditional link state algorithm. The protocol largely relies on the idea of multipoint relays (MPRs). During the flooding process, MPRs are chosen to forward broadcast messages. This technique considerably minimizes message overhead associated to a standard flooding system in which each node retransmits each message after receiving the first copy. In RL-MPEAOLSR, only nodes designated as MPRs generate link state information. Furthermore, the RL-MPEAOLSR node can opt to report just links between itself and its MPR selectors, decreasing the number of control messages flooding the network. Because MPRs function well in large and dense networks, the MPEAOLSR protocol is suited for them. The proposed approach outperforms the conventional Energy Awareness Optimised Link State Routing for Flying Ad-Hoc Networks, according to the results. The technique far outperforms current methods in terms of modern bandwidth consumption of 1478.04kpbs, network density of 95.64%, packet delivery ratio of 95.85%, packet loss ratio of 31.94%, delay in transmission time of 6.981 s and accuracy 6.981%. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023. Decentralized; FANET; Multidimensional Perception and Energy Awareness; Optimized Link State Routing; Reinforcement learning; Routing; UAV Antennas; Disasters; Floods; Internet protocols; Mobile ad hoc networks; Packet networks; Routing protocols; Unmanned aerial vehicles (UAV); Ad-hoc networks; Decentralised; Energy-awareness; Flying ad-hoc network; Multidimensional perception and energy awareness; Multipoint relays; Optimized Link State Routing; Reinforcement learnings; Routings; Uncrewed aerial vehicles; Reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;3;Response
340;Unmanned Aerial Vehicle Cooperative Data Dissemination Based on Graph Neural Networks;Unmanned Aerial Vehicles (UAVs) have critical applications in various real-world scenarios, including mapping unknown environments, military reconnaissance, and post-disaster search and rescue. In these scenarios where communication infrastructure is missing, UAVs will form an ad hoc network and perform tasks in a distributed manner. To efficiently carry out tasks, each UAV must acquire and share global status information and data from neighbors. Meanwhile, UAVs frequently operate in extreme conditions, including storms, lightning, and mountainous areas, which significantly degrade the quality of wireless communication. Additionally, the mobility of UAVs leads to dynamic changes in network topology. Therefore, we propose a method that utilizes graph neural networks (GNN) to learn cooperative data dissemination. This method leverages the network topology relationship and enables UAVs to learn a decision policy based on local data structure, ensuring that all UAVs can recover global information. We train the policy using reinforcement learning that enhances the effectiveness of each transmission. After repeated simulations, the results validate the effectiveness and generalization of the proposed method. © 2024 by the authors.;"cooperative data dissemination; graph neural network; reinforcement learning; UAV";"Ad hoc networks; Antennas; Graph neural networks; Military applications; Military vehicles; Network topology; Unmanned aerial vehicles (UAV); Aerial vehicle; Cooperative data dissemination; Critical applications; Data dissemination; Graph neural networks; Learn+; Network topology; Real-world scenario; Reinforcement learnings; Unmanned aerial vehicle; adult; army; article; controlled study; human; learning; lightning; male; nerve cell network; simulation; unmanned aerial vehicle; wireless communication; Reinforcement learning";"Unmanned Aerial Vehicle Cooperative Data Dissemination Based on Graph Neural Networks Unmanned Aerial Vehicles (UAVs) have critical applications in various real-world scenarios, including mapping unknown environments, military reconnaissance, and post-disaster search and rescue. In these scenarios where communication infrastructure is missing, UAVs will form an ad hoc network and perform tasks in a distributed manner. To efficiently carry out tasks, each UAV must acquire and share global status information and data from neighbors. Meanwhile, UAVs frequently operate in extreme conditions, including storms, lightning, and mountainous areas, which significantly degrade the quality of wireless communication. Additionally, the mobility of UAVs leads to dynamic changes in network topology. Therefore, we propose a method that utilizes graph neural networks (GNN) to learn cooperative data dissemination. This method leverages the network topology relationship and enables UAVs to learn a decision policy based on local data structure, ensuring that all UAVs can recover global information. We train the policy using reinforcement learning that enhances the effectiveness of each transmission. After repeated simulations, the results validate the effectiveness and generalization of the proposed method. © 2024 by the authors. cooperative data dissemination; graph neural network; reinforcement learning; UAV Ad hoc networks; Antennas; Graph neural networks; Military applications; Military vehicles; Network topology; Unmanned aerial vehicles (UAV); Aerial vehicle; Cooperative data dissemination; Critical applications; Data dissemination; Graph neural networks; Learn+; Network topology; Real-world scenario; Reinforcement learnings; Unmanned aerial vehicle; adult; army; article; controlled study; human; learning; lightning; male; nerve cell network; simulation; unmanned aerial vehicle; wireless communication; Reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;3;Response
341;Seismic structural health monitoring of RC framed building using artificial neural network model: a study;Presently the occurrence of strong magnitude earthquake and their effect on urban places has drawn the attention of the researchers for development of quick health monitoring technique. In response to the urgent need for quick rehabilitation of residential areas in seismic-prone regions after an earthquake, conventional methods for health monitoring of affected buildings prove time-consuming and require the assistance of competent personnel. To address this challenge, this research proposes the adoption of an Artificial Neural Network (ANN) model as a fast and robust health monitoring tool for seismic damage identification in Reinforced Concrete (RC) buildings. The developed ANN model, based on a multilayer feed-forward neural network with one hidden layer, utilizes crucial factors such as real-time earthquake ground motion (PGA, PGV, PGD, Time Duration), plinth area, and building height as inputs. Key health monitoring parameters (Inter Story Drift, Displacement, Frequency) for seismic safety are considered as outputs. Trained using the Levenberg-Marquardt algorithm and validated against new earthquake ground motion data, the ANN model demonstrates efficiency and applicability. Numerical simulations, conducted through the finite element software (ETABS) on a typical RC structure with varying plinth areas and building heights, provide the necessary data for ANN model development. Implemented in MATLAB, the results show promising potential for rapid and efficient seismic structural health monitoring, offering an effective means for post-earthquake assessment and rehabilitation of RC structures in seismically active regions. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.;"Artificial neural network; Earthquake ground motion; Finite element software (ETABS); Structural health monitoring; Time history analysis";NULL;"Seismic structural health monitoring of RC framed building using artificial neural network model: a study Presently the occurrence of strong magnitude earthquake and their effect on urban places has drawn the attention of the researchers for development of quick health monitoring technique. In response to the urgent need for quick rehabilitation of residential areas in seismic-prone regions after an earthquake, conventional methods for health monitoring of affected buildings prove time-consuming and require the assistance of competent personnel. To address this challenge, this research proposes the adoption of an Artificial Neural Network (ANN) model as a fast and robust health monitoring tool for seismic damage identification in Reinforced Concrete (RC) buildings. The developed ANN model, based on a multilayer feed-forward neural network with one hidden layer, utilizes crucial factors such as real-time earthquake ground motion (PGA, PGV, PGD, Time Duration), plinth area, and building height as inputs. Key health monitoring parameters (Inter Story Drift, Displacement, Frequency) for seismic safety are considered as outputs. Trained using the Levenberg-Marquardt algorithm and validated against new earthquake ground motion data, the ANN model demonstrates efficiency and applicability. Numerical simulations, conducted through the finite element software (ETABS) on a typical RC structure with varying plinth areas and building heights, provide the necessary data for ANN model development. Implemented in MATLAB, the results show promising potential for rapid and efficient seismic structural health monitoring, offering an effective means for post-earthquake assessment and rehabilitation of RC structures in seismically active regions. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024. Artificial neural network; Earthquake ground motion; Finite element software (ETABS); Structural health monitoring; Time history analysis NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
342;An Intelligent Flood Prediction System Using Deep Learning Techniques and Fine Tuned MobileNet Architecture;A flood can cause significant damage and loss of life and economic disruption. Early warning and accurate forecasting of such disasters can help minimize the effects of these natural disasters by helping with evacuation plans and allocation of resources. The main objective of this study was to develop an automatic model that can predict the precipitation seasons. It was done using the ANNs and the CNN structure. The study utilized real-time images including normal and flood-affected scenes. The results of the study revealed that the CNN models performed well in terms of their accuracy, precision, and F1-score. The CNN models have performed well in terms of their accuracy rates with 96.55 classification accuracy for both 10 and 100 epochs. The IFPS could help authorities identify potential flood threats and take immediate action to protect their communities. The proposed IFPS was evaluated against existing flood prediction tools. Finally, the performance of IFPS is compared with three deep learning algorithms such as ResNet-50, VGG-16, and Inception V2. The results indicated that the deep learning system was more accurate and faster than the traditional methods and the pre-trained models. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd 2024.;"Artificial neural networks; Convolutional neural networks; Early flood prediction systems; Flood prediction tools";NULL;"An Intelligent Flood Prediction System Using Deep Learning Techniques and Fine Tuned MobileNet Architecture A flood can cause significant damage and loss of life and economic disruption. Early warning and accurate forecasting of such disasters can help minimize the effects of these natural disasters by helping with evacuation plans and allocation of resources. The main objective of this study was to develop an automatic model that can predict the precipitation seasons. It was done using the ANNs and the CNN structure. The study utilized real-time images including normal and flood-affected scenes. The results of the study revealed that the CNN models performed well in terms of their accuracy, precision, and F1-score. The CNN models have performed well in terms of their accuracy rates with 96.55 classification accuracy for both 10 and 100 epochs. The IFPS could help authorities identify potential flood threats and take immediate action to protect their communities. The proposed IFPS was evaluated against existing flood prediction tools. Finally, the performance of IFPS is compared with three deep learning algorithms such as ResNet-50, VGG-16, and Inception V2. The results indicated that the deep learning system was more accurate and faster than the traditional methods and the pre-trained models. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd 2024. Artificial neural networks; Convolutional neural networks; Early flood prediction systems; Flood prediction tools NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
343;Advances and gaps in the science and practice of impact-based forecasting of droughts;Advances in impact modeling and numerical weather forecasting have allowed accurate drought monitoring and skilful forecasts that can drive decisions at the regional scale. State-of-the-art drought early-warning systems are currently based on statistical drought indicators, which do not account for dynamic regional vulnerabilities, and hence neglect the socio-economic impact for initiating actions. The transition from conventional physical forecasts of droughts toward impact-based forecasting (IbF) is a recent paradigm shift in early warning services, to ultimately bridge the gap between science and action. The demand to generate predictions of “what the weather will do” underpins the rising interest in drought IbF across all weather-sensitive sectors. Despite the large expected socio-economic benefits, migrating to this new paradigm presents myriad challenges. In this article, we provide a comprehensive overview of drought IbF, outlining the progress made in the field. Additionally, we present a road map highlighting current challenges and limitations in the science and practice of drought IbF and possible ways forward. We identify seven scientific and practical challenges/limitations: the contextual challenge (inadequate accounting for the spatio-sectoral dynamics of vulnerability and exposure), the human-water feedbacks challenge (neglecting how human activities influence the propagation of drought), the typology challenge (oversimplifying drought typology to meteorological), the model challenge (reliance on mainstream machine learning models), and the data challenge (mainly textual) with the linked sectoral and geographical limitations. Our vision is to facilitate the progress of drought IbF and its use in making informed and timely decisions on mitigation measures, thus minimizing the drought impacts globally. This article is categorized under: Science of Water > Water Extremes Science of Water > Methods Science of Water > Water and Environmental Change. © 2023 The Authors. WIREs Water published by Wiley Periodicals LLC.;"drought; drought impact-based forecasting; early action; early warning systems; impacts of drought";"accuracy assessment; demand analysis; drought; early warning system; machine learning; mitigation; paradigm shift; socioeconomic conditions; vulnerability; weather forecasting";"Advances and gaps in the science and practice of impact-based forecasting of droughts Advances in impact modeling and numerical weather forecasting have allowed accurate drought monitoring and skilful forecasts that can drive decisions at the regional scale. State-of-the-art drought early-warning systems are currently based on statistical drought indicators, which do not account for dynamic regional vulnerabilities, and hence neglect the socio-economic impact for initiating actions. The transition from conventional physical forecasts of droughts toward impact-based forecasting (IbF) is a recent paradigm shift in early warning services, to ultimately bridge the gap between science and action. The demand to generate predictions of “what the weather will do” underpins the rising interest in drought IbF across all weather-sensitive sectors. Despite the large expected socio-economic benefits, migrating to this new paradigm presents myriad challenges. In this article, we provide a comprehensive overview of drought IbF, outlining the progress made in the field. Additionally, we present a road map highlighting current challenges and limitations in the science and practice of drought IbF and possible ways forward. We identify seven scientific and practical challenges/limitations: the contextual challenge (inadequate accounting for the spatio-sectoral dynamics of vulnerability and exposure), the human-water feedbacks challenge (neglecting how human activities influence the propagation of drought), the typology challenge (oversimplifying drought typology to meteorological), the model challenge (reliance on mainstream machine learning models), and the data challenge (mainly textual) with the linked sectoral and geographical limitations. Our vision is to facilitate the progress of drought IbF and its use in making informed and timely decisions on mitigation measures, thus minimizing the drought impacts globally. This article is categorized under: Science of Water > Water Extremes Science of Water > Methods Science of Water > Water and Environmental Change. © 2023 The Authors. WIREs Water published by Wiley Periodicals LLC. drought; drought impact-based forecasting; early action; early warning systems; impacts of drought accuracy assessment; demand analysis; drought; early warning system; machine learning; mitigation; paradigm shift; socioeconomic conditions; vulnerability; weather forecasting";-1;Não Classificado;NULL;1.4;Climatological;2;Preparation
344;Wildfire Scenarios for Assessing Risk of Cover Loss in a Megadiverse Zone within the Colombian Caribbean;Rising wildfire incidents in South America, potentially exacerbated by climate change, require an exploration of sustainable approaches for fire risk reduction. This study investigates wildfire-prone meteorological conditions and assesses the susceptibility in Colombia’s megadiverse northern region. Utilizing this knowledge, we apply a machine learning model and the Monte Carlo approach to evaluate sustainability strategies for mitigating fire risk. The findings indicate that a substantial number of fires occur in the southern region, especially in the first two seasons of the year, and in the northeast in the last two seasons. Both are characterized by high temperatures, minimal precipitation, strong winds, and dry conditions. The developed model demonstrates significant predictive accuracy with the HIT, FAR, and POC of 87.9%, 28.3%, and 95.7%, respectively, providing insights into the probabilistic aspects of fire development. Various scenarios showed that a decrease in soil temperature reduces the risk mostly in lower altitudes and leaf skin reservoir content in the highest altitudes, as well as in the north region. Sustainability strategies, such as tree belts, agroforestry mosaics, and forest corridors emerge as crucial measures. The results underscore the importance of proactive measures in mitigating wildfire impact, offering actionable insights for crafting effective sustainability strategies amid escalating fire risks. © 2024 by the authors.;"adaptation; biomass burning; hazard; machine learning; risk; sustainability strategies; vulnerability; wildfires";"South America; adaptation; biomass burning; hazard assessment; machine learning; risk assessment; soil temperature; sustainability; vulnerability; wildfire";"Wildfire Scenarios for Assessing Risk of Cover Loss in a Megadiverse Zone within the Colombian Caribbean Rising wildfire incidents in South America, potentially exacerbated by climate change, require an exploration of sustainable approaches for fire risk reduction. This study investigates wildfire-prone meteorological conditions and assesses the susceptibility in Colombia’s megadiverse northern region. Utilizing this knowledge, we apply a machine learning model and the Monte Carlo approach to evaluate sustainability strategies for mitigating fire risk. The findings indicate that a substantial number of fires occur in the southern region, especially in the first two seasons of the year, and in the northeast in the last two seasons. Both are characterized by high temperatures, minimal precipitation, strong winds, and dry conditions. The developed model demonstrates significant predictive accuracy with the HIT, FAR, and POC of 87.9%, 28.3%, and 95.7%, respectively, providing insights into the probabilistic aspects of fire development. Various scenarios showed that a decrease in soil temperature reduces the risk mostly in lower altitudes and leaf skin reservoir content in the highest altitudes, as well as in the north region. Sustainability strategies, such as tree belts, agroforestry mosaics, and forest corridors emerge as crucial measures. The results underscore the importance of proactive measures in mitigating wildfire impact, offering actionable insights for crafting effective sustainability strategies amid escalating fire risks. © 2024 by the authors. adaptation; biomass burning; hazard; machine learning; risk; sustainability strategies; vulnerability; wildfires South America; adaptation; biomass burning; hazard assessment; machine learning; risk assessment; soil temperature; sustainability; vulnerability; wildfire";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
345;Investigating Land Cover Changes and Their Impact on Land Surface Temperature in Khyber Pakhtunkhwa, Pakistan;"Restoration of degraded land is a significant concern in the 21st century in order to combat the impacts of climate change. For this reason, the provisional government of Khyber Pakhtunkhwa (KPK), Pakistan, initialized a Billion Tree Tsunami Project (BTTP) in 2013 and finished it in 2017. Although a few researchers have investigated the land use transitions under BTTP in the short term by merging all the vegetation types into one, analysis of the long-term benefits of the project and future persistence were missing. Furthermore, the previous studies have not discussed whether the prime objective of the BTTP was achieved. Considering the existing gaps, this research mainly involves analyzing (i) fluctuations in the green fraction by employing a land change modeler (LCM), along with the spatial location of gain-loss and exchange analysis using a high-resolution dataset (GLC30); (ii) forest cover changes under the influence of the BTTP; (iii) impacts of green fraction changes towards land surface temperature (LST) by utilizing the less-explored technique of curve fit linear regression modeling (CFLR); and finally, (iv) assessing the persistence of the NDVI and LST trends by employing the Hurst exponent. Research findings indicate that as an output of BTTP, despite the government’s claim of increasing the forest cover by 2%, a significant gain of grassland (3904.87 km2) was observed at the cost of bare land. In comparison, the overall increase in forest cover was only 0.39%, which does not satisfy the main objective of this project. On the other hand, the CFLRM-based actual contributions of land cover change (LCC) transition to LST indicate a significant decline in LST in the areas with gains in green fraction for both grassland and forest. At the same time, an increase was observed with reverse transitions. Although the results appear positive for climatic impacts in the short term, the HURST model-based persistence analysis revealed that the spatial locations of increasing vegetation and decreasing LST trends fall under the weakly persistent category, therefore these trends may not continue in the near future. Despite some positive impact on LST attributed to the green fraction increase, this project cannot be regarded as a complete success due to its failure to achieve its prime objective. © 2024 by the authors.";"Billion Tree Tsunami Project; climate change; Hurst exponent; land cover change; land surface temperature";"Khyber-Pakhtunkhwa; Pakistan; forest cover; future prospect; grassland; land surface; NDVI; regression analysis; restoration ecology; surface temperature";"Investigating Land Cover Changes and Their Impact on Land Surface Temperature in Khyber Pakhtunkhwa, Pakistan Restoration of degraded land is a significant concern in the 21st century in order to combat the impacts of climate change. For this reason, the provisional government of Khyber Pakhtunkhwa (KPK), Pakistan, initialized a Billion Tree Tsunami Project (BTTP) in 2013 and finished it in 2017. Although a few researchers have investigated the land use transitions under BTTP in the short term by merging all the vegetation types into one, analysis of the long-term benefits of the project and future persistence were missing. Furthermore, the previous studies have not discussed whether the prime objective of the BTTP was achieved. Considering the existing gaps, this research mainly involves analyzing (i) fluctuations in the green fraction by employing a land change modeler (LCM), along with the spatial location of gain-loss and exchange analysis using a high-resolution dataset (GLC30); (ii) forest cover changes under the influence of the BTTP; (iii) impacts of green fraction changes towards land surface temperature (LST) by utilizing the less-explored technique of curve fit linear regression modeling (CFLR); and finally, (iv) assessing the persistence of the NDVI and LST trends by employing the Hurst exponent. Research findings indicate that as an output of BTTP, despite the government’s claim of increasing the forest cover by 2%, a significant gain of grassland (3904.87 km2) was observed at the cost of bare land. In comparison, the overall increase in forest cover was only 0.39%, which does not satisfy the main objective of this project. On the other hand, the CFLRM-based actual contributions of land cover change (LCC) transition to LST indicate a significant decline in LST in the areas with gains in green fraction for both grassland and forest. At the same time, an increase was observed with reverse transitions. Although the results appear positive for climatic impacts in the short term, the HURST model-based persistence analysis revealed that the spatial locations of increasing vegetation and decreasing LST trends fall under the weakly persistent category, therefore these trends may not continue in the near future. Despite some positive impact on LST attributed to the green fraction increase, this project cannot be regarded as a complete success due to its failure to achieve its prime objective. © 2024 by the authors. Billion Tree Tsunami Project; climate change; Hurst exponent; land cover change; land surface temperature Khyber-Pakhtunkhwa; Pakistan; forest cover; future prospect; grassland; land surface; NDVI; regression analysis; restoration ecology; surface temperature";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
346;Tsunami tide prediction in shallow water using recurrent neural networks: model implementation in the Indonesia Tsunami Early Warning System;Near-field tides prediction for tsunami detection in the coastal area is a significant problem of the cable-based tsunami meter system in north Sipora, Indonesia. The problem is caused by its shallow water condition and the unavailability of an applicable model or research for tsunami detection in this area. The problem foundation of shallow water area is its ambient noise level-dependent property that requires preprocessing to improve its feature representation. Moreover, because this shallow water is close to the land area, we must consider a model that can accommodate low prediction time for a Tsunami Early Warning System. Therefore, we propose a recurrent neural network (RNN) model because of its reliable performance for time series forecasting. Our report evaluates variants of the RNN model (the vanilla RNN, LSTM and GRU models) in tides prediction and z-score analysis for tsunami identification. The GRU model overwhelms the other two variants in error scores and time processed (training and prediction). It can achieve median error score distribution of 7.8×10-5 on the L1000-P250 configuration with time prediction under 0.1 s. This lower-time prediction is necessary to ensure the early warning system is going well. Moreover, the GRU model can identify all synthetic tsunami tide spikes (compared with the ground truth result) from magnitude 7.2–8.2 by applying a z-score on the GRU’s prediction. © The Author(s) 2023.;"Deep neural network; Recurrent neural network; Shallow water body; Tides prediction; Tsunami early warning system";"Forecasting; Long short-term memory; Tides; Tsunamis; Indonesia; Model implementation; Near fields; Recurrent neural network model; Shallow water bodies; Shallow waters; Tide prediction; Time predictions; Tsunami detection; Tsunami early-warning systems; Deep neural networks";"Tsunami tide prediction in shallow water using recurrent neural networks: model implementation in the Indonesia Tsunami Early Warning System Near-field tides prediction for tsunami detection in the coastal area is a significant problem of the cable-based tsunami meter system in north Sipora, Indonesia. The problem is caused by its shallow water condition and the unavailability of an applicable model or research for tsunami detection in this area. The problem foundation of shallow water area is its ambient noise level-dependent property that requires preprocessing to improve its feature representation. Moreover, because this shallow water is close to the land area, we must consider a model that can accommodate low prediction time for a Tsunami Early Warning System. Therefore, we propose a recurrent neural network (RNN) model because of its reliable performance for time series forecasting. Our report evaluates variants of the RNN model (the vanilla RNN, LSTM and GRU models) in tides prediction and z-score analysis for tsunami identification. The GRU model overwhelms the other two variants in error scores and time processed (training and prediction). It can achieve median error score distribution of 7.8×10-5 on the L1000-P250 configuration with time prediction under 0.1 s. This lower-time prediction is necessary to ensure the early warning system is going well. Moreover, the GRU model can identify all synthetic tsunami tide spikes (compared with the ground truth result) from magnitude 7.2–8.2 by applying a z-score on the GRU’s prediction. © The Author(s) 2023. Deep neural network; Recurrent neural network; Shallow water body; Tides prediction; Tsunami early warning system Forecasting; Long short-term memory; Tides; Tsunamis; Indonesia; Model implementation; Near fields; Recurrent neural network model; Shallow water bodies; Shallow waters; Tide prediction; Time predictions; Tsunami detection; Tsunami early-warning systems; Deep neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
347;Data interpretation and forecasting of SHM heteroscedastic measurements under typhoon conditions enabled by an enhanced Hierarchical sparse Bayesian Learning model with high robustness;Rapid advances in Structural Health Monitoring (SHM) have led to a new era of Big Data in the past decade. However, collected data may include a huge amount of heteroscedastic data, particularly under extreme events (e.g., typhoons), which makes robust decisions remain a challenging problem. To this end, this article proposed an enhanced Hierarchical Sparse Bayesian Learning (eHSBL) model for SHM field data interpretation and forecasting, uncertainty analysis, as well as correlation analysis. The proposed eHSBL model incorporates the Gaussian kernel function and the Hierarchical Bayesian model for an operational large-scale suspension bridge, i.e., Tsing Ma Bridge (TMB) in Hong Kong, under extreme events exhibiting heterogeneities, strong volatilities, and high uncertainties. The Gaussian kernel function in the eHSBL model is capable of mapping SHM heteroscedastic data into high-dimensional space, and the Hierarchical Bayesian model can be iteratively updated by introducing hyperparameters to achieve the sparse expression of the model, so the model has strong generalization ability and high robustness. Through the incorporation of the extrapolation forecasting algorithm, the eHSBL can carry out the forecasting analysis for the future SHM data. Results show that the eHSBL regression and forecasting results under extreme events can well reflect the strain responses of the structure. The uncertainty and correlation analysis reveal that the eHSBL model error variation is influenced by temperature, traffic loads, and wind speeds, and the corresponding error variation curves and change rules are also provided and summarized, which will shed light on the performance evaluation and early warning for in-service long-span bridges. © 2024;"an enhanced Hierarchical Sparse Bayesian Learning model; Data interpretation and forecasting; SHM heteroscedastic data; Typhoons; Uncertainty and correlation analysis";"Bayesian networks; Correlation methods; Hurricanes; Iterative methods; Learning systems; Storms; Structural health monitoring; Uncertainty analysis; An enhanced hierarchical sparse bayesian learning model; Bayesian learning; Correlation analysis; Data interpretation; Data interpretation and forecasting; Heteroscedastic; Learning models; Sparse bayesian; Structural health monitoring heteroscedastic data; Uncertainty; Uncertainty and correlation analyse; Forecasting";"Data interpretation and forecasting of SHM heteroscedastic measurements under typhoon conditions enabled by an enhanced Hierarchical sparse Bayesian Learning model with high robustness Rapid advances in Structural Health Monitoring (SHM) have led to a new era of Big Data in the past decade. However, collected data may include a huge amount of heteroscedastic data, particularly under extreme events (e.g., typhoons), which makes robust decisions remain a challenging problem. To this end, this article proposed an enhanced Hierarchical Sparse Bayesian Learning (eHSBL) model for SHM field data interpretation and forecasting, uncertainty analysis, as well as correlation analysis. The proposed eHSBL model incorporates the Gaussian kernel function and the Hierarchical Bayesian model for an operational large-scale suspension bridge, i.e., Tsing Ma Bridge (TMB) in Hong Kong, under extreme events exhibiting heterogeneities, strong volatilities, and high uncertainties. The Gaussian kernel function in the eHSBL model is capable of mapping SHM heteroscedastic data into high-dimensional space, and the Hierarchical Bayesian model can be iteratively updated by introducing hyperparameters to achieve the sparse expression of the model, so the model has strong generalization ability and high robustness. Through the incorporation of the extrapolation forecasting algorithm, the eHSBL can carry out the forecasting analysis for the future SHM data. Results show that the eHSBL regression and forecasting results under extreme events can well reflect the strain responses of the structure. The uncertainty and correlation analysis reveal that the eHSBL model error variation is influenced by temperature, traffic loads, and wind speeds, and the corresponding error variation curves and change rules are also provided and summarized, which will shed light on the performance evaluation and early warning for in-service long-span bridges. © 2024 an enhanced Hierarchical Sparse Bayesian Learning model; Data interpretation and forecasting; SHM heteroscedastic data; Typhoons; Uncertainty and correlation analysis Bayesian networks; Correlation methods; Hurricanes; Iterative methods; Learning systems; Storms; Structural health monitoring; Uncertainty analysis; An enhanced hierarchical sparse bayesian learning model; Bayesian learning; Correlation analysis; Data interpretation; Data interpretation and forecasting; Heteroscedastic; Learning models; Sparse bayesian; Structural health monitoring heteroscedastic data; Uncertainty; Uncertainty and correlation analyse; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
348;Short-term drought Index forecasting for hot and semi-humid climate Regions: A novel empirical Fourier decomposition-based ensemble Deep-Random vector functional link strategy;The development of advanced technologies based on computer aid models in the domain of crops and agriculture productively is a modern advancement. Machine learning (ML) based forecasting of the short-term drought indicators (e.g., Standardized Precipitation Evapotranspiration Index (SPEI)) based on individual signals is a complex process that involves several factors including data quality and availability, inherent signal complexity, non-stationarity of climate, and uncertainty in ML models. Recent achievements in the field of Fourier-based signal processing integrated with advanced deep learning approaches have made it possible to produce very accurate intelligent frameworks for multi-temporal drought indicators and fill this gap. In this research, a new ultramodern complementary intelligent framework comprised of the SelectKbest feature selection (FS), Empirical Fourier Decomposition (EFD), and deep ensemble random vector functional link (Deep RVFL) was developed for multi-temporal monthly forecasting of short-term drought indicators for three and six months (SPEI3 and SPEI6) for two different very hot and semi-humid climate zones of Iran. For this purpose, the most influential time lags associated with each drought indicator were indicated using the SelectKbest FS in each zone. Afterwards, the individual SPEI signals were decomposed by the EFD technique imposing the most important lagged components to feed the ML approaches. Here, a new hybrid architecture deep learning model, namely a convolutional neural network coupled with a bidirectional gated recurrent unit (CNN-Bi-GRU) designed for multi-temporal drought forecasting in the next one-and three- months. Two advanced approaches, introducing the convolutional neural network coupled with bidirectional recurrent neural network (CNN-Bi-RNN), and Random vector function link (RVFL) were adopted to validate the main model (EFD-DeepRVFL) in complementary and standalone counterpart forms. The robustness of all the models was examined using several metrics such as coefficient of determination (R2), root mean square error (RMSE), reliability, and squared Chi-square Distance (SquD). The comprehensive assessment of the outcomes of hybrid schemes revealed that EFD-DeepRVFL owing to superior performance (R2|SPEI3(t + 1) = 0.953, R2|SPEI3(t + 3) = 0.837, R2|SPEI6(t + 1) = 0.962, and R2|SPEI6(t + 3) = 0.887) at Ahvaz station and (R2|SPEI3(t + 1) = 0.964, R2|SPEI3(t + 3) = 0.863, R2|SPEI6(t + 1) = 0.935, and R2|SPEI6(t + 3) = 0.839) at Kermanshah station outperformed the EFD-RVFL and CNN-Bi-RNN, respectively. The developed expert system provides early warning of drought conditions, as a decision-making tool, crop yield prediction, and water resources risk assessment. © 2024 Elsevier B.V.;"CNN-BiGRU; Drought indicators; Empirical Fourier decomposition; Multi-temporal forecasting; RVFL; SelectKbest";"Iran; Climate models; Complex networks; Convolution; Convolutional neural networks; Learning systems; Mean square error; Recurrent neural networks; Signal processing; Vectors; Weather forecasting; CNN-BiGRU; Drought indicator; Empirical fourier decomposition; Fourier decomposition; Multi-temporal; Multi-temporal forecasting; Random vector function link; Random vectors; Selectkbest; Vector functions; artificial neural network; climate change; crop yield; decomposition analysis; drought; forecasting method; signal processing; spatiotemporal analysis; Drought";"Short-term drought Index forecasting for hot and semi-humid climate Regions: A novel empirical Fourier decomposition-based ensemble Deep-Random vector functional link strategy The development of advanced technologies based on computer aid models in the domain of crops and agriculture productively is a modern advancement. Machine learning (ML) based forecasting of the short-term drought indicators (e.g., Standardized Precipitation Evapotranspiration Index (SPEI)) based on individual signals is a complex process that involves several factors including data quality and availability, inherent signal complexity, non-stationarity of climate, and uncertainty in ML models. Recent achievements in the field of Fourier-based signal processing integrated with advanced deep learning approaches have made it possible to produce very accurate intelligent frameworks for multi-temporal drought indicators and fill this gap. In this research, a new ultramodern complementary intelligent framework comprised of the SelectKbest feature selection (FS), Empirical Fourier Decomposition (EFD), and deep ensemble random vector functional link (Deep RVFL) was developed for multi-temporal monthly forecasting of short-term drought indicators for three and six months (SPEI3 and SPEI6) for two different very hot and semi-humid climate zones of Iran. For this purpose, the most influential time lags associated with each drought indicator were indicated using the SelectKbest FS in each zone. Afterwards, the individual SPEI signals were decomposed by the EFD technique imposing the most important lagged components to feed the ML approaches. Here, a new hybrid architecture deep learning model, namely a convolutional neural network coupled with a bidirectional gated recurrent unit (CNN-Bi-GRU) designed for multi-temporal drought forecasting in the next one-and three- months. Two advanced approaches, introducing the convolutional neural network coupled with bidirectional recurrent neural network (CNN-Bi-RNN), and Random vector function link (RVFL) were adopted to validate the main model (EFD-DeepRVFL) in complementary and standalone counterpart forms. The robustness of all the models was examined using several metrics such as coefficient of determination (R2), root mean square error (RMSE), reliability, and squared Chi-square Distance (SquD). The comprehensive assessment of the outcomes of hybrid schemes revealed that EFD-DeepRVFL owing to superior performance (R2|SPEI3(t + 1) = 0.953, R2|SPEI3(t + 3) = 0.837, R2|SPEI6(t + 1) = 0.962, and R2|SPEI6(t + 3) = 0.887) at Ahvaz station and (R2|SPEI3(t + 1) = 0.964, R2|SPEI3(t + 3) = 0.863, R2|SPEI6(t + 1) = 0.935, and R2|SPEI6(t + 3) = 0.839) at Kermanshah station outperformed the EFD-RVFL and CNN-Bi-RNN, respectively. The developed expert system provides early warning of drought conditions, as a decision-making tool, crop yield prediction, and water resources risk assessment. © 2024 Elsevier B.V. CNN-BiGRU; Drought indicators; Empirical Fourier decomposition; Multi-temporal forecasting; RVFL; SelectKbest Iran; Climate models; Complex networks; Convolution; Convolutional neural networks; Learning systems; Mean square error; Recurrent neural networks; Signal processing; Vectors; Weather forecasting; CNN-BiGRU; Drought indicator; Empirical fourier decomposition; Fourier decomposition; Multi-temporal; Multi-temporal forecasting; Random vector function link; Random vectors; Selectkbest; Vector functions; artificial neural network; climate change; crop yield; decomposition analysis; drought; forecasting method; signal processing; spatiotemporal analysis; Drought";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
349;An integrated approach for understanding global earthquake patterns and enhancing seismic risk assessment;Earthquakes, as intricate natural phenomena, profoundly impact lives, infrastructure, and the environment. While previous research has explored earthquake patterns through data analysis methods, there has been a gap in examining the time intervals between consecutive earthquakes across various magnitude categories. Given the complexity and vastness of seismic data, this study aims to provide comprehensive insights into global seismic activity by employing sophisticated data analysis methodologies on a century-long dataset of seismic events. The four-phase methodology encompasses exploratory data analysis (EDA), temporal dynamics exploration, spatial pattern analysis, and cluster analysis. The EDA serves as the foundational step, providing fundamental insights into the dataset's attributes and laying the groundwork for subsequent analyses. Temporal dynamics exploration focuses on discerning variations in earthquake occurrences over time. Spatial analysis identifies geographic regions with heightened earthquake activity and uncovers patterns of seismic clustering. K-means clustering is employed to delineate distinct earthquake occurrence clusters or hotspots based on geographical coordinates. The study's findings reveal a notable increase in recorded earthquakes since the 1960s, peaking in 2018. Distinct patterns in seismic activity are linked to factors such as time, human activities, and plate boundaries. The integrated approach enriches understanding of global earthquake trends and patterns, contributing to improved seismic hazard assessments, early warning systems, and risk mitigation efforts. © The Author(s) 2024.;"Clustering; Data science analysis; Earthquakes; Seismic data; Spatiotemporal analysis";NULL;"An integrated approach for understanding global earthquake patterns and enhancing seismic risk assessment Earthquakes, as intricate natural phenomena, profoundly impact lives, infrastructure, and the environment. While previous research has explored earthquake patterns through data analysis methods, there has been a gap in examining the time intervals between consecutive earthquakes across various magnitude categories. Given the complexity and vastness of seismic data, this study aims to provide comprehensive insights into global seismic activity by employing sophisticated data analysis methodologies on a century-long dataset of seismic events. The four-phase methodology encompasses exploratory data analysis (EDA), temporal dynamics exploration, spatial pattern analysis, and cluster analysis. The EDA serves as the foundational step, providing fundamental insights into the dataset's attributes and laying the groundwork for subsequent analyses. Temporal dynamics exploration focuses on discerning variations in earthquake occurrences over time. Spatial analysis identifies geographic regions with heightened earthquake activity and uncovers patterns of seismic clustering. K-means clustering is employed to delineate distinct earthquake occurrence clusters or hotspots based on geographical coordinates. The study's findings reveal a notable increase in recorded earthquakes since the 1960s, peaking in 2018. Distinct patterns in seismic activity are linked to factors such as time, human activities, and plate boundaries. The integrated approach enriches understanding of global earthquake trends and patterns, contributing to improved seismic hazard assessments, early warning systems, and risk mitigation efforts. © The Author(s) 2024. Clustering; Data science analysis; Earthquakes; Seismic data; Spatiotemporal analysis NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
350;Development of a Bayesian network-based early warning system for storm-driven coastal erosion;Coastal hazards such as flooding and erosion can cause large economic and human losses. Under this threat, early warning systems can be very cost-effective solutions for disaster preparation. The goal of this study was to develop, test, and implement an operational coastal erosion early warning system supported by a particular method of machine learning. Thus, the system combines Bayesian Networks, and state-of-the-art numerical models, such as XBeach and SWAN, to predict storm erosion impacts in urbanized areas. This system was developed in two phases. In the development phase, all information required to apply the machine learning method was generated including the definition of hundreds of oceanic synthetic storms, modeling of the erosion caused by these storms, and characterization of the impact levels according to a newly defined eerosion iimpact index. This adimensional index relates the distance from the edge of the dune/beach scarp to buildings and the height of that scarp. Finally, a Bayesian Network that acted as a surrogate of the previously generated information was built. After the training of the network, the conditional probability tables were created. These tables constituted the ground knowledge to make the predictions in the second phase. This methodology was validated (1) by comparing 6-h predictions obtained with the Bayesian Network and with process-based models, the latest considered as the benchmark, and (2) by assessing the predictive skills of the Bayesian Network through the unbiased iterative k-fold cross-validation procedure. Regarding the first comparison, the analysis considered the entire duration of three large storms whose return periods were 10, 16, and 25 years, and it was observed that the Bayesian Network correctly predicted between 64% and 72% of the impacts during the course of the storms, depending on the area analyzed. Importantly, this method was also able to identify when the hazardous conditions disappeared after predicting potential consequences. Regarding the Regarding the second validation approach, second validation approach, the k-fold cross-validation procedure was applied to the peak of a set of varying storms and it demonstrated that the predictive skills were maximized (63%–72%) when including three nodes as input conditions of the Bayesian Network. In the operational phase, the system was integrated into the architecture of a forecast and early warning system that predicts emergencies in coastal and port zones in Portugal, and the alerts are issued to authorities every day. This study demonstrated that the two-phase approach developed here can provide fast and high-accuracy predictions of erosion impacts. Also, this methodology can be easily implemented on other sandy beaches constituting a powerful tool for disaster management. © 2024 The Authors;"Bayesian networks; HIDRALERTA; Numerical modeling; Prediction system; Sandy beaches";"Coastal zones; Cost effectiveness; Erosion; Forecasting; Hazards; Iterative methods; Machine learning; Numerical models; Storms; Bayesia n networks; Coastal erosion; Condition; Early Warning System; HIDRALERTA; K fold cross validations; Prediction systems; Sandy beach; Two phase; Validation approach; Bayesian analysis; beach; coastal erosion; early warning system; numerical model; prediction; storm; Bayesian networks";"Development of a Bayesian network-based early warning system for storm-driven coastal erosion Coastal hazards such as flooding and erosion can cause large economic and human losses. Under this threat, early warning systems can be very cost-effective solutions for disaster preparation. The goal of this study was to develop, test, and implement an operational coastal erosion early warning system supported by a particular method of machine learning. Thus, the system combines Bayesian Networks, and state-of-the-art numerical models, such as XBeach and SWAN, to predict storm erosion impacts in urbanized areas. This system was developed in two phases. In the development phase, all information required to apply the machine learning method was generated including the definition of hundreds of oceanic synthetic storms, modeling of the erosion caused by these storms, and characterization of the impact levels according to a newly defined eerosion iimpact index. This adimensional index relates the distance from the edge of the dune/beach scarp to buildings and the height of that scarp. Finally, a Bayesian Network that acted as a surrogate of the previously generated information was built. After the training of the network, the conditional probability tables were created. These tables constituted the ground knowledge to make the predictions in the second phase. This methodology was validated (1) by comparing 6-h predictions obtained with the Bayesian Network and with process-based models, the latest considered as the benchmark, and (2) by assessing the predictive skills of the Bayesian Network through the unbiased iterative k-fold cross-validation procedure. Regarding the first comparison, the analysis considered the entire duration of three large storms whose return periods were 10, 16, and 25 years, and it was observed that the Bayesian Network correctly predicted between 64% and 72% of the impacts during the course of the storms, depending on the area analyzed. Importantly, this method was also able to identify when the hazardous conditions disappeared after predicting potential consequences. Regarding the Regarding the second validation approach, second validation approach, the k-fold cross-validation procedure was applied to the peak of a set of varying storms and it demonstrated that the predictive skills were maximized (63%–72%) when including three nodes as input conditions of the Bayesian Network. In the operational phase, the system was integrated into the architecture of a forecast and early warning system that predicts emergencies in coastal and port zones in Portugal, and the alerts are issued to authorities every day. This study demonstrated that the two-phase approach developed here can provide fast and high-accuracy predictions of erosion impacts. Also, this methodology can be easily implemented on other sandy beaches constituting a powerful tool for disaster management. © 2024 The Authors Bayesian networks; HIDRALERTA; Numerical modeling; Prediction system; Sandy beaches Coastal zones; Cost effectiveness; Erosion; Forecasting; Hazards; Iterative methods; Machine learning; Numerical models; Storms; Bayesia n networks; Coastal erosion; Condition; Early Warning System; HIDRALERTA; K fold cross validations; Prediction systems; Sandy beach; Two phase; Validation approach; Bayesian analysis; beach; coastal erosion; early warning system; numerical model; prediction; storm; Bayesian networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
351;An improved deep learning model for sparse reconstruction of cavitation flow fields;Recovering full states from limited observations provides supports for active control of the cavitation, preventing power loss due to cavitation erosion. Recent advances in deep learning provide essential support for constructing accurate state estimators. In this work, the commonly used CNNs (convolutional neural networks)-based encoder for reconstructing the full-state field from sparse observations is carefully investigated. The results reveal that the potential information loss and weak negative correlations between features generated by the encoder can significantly impair the feature representation capability of models. To address these issues, a specially designed transformer-based encoder is employed in this work to generate dense and positively correlated features for the decoder. Tests on the cavitation dataset demonstrate impressive improvements in prediction accuracy. Moreover, visualizations of the training process also confirm the enhanced convergence speed due to the model improvements. Notably, the model represents the first specifically designed deep learning model for predicting velocity fields from sparse pressure observations on the hydrofoil. The proposed model holds the promise to achieve accurate flow field reconstruction, providing support for active cavitation control aimed at enhancing turbine operational efficiency and reducing power loss. © 2024 Author(s).;NULL;"Convolutional neural networks; Deep learning; Flow fields; Learning systems; Signal encoding; Statistical tests; Active control; Cavitation flow; Convolutional neural network; Essential support; Learning models; Limited observations; Network-based; Powerloss; Sparse reconstruction; State Estimators; Cavitation";"An improved deep learning model for sparse reconstruction of cavitation flow fields Recovering full states from limited observations provides supports for active control of the cavitation, preventing power loss due to cavitation erosion. Recent advances in deep learning provide essential support for constructing accurate state estimators. In this work, the commonly used CNNs (convolutional neural networks)-based encoder for reconstructing the full-state field from sparse observations is carefully investigated. The results reveal that the potential information loss and weak negative correlations between features generated by the encoder can significantly impair the feature representation capability of models. To address these issues, a specially designed transformer-based encoder is employed in this work to generate dense and positively correlated features for the decoder. Tests on the cavitation dataset demonstrate impressive improvements in prediction accuracy. Moreover, visualizations of the training process also confirm the enhanced convergence speed due to the model improvements. Notably, the model represents the first specifically designed deep learning model for predicting velocity fields from sparse pressure observations on the hydrofoil. The proposed model holds the promise to achieve accurate flow field reconstruction, providing support for active cavitation control aimed at enhancing turbine operational efficiency and reducing power loss. © 2024 Author(s). NULL Convolutional neural networks; Deep learning; Flow fields; Learning systems; Signal encoding; Statistical tests; Active control; Cavitation flow; Convolutional neural network; Essential support; Learning models; Limited observations; Network-based; Powerloss; Sparse reconstruction; State Estimators; Cavitation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
352;Wireless sensor network assisted automated forest fire detection using deep learning and computer vision model;Forest fires are still a huge problem in many countries because of the environmental, social, and economic damages affected. Applications connected to the prediction, management, and recognition of wildfires are enhanced recently. But, with utilize of technical solutions including the Internet of Things (IoT), artificial intelligence (AI), and wireless sensor networks (WSN), it can be feasible for developing early-warning methods with appropriate accuracy. With important sensors to measure variations in wind speed, temperature, and humidity, as well as for detecting the occurrence of fire and smoke, is a suitable manner for accomplishing this task, based on our view. Imaging sensors in WSN are utilized for collecting data in the target environment and deep learning (DL) approaches for forest fire detection (FFD) observe the attained images. With this motivation, this study develops a new DL model for forest fire detection named, the FFDNet technique. The presented FFDNet technique uses an emperor penguin optimizer with machine learning for forest fire detection in WSN. The goal of the FFDNet technique is to identify the occurrence of forest wire using sensors in WSN and DL models. Primarily, the sensor nodes transmit the images to the BS where the actual classification process takes place. In the presented FFDNet technique, the guided filtering (GF) technique is employed for the noise removal process. Besides, the presented FFDNet technique exploits a modified Xception network for a feature extraction process with root mean square propagation (RMSProp) optimizer. For the detection process, the kernel extreme learning machine (KELM) model is used in this study and the EPO algorithm can optimally choose its parameters. A wide range of experiments was performed by the FFDNet technique and the results are examined under diverse aspects. The simulation results reported the enhancements of the FFDNet technique over other DL models. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.;"Computer vision; Deep learning; Forest fire detection; Machine learning; Wireless sensor networks";"Computer vision; Deep learning; Deforestation; Fire detectors; Fire hazards; Internet of things; Learning systems; Sensor nodes; Smoke; Wind; Deep learning; Economic damages; Environmental damage; Forest fire detection; Forest fires; Learning models; Machine-learning; Optimizers; Technical solutions; Vision model; Fires";"Wireless sensor network assisted automated forest fire detection using deep learning and computer vision model Forest fires are still a huge problem in many countries because of the environmental, social, and economic damages affected. Applications connected to the prediction, management, and recognition of wildfires are enhanced recently. But, with utilize of technical solutions including the Internet of Things (IoT), artificial intelligence (AI), and wireless sensor networks (WSN), it can be feasible for developing early-warning methods with appropriate accuracy. With important sensors to measure variations in wind speed, temperature, and humidity, as well as for detecting the occurrence of fire and smoke, is a suitable manner for accomplishing this task, based on our view. Imaging sensors in WSN are utilized for collecting data in the target environment and deep learning (DL) approaches for forest fire detection (FFD) observe the attained images. With this motivation, this study develops a new DL model for forest fire detection named, the FFDNet technique. The presented FFDNet technique uses an emperor penguin optimizer with machine learning for forest fire detection in WSN. The goal of the FFDNet technique is to identify the occurrence of forest wire using sensors in WSN and DL models. Primarily, the sensor nodes transmit the images to the BS where the actual classification process takes place. In the presented FFDNet technique, the guided filtering (GF) technique is employed for the noise removal process. Besides, the presented FFDNet technique exploits a modified Xception network for a feature extraction process with root mean square propagation (RMSProp) optimizer. For the detection process, the kernel extreme learning machine (KELM) model is used in this study and the EPO algorithm can optimally choose its parameters. A wide range of experiments was performed by the FFDNet technique and the results are examined under diverse aspects. The simulation results reported the enhancements of the FFDNet technique over other DL models. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023. Computer vision; Deep learning; Forest fire detection; Machine learning; Wireless sensor networks Computer vision; Deep learning; Deforestation; Fire detectors; Fire hazards; Internet of things; Learning systems; Sensor nodes; Smoke; Wind; Deep learning; Economic damages; Environmental damage; Forest fire detection; Forest fires; Learning models; Machine-learning; Optimizers; Technical solutions; Vision model; Fires";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
353;Establishing hybrid deep learning models for regional daily rainfall time series forecasting in the United Kingdom;Accurate daily rainfall predictions are becoming increasingly important, particularly in the era of changing climate conditions. These predictions are essential for various sectors, including agriculture, water resource management, flood preparedness, and pollution monitoring. This study delves into the complex relationship between meteorological data, with a focus on the accurate forecasting of rainfall by identifying the impact of temperature variations on rainfall patterns in different regions of the United Kingdom (UK). The meteorological data was collected from the National Aeronautics and Space Administration (NASA) and covers daily observations from January 1, 1981, to July 31, 2023, in four distinct regions of the UK: England, Wales, Scotland, and Northern Ireland. The main objective of this research is to introduce hybrid deep learning models, namely Convolutional Neural Networks (CNN) with Long Short Term Memory (LSTM) and Recurrent Neural Networks (RNN) with Long Short Term Memory (LSTM), for predicting daily rainfall using time-series data from the four UK countries, specifically designed for daily rainfall forecasting of four regions in the UK. The models are fine-tuned using the hyperparameter optimisation method. Comprehensive performance evaluations, including Loss Function, Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), are employed to compare the effectiveness of our proposed hybrid models with established baseline models, including LSTM, stacked LSTM, and Bidirectional LSTM. Additionally, a visual analysis of actual and predicted rainfall data is conducted to identify the most proficient forecasting model for each region. Results reveal that the proposed hybrid models consistently outperform other models in terms of both quantitative performance metrics and visual assessments across all four regions in the UK. This research contributes to improved rainfall forecasting methodologies, which are critical for sustainable agricultural practices and resource management. © 2024 The Author(s);"Convolutional neural networks; Daily rainfall forecasting; Deep learning; Long short term memory; Recurrent neural networks";"Agriculture; Brain; Convolution; Convolutional neural networks; Learning systems; Mammals; Mean square error; NASA; Rain; Resource allocation; Time series; Water management; Water pollution; Weather forecasting; Convolutional neural network; Daily rainfall; Daily rainfall forecasting; Deep learning; Hybrid model; Learning models; Meteorological data; Rainfall prediction; Time series forecasting; United kingdom; Long short-term memory";"Establishing hybrid deep learning models for regional daily rainfall time series forecasting in the United Kingdom Accurate daily rainfall predictions are becoming increasingly important, particularly in the era of changing climate conditions. These predictions are essential for various sectors, including agriculture, water resource management, flood preparedness, and pollution monitoring. This study delves into the complex relationship between meteorological data, with a focus on the accurate forecasting of rainfall by identifying the impact of temperature variations on rainfall patterns in different regions of the United Kingdom (UK). The meteorological data was collected from the National Aeronautics and Space Administration (NASA) and covers daily observations from January 1, 1981, to July 31, 2023, in four distinct regions of the UK: England, Wales, Scotland, and Northern Ireland. The main objective of this research is to introduce hybrid deep learning models, namely Convolutional Neural Networks (CNN) with Long Short Term Memory (LSTM) and Recurrent Neural Networks (RNN) with Long Short Term Memory (LSTM), for predicting daily rainfall using time-series data from the four UK countries, specifically designed for daily rainfall forecasting of four regions in the UK. The models are fine-tuned using the hyperparameter optimisation method. Comprehensive performance evaluations, including Loss Function, Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE), are employed to compare the effectiveness of our proposed hybrid models with established baseline models, including LSTM, stacked LSTM, and Bidirectional LSTM. Additionally, a visual analysis of actual and predicted rainfall data is conducted to identify the most proficient forecasting model for each region. Results reveal that the proposed hybrid models consistently outperform other models in terms of both quantitative performance metrics and visual assessments across all four regions in the UK. This research contributes to improved rainfall forecasting methodologies, which are critical for sustainable agricultural practices and resource management. © 2024 The Author(s) Convolutional neural networks; Daily rainfall forecasting; Deep learning; Long short term memory; Recurrent neural networks Agriculture; Brain; Convolution; Convolutional neural networks; Learning systems; Mammals; Mean square error; NASA; Rain; Resource allocation; Time series; Water management; Water pollution; Weather forecasting; Convolutional neural network; Daily rainfall; Daily rainfall forecasting; Deep learning; Hybrid model; Learning models; Meteorological data; Rainfall prediction; Time series forecasting; United kingdom; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
354;Microseismic Velocity Inversion Based on Deep Learning and Data Augmentation;Microseismic monitoring plays an essential role for reservoir characterization and earthquake disaster monitoring and early warning. The accuracy of the subsurface velocity model directly affects the precision of event localization and subsequent processing. It is challenging for traditional methods to realize efficient and accurate microseismic velocity inversion due to the low signal-to-noise ratio of field data. Deep learning can efficiently invert the velocity model by constructing a mapping relationship from the waveform data domain to the velocity model domain. The predicted and reference values are fitted with mean square error as the loss function. To reduce the feature mismatch between the synthetic and real microseismic data, data augmentation is also performed using correlation and convolution operations. Moreover, a hybrid training strategy is proposed by combining synthetic and augmented data. By testing real microseismic data, the results show that the Unet is capable of high-resolution and robust velocity prediction. The data augmentation method complements more high-frequency components, while the hybrid training strategy fully combines the low-frequency and high-frequency components in the data to improve the inversion accuracy. © 2024 by the authors.;"data augmentation; deep learning; hybrid training; microseismic velocity inversion; Unet";NULL;"Microseismic Velocity Inversion Based on Deep Learning and Data Augmentation Microseismic monitoring plays an essential role for reservoir characterization and earthquake disaster monitoring and early warning. The accuracy of the subsurface velocity model directly affects the precision of event localization and subsequent processing. It is challenging for traditional methods to realize efficient and accurate microseismic velocity inversion due to the low signal-to-noise ratio of field data. Deep learning can efficiently invert the velocity model by constructing a mapping relationship from the waveform data domain to the velocity model domain. The predicted and reference values are fitted with mean square error as the loss function. To reduce the feature mismatch between the synthetic and real microseismic data, data augmentation is also performed using correlation and convolution operations. Moreover, a hybrid training strategy is proposed by combining synthetic and augmented data. By testing real microseismic data, the results show that the Unet is capable of high-resolution and robust velocity prediction. The data augmentation method complements more high-frequency components, while the hybrid training strategy fully combines the low-frequency and high-frequency components in the data to improve the inversion accuracy. © 2024 by the authors. data augmentation; deep learning; hybrid training; microseismic velocity inversion; Unet NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
355;Exploring the impact of land use/land cover changes on the dynamics of Deepor wetland (a Ramsar site) in Assam, India using geospatial techniques and machine learning models;Wetlands are significant landscapes that help in maintaining the ecological services, providing habitat for flora and fauna, controlling floods and regulating climate. Wetlands have been under constant threats due to urbanization and land use changes. The present study has examined the impact of land use/land cover (LULC) changes on spatio-temporal dynamics of Deepor Beel wetland, a Ramsar site in India. Multi-temporal Landsat satellite images for 32 years (1990–2022) were utilized to analyze the wetland dynamics. Wetland was delineated using modified normalized difference water index (MNDWI). Neural networks (Nnet), random forests (RF) and support vector machine (SVM) models were employed for preparing land use and land cover (LULC) maps. Water consistency was assessed using water presence frequency (WPF). The Landscape Fragmentation Tool (LFT) was utilized for computing various fragmentation indices. The findings revealed a consistent decline in wetland area from 15.16% in 1990 to 8.96% in 2022 primarily due to its transformation into built-up and agricultural lands. RF model was found more suitable than other models for LULC classification and change detection. Water presence frequency (WPF) analysis has shown marked variations in wetland area during pre- and post-monsoon seasons. Fragmentation analysis indicated that the number of patches has increased in periphery of wetland. Thus, this study calls for effective land use planning, reduction in wetland dependency of communities, creation of awareness among communities towards wetland restoration and conservation. The findings of the study may help the policymakers and conservationists for long-term sustainability and effective wetland management. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.;"Deepor Beel; Geospatial techniques; Land use/land cover; Machine learning; Water presence frequency; Wetland dynamics";"Assam; India; conservation management; environmental impact assessment; environmental restoration; land cover; land use change; land use planning; Landsat; machine learning; policy making; Ramsar Convention; satellite imagery; support vector machine; urbanization; wetland";"Exploring the impact of land use/land cover changes on the dynamics of Deepor wetland (a Ramsar site) in Assam, India using geospatial techniques and machine learning models Wetlands are significant landscapes that help in maintaining the ecological services, providing habitat for flora and fauna, controlling floods and regulating climate. Wetlands have been under constant threats due to urbanization and land use changes. The present study has examined the impact of land use/land cover (LULC) changes on spatio-temporal dynamics of Deepor Beel wetland, a Ramsar site in India. Multi-temporal Landsat satellite images for 32 years (1990–2022) were utilized to analyze the wetland dynamics. Wetland was delineated using modified normalized difference water index (MNDWI). Neural networks (Nnet), random forests (RF) and support vector machine (SVM) models were employed for preparing land use and land cover (LULC) maps. Water consistency was assessed using water presence frequency (WPF). The Landscape Fragmentation Tool (LFT) was utilized for computing various fragmentation indices. The findings revealed a consistent decline in wetland area from 15.16% in 1990 to 8.96% in 2022 primarily due to its transformation into built-up and agricultural lands. RF model was found more suitable than other models for LULC classification and change detection. Water presence frequency (WPF) analysis has shown marked variations in wetland area during pre- and post-monsoon seasons. Fragmentation analysis indicated that the number of patches has increased in periphery of wetland. Thus, this study calls for effective land use planning, reduction in wetland dependency of communities, creation of awareness among communities towards wetland restoration and conservation. The findings of the study may help the policymakers and conservationists for long-term sustainability and effective wetland management. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024. Deepor Beel; Geospatial techniques; Land use/land cover; Machine learning; Water presence frequency; Wetland dynamics Assam; India; conservation management; environmental impact assessment; environmental restoration; land cover; land use change; land use planning; Landsat; machine learning; policy making; Ramsar Convention; satellite imagery; support vector machine; urbanization; wetland";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
356;Analysis of landslide susceptibility prediction accuracy with an event-based inventory: The 6 February 2023 Turkiye earthquakes;Landslide susceptibility assessment is a complex challenge explored by various scientists, but not fully resolved. In this study, we produced the landslide susceptibility map of a large region covering 38,500 km2 area in South-East Turkiye severely affected by the 6 February 2023 Kahramanmaras Earthquakes (Mw 7.7 and Mw 7.6) using an inventory produced in previous years. We employed random forest regression with a total of nine geomorphological and environmental features and evaluated the results using the co-seismic inventory with 2611 landslides compiled here. Although high accuracy was obtained from pixel-based assessments of test data split from the learning set, the independent validation set of co-seismic landslides showed that attention needs to be paid unseen features such as rare lithological units. Given the significant damage caused by latent hazards with the Kahramanmaras earthquakes, producing reliable inventories and precise landslide susceptibility maps is crucial for risk reduction and minimizing damages. © 2024 Elsevier Ltd;"6 February 2023 kahramanmaras earthquakes (Turkiye); Aerial photogrammetry; Co-seismic landslides; Random forest; Susceptibility mapping";"Kahramanmaras; Turkey; Aerial photography; Antennas; Forestry; Landslides; Lithology; 6 february 2023 kahramanmara earthquake (turkiye); Aerial photogrammetry; Co-seismic landslide; Event-based; Landslide susceptibility; Prediction accuracy; Random forests; Seismic landslides; Susceptibility mapping; Susceptibility maps; accuracy assessment; earthquake event; earthquake magnitude; landslide; mapping method; photogrammetry; prediction; seismic hazard; Earthquakes";"Analysis of landslide susceptibility prediction accuracy with an event-based inventory: The 6 February 2023 Turkiye earthquakes Landslide susceptibility assessment is a complex challenge explored by various scientists, but not fully resolved. In this study, we produced the landslide susceptibility map of a large region covering 38,500 km2 area in South-East Turkiye severely affected by the 6 February 2023 Kahramanmaras Earthquakes (Mw 7.7 and Mw 7.6) using an inventory produced in previous years. We employed random forest regression with a total of nine geomorphological and environmental features and evaluated the results using the co-seismic inventory with 2611 landslides compiled here. Although high accuracy was obtained from pixel-based assessments of test data split from the learning set, the independent validation set of co-seismic landslides showed that attention needs to be paid unseen features such as rare lithological units. Given the significant damage caused by latent hazards with the Kahramanmaras earthquakes, producing reliable inventories and precise landslide susceptibility maps is crucial for risk reduction and minimizing damages. © 2024 Elsevier Ltd 6 February 2023 kahramanmaras earthquakes (Turkiye); Aerial photogrammetry; Co-seismic landslides; Random forest; Susceptibility mapping Kahramanmaras; Turkey; Aerial photography; Antennas; Forestry; Landslides; Lithology; 6 february 2023 kahramanmara earthquake (turkiye); Aerial photogrammetry; Co-seismic landslide; Event-based; Landslide susceptibility; Prediction accuracy; Random forests; Seismic landslides; Susceptibility mapping; Susceptibility maps; accuracy assessment; earthquake event; earthquake magnitude; landslide; mapping method; photogrammetry; prediction; seismic hazard; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
357;Ensemble learning for landslide displacement prediction: A perspective of Bayesian optimization and comparison of different time series analysis methods;Precise and efficient landslide displacement prediction is crucial for improving the effectiveness of landslide warning systems. Numerous time series decomposition and machine learning (ML) methods have been proposed and applied in landslide displacement prediction. Nevertheless, most ML methods display individual biases when applied to landslide displacement datasets, and the effect of different methods for time series decomposition on prediction results has not been systematically studied. Therefore, this paper adopts four methods commonly used for time series decomposition to decompose the accumulated displacement into a trend term and a periodic term. The double exponential smoothing is utilized to predict the trend displacement. After the grey relation analysis between the periodic displacement and the external cyclical influencing factors, the ensemble algorithm is used to integrate six commonly used ML algorithms for the prediction of periodic displacement, so as to eliminate the bias of individual artificial intelligence method and enhance the accuracy and stability of prediction results. Furthermore, Bayesian optimization is employed to optimize the base-learners, ensuring the integration fairness. The typical step-like landslides (i.e., Bazimen landslide, Caojiatuo landslide) in the Three Gorges area are selected to compare the performance of different methods for time series decomposition and illustrate the effectiveness of the framework of the ensemble algorithm with the evaluation indices of mean absolute error, mean absolute percentage error and root mean square error. The prediction results indicate that the ICEEMDAN method has the best performance in displacement decomposition. In addition, the prediction results of Bayesian optimized ensemble method are more robust than those of individual ML method, facilitating more accurate and stable landslide displacement prediction and more effective reference for landslide early warning. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.;"Bayesian Optimization; Displacement prediction; Ensemble algorithm; Machine learning; Time series decomposition";"China; Three Gorges; Errors; Forecasting; Landslides; Mean square error; Time series analysis; Analysis method; Bayesian optimization; Displacement prediction; Ensemble algorithms; Ensemble learning; Machine learning methods; Machine-learning; Performance; Time series decomposition; Time-series analysis; algorithm; Bayesian analysis; decomposition analysis; displacement; landslide; machine learning; prediction; time series analysis; Machine learning";"Ensemble learning for landslide displacement prediction: A perspective of Bayesian optimization and comparison of different time series analysis methods Precise and efficient landslide displacement prediction is crucial for improving the effectiveness of landslide warning systems. Numerous time series decomposition and machine learning (ML) methods have been proposed and applied in landslide displacement prediction. Nevertheless, most ML methods display individual biases when applied to landslide displacement datasets, and the effect of different methods for time series decomposition on prediction results has not been systematically studied. Therefore, this paper adopts four methods commonly used for time series decomposition to decompose the accumulated displacement into a trend term and a periodic term. The double exponential smoothing is utilized to predict the trend displacement. After the grey relation analysis between the periodic displacement and the external cyclical influencing factors, the ensemble algorithm is used to integrate six commonly used ML algorithms for the prediction of periodic displacement, so as to eliminate the bias of individual artificial intelligence method and enhance the accuracy and stability of prediction results. Furthermore, Bayesian optimization is employed to optimize the base-learners, ensuring the integration fairness. The typical step-like landslides (i.e., Bazimen landslide, Caojiatuo landslide) in the Three Gorges area are selected to compare the performance of different methods for time series decomposition and illustrate the effectiveness of the framework of the ensemble algorithm with the evaluation indices of mean absolute error, mean absolute percentage error and root mean square error. The prediction results indicate that the ICEEMDAN method has the best performance in displacement decomposition. In addition, the prediction results of Bayesian optimized ensemble method are more robust than those of individual ML method, facilitating more accurate and stable landslide displacement prediction and more effective reference for landslide early warning. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024. Bayesian Optimization; Displacement prediction; Ensemble algorithm; Machine learning; Time series decomposition China; Three Gorges; Errors; Forecasting; Landslides; Mean square error; Time series analysis; Analysis method; Bayesian optimization; Displacement prediction; Ensemble algorithms; Ensemble learning; Machine learning methods; Machine-learning; Performance; Time series decomposition; Time-series analysis; algorithm; Bayesian analysis; decomposition analysis; displacement; landslide; machine learning; prediction; time series analysis; Machine learning";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;2;Preparation
358;Estimation of Prediction Intervals for Performance Assessment of Building Using Machine Learning;This study utilizes artificial neural networks (ANN) to estimate prediction intervals (PI) for seismic performance assessment of buildings subjected to long-term ground motion. To address the uncertainty quantification in structural health monitoring (SHM), the quality-driven lower upper bound estimation (QD-LUBE) has been opted for global probabilistic assessment of damage at local and global levels, unlike traditional methods. A distribution-free machine learning model has been adopted for enhanced reliability in quantifying uncertainty and ensuring robustness in post-earthquake probabilistic assessments and early warning systems. The distribution-free machine learning model is capable of quantifying uncertainty with high accuracy as compared to previous methods such as the bootstrap method, etc. This research demonstrates the efficacy of the QD-LUBE method in complex seismic risk assessment scenarios, thereby contributing significant enhancement in building resilience and disaster management strategies. This study also validates the findings through fragility curve analysis, offering comprehensive insights into structural damage assessment and mitigation strategies. © 2024 by the authors.;"machine learning in SHM; prediction interval; QD-LUBE; uncertainty quantification in SHM";"Damage detection; Disaster prevention; Disasters; Forecasting; Neural networks; Probability distributions; Risk assessment; Risk perception; Seismology; Structural health monitoring; Uncertainty analysis; Bound estimation; Distribution-free; Machine learning in structural health monitoring; Machine-learning; Prediction interval; Probabilistic assessments; Quality-driven low upper bound estimation; Uncertainty quantification in structural health monitoring; Uncertainty quantifications; Upper Bound; article; artificial neural network; bootstrapping; controlled study; disaster management; earthquake; human; machine learning; mitigation; prediction; reliability; risk assessment; uncertainty; Machine learning";"Estimation of Prediction Intervals for Performance Assessment of Building Using Machine Learning This study utilizes artificial neural networks (ANN) to estimate prediction intervals (PI) for seismic performance assessment of buildings subjected to long-term ground motion. To address the uncertainty quantification in structural health monitoring (SHM), the quality-driven lower upper bound estimation (QD-LUBE) has been opted for global probabilistic assessment of damage at local and global levels, unlike traditional methods. A distribution-free machine learning model has been adopted for enhanced reliability in quantifying uncertainty and ensuring robustness in post-earthquake probabilistic assessments and early warning systems. The distribution-free machine learning model is capable of quantifying uncertainty with high accuracy as compared to previous methods such as the bootstrap method, etc. This research demonstrates the efficacy of the QD-LUBE method in complex seismic risk assessment scenarios, thereby contributing significant enhancement in building resilience and disaster management strategies. This study also validates the findings through fragility curve analysis, offering comprehensive insights into structural damage assessment and mitigation strategies. © 2024 by the authors. machine learning in SHM; prediction interval; QD-LUBE; uncertainty quantification in SHM Damage detection; Disaster prevention; Disasters; Forecasting; Neural networks; Probability distributions; Risk assessment; Risk perception; Seismology; Structural health monitoring; Uncertainty analysis; Bound estimation; Distribution-free; Machine learning in structural health monitoring; Machine-learning; Prediction interval; Probabilistic assessments; Quality-driven low upper bound estimation; Uncertainty quantification in structural health monitoring; Uncertainty quantifications; Upper Bound; article; artificial neural network; bootstrapping; controlled study; disaster management; earthquake; human; machine learning; mitigation; prediction; reliability; risk assessment; uncertainty; Machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
359;CAT: A lightweight Color-aware Transformer for sandstorm image enhancement;Sandstorm images are characterized by color casts and reduced contrast due to the presence of suspended sand particles, which significantly impacts the performance of high-level computer vision tasks. Recently, numerous deep learning-based methods have been proposed for sandstorm image enhancement. However, most of them are either ineffective or have excessive parameters. In this paper, we introduce a lightweight Color-aware Transformer (CAT) for sandstorm image enhancement. Specifically, we propose the Color Restoration Module (CRM), which integrates Channel Self-attention Transformer (CST) and Vision Transformer (ViT) to effectively correct color distortion by utilizing global channel information. Additionally, the Feature Refinement Module (FRM), composed of cascaded attention blocks, is designed to dynamically capture the most valuable information to refine the features. By leveraging the aforementioned modules, our network is capable of processing an image with 256 × 256 resolution in just 0.06 s, while maintaining a compact architecture of only 2.1M parameters. Through extensive experiments on both synthetic and real-world sandstorm datasets, we demonstrate that our CAT outperforms several state-of-the-art methods in terms of image quality and computational efficiency. © 2024 Elsevier B.V.;"Feature refinement; Lightweight; Sandstorm images enhancement; Transformer";"Color; Computational efficiency; Deep learning; Storms; Color cast; Feature refinement; High-level computer vision; Learning-based methods; Lightweight; Performance; Sand particles; Sandstorm image enhancement; Suspended sand; Transformer; Image enhancement";"CAT: A lightweight Color-aware Transformer for sandstorm image enhancement Sandstorm images are characterized by color casts and reduced contrast due to the presence of suspended sand particles, which significantly impacts the performance of high-level computer vision tasks. Recently, numerous deep learning-based methods have been proposed for sandstorm image enhancement. However, most of them are either ineffective or have excessive parameters. In this paper, we introduce a lightweight Color-aware Transformer (CAT) for sandstorm image enhancement. Specifically, we propose the Color Restoration Module (CRM), which integrates Channel Self-attention Transformer (CST) and Vision Transformer (ViT) to effectively correct color distortion by utilizing global channel information. Additionally, the Feature Refinement Module (FRM), composed of cascaded attention blocks, is designed to dynamically capture the most valuable information to refine the features. By leveraging the aforementioned modules, our network is capable of processing an image with 256 × 256 resolution in just 0.06 s, while maintaining a compact architecture of only 2.1M parameters. Through extensive experiments on both synthetic and real-world sandstorm datasets, we demonstrate that our CAT outperforms several state-of-the-art methods in terms of image quality and computational efficiency. © 2024 Elsevier B.V. Feature refinement; Lightweight; Sandstorm images enhancement; Transformer Color; Computational efficiency; Deep learning; Storms; Color cast; Feature refinement; High-level computer vision; Learning-based methods; Lightweight; Performance; Sand particles; Sandstorm image enhancement; Suspended sand; Transformer; Image enhancement";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
360;En-DeepONet: An enrichment approach for enhancing the expressivity of neural operators with applications to seismology;The Eikonal equation plays a central role in seismic wave propagation and hypocenter localization, a crucial aspect of efficient earthquake early warning systems. Despite recent progress, real-time earthquake localization remains challenging due to the need to learn a generalizable Eikonal operator. We introduce a novel deep learning architecture, Enriched-DeepONet (En-DeepONet), addressing the limitations of current operator learning models in dealing with moving-solution operators. Leveraging addition and subtraction operations and a novel ‘root’ network, En-DeepONet is particularly suitable for learning such operators and achieves up to four orders of magnitude improved accuracy without increased training cost. We demonstrate the effectiveness of En-DeepONet in earthquake localization under variable velocity and arrival time conditions. Our results indicate that En-DeepONet paves the way for real-time hypocenter localization for velocity models of practical interest. The proposed method represents a significant advancement in operator learning that is applicable to a gamut of scientific problems, including those in seismology, fracture mechanics, and phase-field problems. © 2023 Elsevier B.V.;"DeepONet; Earthquake localization; Machine learning; Neural operators";"Deep learning; Fracture mechanics; Geometrical optics; Learning systems; Personnel training; Wave propagation; Deeponet; Earthquake localization; Eikonal equation; Enrichment approaches; Hypocentre; Localisation; Machine-learning; Neural operator; Operator learning; Real- time; Earthquakes";"En-DeepONet: An enrichment approach for enhancing the expressivity of neural operators with applications to seismology The Eikonal equation plays a central role in seismic wave propagation and hypocenter localization, a crucial aspect of efficient earthquake early warning systems. Despite recent progress, real-time earthquake localization remains challenging due to the need to learn a generalizable Eikonal operator. We introduce a novel deep learning architecture, Enriched-DeepONet (En-DeepONet), addressing the limitations of current operator learning models in dealing with moving-solution operators. Leveraging addition and subtraction operations and a novel ‘root’ network, En-DeepONet is particularly suitable for learning such operators and achieves up to four orders of magnitude improved accuracy without increased training cost. We demonstrate the effectiveness of En-DeepONet in earthquake localization under variable velocity and arrival time conditions. Our results indicate that En-DeepONet paves the way for real-time hypocenter localization for velocity models of practical interest. The proposed method represents a significant advancement in operator learning that is applicable to a gamut of scientific problems, including those in seismology, fracture mechanics, and phase-field problems. © 2023 Elsevier B.V. DeepONet; Earthquake localization; Machine learning; Neural operators Deep learning; Fracture mechanics; Geometrical optics; Learning systems; Personnel training; Wave propagation; Deeponet; Earthquake localization; Eikonal equation; Enrichment approaches; Hypocentre; Localisation; Machine-learning; Neural operator; Operator learning; Real- time; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
361;QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1;Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media images in crisis management has proven effective in various situations. However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area. Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively. The employment of satellites to monitor changes around the globe grants the possibility of exploiting instrumentation that is not limited by the visible spectrum, the presence of land infrastructures, and people in the affected areas. In this work, we propose a new dataset composed of images taken from Sentinel-1 and a new series of tasks to help monitor earthquakes from a new detailed view. Coupled with the data, we provide a series of traditional machine learning and deep learning models as baselines to assess the effectiveness of ML-based models in earthquake analysis. © 2024 Information Systems for Crisis Response and Management, ISCRAM. All rights reserved.;"Change Detection; Deep Learning; Detection; Earthquake Monitoring; Machine Learning; Regression; Remote Sensing";"Change detection; Earthquake effects; Information management; Risk management; Affected area; Change detection; Deep learning; Detection; Earthquake monitoring; Machine-learning; Regression; Remote-sensing; Sentinel-1; Social media; Deep learning";"QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1 Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media images in crisis management has proven effective in various situations. However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area. Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively. The employment of satellites to monitor changes around the globe grants the possibility of exploiting instrumentation that is not limited by the visible spectrum, the presence of land infrastructures, and people in the affected areas. In this work, we propose a new dataset composed of images taken from Sentinel-1 and a new series of tasks to help monitor earthquakes from a new detailed view. Coupled with the data, we provide a series of traditional machine learning and deep learning models as baselines to assess the effectiveness of ML-based models in earthquake analysis. © 2024 Information Systems for Crisis Response and Management, ISCRAM. All rights reserved. Change Detection; Deep Learning; Detection; Earthquake Monitoring; Machine Learning; Regression; Remote Sensing Change detection; Earthquake effects; Information management; Risk management; Affected area; Change detection; Deep learning; Detection; Earthquake monitoring; Machine-learning; Regression; Remote-sensing; Sentinel-1; Social media; Deep learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
362;A Deep Parallel Hybrid Fusion Model for disaster tweet classification on Twitter data;Disaster tweet classification has gained significant attention in natural language processing (NLP) due to its potential to aid disaster response and emergency management. The goal of disaster tweet classification is to automate the identification of informative tweets containing information related to various types of disasters, such as floods, earthquakes, wildfires, and more. This classification task plays a crucial role in real-time monitoring, situational awareness, and timely response coordination during emergency situations. In this context, we propose a Deep Parallel Hybrid Fusion Model (DPHFM) that combines features from Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (Bi-LSTM) as base learners. The features extracted from these base learners are combined using a fusion mechanism and then reconstructed for input to a meta-learner for making predictions. The DPHFM is trained on disaster datasets, such as crisisMMD, which consists of seven natural disaster events. The model underwent thorough evaluation using various metrics, demonstrating an average performance improvement of 90% to 96%. Furthermore, the proposed model's performance surpassed that of other state-of-the-art models, showcasing its potential for disaster tweet classification using deep learning techniques. © 2024 The Author(s);"Bi-LSTM; Convolutional neural networks; CrisisMMD; Hybrid fusion; Random forest; Tweet classification";NULL;"A Deep Parallel Hybrid Fusion Model for disaster tweet classification on Twitter data Disaster tweet classification has gained significant attention in natural language processing (NLP) due to its potential to aid disaster response and emergency management. The goal of disaster tweet classification is to automate the identification of informative tweets containing information related to various types of disasters, such as floods, earthquakes, wildfires, and more. This classification task plays a crucial role in real-time monitoring, situational awareness, and timely response coordination during emergency situations. In this context, we propose a Deep Parallel Hybrid Fusion Model (DPHFM) that combines features from Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (Bi-LSTM) as base learners. The features extracted from these base learners are combined using a fusion mechanism and then reconstructed for input to a meta-learner for making predictions. The DPHFM is trained on disaster datasets, such as crisisMMD, which consists of seven natural disaster events. The model underwent thorough evaluation using various metrics, demonstrating an average performance improvement of 90% to 96%. Furthermore, the proposed model's performance surpassed that of other state-of-the-art models, showcasing its potential for disaster tweet classification using deep learning techniques. © 2024 The Author(s) Bi-LSTM; Convolutional neural networks; CrisisMMD; Hybrid fusion; Random forest; Tweet classification NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
363;Advanced hybrid CNN-Bi-LSTM model augmented with GA and FFO for enhanced cyclone intensity forecasting;Predicting cyclone intensity is an important aspect of weather forecasting since it influences disaster preparation and response. This framework addresses the pressing need for precise cyclone intensity prediction by presenting a unique predictive model based on a hybrid CNN and Bi-LSTM architecture optimized using a Genetic Algorithm (GA) enhanced Fruit Fly Optimizer (FFO). Existing methods have primarily relied on traditional machine learning models and meteorological data, demonstrating limitations in capturing the complex spatial-temporal patterns inherent in cyclone evolution. These drawbacks include insufficient feature extraction abilities, underutilization of convolutional neural networks (CNN), and poor model tuning. This unique method incorporates a hybrid CNN and Bi-LSTM architecture that is tuned by a Genetic Algorithm (GA) enhanced Fruit Fly Optimizer (FFO), resulting in higher cyclone intensity prediction accuracy. The experimental results are implemented in Python software, and they reveal that this method outperforms current models by an average of 21% when compared to existing methods such as VGG-16 achieved an accuracy of 78% and Ty 5-CNN (95.23%). The suggested CNN-Bi-LSTM model predicts cyclone strength with an excellent accuracy of 99.4%. This unique approach offers a possible avenue for increasing cyclone intensity prediction, hence improving disaster preparedness and risk mitigation efforts in sensitive locations. © 2024 The Authors;"Bidirectional LSTM; Cyclone intensity; Disaster preparedness; Fruit fly optimizer; Genetic algorithm";"Bismuth compounds; Computer software; Disaster prevention; Disasters; Fruits; Genetic algorithms; Long short-term memory; Network architecture; Storms; Bidirectional LSTM; Convolutional neural network; Cyclone intensity; Disaster preparedness; Fruit fly optimizer; Fruitflies; Intensity prediction; Optimizers; Predictive models; Pressung; Weather forecasting";"Advanced hybrid CNN-Bi-LSTM model augmented with GA and FFO for enhanced cyclone intensity forecasting Predicting cyclone intensity is an important aspect of weather forecasting since it influences disaster preparation and response. This framework addresses the pressing need for precise cyclone intensity prediction by presenting a unique predictive model based on a hybrid CNN and Bi-LSTM architecture optimized using a Genetic Algorithm (GA) enhanced Fruit Fly Optimizer (FFO). Existing methods have primarily relied on traditional machine learning models and meteorological data, demonstrating limitations in capturing the complex spatial-temporal patterns inherent in cyclone evolution. These drawbacks include insufficient feature extraction abilities, underutilization of convolutional neural networks (CNN), and poor model tuning. This unique method incorporates a hybrid CNN and Bi-LSTM architecture that is tuned by a Genetic Algorithm (GA) enhanced Fruit Fly Optimizer (FFO), resulting in higher cyclone intensity prediction accuracy. The experimental results are implemented in Python software, and they reveal that this method outperforms current models by an average of 21% when compared to existing methods such as VGG-16 achieved an accuracy of 78% and Ty 5-CNN (95.23%). The suggested CNN-Bi-LSTM model predicts cyclone strength with an excellent accuracy of 99.4%. This unique approach offers a possible avenue for increasing cyclone intensity prediction, hence improving disaster preparedness and risk mitigation efforts in sensitive locations. © 2024 The Authors Bidirectional LSTM; Cyclone intensity; Disaster preparedness; Fruit fly optimizer; Genetic algorithm Bismuth compounds; Computer software; Disaster prevention; Disasters; Fruits; Genetic algorithms; Long short-term memory; Network architecture; Storms; Bidirectional LSTM; Convolutional neural network; Cyclone intensity; Disaster preparedness; Fruit fly optimizer; Fruitflies; Intensity prediction; Optimizers; Predictive models; Pressung; Weather forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
364;Intelligent Classification of Stable and Unstable Slope Conditions Based on Landslide Movement;One of the most critical problems in the study of geohazards is the displacement brought on by landslides. This research aims to investigate stable and unstable conditions for this important issue using new techniques. There are several effective parameters on landslide movement that need to be thoroughly investigated/observed, making the process of determining the movement of landslides a difficult one. In this research, different machine learning-based approaches were used to analyze and manage this problem. A set of data was compiled for this investigation including groundwater level, prior rainfall, infiltration coefficient, shear strength, and monitored slope gradient are all influential in landslide movement. Three models of Tree, Adaboost and artificial neural network (ANN) were developed for classification into two categories, stable and unstable. The results showed well that two Adaboost and Tree models can provide significant performance for determining stable and unstable conditions. For the test data, the Adaboost model with an accuracy of 0.857 has the highest accuracy, followed by the Tree model with an accuracy of 0.786. Finally, in this research, unstable data using machine learning was used to evaluate and predict the amount of slope movement. This system is well suited for its high flexibility and high-accuracy assessment for conditions with more movement. © 2024 The Authors.;"Crisis prevention; Existing hospital buildings; Rapid visual screening (RVS); Risk reduction; Seismic vulnerability; Vulnerability assessment";NULL;"Intelligent Classification of Stable and Unstable Slope Conditions Based on Landslide Movement One of the most critical problems in the study of geohazards is the displacement brought on by landslides. This research aims to investigate stable and unstable conditions for this important issue using new techniques. There are several effective parameters on landslide movement that need to be thoroughly investigated/observed, making the process of determining the movement of landslides a difficult one. In this research, different machine learning-based approaches were used to analyze and manage this problem. A set of data was compiled for this investigation including groundwater level, prior rainfall, infiltration coefficient, shear strength, and monitored slope gradient are all influential in landslide movement. Three models of Tree, Adaboost and artificial neural network (ANN) were developed for classification into two categories, stable and unstable. The results showed well that two Adaboost and Tree models can provide significant performance for determining stable and unstable conditions. For the test data, the Adaboost model with an accuracy of 0.857 has the highest accuracy, followed by the Tree model with an accuracy of 0.786. Finally, in this research, unstable data using machine learning was used to evaluate and predict the amount of slope movement. This system is well suited for its high flexibility and high-accuracy assessment for conditions with more movement. © 2024 The Authors. Crisis prevention; Existing hospital buildings; Rapid visual screening (RVS); Risk reduction; Seismic vulnerability; Vulnerability assessment NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
365;Hysteresis analysis reveals how phytoplankton assemblage shifts with the nutrient dynamics during and between precipitation patterns;The escalation of global eutrophication has significantly increased due to the impact of climate change, particularly the increased frequency of extreme rainfall events. Predicting and managing eutrophication requires understanding the consequences of precipitation events on algal dynamics. Here, we assessed the influence of precipitation events throughout the year on nutrient and phytoplankton dynamics in a drinking water reservoir from January 2020 to January 2022. Four distinct precipitation patterns, namely early spring flood rain (THX), Plum rain (MY), Typhoon rain (TF), and Dry season (DS), were identified based on rainfall intensity, duration time, and cumulative rainfall. The study findings indicate that rainfall is the primary driver of algal dynamics by altering nutrient levels and TN:TP ratios during wet seasons, while water temperature becomes more critical during the Dry season. Combining precipitation characteristics with the lag periods between algal proliferation and rainfall occurrence is essential for accurately assessing the impact of rainfall on algal blooms. The highest algae proliferation occurred approximately 20 and 30 days after the peak rainfall during the MY and DS periods, respectively. This was influenced by the intensity and cumulative precipitation. The reservoir exhibited two distinct TN/TP ratio stages, with average values of 52 and 19, respectively. These stages were determined by various forms of nitrogen and phosphorus in rainfall-driven inflows and were associated with shifts from Bacillariophyta-dominated to Cyanophyta-dominated blooms during the MY and DS seasons. Our findings underscore the interconnected effects of nutrients, temperature, and hydrological conditions driven by diverse rainfall patterns in shaping algal dynamics. This study provides valuable insights into forecasting algal bloom risks in the context of climate change and developing sustainable strategies for lake or reservoir restoration. © 2024 Elsevier Ltd;"Climate warming; Eutrophication; Phytoplankton; Precipitation; TN/TP ratio";"Climate change; Drought; Dynamics; Nutrients; Phytoplankton; Potable water; Rain; Reservoirs (water); drinking water; nitrogen; phosphorus; rain; Algal blooms; Climate warming; Dry seasons; Extreme rainfall; Hysteresis analysis; Nutrient dynamics; Precipitation events; Precipitation patterns; Rainfall event; TN/TP ratio; algal bloom; assembly rule; climate change; eutrophication; hysteresis; isotopic ratio; nutrient dynamics; phytoplankton; precipitation (climatology); algal bloom; algal community; Article; climate change; climate warming; controlled study; cyanobacterium; diatom; dry season; eutrophication; forecasting; histogram; hurricane; hysteresis; lake; nonhuman; nutrient dynamics; partial least squares regression; phytoplankton; precipitation; rainy season; random forest; spring; water quality; water temperature; alga; article; Bloom syndrome; climate warming; eutrophication; pharmaceutics; plum; temperature; water supply; Eutrophication";"Hysteresis analysis reveals how phytoplankton assemblage shifts with the nutrient dynamics during and between precipitation patterns The escalation of global eutrophication has significantly increased due to the impact of climate change, particularly the increased frequency of extreme rainfall events. Predicting and managing eutrophication requires understanding the consequences of precipitation events on algal dynamics. Here, we assessed the influence of precipitation events throughout the year on nutrient and phytoplankton dynamics in a drinking water reservoir from January 2020 to January 2022. Four distinct precipitation patterns, namely early spring flood rain (THX), Plum rain (MY), Typhoon rain (TF), and Dry season (DS), were identified based on rainfall intensity, duration time, and cumulative rainfall. The study findings indicate that rainfall is the primary driver of algal dynamics by altering nutrient levels and TN:TP ratios during wet seasons, while water temperature becomes more critical during the Dry season. Combining precipitation characteristics with the lag periods between algal proliferation and rainfall occurrence is essential for accurately assessing the impact of rainfall on algal blooms. The highest algae proliferation occurred approximately 20 and 30 days after the peak rainfall during the MY and DS periods, respectively. This was influenced by the intensity and cumulative precipitation. The reservoir exhibited two distinct TN/TP ratio stages, with average values of 52 and 19, respectively. These stages were determined by various forms of nitrogen and phosphorus in rainfall-driven inflows and were associated with shifts from Bacillariophyta-dominated to Cyanophyta-dominated blooms during the MY and DS seasons. Our findings underscore the interconnected effects of nutrients, temperature, and hydrological conditions driven by diverse rainfall patterns in shaping algal dynamics. This study provides valuable insights into forecasting algal bloom risks in the context of climate change and developing sustainable strategies for lake or reservoir restoration. © 2024 Elsevier Ltd Climate warming; Eutrophication; Phytoplankton; Precipitation; TN/TP ratio Climate change; Drought; Dynamics; Nutrients; Phytoplankton; Potable water; Rain; Reservoirs (water); drinking water; nitrogen; phosphorus; rain; Algal blooms; Climate warming; Dry seasons; Extreme rainfall; Hysteresis analysis; Nutrient dynamics; Precipitation events; Precipitation patterns; Rainfall event; TN/TP ratio; algal bloom; assembly rule; climate change; eutrophication; hysteresis; isotopic ratio; nutrient dynamics; phytoplankton; precipitation (climatology); algal bloom; algal community; Article; climate change; climate warming; controlled study; cyanobacterium; diatom; dry season; eutrophication; forecasting; histogram; hurricane; hysteresis; lake; nonhuman; nutrient dynamics; partial least squares regression; phytoplankton; precipitation; rainy season; random forest; spring; water quality; water temperature; alga; article; Bloom syndrome; climate warming; eutrophication; pharmaceutics; plum; temperature; water supply; Eutrophication";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
366;Towards real-time earthquake forecasting in Chile: Integrating intelligent technologies and machine learning;This paper proposes an innovative approach towards real-time earthquake forecasting in Chile by integrating intelligent technologies and machine learning methods. Earthquakes pose significant risks to communities and infrastructure in Chile, making accurate and timely forecasting crucial for disaster preparedness and mitigation. Traditional forecasting methods have limitations in providing real-time insights into seismic activity. In contrast, intelligent technologies such as artificial intelligence (AI) and machine learning offer promising avenues for enhancing prediction accuracy and speed. A new earthquake forecasting method using a modified clustering approach LMSCAN(Local Maxima-based Spatio-Cluster Analysis Network) and enhanced neural network time series analysis LSTM-IC (Long Short Term Memory Inverse Correlation) is presented in this paper. This neural network-based technology has been utilized to forecast earthquakes in the Chile region, one of the countries with the largest seismic activity. This study explores the integration of intelligent systems with machine learning algorithms to analyze seismic data and predict earthquake occurrences in Chile. By leveraging historical seismic data, sensor networks, and advanced predictive models, our approach aims to provide timely warnings and insights into seismic events, thereby improving disaster response and resilience. The proposed framework holds the potential to revolutionize earthquake forecasting by enabling real-time monitoring and proactive measures to safeguard communities and infrastructure in Chile and beyond. The remarkable 95 % accuracy achieved by this model is a testament to its exceptional learning process, which sets it apart from other models. Its ability to learn and adapt to new data is unparalleled, allowing it to forecast incredibly precisely and produce highly reliable results. © 2024 Elsevier Ltd;"Clustering; Correlation; Earthquake; LSTM; Observations of Time Series";"Cluster analysis; Disaster prevention; Earthquakes; Forecasting; Intelligent systems; Learning algorithms; Learning systems; Long short-term memory; Seismic response; Seismic waves; Sensor networks; Clusterings; Correlation; Earthquake forecasting; Intelligent machine; Intelligent technology; LSTM; Observation of time series; Real- time; Technology learning; Times series; Time series analysis";"Towards real-time earthquake forecasting in Chile: Integrating intelligent technologies and machine learning This paper proposes an innovative approach towards real-time earthquake forecasting in Chile by integrating intelligent technologies and machine learning methods. Earthquakes pose significant risks to communities and infrastructure in Chile, making accurate and timely forecasting crucial for disaster preparedness and mitigation. Traditional forecasting methods have limitations in providing real-time insights into seismic activity. In contrast, intelligent technologies such as artificial intelligence (AI) and machine learning offer promising avenues for enhancing prediction accuracy and speed. A new earthquake forecasting method using a modified clustering approach LMSCAN(Local Maxima-based Spatio-Cluster Analysis Network) and enhanced neural network time series analysis LSTM-IC (Long Short Term Memory Inverse Correlation) is presented in this paper. This neural network-based technology has been utilized to forecast earthquakes in the Chile region, one of the countries with the largest seismic activity. This study explores the integration of intelligent systems with machine learning algorithms to analyze seismic data and predict earthquake occurrences in Chile. By leveraging historical seismic data, sensor networks, and advanced predictive models, our approach aims to provide timely warnings and insights into seismic events, thereby improving disaster response and resilience. The proposed framework holds the potential to revolutionize earthquake forecasting by enabling real-time monitoring and proactive measures to safeguard communities and infrastructure in Chile and beyond. The remarkable 95 % accuracy achieved by this model is a testament to its exceptional learning process, which sets it apart from other models. Its ability to learn and adapt to new data is unparalleled, allowing it to forecast incredibly precisely and produce highly reliable results. © 2024 Elsevier Ltd Clustering; Correlation; Earthquake; LSTM; Observations of Time Series Cluster analysis; Disaster prevention; Earthquakes; Forecasting; Intelligent systems; Learning algorithms; Learning systems; Long short-term memory; Seismic response; Seismic waves; Sensor networks; Clusterings; Correlation; Earthquake forecasting; Intelligent machine; Intelligent technology; LSTM; Observation of time series; Real- time; Technology learning; Times series; Time series analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
367;Object Detection and Classification in Human Rescue Operations: Deep Learning Strategies for Flooded Environments;Rescue efforts might be significantly complicated in flooded areas. In this study, we examine and evaluate the state-of-the-art in object detection and image enhancement techniques in flooded situations for the purpose of human rescue operations using various image processing, object detection, and low light image enhancement approaches. Partial visible images are difficult due to poor light, low contrast, and scattering. Faster R-CNN, YOLO (You Only Look Once), and SSD (Single Shot Detector) are just a few of the popular object identification methods. Advanced deep learning-based low-light enhancement approaches increase image quality by amplifying faint features, decreasing noise, and correcting color imbalances. These models use auto encoders, generative adversarial networks, and attention processes to rebuild images better than classic enhancement methods, making them useful for rescue pre-processing. The findings emphasized the role of real-time data analysis and communication systems in improving response times and operational efficiency. The application of Generative Adversarial Networks significantly improved the clarity and color accuracy of underwater images. These methods address water's refractive characteristics, floating debris, and human occlusion. For efficient and complete disaster management throughout all phases, subsequent attempts should focus on blending disaster management expertise, image processing techniques, and machine learning tools, as outlined by our study. This research can improve flood monitoring systems and disaster preparedness, response, and recovery. Copyright: ©2024 The authors.;"convolutional neural networks (CNN); disaster management; human rescue operations; low light image enhancement; object detection";NULL;"Object Detection and Classification in Human Rescue Operations: Deep Learning Strategies for Flooded Environments Rescue efforts might be significantly complicated in flooded areas. In this study, we examine and evaluate the state-of-the-art in object detection and image enhancement techniques in flooded situations for the purpose of human rescue operations using various image processing, object detection, and low light image enhancement approaches. Partial visible images are difficult due to poor light, low contrast, and scattering. Faster R-CNN, YOLO (You Only Look Once), and SSD (Single Shot Detector) are just a few of the popular object identification methods. Advanced deep learning-based low-light enhancement approaches increase image quality by amplifying faint features, decreasing noise, and correcting color imbalances. These models use auto encoders, generative adversarial networks, and attention processes to rebuild images better than classic enhancement methods, making them useful for rescue pre-processing. The findings emphasized the role of real-time data analysis and communication systems in improving response times and operational efficiency. The application of Generative Adversarial Networks significantly improved the clarity and color accuracy of underwater images. These methods address water's refractive characteristics, floating debris, and human occlusion. For efficient and complete disaster management throughout all phases, subsequent attempts should focus on blending disaster management expertise, image processing techniques, and machine learning tools, as outlined by our study. This research can improve flood monitoring systems and disaster preparedness, response, and recovery. Copyright: ©2024 The authors. convolutional neural networks (CNN); disaster management; human rescue operations; low light image enhancement; object detection NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
368;Bias-Compensation Augmentation Learning for Semantic Segmentation in UAV Networks;In the realm of emergency disaster relief, it is paramount to attain a thorough comprehension of the semantic information associated with the local disaster scene for strategic rescue path planning and immediate rescue operations for affected individuals. Unmanned aerial vehicle (UAV) networks are widely utilized for rapid data collection in the aftermath of disasters due to their flexibility and maneuverability, assisting in rescue decision-making. However, some disasters, such as seismic events and floods, have disrupted the initially structured ground shape information, leading to a disparate distribution of data collected by various UAV groups. This exposes traditional semantic segmentation models susceptible to shortcut bias, posing challenges in adapting to semantic segmentation tasks in disaster scenarios. Thus, this article proposes a bias-compensation augmentation learning-based semantic segmentation framework, which substantially enhances the extraction capability of semantic information. Initially, we exploit an artificial augmentation neural network for bias-awareness to determine the relative bias values of the collected image data. Subsequently, considering the limited computing power resources in UAV networks, we present a bias compensation computation offloading strategy to achieve a relatively balanced distribution of semantic information across UAV nodes, optimizing the tradeoff between network scheduling efficiency and model accuracy. We conduct reconstruction validation on the FloodNet data set, and a plethora of experimental results demonstrate that, compared to traditional methods, this approach greatly improves the accuracy of pixel-level semantic segmentation by over 86.5%. Moreover, the average combined processing time is also reduced by over 50%, enhancing the utilization efficiency of limited computational resources.  © 2014 IEEE.;"Bias compensation augmentation learning; computing power resource scheduling; semantic segmentation; unmanned aerial vehicle (UAV) networks";"Antennas; Computational efficiency; Decision making; Disaster prevention; Economic and social effects; Floods; Job analysis; Maneuverability; Motion planning; Semantics; Unmanned aerial vehicles (UAV); Adaptation models; Aerial vehicle; Bias compensation; Bias compensation augmentation learning; Computational modelling; Computing power; Computing power resource scheduling; Power resources; Processor scheduling; Resource-scheduling; Semantic segmentation; Task analysis; Unmanned aerial vehicle network; Vehicle network; Disasters";"Bias-Compensation Augmentation Learning for Semantic Segmentation in UAV Networks In the realm of emergency disaster relief, it is paramount to attain a thorough comprehension of the semantic information associated with the local disaster scene for strategic rescue path planning and immediate rescue operations for affected individuals. Unmanned aerial vehicle (UAV) networks are widely utilized for rapid data collection in the aftermath of disasters due to their flexibility and maneuverability, assisting in rescue decision-making. However, some disasters, such as seismic events and floods, have disrupted the initially structured ground shape information, leading to a disparate distribution of data collected by various UAV groups. This exposes traditional semantic segmentation models susceptible to shortcut bias, posing challenges in adapting to semantic segmentation tasks in disaster scenarios. Thus, this article proposes a bias-compensation augmentation learning-based semantic segmentation framework, which substantially enhances the extraction capability of semantic information. Initially, we exploit an artificial augmentation neural network for bias-awareness to determine the relative bias values of the collected image data. Subsequently, considering the limited computing power resources in UAV networks, we present a bias compensation computation offloading strategy to achieve a relatively balanced distribution of semantic information across UAV nodes, optimizing the tradeoff between network scheduling efficiency and model accuracy. We conduct reconstruction validation on the FloodNet data set, and a plethora of experimental results demonstrate that, compared to traditional methods, this approach greatly improves the accuracy of pixel-level semantic segmentation by over 86.5%. Moreover, the average combined processing time is also reduced by over 50%, enhancing the utilization efficiency of limited computational resources.  © 2014 IEEE. Bias compensation augmentation learning; computing power resource scheduling; semantic segmentation; unmanned aerial vehicle (UAV) networks Antennas; Computational efficiency; Decision making; Disaster prevention; Economic and social effects; Floods; Job analysis; Maneuverability; Motion planning; Semantics; Unmanned aerial vehicles (UAV); Adaptation models; Aerial vehicle; Bias compensation; Bias compensation augmentation learning; Computational modelling; Computing power; Computing power resource scheduling; Power resources; Processor scheduling; Resource-scheduling; Semantic segmentation; Task analysis; Unmanned aerial vehicle network; Vehicle network; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
369;Research on Urban Storm Flood Simulation by Coupling K-means Machine Learning Algorithm and GIS Spatial Analysis Technology into SWMM Model;Accurate flood simulation has significant practical implications for urban flood management. The focus of this study is to develop a new flood model (K-SWMMG) based on the Storm Water Management Model (SWMM), which innovatively couples the K-means clustering machine learning algorithm and GIS spatial analysis techniques. The K-means clustering machine learning algorithm is used to determine the uncertain parameters of the SWMM model, while GIS spatial analysis techniques enhance the two-dimensional realism of flood simulation. We applied the K-SWMMG model to six historical observed flood events in a specific catchment area in Zhengzhou City, using rainfall and flow data. The study shows that: 1) K-SWMMG optimizes the sub-basin division method of urban stormwater models, avoiding the tedious and complex parameter calibration process, and improving modeling efficiency to some extent. 2) The two-dimensional visualization of inundation provided by GIS spatial analysis techniques better meets the production requirements of current urban flood simulation. 3) K-SWMMG outperforms SWMM in terms of simulation performance, with improvements in absolute error (AE), relative error (RE), Nash-Sutcliffe efficiency coefficient (NSE), and coefficient of determination (R2) by 0.019m, 5.36%, 0.068, and 0.042, respectively. The findings can provide scientific decision-making references for urban flood forecasting and early warning. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"City functional area; GIS; K-means clustering algorithm; K-SWMMG; Uncertainty parameters; Urban flood forecast";"China; Henan; Zhengzhou; Bayesian networks; Catchments; Decision making; Efficiency; Flood control; K-means clustering; Learning algorithms; Machine learning; Parameter estimation; Storms; Uncertainty analysis; Weather forecasting; City functional area; Flood forecast; Flood simulation; Functional areas; K-means clustering algorithms; K-SWMMG; Spatial analysis; Uncertainty parameters; Urban flood forecast; Urban floods; algorithm; cluster analysis; flood control; flooding; GIS; machine learning; uncertainty analysis; water management; Floods";"Research on Urban Storm Flood Simulation by Coupling K-means Machine Learning Algorithm and GIS Spatial Analysis Technology into SWMM Model Accurate flood simulation has significant practical implications for urban flood management. The focus of this study is to develop a new flood model (K-SWMMG) based on the Storm Water Management Model (SWMM), which innovatively couples the K-means clustering machine learning algorithm and GIS spatial analysis techniques. The K-means clustering machine learning algorithm is used to determine the uncertain parameters of the SWMM model, while GIS spatial analysis techniques enhance the two-dimensional realism of flood simulation. We applied the K-SWMMG model to six historical observed flood events in a specific catchment area in Zhengzhou City, using rainfall and flow data. The study shows that: 1) K-SWMMG optimizes the sub-basin division method of urban stormwater models, avoiding the tedious and complex parameter calibration process, and improving modeling efficiency to some extent. 2) The two-dimensional visualization of inundation provided by GIS spatial analysis techniques better meets the production requirements of current urban flood simulation. 3) K-SWMMG outperforms SWMM in terms of simulation performance, with improvements in absolute error (AE), relative error (RE), Nash-Sutcliffe efficiency coefficient (NSE), and coefficient of determination (R2) by 0.019m, 5.36%, 0.068, and 0.042, respectively. The findings can provide scientific decision-making references for urban flood forecasting and early warning. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. City functional area; GIS; K-means clustering algorithm; K-SWMMG; Uncertainty parameters; Urban flood forecast China; Henan; Zhengzhou; Bayesian networks; Catchments; Decision making; Efficiency; Flood control; K-means clustering; Learning algorithms; Machine learning; Parameter estimation; Storms; Uncertainty analysis; Weather forecasting; City functional area; Flood forecast; Flood simulation; Functional areas; K-means clustering algorithms; K-SWMMG; Spatial analysis; Uncertainty parameters; Urban flood forecast; Urban floods; algorithm; cluster analysis; flood control; flooding; GIS; machine learning; uncertainty analysis; water management; Floods";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.2;Hydrological;1;Prevention
370;Displacement Prediction Method for Bank Landslide Based on SSA-VMD and LSTM Model;"Landslide displacement prediction is of great significance for the prevention and early warning of slope hazards. In order to enhance the extraction of landslide historical monitoring signals, a landslide displacement prediction method is proposed based on the decomposition of monitoring data before prediction. Firstly, based on the idea of temporal addition, the sparrow search algorithm (SSA) coupled with the variational modal decomposition (VMD) algorithm is used to decompose the total landslide displacement into trend item, periodic item and random item; then, the displacement values of the subitems are fitted by using the long and short-term memory (LSTM) neural network, and the predicted cumulative landslide displacement is obtained by adding up the predicted values of the three subsequences. Finally, the historical measured data of the Shuping landslide is taken as an example. Considering the effects of seasonal rainfall and reservoir water level rise and fall, the displacement of this landslide is predicted, and the prediction results of other traditional models are compared. The results show that the landslide displacement prediction model of SSA-VMD coupled with LSTM can predict landslide displacement more accurately and capture the characteristics of historical signals, which can be used as a reference for landslide displacement prediction. © 2024 by the authors.";"bank landslide; landslide displacement prediction; long and short-term memory neural network; sparrow search algorithm; time series; variational modal decomposition";NULL;"Displacement Prediction Method for Bank Landslide Based on SSA-VMD and LSTM Model Landslide displacement prediction is of great significance for the prevention and early warning of slope hazards. In order to enhance the extraction of landslide historical monitoring signals, a landslide displacement prediction method is proposed based on the decomposition of monitoring data before prediction. Firstly, based on the idea of temporal addition, the sparrow search algorithm (SSA) coupled with the variational modal decomposition (VMD) algorithm is used to decompose the total landslide displacement into trend item, periodic item and random item; then, the displacement values of the subitems are fitted by using the long and short-term memory (LSTM) neural network, and the predicted cumulative landslide displacement is obtained by adding up the predicted values of the three subsequences. Finally, the historical measured data of the Shuping landslide is taken as an example. Considering the effects of seasonal rainfall and reservoir water level rise and fall, the displacement of this landslide is predicted, and the prediction results of other traditional models are compared. The results show that the landslide displacement prediction model of SSA-VMD coupled with LSTM can predict landslide displacement more accurately and capture the characteristics of historical signals, which can be used as a reference for landslide displacement prediction. © 2024 by the authors. bank landslide; landslide displacement prediction; long and short-term memory neural network; sparrow search algorithm; time series; variational modal decomposition NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
371;PLANES4LOD2: Reconstruction of LoD-2 building models using a depth attention-based fully convolutional neural network;Level of detail (LoD)-2 reconstruction is an inevitable task in digital twin-related applications such as disaster management, flood simulation, landslide simulation and solar panel recommendation. However, there is a lack of capable methods that can exploit fine details in RGB imagery and mitigate noise in photogrammetric digital surface models (DSMs). Our investigation is focused on the use of roof planes to achieve a geometrically complete and correct, and topologically consistent LoD-2 building reconstruction. Using UNet with the EfficientNet-B3 backbone, the developed approach starts with jointly predicting building sections and roof planes from the orthorectified RGB imagery and a photogrammetric DSM. The detected sections and planes are then vectorized by employing tree search and simplified with the Douglas Peucker algorithm. Subsequently, height values from the noisy input DSM and the vectorized image-based (and simplified) roof planes are used to derive 3D-planes. Finally, the building model is formed by computing plane intersections as the ridge lines. This study demonstrates that a well-designed depth attention module (DAM), which is the bottleneck of the UNet, can achieve a very good use of both spectral and depth features. The resultant 1-to-n correspondence between building section and roof plane benefits accurate and consistent building model reconstruction. Furthermore, it leads to a superior generalization capability of the proposed method. Experiments with 1437 buildings from the cities Cologne and Braunschweig, Germany, demonstrate the success of the proposed workflow in reconstructing compound buildings with complex roof structures. The achieved geometric mean absolute error (MAE) is 1.06 m and 0.24 m respectively. Comprehensive comparative evaluations showcase the superiority of the approach in terms of geometric completeness and accuracy, and topological consistence with. The improvement over SAT2LOD2 (Gui and Qin, 2021) is 1.12 m in Cologne (data accessible at https://github.com/dlrPHS/GPUB) and 0.47 m in Braunschweig in geometrical MAE. © 2024 The Author(s);"Building reconstruction; Depth attention module; Digital surface model; Images; Instance segmentation";"Braunschweig; Cologne; Germany; Lower Saxony; North Rhine-Westphalia; Buildings; Convolutional neural networks; Disaster prevention; Disasters; Geometry; Image segmentation; Photogrammetry; Roofs; Topology; Building model; Building reconstruction; Convolutional neural network; Depth attention module; Digital surface models; Disaster management; Image; Instance segmentation; Level-of-detail; Mean absolute error; artificial neural network; building construction; digital image; disaster management; landslide; simulation; Image reconstruction";"PLANES4LOD2: Reconstruction of LoD-2 building models using a depth attention-based fully convolutional neural network Level of detail (LoD)-2 reconstruction is an inevitable task in digital twin-related applications such as disaster management, flood simulation, landslide simulation and solar panel recommendation. However, there is a lack of capable methods that can exploit fine details in RGB imagery and mitigate noise in photogrammetric digital surface models (DSMs). Our investigation is focused on the use of roof planes to achieve a geometrically complete and correct, and topologically consistent LoD-2 building reconstruction. Using UNet with the EfficientNet-B3 backbone, the developed approach starts with jointly predicting building sections and roof planes from the orthorectified RGB imagery and a photogrammetric DSM. The detected sections and planes are then vectorized by employing tree search and simplified with the Douglas Peucker algorithm. Subsequently, height values from the noisy input DSM and the vectorized image-based (and simplified) roof planes are used to derive 3D-planes. Finally, the building model is formed by computing plane intersections as the ridge lines. This study demonstrates that a well-designed depth attention module (DAM), which is the bottleneck of the UNet, can achieve a very good use of both spectral and depth features. The resultant 1-to-n correspondence between building section and roof plane benefits accurate and consistent building model reconstruction. Furthermore, it leads to a superior generalization capability of the proposed method. Experiments with 1437 buildings from the cities Cologne and Braunschweig, Germany, demonstrate the success of the proposed workflow in reconstructing compound buildings with complex roof structures. The achieved geometric mean absolute error (MAE) is 1.06 m and 0.24 m respectively. Comprehensive comparative evaluations showcase the superiority of the approach in terms of geometric completeness and accuracy, and topological consistence with. The improvement over SAT2LOD2 (Gui and Qin, 2021) is 1.12 m in Cologne (data accessible at https://github.com/dlrPHS/GPUB) and 0.47 m in Braunschweig in geometrical MAE. © 2024 The Author(s) Building reconstruction; Depth attention module; Digital surface model; Images; Instance segmentation Braunschweig; Cologne; Germany; Lower Saxony; North Rhine-Westphalia; Buildings; Convolutional neural networks; Disaster prevention; Disasters; Geometry; Image segmentation; Photogrammetry; Roofs; Topology; Building model; Building reconstruction; Convolutional neural network; Depth attention module; Digital surface models; Disaster management; Image; Instance segmentation; Level-of-detail; Mean absolute error; artificial neural network; building construction; digital image; disaster management; landslide; simulation; Image reconstruction";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
372;Zero-shot knowledge transfer for seismic damage diagnosis through multi-channel 1D CNN integrated with autoencoder-based domain adaptation;Accurate and timely structural damage diagnosis is crucial to efficient disaster response and city renovation in post-earthquake events. The scarcity of labeled data hinders the powerful deep learning techniques from in-domain damage detection on target structures. Cross-domain transfer learning has emerged as a captivating strategy through digging knowledge from the abundant source domain to detect the damage in the target domain. However, the heterogeneity among multi-domain structures poses the challenge in seismic damage diagnosis. This study proposes a novel zero-shot knowledge transfer approach for seismic damage diagnosis through multi-channel one-dimensional convolutional neural networks (1D CNN) integrated with deep autoencoder (DAE)-based domain adaptation (DA). The framework consists of three modules, namely, data preprocessor adaptive to seismic vibration signals, DAE-based DA module, and damage diagnosis via multi-channel 1D CNN. The DA module is customized to seamlessly translate the unseen target-domain data to mimic latent representation via a DAE pretrained on the source data, thus realizing rigorous zero-shot learning. Imbalanced data distribution is also considered during the network training and testing. Two representative phases of knowledge transfer are performed to substantiate the knowledge transferability of the proposed method. The first phase involves multi-class damage quantification on two ASCE benchmark models from the simplified model to the refined one, and the second phase conducts binary damage detection on a three-story reinforced frame structure from the finite element numerical model to the laboratory-tested physical model. Both examples show that the proposed method exhibits high prediction accuracy and a lower false negative rate in achieving zero-shot knowledge transfer for cross-domain structural damage diagnosis. With a delicate network design for diverse data, the proposed knowledge transfer framework can be further extended from the present zero-shot approach to the few-shot learning paradigm, thus suggesting a feasible algorithm adaptability and promising engineering applicability. © 2024 Elsevier Ltd;"Domain adaptation; Knowledge transfer; Seismic damage; Structural damage diagnosis; Zero-shot learning";"Convolutional neural networks; Damage detection; Knowledge management; Learning systems; Seismology; Structural analysis; Transfer learning; Zero-shot learning; Adaptation module; Auto encoders; Cross-domain; Damage diagnosis; Domain adaptation; Knowledge transfer; Multi channel; Seismic damage; Structural damage diagnosis; Target domain; Deep learning";"Zero-shot knowledge transfer for seismic damage diagnosis through multi-channel 1D CNN integrated with autoencoder-based domain adaptation Accurate and timely structural damage diagnosis is crucial to efficient disaster response and city renovation in post-earthquake events. The scarcity of labeled data hinders the powerful deep learning techniques from in-domain damage detection on target structures. Cross-domain transfer learning has emerged as a captivating strategy through digging knowledge from the abundant source domain to detect the damage in the target domain. However, the heterogeneity among multi-domain structures poses the challenge in seismic damage diagnosis. This study proposes a novel zero-shot knowledge transfer approach for seismic damage diagnosis through multi-channel one-dimensional convolutional neural networks (1D CNN) integrated with deep autoencoder (DAE)-based domain adaptation (DA). The framework consists of three modules, namely, data preprocessor adaptive to seismic vibration signals, DAE-based DA module, and damage diagnosis via multi-channel 1D CNN. The DA module is customized to seamlessly translate the unseen target-domain data to mimic latent representation via a DAE pretrained on the source data, thus realizing rigorous zero-shot learning. Imbalanced data distribution is also considered during the network training and testing. Two representative phases of knowledge transfer are performed to substantiate the knowledge transferability of the proposed method. The first phase involves multi-class damage quantification on two ASCE benchmark models from the simplified model to the refined one, and the second phase conducts binary damage detection on a three-story reinforced frame structure from the finite element numerical model to the laboratory-tested physical model. Both examples show that the proposed method exhibits high prediction accuracy and a lower false negative rate in achieving zero-shot knowledge transfer for cross-domain structural damage diagnosis. With a delicate network design for diverse data, the proposed knowledge transfer framework can be further extended from the present zero-shot approach to the few-shot learning paradigm, thus suggesting a feasible algorithm adaptability and promising engineering applicability. © 2024 Elsevier Ltd Domain adaptation; Knowledge transfer; Seismic damage; Structural damage diagnosis; Zero-shot learning Convolutional neural networks; Damage detection; Knowledge management; Learning systems; Seismology; Structural analysis; Transfer learning; Zero-shot learning; Adaptation module; Auto encoders; Cross-domain; Damage diagnosis; Domain adaptation; Knowledge transfer; Multi channel; Seismic damage; Structural damage diagnosis; Target domain; Deep learning";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;3;Response
373;IMPLEMENTATION OF MAPPING-BASED MACHINE LEARNING ALGORITHM AS NON-STRUCTURAL DISASTER MITIGATION TO DETECT LANDSLIDE SUSCEPTIBILITY IN TAKARI DISTRICT;This research is primarily dedicated to providing a comprehensive exposition of the methodology applied in the deployment of a cartographic-based machine learning algorithm designed for the precise identification of areas susceptible to landslides within the geographical confines of the Takari District. This research delves into the application of mapping-based machine learning algorithms in the domain of non-structural disaster mitigation, with a specific emphasis on the detection of landslide susceptibility within the Takari District. A range of machine learning algorithms, including Support Vector Machine, Naive Bayes Classifier, Ordinal Logistic Regression, Random Forest, and Decision Tree, were harnessed to evaluate rainfall data within the context of landslide susceptibility. An evaluation of model performance, anchored in accuracy and Kappa metrics, unveiled that both the Ordinal Logistic Regression and Random Forest models exhibited noteworthy precision, reaching a commendable 74.36%. Nevertheless, a meticulous examination of Kappa values disclosed the ascendancy of the Random Forest model, which achieved a superior Kappa value of 0.5397. As portrayed in the visual representation provided, it becomes manifest that the Random Forest algorithm's prognostications yield 66 instances of cloudy atmospheric conditions, 48 occurrences of light precipitation, and 3 episodes of moderate rainfall. These predictions are influenced by several factors, including average temperature, humidity levels, wind speed, duration of sunlight, and wind direction at maximum speed. Consequently, this comprehensive analysis underscores the Random Forest algorithm as the most efficacious model for landslide susceptibility prediction. Furthermore, the study seamlessly integrated overlay maps, encompassing the Slope Inclination Map of the Takari District, Geological Map of the Takari District, and Soil Type Map of the Takari District, to contribute to the formulation of a definitive map delineating the susceptibility to landslides in the Takari District. Furthermore, further research could conduct spatial validation of the model predictions using additional datasets or remote sensing data to validate the accuracy of the landslide susceptibility map and ensure its applicability across different geographical regions. © 2024 Author(s).;"Landslide Susceptibility; Machine Learning; Mapping; Non-Structural Mitigation";NULL;"IMPLEMENTATION OF MAPPING-BASED MACHINE LEARNING ALGORITHM AS NON-STRUCTURAL DISASTER MITIGATION TO DETECT LANDSLIDE SUSCEPTIBILITY IN TAKARI DISTRICT This research is primarily dedicated to providing a comprehensive exposition of the methodology applied in the deployment of a cartographic-based machine learning algorithm designed for the precise identification of areas susceptible to landslides within the geographical confines of the Takari District. This research delves into the application of mapping-based machine learning algorithms in the domain of non-structural disaster mitigation, with a specific emphasis on the detection of landslide susceptibility within the Takari District. A range of machine learning algorithms, including Support Vector Machine, Naive Bayes Classifier, Ordinal Logistic Regression, Random Forest, and Decision Tree, were harnessed to evaluate rainfall data within the context of landslide susceptibility. An evaluation of model performance, anchored in accuracy and Kappa metrics, unveiled that both the Ordinal Logistic Regression and Random Forest models exhibited noteworthy precision, reaching a commendable 74.36%. Nevertheless, a meticulous examination of Kappa values disclosed the ascendancy of the Random Forest model, which achieved a superior Kappa value of 0.5397. As portrayed in the visual representation provided, it becomes manifest that the Random Forest algorithm's prognostications yield 66 instances of cloudy atmospheric conditions, 48 occurrences of light precipitation, and 3 episodes of moderate rainfall. These predictions are influenced by several factors, including average temperature, humidity levels, wind speed, duration of sunlight, and wind direction at maximum speed. Consequently, this comprehensive analysis underscores the Random Forest algorithm as the most efficacious model for landslide susceptibility prediction. Furthermore, the study seamlessly integrated overlay maps, encompassing the Slope Inclination Map of the Takari District, Geological Map of the Takari District, and Soil Type Map of the Takari District, to contribute to the formulation of a definitive map delineating the susceptibility to landslides in the Takari District. Furthermore, further research could conduct spatial validation of the model predictions using additional datasets or remote sensing data to validate the accuracy of the landslide susceptibility map and ensure its applicability across different geographical regions. © 2024 Author(s). Landslide Susceptibility; Machine Learning; Mapping; Non-Structural Mitigation NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
374;Advancing rapid urban flood prediction: a spatiotemporal deep learning approach with uneven rainfall and attention mechanism;Urban floods pose a significant threat to human communities, making its prediction essential for comprehensive flood risk assessment and the formulation of effective resource allocation strategies. Data-driven deep learning approaches have gained traction in urban emergency flood prediction, addressing the efficiency constraints of physical models. However, the spatial structure of rainfall, which has a profound influence on urban flooding, is often overlooked in many deep learning investigations. In this study, we introduce a novel deep learning model known as CRU-Net equipped with an attention mechanism to predict inundation depths in urban terrains based on spatiotemporal rainfall patterns. This method utilizes eight topographic parameters related to the height of urban waterlogging, combined with spatial rainfall data as inputs to the model. Comparative evaluations between the developed CRU-Net and two other deep learning models, U-Net and ResU-Net, reveal that CRU-Net adeptly interprets the spatiotemporal traits of rainfall and accurately estimates flood depths, emphasizing deep inundation and flood-vulnerable regions. The model demonstrates exceptional accuracy, evidenced by a root mean square error of 0.054 m and a Nash–Sutcliffe efficiency of 0.975. CRU-Net also accurately predicts over 80% of inundation locations with depths exceeding 0.3 m. Remarkably, CRU-Net delivers predictions for 3 million grids in 2.9 s, showcasing its efficiency. © 2024 IWA Publishing. All rights reserved.;"CRU-Net; deep learning; rapid prediction; spatiotemporal rainfall; urban flooding";NULL;"Advancing rapid urban flood prediction: a spatiotemporal deep learning approach with uneven rainfall and attention mechanism Urban floods pose a significant threat to human communities, making its prediction essential for comprehensive flood risk assessment and the formulation of effective resource allocation strategies. Data-driven deep learning approaches have gained traction in urban emergency flood prediction, addressing the efficiency constraints of physical models. However, the spatial structure of rainfall, which has a profound influence on urban flooding, is often overlooked in many deep learning investigations. In this study, we introduce a novel deep learning model known as CRU-Net equipped with an attention mechanism to predict inundation depths in urban terrains based on spatiotemporal rainfall patterns. This method utilizes eight topographic parameters related to the height of urban waterlogging, combined with spatial rainfall data as inputs to the model. Comparative evaluations between the developed CRU-Net and two other deep learning models, U-Net and ResU-Net, reveal that CRU-Net adeptly interprets the spatiotemporal traits of rainfall and accurately estimates flood depths, emphasizing deep inundation and flood-vulnerable regions. The model demonstrates exceptional accuracy, evidenced by a root mean square error of 0.054 m and a Nash–Sutcliffe efficiency of 0.975. CRU-Net also accurately predicts over 80% of inundation locations with depths exceeding 0.3 m. Remarkably, CRU-Net delivers predictions for 3 million grids in 2.9 s, showcasing its efficiency. © 2024 IWA Publishing. All rights reserved. CRU-Net; deep learning; rapid prediction; spatiotemporal rainfall; urban flooding NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
375;Prediction of shear stress distribution in compound channel with smooth converging floodplains;Climate change can have a profound impact on river flooding, leading to increased frequency and severity of floods. To mitigate these effects, it is crucial to focus on enhancing early warning systems and bolstering infrastructure resilience through improved forecasting. This proactive approach enables communities to better plan for and respond to flood events, thereby minimizing the adverse consequences of climate change on river floods. During river flooding, the channels often take on a compound nature, with varying geometries along the flow length. This complexity arises from construction and agricultural activities along the floodplains, resulting in converging, diverging, or skewed compound channels. Modelling the flow in these channels requires consideration of additional momentum transfer factors. In this study, machine learning techniques, including Gene Expression Programming (GEP), Artificial Neural Networks (ANN), and Support Vector Machines (SVM), were employed. The focus was on a compound channel with converging floodplains, predicting the shear force carried by the floodplains in terms of non-dimensional flow and hydraulic parameters. The findings indicate that the proposed ANN model outperformed GEP, SVM, and other established approaches in accurately predicting floodplain shear force. This research underscores the efficacy of utilizing machine learning techniques in the examination of river hydraulics.  © 2024 Vijay Kaushik et al., published by Sciendo.;"Compound channel; Converging floodplains; Machine learning approaches; Non-dimensional parameters; Shear stress distribution";"Floods; Forecasting; Gene expression; Learning algorithms; Learning systems; Neural networks; Rivers; Shear flow; Shear stress; Stress concentration; Support vector machines; Compound channel; Converging floodplain; Flood plains; Gene-expression programming; Machine learning approaches; Machine learning techniques; Non-dimensional parameters; River flooding; Shear force; Shear stress distribution; artificial neural network; channel flow; early warning system; flood; flooding; floodplain; parameter estimation; shear stress; Climate change";"Prediction of shear stress distribution in compound channel with smooth converging floodplains Climate change can have a profound impact on river flooding, leading to increased frequency and severity of floods. To mitigate these effects, it is crucial to focus on enhancing early warning systems and bolstering infrastructure resilience through improved forecasting. This proactive approach enables communities to better plan for and respond to flood events, thereby minimizing the adverse consequences of climate change on river floods. During river flooding, the channels often take on a compound nature, with varying geometries along the flow length. This complexity arises from construction and agricultural activities along the floodplains, resulting in converging, diverging, or skewed compound channels. Modelling the flow in these channels requires consideration of additional momentum transfer factors. In this study, machine learning techniques, including Gene Expression Programming (GEP), Artificial Neural Networks (ANN), and Support Vector Machines (SVM), were employed. The focus was on a compound channel with converging floodplains, predicting the shear force carried by the floodplains in terms of non-dimensional flow and hydraulic parameters. The findings indicate that the proposed ANN model outperformed GEP, SVM, and other established approaches in accurately predicting floodplain shear force. This research underscores the efficacy of utilizing machine learning techniques in the examination of river hydraulics.  © 2024 Vijay Kaushik et al., published by Sciendo. Compound channel; Converging floodplains; Machine learning approaches; Non-dimensional parameters; Shear stress distribution Floods; Forecasting; Gene expression; Learning algorithms; Learning systems; Neural networks; Rivers; Shear flow; Shear stress; Stress concentration; Support vector machines; Compound channel; Converging floodplain; Flood plains; Gene-expression programming; Machine learning approaches; Machine learning techniques; Non-dimensional parameters; River flooding; Shear force; Shear stress distribution; artificial neural network; channel flow; early warning system; flood; flooding; floodplain; parameter estimation; shear stress; Climate change";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
376;Towards a Multimedia Big Data-Driven Approach for Earthquake Monitoring and Forecasting Early Warning System;Digital networks have fundamentally changed the way that people think by enabling them to share their location and other personal information for the benefit of their communities. The popularity of geo-social networks (GN) like Instagram, Twitter, Facebook, and Flickr has increased significantly in recent years. As a result, everyone in the world may now express their opinions, immediately report an occurrence, and interact with others. GN data therefore gives comprehensive data on individual current developments. By evaluating geo-social data in real-time, modern GN may be used as digital assets for countries and their governments. Hybrid lion-optimized random forest (HLO-RF) is the proposed technique of this research. It seamlessly integrates Lion Optimizing with random forest methods to improve complicated data-driven activities' forecasting accuracy. The novel technique promotes effective and resilient decision-making in several applications. To explore GN while gathering information and rendering decisions in real-time while monitoring and forecasting various natural occurrences, we offer an effective system and Machine Learning (ML) technique in this research. To predict the early warning system using HLO-RF. To monitor earthquake occurrences, and incidents in real-time, make potential real-time decisions, and assist in planning for the future, unique in a real-world setting, the proposed system is designed and used. We demonstrate that the proposed system has increased performance and can analyze a massive quantity of GN data in real-time, while simultaneously identifying any occurrence. © 2024 Slovene Society Informatika. All rights reserved.;"big data; earthquake; Geo-social networks (GN); hybrid lion-optimized random forest (HLO-RF)";"Behavioral research; Big data; Decision making; Forecasting; Social networking (online); Data-driven approach; Early Warning System; Earthquake forecasting; Earthquake monitoring; Geo-social network; Geo-social networks; Hybrid lion-optimized random forest; Network data; Random forests; Real- time; Earthquakes";"Towards a Multimedia Big Data-Driven Approach for Earthquake Monitoring and Forecasting Early Warning System Digital networks have fundamentally changed the way that people think by enabling them to share their location and other personal information for the benefit of their communities. The popularity of geo-social networks (GN) like Instagram, Twitter, Facebook, and Flickr has increased significantly in recent years. As a result, everyone in the world may now express their opinions, immediately report an occurrence, and interact with others. GN data therefore gives comprehensive data on individual current developments. By evaluating geo-social data in real-time, modern GN may be used as digital assets for countries and their governments. Hybrid lion-optimized random forest (HLO-RF) is the proposed technique of this research. It seamlessly integrates Lion Optimizing with random forest methods to improve complicated data-driven activities' forecasting accuracy. The novel technique promotes effective and resilient decision-making in several applications. To explore GN while gathering information and rendering decisions in real-time while monitoring and forecasting various natural occurrences, we offer an effective system and Machine Learning (ML) technique in this research. To predict the early warning system using HLO-RF. To monitor earthquake occurrences, and incidents in real-time, make potential real-time decisions, and assist in planning for the future, unique in a real-world setting, the proposed system is designed and used. We demonstrate that the proposed system has increased performance and can analyze a massive quantity of GN data in real-time, while simultaneously identifying any occurrence. © 2024 Slovene Society Informatika. All rights reserved. big data; earthquake; Geo-social networks (GN); hybrid lion-optimized random forest (HLO-RF) Behavioral research; Big data; Decision making; Forecasting; Social networking (online); Data-driven approach; Early Warning System; Earthquake forecasting; Earthquake monitoring; Geo-social network; Geo-social networks; Hybrid lion-optimized random forest; Network data; Random forests; Real- time; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
377;Optimizing Residential Construction Site Selection in Mountainous Regions Using Geospatial Data and eXplainable AI;The rapid urbanization of Abha and its surrounding cities in Saudi Arabia’s mountainous regions poses challenges for sustainable and secure development. This study aimed to identify suitable sites for eco-friendly and safe building complexes amidst complex geophysical, geoecological, and socio-economic factors, integrating natural hazards assessment and risk management. Employing the Fuzzy Analytic Hierarchy Process (Fuzzy-AHP), the study constructed a suitability model incorporating sixteen parameters. Additionally, a Deep Neural Network (DNN) based on eXplainable Artificial Intelligence (XAI) conducted sensitivity analyses to assess the parameters’ influence on optimal location decision making. The results reveal slope as the most crucial parameter (22.90%), followed by altitude and land use/land cover (13.24%), emphasizing topography and environmental considerations. Drainage density (11.36%) and rainfall patterns (9.15%) are also significant for flood defense and water management. Only 12.21% of the study area is deemed “highly suitable”, with “no-build zones” designated for safety and environmental protection. DNN-based XAI demonstrates the positive impact of variables like the NDVI and municipal solid waste generation on site selection, informing waste management and ecological preservation strategies. This integrated methodology provides actionable insights for sustainable and safe residential development in Abha, aiding informed decision making and balancing urban expansion with environmental conservation and hazard risk reduction. © 2024 by the authors.;"artificial intelligence; decision-making framework; GIS-based site selection; mountainous terrain; risk assessment; sustainable urbanization";"Saudi Arabia; altitude; artificial intelligence; decision making; GIS; municipal solid waste; NDVI; residential development; risk assessment; site selection; sustainability; urbanization";"Optimizing Residential Construction Site Selection in Mountainous Regions Using Geospatial Data and eXplainable AI The rapid urbanization of Abha and its surrounding cities in Saudi Arabia’s mountainous regions poses challenges for sustainable and secure development. This study aimed to identify suitable sites for eco-friendly and safe building complexes amidst complex geophysical, geoecological, and socio-economic factors, integrating natural hazards assessment and risk management. Employing the Fuzzy Analytic Hierarchy Process (Fuzzy-AHP), the study constructed a suitability model incorporating sixteen parameters. Additionally, a Deep Neural Network (DNN) based on eXplainable Artificial Intelligence (XAI) conducted sensitivity analyses to assess the parameters’ influence on optimal location decision making. The results reveal slope as the most crucial parameter (22.90%), followed by altitude and land use/land cover (13.24%), emphasizing topography and environmental considerations. Drainage density (11.36%) and rainfall patterns (9.15%) are also significant for flood defense and water management. Only 12.21% of the study area is deemed “highly suitable”, with “no-build zones” designated for safety and environmental protection. DNN-based XAI demonstrates the positive impact of variables like the NDVI and municipal solid waste generation on site selection, informing waste management and ecological preservation strategies. This integrated methodology provides actionable insights for sustainable and safe residential development in Abha, aiding informed decision making and balancing urban expansion with environmental conservation and hazard risk reduction. © 2024 by the authors. artificial intelligence; decision-making framework; GIS-based site selection; mountainous terrain; risk assessment; sustainable urbanization Saudi Arabia; altitude; artificial intelligence; decision making; GIS; municipal solid waste; NDVI; residential development; risk assessment; site selection; sustainability; urbanization";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
378;Performance of partially grouted reinforced masonry walls with bed-joint reinforcement: parametric and optimization investigation;Post-seismic assessments have indicated that the responses of masonry walls to seismic forces may be classified into two key categories: in-plane global mechanisms and out-of-plane cyclical actions occurring orthogonally to the wall. The initial phase of this investigation scrutinized the widely accepted shear strength models for in-plane shear resistance, V n, of reinforced masonry (RM) walls. Subsequently, using a dataset comprising 78 samples of fully grouted hollow concrete block (HCB) RM walls, a detailed examination of the sensitivity of experimental shear strength, Vexp , to geometric, mechanical, and reinforcement characteristics of the wall was undertaken. The study's second phase entailed a parametric evaluation using finite element analysis to appraise the sensitivity of lateral drift to wall geometry and bed-joint reinforcement attributes. The third phase of the research introduced an informational model for estimating the lateral drift of partially grouted RM walls, incorporating BJ and vertical reinforcement. The model was established utilizing data from 44 full-scale in-plane cyclic tests on clay brick walls and 32 tests on HCB walls. The investigation further presented a multi-objective optimization methodology to ascertain the optimal vertical and BJR ratios, ρ v and ρ BJ. A graphical user interface and an accompanying empirical equation were also devised to simplify the analysis and design process for reinforced masonry walls, obviating the need for lengthy analyses. Increasing the BJR size from 6 to 8 mm resulted in a 30% increase in Vn for specimens with six BJR rows, while increasing the number of BJR rows from six to ten led to 16% rise in Vn , as demonstrated by numerical modeling validated against experimental tests.The findings of the study highlight a notable dependence of the lateral drift capacity of reinforced masonry walls on wall geometry and ρ BJ ratio. These revelations provide invaluable insights for designing earthquake-resistant masonry edifices and formulating rehabilitation strategies for existing masonry structures deficient in seismic resilience. © 2023, Wroclaw University of Science and Technology.;"Artificial neural network; Bed-joint reinforcement; Clay brick; Hollow concrete block; Lateral drift; Masonry; Multi-objective optimization; Shear wall";"Concrete construction; Earthquakes; Geometry; Graphical user interfaces; Grouting; Mortar; Neural networks; Reinforced concrete; Retaining walls; Bed joints; Bed-joint reinforcement; Clay bricks; Hollow concrete block; Joint reinforcement; Lateral drifts; Masonry; Multi-objectives optimization; Reinforced masonry walls; Shears strength; Multiobjective optimization";"Performance of partially grouted reinforced masonry walls with bed-joint reinforcement: parametric and optimization investigation Post-seismic assessments have indicated that the responses of masonry walls to seismic forces may be classified into two key categories: in-plane global mechanisms and out-of-plane cyclical actions occurring orthogonally to the wall. The initial phase of this investigation scrutinized the widely accepted shear strength models for in-plane shear resistance, V n, of reinforced masonry (RM) walls. Subsequently, using a dataset comprising 78 samples of fully grouted hollow concrete block (HCB) RM walls, a detailed examination of the sensitivity of experimental shear strength, Vexp , to geometric, mechanical, and reinforcement characteristics of the wall was undertaken. The study's second phase entailed a parametric evaluation using finite element analysis to appraise the sensitivity of lateral drift to wall geometry and bed-joint reinforcement attributes. The third phase of the research introduced an informational model for estimating the lateral drift of partially grouted RM walls, incorporating BJ and vertical reinforcement. The model was established utilizing data from 44 full-scale in-plane cyclic tests on clay brick walls and 32 tests on HCB walls. The investigation further presented a multi-objective optimization methodology to ascertain the optimal vertical and BJR ratios, ρ v and ρ BJ. A graphical user interface and an accompanying empirical equation were also devised to simplify the analysis and design process for reinforced masonry walls, obviating the need for lengthy analyses. Increasing the BJR size from 6 to 8 mm resulted in a 30% increase in Vn for specimens with six BJR rows, while increasing the number of BJR rows from six to ten led to 16% rise in Vn , as demonstrated by numerical modeling validated against experimental tests.The findings of the study highlight a notable dependence of the lateral drift capacity of reinforced masonry walls on wall geometry and ρ BJ ratio. These revelations provide invaluable insights for designing earthquake-resistant masonry edifices and formulating rehabilitation strategies for existing masonry structures deficient in seismic resilience. © 2023, Wroclaw University of Science and Technology. Artificial neural network; Bed-joint reinforcement; Clay brick; Hollow concrete block; Lateral drift; Masonry; Multi-objective optimization; Shear wall Concrete construction; Earthquakes; Geometry; Graphical user interfaces; Grouting; Mortar; Neural networks; Reinforced concrete; Retaining walls; Bed joints; Bed-joint reinforcement; Clay bricks; Hollow concrete block; Joint reinforcement; Lateral drifts; Masonry; Multi-objectives optimization; Reinforced masonry walls; Shears strength; Multiobjective optimization";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
379;A novel hybrid machine learning model for rapid assessment of wave and storm surge responses over an extended coastal region;Storm surge and waves are responsible for a substantial portion of tropical and extratropical cyclones-related damages. While high-fidelity numerical models have significantly advanced the simulation accuracy of storm surge and waves, they are not practical to be employed for probabilistic analysis, risk assessment or rapid prediction due to their high computational demands. In this study, a novel hybrid model combining dimensionality reduction and data-driven techniques is developed for rapid assessment of waves and storm surge responses over an extended coastal region. Specifically, the hybrid model simultaneously identifies a low-dimensional representation of the high-dimensional spatial system based on a deep autoencoder (DAE) while mapping the storm parameters to the obtained low-dimensional latent space using a deep neural network (DNN). To train the hybrid model, a combined weighted loss function is designed to encourage a balance between DAE and DNN training and achieve the best accuracy. The performance of the hybrid model is evaluated through a case study using the synthetic data from the North Atlantic Comprehensive Coastal Study (NACCS) covering critical regions within New York and New Jersey. In addition, the proposed approach is compared with two decoupled models where the regression model is based on DNN and the reduction techniques are either principal component analysis (PCA) or DAE which are trained separately from the DNN model. High accuracy and computational efficiency are observed for the hybrid model which could be readily implemented as part of early warning systems or probabilistic risk assessment of waves and storm surge. © 2024 The Authors;"Deep autoencoder; Deep learning; Significant wave height; Storm surge";"Atlantic Ocean; Atlantic Ocean (North); New Jersey; New York [United States]; United States; Computational efficiency; Deep neural networks; Floods; Hurricanes; Learning systems; Principal component analysis; Regression analysis; Risk analysis; Storms; Auto encoders; Coastal regions; Deep autoencoder; Deep learning; Hybrid machine learning; Hybrid model; Rapid assessment; Significant wave height; Storm surges; Storm waves; assessment method; coastal engineering; early warning system; machine learning; ocean wave; storm surge; wave height; Risk assessment";"A novel hybrid machine learning model for rapid assessment of wave and storm surge responses over an extended coastal region Storm surge and waves are responsible for a substantial portion of tropical and extratropical cyclones-related damages. While high-fidelity numerical models have significantly advanced the simulation accuracy of storm surge and waves, they are not practical to be employed for probabilistic analysis, risk assessment or rapid prediction due to their high computational demands. In this study, a novel hybrid model combining dimensionality reduction and data-driven techniques is developed for rapid assessment of waves and storm surge responses over an extended coastal region. Specifically, the hybrid model simultaneously identifies a low-dimensional representation of the high-dimensional spatial system based on a deep autoencoder (DAE) while mapping the storm parameters to the obtained low-dimensional latent space using a deep neural network (DNN). To train the hybrid model, a combined weighted loss function is designed to encourage a balance between DAE and DNN training and achieve the best accuracy. The performance of the hybrid model is evaluated through a case study using the synthetic data from the North Atlantic Comprehensive Coastal Study (NACCS) covering critical regions within New York and New Jersey. In addition, the proposed approach is compared with two decoupled models where the regression model is based on DNN and the reduction techniques are either principal component analysis (PCA) or DAE which are trained separately from the DNN model. High accuracy and computational efficiency are observed for the hybrid model which could be readily implemented as part of early warning systems or probabilistic risk assessment of waves and storm surge. © 2024 The Authors Deep autoencoder; Deep learning; Significant wave height; Storm surge Atlantic Ocean; Atlantic Ocean (North); New Jersey; New York [United States]; United States; Computational efficiency; Deep neural networks; Floods; Hurricanes; Learning systems; Principal component analysis; Regression analysis; Risk analysis; Storms; Auto encoders; Coastal regions; Deep autoencoder; Deep learning; Hybrid machine learning; Hybrid model; Rapid assessment; Significant wave height; Storm surges; Storm waves; assessment method; coastal engineering; early warning system; machine learning; ocean wave; storm surge; wave height; Risk assessment";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.3;Meteorological;2;Preparation
380;A Rapid Assessment Method for Flood Risk Mapping Integrating Aerial Point Clouds and Deep Learning;In recent years, floods have brought renewed attention and requirement for real-time and city-scaled flood forecasting due to climate change and urbanization. In this study, a rapid assessment method for flood risk mapping is proposed by integrating aerial point clouds and deep learning technique that is capable of superior modeling efficiency and analysis accuracy for flood risk mapping. The method includes four application modules, i.e., data acquisition and preprocessing by oblique photography, large-scale point clouds segmentation by RandLA-Net, high-precision digital elevation model (DEM) reconstruction by modified hierarchical smoothing filtering algorithm, and hydrodynamics simulation based on hydrodynamics. To demonstrate the advantages of the proposed rapid assessment method more clearly, a case study is conducted in a local area of the South-to-North Water Transfer Project in China. The proposed method achieved 70.85% in mean intersection over union (mIoU) and 88.70% in overall accuracy (OAcc), outperforming the PointNet and PointNet++ networks. For the case point cloud containing nearly 50 million points, the computation time is less than 9 min, while the computation times for PointNet and PointNet++ are both more than 24 h. Then, high-precision DEM reconstruction by proposed hierarchical smoothing method with topographic feature embedding. These results demonstrate the efficiency and accuracy of the proposed method in processing large-scale 3D point clouds and rapid assessment of flood risk, providing a new perspective and effective solution for flood risk mapping in the field of spatial information science. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.;"DEM reconstruction; Flood risk mapping; Hydrodynamics simulation; Point clouds segmentation";"China; Aerial photography; Climate change; Data acquisition; Deep learning; Efficiency; Flood control; Floods; Hydrodynamics; Learning systems; Mapping; Risk assessment; Surveying; Weather forecasting; Digital elevation model reconstruction; Flood risk mapping; Flood risks; High-precision; Hydrodynamic simulation; Large-scales; Point cloud segmentation; Point-clouds; Rapid assessment; Risk mappings; digital elevation model; flood; hydrodynamics; machine learning; mapping; reconstruction; risk assessment; segmentation; Antennas";"A Rapid Assessment Method for Flood Risk Mapping Integrating Aerial Point Clouds and Deep Learning In recent years, floods have brought renewed attention and requirement for real-time and city-scaled flood forecasting due to climate change and urbanization. In this study, a rapid assessment method for flood risk mapping is proposed by integrating aerial point clouds and deep learning technique that is capable of superior modeling efficiency and analysis accuracy for flood risk mapping. The method includes four application modules, i.e., data acquisition and preprocessing by oblique photography, large-scale point clouds segmentation by RandLA-Net, high-precision digital elevation model (DEM) reconstruction by modified hierarchical smoothing filtering algorithm, and hydrodynamics simulation based on hydrodynamics. To demonstrate the advantages of the proposed rapid assessment method more clearly, a case study is conducted in a local area of the South-to-North Water Transfer Project in China. The proposed method achieved 70.85% in mean intersection over union (mIoU) and 88.70% in overall accuracy (OAcc), outperforming the PointNet and PointNet++ networks. For the case point cloud containing nearly 50 million points, the computation time is less than 9 min, while the computation times for PointNet and PointNet++ are both more than 24 h. Then, high-precision DEM reconstruction by proposed hierarchical smoothing method with topographic feature embedding. These results demonstrate the efficiency and accuracy of the proposed method in processing large-scale 3D point clouds and rapid assessment of flood risk, providing a new perspective and effective solution for flood risk mapping in the field of spatial information science. © The Author(s), under exclusive licence to Springer Nature B.V. 2024. DEM reconstruction; Flood risk mapping; Hydrodynamics simulation; Point clouds segmentation China; Aerial photography; Climate change; Data acquisition; Deep learning; Efficiency; Flood control; Floods; Hydrodynamics; Learning systems; Mapping; Risk assessment; Surveying; Weather forecasting; Digital elevation model reconstruction; Flood risk mapping; Flood risks; High-precision; Hydrodynamic simulation; Large-scales; Point cloud segmentation; Point-clouds; Rapid assessment; Risk mappings; digital elevation model; flood; hydrodynamics; machine learning; mapping; reconstruction; risk assessment; segmentation; Antennas";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
381;Fast earthquake recognition method based on DAS and one dimensional QRE-net;Earthquake early warning can provide important information before the occurrence of destructive disasters to reduce the possible loss, and the requirements for earthquake identification time and accuracy are strict. As a new seismic monitoring technology, distributed optical fiber acoustic sensing (DAS) has the advantages of wide coverage and spatially dense detection. DAS can be deployed in earthquake-prone areas, thus saving the time for seismic waves to reach the sensors. In order to reduce the recognition time and improve the accuracy, this paper proposes a fast earthquake recognition method based on DAS and quickly recognition earthquake network (QRE-net). After preprocessing the seismic data collected in DAS, QRE-net identifies the seismic signals. The effectiveness of the proposed method is tested on natural seismic data collected by DAS, when DAS detect 1s seismic signals, recognition accuracy reaches 88.74%. It is believed that this method will play an important role in earthquake early warning. © 2024 Elsevier B.V.;"Deep learning; Distributed optical fiber acoustic sensing; Earthquake recognition; EEW";"Deep learning; Optical fibers; Seismic response; Seismic waves; Acoustic sensing; Deep learning; Distributed optical fiber; Distributed optical fiber acoustic sensing; Earthquake early warning; Earthquake networks; Earthquake recognition; EEW; Recognition methods; Seismic datas; Earthquakes";"Fast earthquake recognition method based on DAS and one dimensional QRE-net Earthquake early warning can provide important information before the occurrence of destructive disasters to reduce the possible loss, and the requirements for earthquake identification time and accuracy are strict. As a new seismic monitoring technology, distributed optical fiber acoustic sensing (DAS) has the advantages of wide coverage and spatially dense detection. DAS can be deployed in earthquake-prone areas, thus saving the time for seismic waves to reach the sensors. In order to reduce the recognition time and improve the accuracy, this paper proposes a fast earthquake recognition method based on DAS and quickly recognition earthquake network (QRE-net). After preprocessing the seismic data collected in DAS, QRE-net identifies the seismic signals. The effectiveness of the proposed method is tested on natural seismic data collected by DAS, when DAS detect 1s seismic signals, recognition accuracy reaches 88.74%. It is believed that this method will play an important role in earthquake early warning. © 2024 Elsevier B.V. Deep learning; Distributed optical fiber acoustic sensing; Earthquake recognition; EEW Deep learning; Optical fibers; Seismic response; Seismic waves; Acoustic sensing; Deep learning; Distributed optical fiber; Distributed optical fiber acoustic sensing; Earthquake early warning; Earthquake networks; Earthquake recognition; EEW; Recognition methods; Seismic datas; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
382;Digital Twin Research on Masonry–Timber Architectural Heritage Pathology Cracks Using 3D Laser Scanning and Deep Learning Model;Due to various factors such as aging, natural environment erosion, and man-made destruction, architectural heritage has formed various diseases and cracks, especially in pathology cracks, which are the most typical masonry–timber architectural heritages, directly affecting the structural stability of masonry–timber buildings. This paper uses artificial intelligence and architecture and other multi-disciplinary research methods, taking James Jackson Gymnasium, a famous masonry–timber architectural heritage in Wuhan, as an example, using 3D laser scanning technology to obtain disease details and crack data of architectural heritage, using a Mask R-CNN model to detect crack area, using an FCN model to identify and calculate single cracks, and finally summarizing the type, location, and characteristics of cracks, analyzing the causes of cracks, and then putting forward corresponding hierarchical restoration strategies. The research results build a set of detection and repair systems of masonry–timber architectural heritage pathology cracks, which provide a set of accurate and objective pathology cracks data for architectural heritage protection and repair, and provide a reference for architectural heritage repair. © 2024 by the authors.;"3D laser scanning; architectural heritage; artificial intelligence application; deep learning; pathology cracks";"Architecture; Deep learning; E-learning; Image reconstruction; Laser applications; Pathology; Stability; Three dimensional computer graphics; 3D Laser scanning; Architectural heritage; Artificial intelligence application; Deep learning; Learning models; Multi-disciplinary research; Natural environments; Pathology crack; Structural stabilities; Timber buildings; Timber";"Digital Twin Research on Masonry–Timber Architectural Heritage Pathology Cracks Using 3D Laser Scanning and Deep Learning Model Due to various factors such as aging, natural environment erosion, and man-made destruction, architectural heritage has formed various diseases and cracks, especially in pathology cracks, which are the most typical masonry–timber architectural heritages, directly affecting the structural stability of masonry–timber buildings. This paper uses artificial intelligence and architecture and other multi-disciplinary research methods, taking James Jackson Gymnasium, a famous masonry–timber architectural heritage in Wuhan, as an example, using 3D laser scanning technology to obtain disease details and crack data of architectural heritage, using a Mask R-CNN model to detect crack area, using an FCN model to identify and calculate single cracks, and finally summarizing the type, location, and characteristics of cracks, analyzing the causes of cracks, and then putting forward corresponding hierarchical restoration strategies. The research results build a set of detection and repair systems of masonry–timber architectural heritage pathology cracks, which provide a set of accurate and objective pathology cracks data for architectural heritage protection and repair, and provide a reference for architectural heritage repair. © 2024 by the authors. 3D laser scanning; architectural heritage; artificial intelligence application; deep learning; pathology cracks Architecture; Deep learning; E-learning; Image reconstruction; Laser applications; Pathology; Stability; Three dimensional computer graphics; 3D Laser scanning; Architectural heritage; Artificial intelligence application; Deep learning; Learning models; Multi-disciplinary research; Natural environments; Pathology crack; Structural stabilities; Timber buildings; Timber";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;4;Recovery
383;The development of a weather-type statistical downscaling model for wave climate based on wave clustering;Reliable long-term wave data are significant for understanding changes and variability of ocean waves, which has important implications for coastal engineering, land erosion, and coastal flooding. This study develops a regression-guided weather-type statistical method for modelling regional or global significant wave height Hs. The model classifies the atmospheric circulation patterns (predictor) through the regression-guided clustering approach, linking the atmospheric circulation with clustered regional Hs (predictand). It is applied in the Chinese marginal seas and also the global ocean. A comprehensive skill assessment shows robust skill and computationally efficiency of the model in capturing climatology and variability of both mean and extreme Hs in the Chinese marginal seas and global oceans. Furthermore, the reconstructed global Hs data show similar seasonal trends as the ERA5 data, with a gradual decrease in Hs observed during boreal summer in the central Pacific and western North Atlantic regions at lower latitudes. This method proves to be robust for both regional and global Hs reconstruction, and also applicable for Hs climate prediction and projections. © 2024 Elsevier Ltd;"Extreme waves; Statistical downscaling; Wave climate; Wave clustering; Weather-type";"China; Climate models; Climatology; Coastal engineering; Floods; Oceanography; Clusterings; Extreme waves; Global ocean; Land erosion; Marginal seas; Statistical downscaling; Wave climates; Wave clustering; Wave data; Weather types; cluster analysis; downscaling; statistical analysis; wave climate; wave modeling; weather; Water waves";"The development of a weather-type statistical downscaling model for wave climate based on wave clustering Reliable long-term wave data are significant for understanding changes and variability of ocean waves, which has important implications for coastal engineering, land erosion, and coastal flooding. This study develops a regression-guided weather-type statistical method for modelling regional or global significant wave height Hs. The model classifies the atmospheric circulation patterns (predictor) through the regression-guided clustering approach, linking the atmospheric circulation with clustered regional Hs (predictand). It is applied in the Chinese marginal seas and also the global ocean. A comprehensive skill assessment shows robust skill and computationally efficiency of the model in capturing climatology and variability of both mean and extreme Hs in the Chinese marginal seas and global oceans. Furthermore, the reconstructed global Hs data show similar seasonal trends as the ERA5 data, with a gradual decrease in Hs observed during boreal summer in the central Pacific and western North Atlantic regions at lower latitudes. This method proves to be robust for both regional and global Hs reconstruction, and also applicable for Hs climate prediction and projections. © 2024 Elsevier Ltd Extreme waves; Statistical downscaling; Wave climate; Wave clustering; Weather-type China; Climate models; Climatology; Coastal engineering; Floods; Oceanography; Clusterings; Extreme waves; Global ocean; Land erosion; Marginal seas; Statistical downscaling; Wave climates; Wave clustering; Wave data; Weather types; cluster analysis; downscaling; statistical analysis; wave climate; wave modeling; weather; Water waves";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
384;Enhanced prediction of vegetation responses to extreme drought using deep learning and Earth observation data;The advent of abundant Earth observation data enables the development of novel predictive methods for forecasting climate impacts on the state and health of terrestrial ecosystems. Here, we predict the spatial and temporal variations of land surface reflectance and vegetation greenness, measuring the density of green vegetation and active foliage area, conditioned on current and past weather and the local topography. We train two alternative recurrent deep learning models that combine Long Short-Term Memory cells with convolutional layers (ConvLSTM) for forecasting the spatially resolved deviation of surface reflectance across a heterogeneous landscape from a specified initial state. Using data from diverse ecosystems and land cover types across Europe and following a standardized model evaluation framework (EarthNet2021 Challenge), our results indicate increased performance in predicting surface greenness during extreme drought events of the models presented here, compared to currently published benchmarks. This demonstrates how deep learning methods for optical Earth observation time series enable an early-warning of vegetation responses to the impacts of climatic extreme events, such as the drought-related loss of green foliage. © 2024 The Author(s);"ConvLSTM; Drought impact forecasting; EarthNet2021; NDVI; Sentinel-2";"Europe; drought; extreme event; land surface; landscape change; spatiotemporal analysis";"Enhanced prediction of vegetation responses to extreme drought using deep learning and Earth observation data The advent of abundant Earth observation data enables the development of novel predictive methods for forecasting climate impacts on the state and health of terrestrial ecosystems. Here, we predict the spatial and temporal variations of land surface reflectance and vegetation greenness, measuring the density of green vegetation and active foliage area, conditioned on current and past weather and the local topography. We train two alternative recurrent deep learning models that combine Long Short-Term Memory cells with convolutional layers (ConvLSTM) for forecasting the spatially resolved deviation of surface reflectance across a heterogeneous landscape from a specified initial state. Using data from diverse ecosystems and land cover types across Europe and following a standardized model evaluation framework (EarthNet2021 Challenge), our results indicate increased performance in predicting surface greenness during extreme drought events of the models presented here, compared to currently published benchmarks. This demonstrates how deep learning methods for optical Earth observation time series enable an early-warning of vegetation responses to the impacts of climatic extreme events, such as the drought-related loss of green foliage. © 2024 The Author(s) ConvLSTM; Drought impact forecasting; EarthNet2021; NDVI; Sentinel-2 Europe; drought; extreme event; land surface; landscape change; spatiotemporal analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
385;An automated approach for human-animal conflict minimisation in Assam and protection of wildlife around the Kaziranga National Park using YOLO and SENet Attention Framework;Human-animal conflict in Assam, India's north-eastern state, is rising continuously. Because it occurs year-round, it damages agricultural productivity and kills people and animals, including elephants. When a herd of wild elephants emerges from a deep forest and enters human-inhabited territory around the Kaziranga National Park (KNP) in Assam, an alert must be sounded for the neighbourhood residents and forest workers to prevent conflicts. Another concern is that many wild animals die near the KNP while crossing the national highway NH-37 which traverses the area. During floods, animals flee to the highlands for food and shelter. An automated animal identification and warning system near the KNP may reduce human-animal confrontations. This paper reports the design of a system that attempts to address the above concerns. Artificial Intelligence (AI)-based strategies are utilized to recognize wild animals from live video sequences, provide warnings to avoid encounters, and protect humans and animals. Deep learning models and YoloV5 with the SENet attention layer are used to recognize wild animals in real-time. This model is trained using a public and customized dataset of animal species. Cameras attached to the cloud-based AI system take photographs from several KNP locations to confirm the model. The model's 96% accuracy in animal photographs and videos taken day and night and in feed from contemporaneous location has shown its utility. The model also improves reliability by 1–13% over previous methods. © 2023;"Animal detection; Computer vision; Deep learning; Human-animal conflict; Kaziranga; Object detection; Yolo";"Assam; India; Kaziranga National Park; artificial intelligence; computer vision; detection method; early warning system; neighborhood; spatiotemporal analysis; wild population";"An automated approach for human-animal conflict minimisation in Assam and protection of wildlife around the Kaziranga National Park using YOLO and SENet Attention Framework Human-animal conflict in Assam, India's north-eastern state, is rising continuously. Because it occurs year-round, it damages agricultural productivity and kills people and animals, including elephants. When a herd of wild elephants emerges from a deep forest and enters human-inhabited territory around the Kaziranga National Park (KNP) in Assam, an alert must be sounded for the neighbourhood residents and forest workers to prevent conflicts. Another concern is that many wild animals die near the KNP while crossing the national highway NH-37 which traverses the area. During floods, animals flee to the highlands for food and shelter. An automated animal identification and warning system near the KNP may reduce human-animal confrontations. This paper reports the design of a system that attempts to address the above concerns. Artificial Intelligence (AI)-based strategies are utilized to recognize wild animals from live video sequences, provide warnings to avoid encounters, and protect humans and animals. Deep learning models and YoloV5 with the SENet attention layer are used to recognize wild animals in real-time. This model is trained using a public and customized dataset of animal species. Cameras attached to the cloud-based AI system take photographs from several KNP locations to confirm the model. The model's 96% accuracy in animal photographs and videos taken day and night and in feed from contemporaneous location has shown its utility. The model also improves reliability by 1–13% over previous methods. © 2023 Animal detection; Computer vision; Deep learning; Human-animal conflict; Kaziranga; Object detection; Yolo Assam; India; Kaziranga National Park; artificial intelligence; computer vision; detection method; early warning system; neighborhood; spatiotemporal analysis; wild population";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
386;Incorporating spatial autocorrelation into deformable ConvLSTM for hourly precipitation forecasting;Hourly precipitation forecasting is considered a spatiotemporal sequence forecasting problem that plays an increasingly important role in early warning of rainfall-induced floods and secondary disasters. Current popular precipitation forecasting methods usually ignore the spatial autocorrelation features, resulting in limited spatial information representations and extractions. In this work, a deformable convolution long short-term memory model considering spatial autocorrelation (DConvLSTM-SAC) is proposed for short-term precipitation forecasting. The proposed model extracts irregularly distributed precipitation information by deformable convolution with better generalization ability and calculates the spatial autocorrelation of inputs by calculating the local Moran index of precipitation images to fully utilize the spatial distribution information of features. In the experiment, the consecutive 3-h precipitation image sequences are used as the input data for the model to predict the precipitation images for the next 3 h. Four commonly used heuristic forecasting methods are adopted as baseline models, including decision tree regression (DTR), random forest regression (RF), recurrent neural network (RNN) and convolutional long short-term memory (ConvLSTM) methods. The proposed DConvLSTM-SAC model exhibits higher predictive accuracy than the four baseline models during the experiments and effectively improves the spatial patterns, with the most significant improvement in the first-hour lead time. The average R2 values for the first-hour prediction are 0.842, 0.838, 0.834, 0.850 and 0.892 for the DTR, RF, RNN, ConvLSTM and DConvLSTM-SAC models, respectively. As a result, the average R2 increased by 4.96%, and the average RMSE (MAE) decreased by 15.21% (9.01%) for the proposed DConvLSTM-SAC model relative to the ConvLSTM method during the first-hour prediction. © 2024 Elsevier Ltd;"ConvLSTM; Deformable convolution; Short-term precipitation forecasting; Spatial autocorrelation";"Autocorrelation; Brain; Convolution; Decision trees; Heuristic methods; Weather forecasting; Autocorrelation modeling; Baseline models; Convolutional long short-term memory; Decision tree regression; Deformable convolution; Forecasting methods; Memory modeling; Precipitation forecasting; Short-term precipitation forecasting; Spatial autocorrelations; autocorrelation; computer simulation; numerical model; precipitation assessment; precipitation intensity; spatiotemporal analysis; weather forecasting; Long short-term memory";"Incorporating spatial autocorrelation into deformable ConvLSTM for hourly precipitation forecasting Hourly precipitation forecasting is considered a spatiotemporal sequence forecasting problem that plays an increasingly important role in early warning of rainfall-induced floods and secondary disasters. Current popular precipitation forecasting methods usually ignore the spatial autocorrelation features, resulting in limited spatial information representations and extractions. In this work, a deformable convolution long short-term memory model considering spatial autocorrelation (DConvLSTM-SAC) is proposed for short-term precipitation forecasting. The proposed model extracts irregularly distributed precipitation information by deformable convolution with better generalization ability and calculates the spatial autocorrelation of inputs by calculating the local Moran index of precipitation images to fully utilize the spatial distribution information of features. In the experiment, the consecutive 3-h precipitation image sequences are used as the input data for the model to predict the precipitation images for the next 3 h. Four commonly used heuristic forecasting methods are adopted as baseline models, including decision tree regression (DTR), random forest regression (RF), recurrent neural network (RNN) and convolutional long short-term memory (ConvLSTM) methods. The proposed DConvLSTM-SAC model exhibits higher predictive accuracy than the four baseline models during the experiments and effectively improves the spatial patterns, with the most significant improvement in the first-hour lead time. The average R2 values for the first-hour prediction are 0.842, 0.838, 0.834, 0.850 and 0.892 for the DTR, RF, RNN, ConvLSTM and DConvLSTM-SAC models, respectively. As a result, the average R2 increased by 4.96%, and the average RMSE (MAE) decreased by 15.21% (9.01%) for the proposed DConvLSTM-SAC model relative to the ConvLSTM method during the first-hour prediction. © 2024 Elsevier Ltd ConvLSTM; Deformable convolution; Short-term precipitation forecasting; Spatial autocorrelation Autocorrelation; Brain; Convolution; Decision trees; Heuristic methods; Weather forecasting; Autocorrelation modeling; Baseline models; Convolutional long short-term memory; Decision tree regression; Deformable convolution; Forecasting methods; Memory modeling; Precipitation forecasting; Short-term precipitation forecasting; Spatial autocorrelations; autocorrelation; computer simulation; numerical model; precipitation assessment; precipitation intensity; spatiotemporal analysis; weather forecasting; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
387;Ground motion prediction model for shallow crustal earthquakes in Japan based on XGBoost with Bayesian optimization;Ground motion prediction is an important and complex research subject in earthquake engineering and traditional approaches based on statistical regression have much room for improvement in prediction accuracy. Utilizing the 67,164 ground motion records from KiK-net and K-Net for 777 shallow crustal earthquakes between 1997 and 2019 in Japan, this paper proposes a ground motion prediction model XGBoost-SC based on the machine learning algorithm of eXtreme Gradient Boosting (XGBoost) for Japan. Magnitude, focal depth, hypo-central distance, Vs30, site altitude, and focal mechanism were used as the feature parameters, and XGBoost, Random Forest, and Deep Neural Networks (DNN) algorithms were selected for model training while Bayesian optimization was used to search for optimized hyperparameters to improve the prediction accuracy. XGBoost algorithm was selected for further study based on the comparison of results from the three algorithms. Residual change with magnitude and hypo-central distance, the probability distribution of residuals, residual standard deviation (σ), residual mean squared error (MSE), and Pearson correlation coefficient (R) were used as the evaluation parameters, and a comparison study was performed against the ground motion prediction equation based on traditional approaches. Actual earthquake events were selected to compare the prediction results against the observation records. To further validate and explain the proposed model, SHapley Additive exPlanations (SHAP) analysis was performed to explain the impact of selected feature parameters on the proposed model. The results demonstrate that the proposed XGBoost-SC model has good prediction stability for all periods, and its residual errors are smaller than those of other models. Therefore, the proposed model can better reflect the ground motion attenuation for shallow crustal earthquakes in Japan and can serve as a better model for ground motion prediction in future aseismic design and earthquake disaster mitigation efforts. © 2023 Elsevier Ltd;"Bayesian optimization; Ground motion prediction model; Machine learning; SHAP";"Japan; Adaptive boosting; Bayesian networks; Deep neural networks; Earthquake engineering; Equations of motion; Forestry; Learning systems; Mean square error; Motion estimation; Parameter estimation; Probability distributions; Bayesian optimization; Central distance; Crustal earthquakes; Ground motion prediction; Ground-motion prediction models; Machine-learning; Prediction accuracy; Shapley; Shapley additive explanation; Traditional approaches; Bayesian analysis; crustal structure; earthquake magnitude; ground motion; optimization; prediction; software; Forecasting";"Ground motion prediction model for shallow crustal earthquakes in Japan based on XGBoost with Bayesian optimization Ground motion prediction is an important and complex research subject in earthquake engineering and traditional approaches based on statistical regression have much room for improvement in prediction accuracy. Utilizing the 67,164 ground motion records from KiK-net and K-Net for 777 shallow crustal earthquakes between 1997 and 2019 in Japan, this paper proposes a ground motion prediction model XGBoost-SC based on the machine learning algorithm of eXtreme Gradient Boosting (XGBoost) for Japan. Magnitude, focal depth, hypo-central distance, Vs30, site altitude, and focal mechanism were used as the feature parameters, and XGBoost, Random Forest, and Deep Neural Networks (DNN) algorithms were selected for model training while Bayesian optimization was used to search for optimized hyperparameters to improve the prediction accuracy. XGBoost algorithm was selected for further study based on the comparison of results from the three algorithms. Residual change with magnitude and hypo-central distance, the probability distribution of residuals, residual standard deviation (σ), residual mean squared error (MSE), and Pearson correlation coefficient (R) were used as the evaluation parameters, and a comparison study was performed against the ground motion prediction equation based on traditional approaches. Actual earthquake events were selected to compare the prediction results against the observation records. To further validate and explain the proposed model, SHapley Additive exPlanations (SHAP) analysis was performed to explain the impact of selected feature parameters on the proposed model. The results demonstrate that the proposed XGBoost-SC model has good prediction stability for all periods, and its residual errors are smaller than those of other models. Therefore, the proposed model can better reflect the ground motion attenuation for shallow crustal earthquakes in Japan and can serve as a better model for ground motion prediction in future aseismic design and earthquake disaster mitigation efforts. © 2023 Elsevier Ltd Bayesian optimization; Ground motion prediction model; Machine learning; SHAP Japan; Adaptive boosting; Bayesian networks; Deep neural networks; Earthquake engineering; Equations of motion; Forestry; Learning systems; Mean square error; Motion estimation; Parameter estimation; Probability distributions; Bayesian optimization; Central distance; Crustal earthquakes; Ground motion prediction; Ground-motion prediction models; Machine-learning; Prediction accuracy; Shapley; Shapley additive explanation; Traditional approaches; Bayesian analysis; crustal structure; earthquake magnitude; ground motion; optimization; prediction; software; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
388;Modeling wildland fire burn severity in California using a spatial Super Learner approach;Given the increasing prevalence of wildland fires in the Western US, there is a critical need to develop tools to understand and accurately predict burn severity. We develop a novel machine learning model to predict post-fire burn severity using pre-fire remotely sensed data. Hydrological, ecological, and topographical variables collected from four regions of California — the site of the Kincade fire (2019), the CZU Lightning Complex fire (2020), the Windy fire (2021), and the KNP Fire (2021) — are used as predictors of the differenced normalized burn ratio. We hypothesize that a Super Learner (SL) algorithm that accounts for spatial autocorrelation using Vecchia’s Gaussian approximation will accurately model burn severity. We use a cross-validation study to show that the spatial SL model can predict burn severity with reasonable classification accuracy, including high burn severity events. After fitting and verifying the performance of the SL model, we use interpretable machine learning tools to determine the main drivers of severe burn damage, including greenness, elevation, and fire weather variables. These findings provide actionable insights that enable communities to strategize interventions, such as early fire detection systems, pre-fire season vegetation clearing activities, and resource allocation during emergency responses. When implemented, this model has the potential to minimize the loss of human life, property, resources, and ecosystems in California. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.;"Ensemble prediction; Kriging; Machine learning; Remote sensing";"California; United States; algorithm; kriging; machine learning; modeling; prediction; remote sensing; vegetation; wildfire";"Modeling wildland fire burn severity in California using a spatial Super Learner approach Given the increasing prevalence of wildland fires in the Western US, there is a critical need to develop tools to understand and accurately predict burn severity. We develop a novel machine learning model to predict post-fire burn severity using pre-fire remotely sensed data. Hydrological, ecological, and topographical variables collected from four regions of California — the site of the Kincade fire (2019), the CZU Lightning Complex fire (2020), the Windy fire (2021), and the KNP Fire (2021) — are used as predictors of the differenced normalized burn ratio. We hypothesize that a Super Learner (SL) algorithm that accounts for spatial autocorrelation using Vecchia’s Gaussian approximation will accurately model burn severity. We use a cross-validation study to show that the spatial SL model can predict burn severity with reasonable classification accuracy, including high burn severity events. After fitting and verifying the performance of the SL model, we use interpretable machine learning tools to determine the main drivers of severe burn damage, including greenness, elevation, and fire weather variables. These findings provide actionable insights that enable communities to strategize interventions, such as early fire detection systems, pre-fire season vegetation clearing activities, and resource allocation during emergency responses. When implemented, this model has the potential to minimize the loss of human life, property, resources, and ecosystems in California. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024. Ensemble prediction; Kriging; Machine learning; Remote sensing California; United States; algorithm; kriging; machine learning; modeling; prediction; remote sensing; vegetation; wildfire";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
389;Hurricane Ian Damage Assessment Using Aerial Imagery and LiDAR: A Case Study of Estero Island, Florida;Remote sensing techniques have emerged as an essential tool for conducting damage assessments and are commonly used to improve disaster recovery planning and community resilience policies. The objective of this study was to use aerial imagery data and LiDAR to identify the hardest hit areas, quantify the extent of damages, and compare pre- and post-storm beach morphology conditions in Estero Island, Florida, relating to Hurricane Ian in 2022. This study identified >2400 structures that were impacted by Hurricane Ian, with 170 structures suffering extensive damage. Clustering of heavily damaged buildings was observed on the northern and central portions of the island, with lower levels of damage clustered on the southern part. Among the ‘severely damaged’ and ‘destroyed’ structures were seven mobile home subdivisions. The total assessed value of the heavily damaged structures was estimated at over USD 200 million. The results also indicated substantial post-storm debris and sand deposition across the entire island. Remote sensing provides advanced techniques that can help prioritize emergency response efforts after catastrophic impacts from a natural disaster. © 2024 by the authors.;"beach morphology; emergency response; hurricane; JABLTCX; LiDAR; remote sensing; supervised classification";NULL;"Hurricane Ian Damage Assessment Using Aerial Imagery and LiDAR: A Case Study of Estero Island, Florida Remote sensing techniques have emerged as an essential tool for conducting damage assessments and are commonly used to improve disaster recovery planning and community resilience policies. The objective of this study was to use aerial imagery data and LiDAR to identify the hardest hit areas, quantify the extent of damages, and compare pre- and post-storm beach morphology conditions in Estero Island, Florida, relating to Hurricane Ian in 2022. This study identified >2400 structures that were impacted by Hurricane Ian, with 170 structures suffering extensive damage. Clustering of heavily damaged buildings was observed on the northern and central portions of the island, with lower levels of damage clustered on the southern part. Among the ‘severely damaged’ and ‘destroyed’ structures were seven mobile home subdivisions. The total assessed value of the heavily damaged structures was estimated at over USD 200 million. The results also indicated substantial post-storm debris and sand deposition across the entire island. Remote sensing provides advanced techniques that can help prioritize emergency response efforts after catastrophic impacts from a natural disaster. © 2024 by the authors. beach morphology; emergency response; hurricane; JABLTCX; LiDAR; remote sensing; supervised classification NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;3;Response
390;Multi-hazard could exacerbate in coastal Bangladesh in the context of climate change;"Floods and landslides have cascading effects on coastal areas of Bangladesh. This study aims to develop multi-hazard maps (e.g., floods and landslides) for the coastal region, by integrating a genetic algorithm optimizer with Long Short-Term Memory deep learning algorithm (GA-LSTM-DLA), global climate model, and geospatial data. It assesses vulnerabilities to multiple hazards and projects future risks considering the Australian Community Climate and Earth System Simulator Earth System Model 1.5 (ACCESS-ESM1-5) under three climate scenarios, i.e., Shared Socioeconomic Pathways1-2.6, 2–4.5, and 5–8.5 (SSP1-2.6, SSP2-4.5, and SSP5-8.5). The Receiver Operating Characteristic (ROC) curve is used to assess the performance of GA-LSTM-DLA model. According to important feature obtained by random forest (RF) algorithm, most important features for flash, riverine, tidal floods, and landslides are drainage density, rainfall, and geology, respectively. The baseline susceptibility maps for flash, riverine, tidal floods, and landslides are initially recorded as 0.11%, 7.99%, 3.46%, and 0.03%. Projections for the year 2100, under different SSPs, show significant increase, i.e., SSP1-2.6—flash 1.31%, riverine 23.74%, tidal 26.97%, landslides 2.05%; SSP2-4.5—flash 0.12%, riverine 30.97%, tidal 10.23%, landslides 0.18%; SSP5-8.5—flash 0.02%, riverine 17.42%, tidal 6.19%, landslides 0.14%. These projections highlight urgent need for mitigation and adaptation measures against hazards susceptibility, particularly under more extreme socioeconomic scenarios. Overall, the findings of this work are critical for policymakers to develop informed strategies for climate resilience, sustainable development, and disaster risk reduction in the coastal region of Bangladesh. © 2024 Elsevier Ltd";"Coastal region; Deep learning algorithm; Global climate model; Multi-hazard; Sustainable development";"Climate change; Earth (planet); Earth system models; Floods; Forestry; Genetic algorithms; Hazards; Learning algorithms; Long short-term memory; Risk assessment; Risk perception; Sustainable development; Bangladesh; Cascading effects; Coastal regions; Deep learning algorithm; Global climate model; Global climates; Important features; Multi-hazards; Optimizers; Socio-economics; Climate models";"Multi-hazard could exacerbate in coastal Bangladesh in the context of climate change Floods and landslides have cascading effects on coastal areas of Bangladesh. This study aims to develop multi-hazard maps (e.g., floods and landslides) for the coastal region, by integrating a genetic algorithm optimizer with Long Short-Term Memory deep learning algorithm (GA-LSTM-DLA), global climate model, and geospatial data. It assesses vulnerabilities to multiple hazards and projects future risks considering the Australian Community Climate and Earth System Simulator Earth System Model 1.5 (ACCESS-ESM1-5) under three climate scenarios, i.e., Shared Socioeconomic Pathways1-2.6, 2–4.5, and 5–8.5 (SSP1-2.6, SSP2-4.5, and SSP5-8.5). The Receiver Operating Characteristic (ROC) curve is used to assess the performance of GA-LSTM-DLA model. According to important feature obtained by random forest (RF) algorithm, most important features for flash, riverine, tidal floods, and landslides are drainage density, rainfall, and geology, respectively. The baseline susceptibility maps for flash, riverine, tidal floods, and landslides are initially recorded as 0.11%, 7.99%, 3.46%, and 0.03%. Projections for the year 2100, under different SSPs, show significant increase, i.e., SSP1-2.6—flash 1.31%, riverine 23.74%, tidal 26.97%, landslides 2.05%; SSP2-4.5—flash 0.12%, riverine 30.97%, tidal 10.23%, landslides 0.18%; SSP5-8.5—flash 0.02%, riverine 17.42%, tidal 6.19%, landslides 0.14%. These projections highlight urgent need for mitigation and adaptation measures against hazards susceptibility, particularly under more extreme socioeconomic scenarios. Overall, the findings of this work are critical for policymakers to develop informed strategies for climate resilience, sustainable development, and disaster risk reduction in the coastal region of Bangladesh. © 2024 Elsevier Ltd Coastal region; Deep learning algorithm; Global climate model; Multi-hazard; Sustainable development Climate change; Earth (planet); Earth system models; Floods; Forestry; Genetic algorithms; Hazards; Learning algorithms; Long short-term memory; Risk assessment; Risk perception; Sustainable development; Bangladesh; Cascading effects; Coastal regions; Deep learning algorithm; Global climate model; Global climates; Important features; Multi-hazards; Optimizers; Socio-economics; Climate models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
391;Location of Disaster Management Bases Using Spatial Analysis;Pre-crisis management involves the optimal selection of relief and rescue centers to minimize vulnerability. Iran is particularly vulnerable due to its location on the Alpine-Himalaya seismic belt, resulting in an average death rate six times higher than the global average during earthquakes. Therefore, selecting appropriate relief and rescue centers is crucial to Iran’s disaster preparedness. When selecting the placement of rescue centers, accessibility and the appropriateness of the land should be taken into account as well as the distance from high-risk areas. The location of these centers does not require any particular combinations. To address this issue, a study was conducted utilizing GIS, artificial neural networks, fuzzy logic, and mathematical models to determine the optimal placement based on 12 indicators within two clusters: natural and human. To examine the information layers of the initial stage, a spatial data repository concerning the variables impacting the placement of these centers was established using ARCGIS. Using functions and algorithms such as Fuzzy Logic in IDRISI, TOPSIS, and VIKOR software, the layers were assessed for weightage before being overlaid. The study’s analysis of the models used revealed that the positioning priority limits of the areas differed across all four models. Notably, the areas with high desirability varied to a greater extent: the fuzzy model varied by 9.3%, neural network by 12.4%, VIKOR by 4.5%, and TOPSIS by 16.2%. The variance in results can be attributed to the differing levels of risk acceptance and non-acceptance in each model. Additionally, the study yielded other significant findings such as the correlation between study area size and model accuracy. Specifically, smaller study areas exhibited higher model accuracy. The research also demonstrated that both fuzzy and VIKOR models achieved greater accuracy. As a result, employing these models in crisis management planning, particularly in pre-crisis management for identifying rescue center locations, would be highly advantageous and increase the precision of these endeavors. © Systems Engineering Society of China and Springer-Verlag GmbH Germany 2023.;"Crisis management; earthquake risk; GIS; site selection";"Computer circuits; Disaster prevention; Disasters; Earthquakes; Fuzzy inference; Fuzzy neural networks; Geographic information systems; Location; Risk assessment; Crisis management; Disaster management; Earthquake risk; Fuzzy-Logic; Himalayas; Management basis; Modeling accuracy; Optimal selection; Spatial analysis; Study areas; Site selection";"Location of Disaster Management Bases Using Spatial Analysis Pre-crisis management involves the optimal selection of relief and rescue centers to minimize vulnerability. Iran is particularly vulnerable due to its location on the Alpine-Himalaya seismic belt, resulting in an average death rate six times higher than the global average during earthquakes. Therefore, selecting appropriate relief and rescue centers is crucial to Iran’s disaster preparedness. When selecting the placement of rescue centers, accessibility and the appropriateness of the land should be taken into account as well as the distance from high-risk areas. The location of these centers does not require any particular combinations. To address this issue, a study was conducted utilizing GIS, artificial neural networks, fuzzy logic, and mathematical models to determine the optimal placement based on 12 indicators within two clusters: natural and human. To examine the information layers of the initial stage, a spatial data repository concerning the variables impacting the placement of these centers was established using ARCGIS. Using functions and algorithms such as Fuzzy Logic in IDRISI, TOPSIS, and VIKOR software, the layers were assessed for weightage before being overlaid. The study’s analysis of the models used revealed that the positioning priority limits of the areas differed across all four models. Notably, the areas with high desirability varied to a greater extent: the fuzzy model varied by 9.3%, neural network by 12.4%, VIKOR by 4.5%, and TOPSIS by 16.2%. The variance in results can be attributed to the differing levels of risk acceptance and non-acceptance in each model. Additionally, the study yielded other significant findings such as the correlation between study area size and model accuracy. Specifically, smaller study areas exhibited higher model accuracy. The research also demonstrated that both fuzzy and VIKOR models achieved greater accuracy. As a result, employing these models in crisis management planning, particularly in pre-crisis management for identifying rescue center locations, would be highly advantageous and increase the precision of these endeavors. © Systems Engineering Society of China and Springer-Verlag GmbH Germany 2023. Crisis management; earthquake risk; GIS; site selection Computer circuits; Disaster prevention; Disasters; Earthquakes; Fuzzy inference; Fuzzy neural networks; Geographic information systems; Location; Risk assessment; Crisis management; Disaster management; Earthquake risk; Fuzzy-Logic; Himalayas; Management basis; Modeling accuracy; Optimal selection; Spatial analysis; Study areas; Site selection";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;2;Preparation
392;Spatio-attention-based network to improve heavy rainfall prediction over the complex terrain of Assam;Heavy rainfall events prediction at the local scale imposes a big challenge for meteorological agencies over the complex terrain areas in India such as Assam, Uttarakhand, and Himachal Pradesh and causes flash floods with severe consequences throughout the area causing a huge socio-economical loss over these regions. Assam is currently experiencing severe flooding in June 2023. Due to the limits of deterministic numerical weather models in accurately forecasting these events, this work investigates the incorporation of deep learning (DL) models, particularly spatial attention-based U-Net, using simulated daily collected rainfall outputs from various parametrization schemes. This is a pioneering effort to improve district-scale rainfall using the spatio-attention U-Net DL method, particularly over the orographically complex region such as Assam. The proposed model outperformed individual and ensemble Weather Research and Forecasting (WRF) model outputs over four days in June 2022, demonstrating greater abilities to forecast rainfall at the district scale with a mean absolute error of less than 10 mm. Additionally, the proposed model considerably outperformed WRF models by 51.3% in categorical rainfall prediction, achieving a high prediction accuracy of 91.9%. Furthermore, the proposed model has demonstrated improved spatial variation as compared to the WRF model by correctly predicting severe rainfall occurrences at the district scale, including Barpeta, Kamrup, Kokrajhar, and Nalbari. The WRF projections regularly underestimated rainfall intensity (< 100 mm), whereas the DL model's estimates matched actual rainfall readings from the India Meteorological Department (> 150 mm). On the quantitative estimation of rainfall thresholds using different skill scores, Equitable threat score values are more than 0.5 for all categories for the proposed model. In a nutshell, the findings of the study have direct implications for improving early warning systems and associated follow-up action in terms of developing efficient strategies toward better preparedness, mitigation, and adaptation measures over complex hilly regions to reduce loss of lives and properties. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.;"Deep learning (DL) models; Equitable threat score; Heavy rainfall events; U-Net";"Complex networks; Deep learning; Floods; Learning systems; Weather forecasting; Complex terrains; Deep learning  model; Equitable threat score; Heavy rainfall; Heavy rainfall event; Learning models; Rainfall event; Rainfall prediction; U-net; Weather research and forecasting models; Rain";"Spatio-attention-based network to improve heavy rainfall prediction over the complex terrain of Assam Heavy rainfall events prediction at the local scale imposes a big challenge for meteorological agencies over the complex terrain areas in India such as Assam, Uttarakhand, and Himachal Pradesh and causes flash floods with severe consequences throughout the area causing a huge socio-economical loss over these regions. Assam is currently experiencing severe flooding in June 2023. Due to the limits of deterministic numerical weather models in accurately forecasting these events, this work investigates the incorporation of deep learning (DL) models, particularly spatial attention-based U-Net, using simulated daily collected rainfall outputs from various parametrization schemes. This is a pioneering effort to improve district-scale rainfall using the spatio-attention U-Net DL method, particularly over the orographically complex region such as Assam. The proposed model outperformed individual and ensemble Weather Research and Forecasting (WRF) model outputs over four days in June 2022, demonstrating greater abilities to forecast rainfall at the district scale with a mean absolute error of less than 10 mm. Additionally, the proposed model considerably outperformed WRF models by 51.3% in categorical rainfall prediction, achieving a high prediction accuracy of 91.9%. Furthermore, the proposed model has demonstrated improved spatial variation as compared to the WRF model by correctly predicting severe rainfall occurrences at the district scale, including Barpeta, Kamrup, Kokrajhar, and Nalbari. The WRF projections regularly underestimated rainfall intensity (< 100 mm), whereas the DL model's estimates matched actual rainfall readings from the India Meteorological Department (> 150 mm). On the quantitative estimation of rainfall thresholds using different skill scores, Equitable threat score values are more than 0.5 for all categories for the proposed model. In a nutshell, the findings of the study have direct implications for improving early warning systems and associated follow-up action in terms of developing efficient strategies toward better preparedness, mitigation, and adaptation measures over complex hilly regions to reduce loss of lives and properties. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024. Deep learning (DL) models; Equitable threat score; Heavy rainfall events; U-Net Complex networks; Deep learning; Floods; Learning systems; Weather forecasting; Complex terrains; Deep learning  model; Equitable threat score; Heavy rainfall; Heavy rainfall event; Learning models; Rainfall event; Rainfall prediction; U-net; Weather research and forecasting models; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
393;Variational data augmentation for a learning-based granular predictive model of power outages;As the trend in climate change continues, extreme weather events are expected to occur with increasing frequency and severity and pose a significant threat to the electric power infrastructure. Regardless of the efforts a utility puts towards hardening the grid, storm-induced damage to the utility assets such as cables and distributed energy resources (DERs) that are particularly vulnerable to such events is unavoidable. Access to a highly granular, in space and time, outage forecasting tool with long lead times (i.e., days ahead) will enhance the efficiency of service restoration efforts. In this study, we propose to develop and implement a multi-model framework as an operational tool based on a granular and multi-day outage forecasting model using operational numerical weather prediction model forecasts and detailed component outage information. An innovative two-layered recurrent neural network, i.e., a long-short-term-memory (LSTM)-based variational autoencoder (VAE) framework and a sliding window are used to address the uneven distribution of different types of weather events and make better use of the time-series data. Case studies are performed to demonstrate the performance of the new framework. © 2024;"Outage prediction; Recurrent neural networks; Variational autoencoder; Variational data augmentation; Weather-related outages";"Climate change; Energy resources; Long short-term memory; Multilayer neural networks; Network layers; Weather forecasting; Weather information services; Auto encoders; Data augmentation; Electric power infrastructure; Extreme weather events; Outage prediction; Power outage; Predictive models; Variational autoencoder; Variational data augmentation; Weather-related outage; Learning systems";"Variational data augmentation for a learning-based granular predictive model of power outages As the trend in climate change continues, extreme weather events are expected to occur with increasing frequency and severity and pose a significant threat to the electric power infrastructure. Regardless of the efforts a utility puts towards hardening the grid, storm-induced damage to the utility assets such as cables and distributed energy resources (DERs) that are particularly vulnerable to such events is unavoidable. Access to a highly granular, in space and time, outage forecasting tool with long lead times (i.e., days ahead) will enhance the efficiency of service restoration efforts. In this study, we propose to develop and implement a multi-model framework as an operational tool based on a granular and multi-day outage forecasting model using operational numerical weather prediction model forecasts and detailed component outage information. An innovative two-layered recurrent neural network, i.e., a long-short-term-memory (LSTM)-based variational autoencoder (VAE) framework and a sliding window are used to address the uneven distribution of different types of weather events and make better use of the time-series data. Case studies are performed to demonstrate the performance of the new framework. © 2024 Outage prediction; Recurrent neural networks; Variational autoencoder; Variational data augmentation; Weather-related outages Climate change; Energy resources; Long short-term memory; Multilayer neural networks; Network layers; Weather forecasting; Weather information services; Auto encoders; Data augmentation; Electric power infrastructure; Extreme weather events; Outage prediction; Power outage; Predictive models; Variational autoencoder; Variational data augmentation; Weather-related outage; Learning systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
394;Designing an Image Classification Model on Emergency Incident Images using a Convolutional Neural Network for iRESPOND;In the face of increasing natural disasters and emergencies, there is a growing need for effective geospatial information systems to process and classify emergency reports in real time. This work presents a new Convolutional Neural Network (CNN) model that is intended to classify emergency images taken and delivered to the iRESPOND system. Through the utilization of training phases and various tools, frameworks, and techniques, the authors effectively used deep learning to develop the CNN model. This model improves disaster response and mitigation by enabling the iRESPOND system to categorize emergency incidents rapidly. The results showcase the model’s commendable performance, achieving a high accuracy of 95.02% on the test set. A comprehensive evaluation, including precision, recall, and F1-score metrics for individual classes, illuminates the model’s strengths and areas for improvement. Noteworthy is the model’s proficiency in classes such as ‘flood’, ‘infrastructure_damage’, ‘no_damage_buildings_street’, ‘no_damage_ water_related’, and ‘no_damage_wildlife_forest’, reflecting robust predictive capabilities in specific emergency scenarios. The interpretability of the CNN model is augmented through visualization techniques like LIME, Grad-CAM, and Grad-CAM++. Also, a visualization report featuring the original image alongside interpretability overlays provides information on the characteristics and areas of the original images that influence the model’s decisions. In conclusion, the model demonstrates efficacy in rapidly categorizing emergency incidents, providing a valuable tool for the response team. The recommendations for future work underscore the continuous refinement required for optimal performance, including addressing class imbalances, fine-tuning hyperparameters, exploring ensemble models, and expanding the diverse image datasets. © 2024 Seventh Sense Research Group®;"Emergency response; Keras; Machine learning; Resnet50; Tensorflow";NULL;"Designing an Image Classification Model on Emergency Incident Images using a Convolutional Neural Network for iRESPOND In the face of increasing natural disasters and emergencies, there is a growing need for effective geospatial information systems to process and classify emergency reports in real time. This work presents a new Convolutional Neural Network (CNN) model that is intended to classify emergency images taken and delivered to the iRESPOND system. Through the utilization of training phases and various tools, frameworks, and techniques, the authors effectively used deep learning to develop the CNN model. This model improves disaster response and mitigation by enabling the iRESPOND system to categorize emergency incidents rapidly. The results showcase the model’s commendable performance, achieving a high accuracy of 95.02% on the test set. A comprehensive evaluation, including precision, recall, and F1-score metrics for individual classes, illuminates the model’s strengths and areas for improvement. Noteworthy is the model’s proficiency in classes such as ‘flood’, ‘infrastructure_damage’, ‘no_damage_buildings_street’, ‘no_damage_ water_related’, and ‘no_damage_wildlife_forest’, reflecting robust predictive capabilities in specific emergency scenarios. The interpretability of the CNN model is augmented through visualization techniques like LIME, Grad-CAM, and Grad-CAM++. Also, a visualization report featuring the original image alongside interpretability overlays provides information on the characteristics and areas of the original images that influence the model’s decisions. In conclusion, the model demonstrates efficacy in rapidly categorizing emergency incidents, providing a valuable tool for the response team. The recommendations for future work underscore the continuous refinement required for optimal performance, including addressing class imbalances, fine-tuning hyperparameters, exploring ensemble models, and expanding the diverse image datasets. © 2024 Seventh Sense Research Group® Emergency response; Keras; Machine learning; Resnet50; Tensorflow NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
395;The combination mode of forest and SVM for power network disaster response failure identification;The damage to power transmission and distribution equipment in different regions is inseparable from the erosion of natural and artificial disasters, affecting the power grid's regular power supply and users' typical power consumption. Therefore, research on failure risk prediction technology for power transmission and distribution equipment under different disasters has increasingly become critical to power grid reliability. Based on the combination mode of forest and Support Vector Machine (SVM), this study proposes a disaster response failure prediction model that considers the time cumulative effect of disaster elements. Based on this model, the failure degree indicators of different disaster elements are constructed by analyzing the relationship between disaster frequency and time. Then, the historical disaster data is used to calculate the index and form a training set with the monitoring data. The binary decision graph transformation method is combined with the adjacent node priority method to obtain the minimum cut set, and the SVM regression method is used to train to get the prediction model. The experimental results show that the effectiveness and accuracy of the proposed method are verified through the analysis of numerical examples and comparison of various techniques. The model's accuracy is 8.63 % higher than that of the traditional disaster prediction model, and the error rate of disaster failure prediction analysis is not more than 0.01. © 2024;"Convolution neural network (CNN); Coping failure; Disaster evaluation state; Failure probability; Grid response task; One-sided selection function; Power grid disasters; Random forest (RF) probability distribution; Real-time (RT) type; Risk warning; Support vector machine (SVM) regression";"Damage detection; Electric power transmission networks; Failure (mechanical); Forecasting; Graph theory; Numerical methods; Outages; Power transmission; Probability distributions; Regression analysis; Support vector machines; Convolution neural network; Coping failure; Disaster evaluation; Disaster evaluation state; Failure Probability; Grid response task; One-sided selection function; Power grid disaster; Power grids; Probability: distributions; Random forest  probability distribution; Random forests; Real- time; Real-time  type; Risk warnings; Selection function; Support vector machine regressions; Electric power transmission";"The combination mode of forest and SVM for power network disaster response failure identification The damage to power transmission and distribution equipment in different regions is inseparable from the erosion of natural and artificial disasters, affecting the power grid's regular power supply and users' typical power consumption. Therefore, research on failure risk prediction technology for power transmission and distribution equipment under different disasters has increasingly become critical to power grid reliability. Based on the combination mode of forest and Support Vector Machine (SVM), this study proposes a disaster response failure prediction model that considers the time cumulative effect of disaster elements. Based on this model, the failure degree indicators of different disaster elements are constructed by analyzing the relationship between disaster frequency and time. Then, the historical disaster data is used to calculate the index and form a training set with the monitoring data. The binary decision graph transformation method is combined with the adjacent node priority method to obtain the minimum cut set, and the SVM regression method is used to train to get the prediction model. The experimental results show that the effectiveness and accuracy of the proposed method are verified through the analysis of numerical examples and comparison of various techniques. The model's accuracy is 8.63 % higher than that of the traditional disaster prediction model, and the error rate of disaster failure prediction analysis is not more than 0.01. © 2024 Convolution neural network (CNN); Coping failure; Disaster evaluation state; Failure probability; Grid response task; One-sided selection function; Power grid disasters; Random forest (RF) probability distribution; Real-time (RT) type; Risk warning; Support vector machine (SVM) regression Damage detection; Electric power transmission networks; Failure (mechanical); Forecasting; Graph theory; Numerical methods; Outages; Power transmission; Probability distributions; Regression analysis; Support vector machines; Convolution neural network; Coping failure; Disaster evaluation; Disaster evaluation state; Failure Probability; Grid response task; One-sided selection function; Power grid disaster; Power grids; Probability: distributions; Random forest  probability distribution; Random forests; Real- time; Real-time  type; Risk warnings; Selection function; Support vector machine regressions; Electric power transmission";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
396;Flood Insights: Integrating Remote and Social Sensing Data for Flood Exposure, Damage, and Urgent Needs Mapping;The absence of comprehensive situational awareness information poses a significant challenge for humanitarian organizations during their response efforts. We present Flood Insights, an end-to-end system that ingests data from multiple non-traditional data sources such as remote sensing, social sensing, and geospatial data. We employ state-of-the-art natural language processing and computer vision models to identify flood exposure, ground-level damage and flood reports, and most importantly, urgent needs of affected people. We deploy and test the system during a recent real-world catastrophe, the 2022 Pakistan floods, to surface critical situational and damage information at the district level. We validated the system's effectiveness through geographic regression analysis using official ground-truth data, showcasing its strong performance and explanatory power. Moreover, the system was commended by the United Nations Development Programme stationed in Pakistan, as well as local authorities, for pinpointing hard-hit districts and enhancing disaster response. Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.;NULL;"Disasters; Natural language processing systems; Regression analysis; Remote sensing; Awareness information; Data-source; End-to-end systems; Geo-spatial data; Non-traditional; Pakistan; Remote-sensing; Sensing data; Situational awareness; Social sensing; Floods";"Flood Insights: Integrating Remote and Social Sensing Data for Flood Exposure, Damage, and Urgent Needs Mapping The absence of comprehensive situational awareness information poses a significant challenge for humanitarian organizations during their response efforts. We present Flood Insights, an end-to-end system that ingests data from multiple non-traditional data sources such as remote sensing, social sensing, and geospatial data. We employ state-of-the-art natural language processing and computer vision models to identify flood exposure, ground-level damage and flood reports, and most importantly, urgent needs of affected people. We deploy and test the system during a recent real-world catastrophe, the 2022 Pakistan floods, to surface critical situational and damage information at the district level. We validated the system's effectiveness through geographic regression analysis using official ground-truth data, showcasing its strong performance and explanatory power. Moreover, the system was commended by the United Nations Development Programme stationed in Pakistan, as well as local authorities, for pinpointing hard-hit districts and enhancing disaster response. Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. NULL Disasters; Natural language processing systems; Regression analysis; Remote sensing; Awareness information; Data-source; End-to-end systems; Geo-spatial data; Non-traditional; Pakistan; Remote-sensing; Sensing data; Situational awareness; Social sensing; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
397;Early warning methods of chemical agent channeling in polymer–surfactant flooding reservoirs;In the chemical flooding process, the premature breakthrough of chemical agents in production wells results in a large waste of chemical agents and increases the volume and processing difficulty of the produced fluids. The early warning method of chemical agent channeling can predict the strength of agent channeling in advance. The practice of chemical flooding shows that the production performance can be used for early warning of chemical agent channeling. In this paper, we analyze the relationship between cumulative oil production and cumulative polymer production of production wells in polymer–surfactant flooding. Three types of curves according to the enhanced oil production characteristics of chemical flooding, including convex type, S-type, and concave type. We use the drop speed of the water–oil ratio and the rapid-decline speed of water cut as early warning indicators to predict the channeling coefficient. A Latin hypercube experimental design method is used to design a polymer–surfactant flooding scheme with the main control factors of the channeling coefficient and early warning indicators. Numerical simulation is used to calculate samples of the channeling coefficient and early warning indicators under various conditions. The drop speed of the water–oil ratio reference value model and the rapid-decline speed of the water cut reference value model are determined with a multiple regression method. A prediction model for the chemical agent channeling coefficient is established using the deviation coefficient of an early warning index. The method is applied in the Ng54-61 polymer–surfactant flooding pilot area in the west of the Gudong Seventh District, Shengli Oilfield, China, and the error between the predicted result and the actual value is less than 10%. This research is helpful in taking effective antichanneling measures and improving the oil recovery degree of chemical flooding reservoirs. © 2024 The Authors. Energy Science & Engineering published by Society of Chemical Industry and John Wiley & Sons Ltd.;"chemical agent channeling; dynamic monitoring; early warning method; multiple regression; polymer–surfactant flooding";"Enhanced recovery; Floods; Forecasting; Oil well flooding; Regression analysis; Reservoirs (water); Surface active agents; Chemical agent; Chemical agent channeling; Chemical flooding; Dynamic monitoring; Early warning indicators; Early-warning method; Multiple regressions; Polymer surfactants; Polymer–surfactant flooding; Surfactant flooding; Drops";"Early warning methods of chemical agent channeling in polymer–surfactant flooding reservoirs In the chemical flooding process, the premature breakthrough of chemical agents in production wells results in a large waste of chemical agents and increases the volume and processing difficulty of the produced fluids. The early warning method of chemical agent channeling can predict the strength of agent channeling in advance. The practice of chemical flooding shows that the production performance can be used for early warning of chemical agent channeling. In this paper, we analyze the relationship between cumulative oil production and cumulative polymer production of production wells in polymer–surfactant flooding. Three types of curves according to the enhanced oil production characteristics of chemical flooding, including convex type, S-type, and concave type. We use the drop speed of the water–oil ratio and the rapid-decline speed of water cut as early warning indicators to predict the channeling coefficient. A Latin hypercube experimental design method is used to design a polymer–surfactant flooding scheme with the main control factors of the channeling coefficient and early warning indicators. Numerical simulation is used to calculate samples of the channeling coefficient and early warning indicators under various conditions. The drop speed of the water–oil ratio reference value model and the rapid-decline speed of the water cut reference value model are determined with a multiple regression method. A prediction model for the chemical agent channeling coefficient is established using the deviation coefficient of an early warning index. The method is applied in the Ng54-61 polymer–surfactant flooding pilot area in the west of the Gudong Seventh District, Shengli Oilfield, China, and the error between the predicted result and the actual value is less than 10%. This research is helpful in taking effective antichanneling measures and improving the oil recovery degree of chemical flooding reservoirs. © 2024 The Authors. Energy Science & Engineering published by Society of Chemical Industry and John Wiley & Sons Ltd. chemical agent channeling; dynamic monitoring; early warning method; multiple regression; polymer–surfactant flooding Enhanced recovery; Floods; Forecasting; Oil well flooding; Regression analysis; Reservoirs (water); Surface active agents; Chemical agent; Chemical agent channeling; Chemical flooding; Dynamic monitoring; Early warning indicators; Early-warning method; Multiple regressions; Polymer surfactants; Polymer–surfactant flooding; Surfactant flooding; Drops";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
398;Underwater Rescue Target Detection Based on Acoustic Images;In order to effectively respond to floods and water emergencies that result in the drowning of missing persons, timely and effective search and rescue is a very critical step in underwater rescue. Due to the complex underwater environment and low visibility, unmanned underwater vehicles (UUVs) with sonar are more efficient than traditional manual search and rescue methods to conduct active searches using deep learning algorithms. In this paper, we constructed a sound-based rescue target dataset that encompasses both the source and target domains using deep transfer learning techniques. For the underwater acoustic rescue target detection of small targets, which lack image feature accuracy, this paper proposes a two-branch convolution module and improves the YOLOv5s algorithm model to design an acoustic rescue small target detection algorithm model. For an underwater rescue target dataset based on acoustic images with a small sample acoustic dataset, a direct fine-tuning using optical image pre-training lacks cross-domain adaptability due to the different statistical properties of optical and acoustic images. This paper therefore proposes a heterogeneous information hierarchical migration learning method. For the false detection of acoustic rescue targets in a complex underwater background, the network layer is frozen during the hierarchical migration of heterogeneous information to improve the detection accuracy. In addition, in order to be more applicable to the embedded devices carried by underwater UAVs, an underwater acoustic rescue target detection algorithm based on ShuffleNetv2 is proposed to improve the two-branch convolutional module and the backbone network of YOLOv5s algorithm, and to create a lightweight model based on hierarchical migration of heterogeneous information. Through extensive comparative experiments conducted on various acoustic images, we have thoroughly validated the feasibility and effectiveness of our method. Our approach has demonstrated state-of-the-art performance in underwater search and rescue target detection tasks. © 2024 by the authors.;"acoustic small target detection; deep learning; deep migration learning; lightweight network; underwater acoustic rescue target detection";"Autonomous underwater vehicles; Complex networks; Convolution; Convolutional neural networks; Deep learning; Geometrical optics; Image enhancement; Learning algorithms; Learning systems; Network layers; Signal detection; water; Acoustic small target detection; Deep learning; Deep migration learning; Heterogeneous information; Lightweight network; Search and rescue; Small target detection; Targets detection; Underwater acoustic rescue target detection; Underwater rescues; aged; algorithm; article; controlled study; deep learning; detection algorithm; diagnosis; drowning; echolocation; flooding; human; learning; learning algorithm; transfer of learning; visibility; Underwater acoustics";"Underwater Rescue Target Detection Based on Acoustic Images In order to effectively respond to floods and water emergencies that result in the drowning of missing persons, timely and effective search and rescue is a very critical step in underwater rescue. Due to the complex underwater environment and low visibility, unmanned underwater vehicles (UUVs) with sonar are more efficient than traditional manual search and rescue methods to conduct active searches using deep learning algorithms. In this paper, we constructed a sound-based rescue target dataset that encompasses both the source and target domains using deep transfer learning techniques. For the underwater acoustic rescue target detection of small targets, which lack image feature accuracy, this paper proposes a two-branch convolution module and improves the YOLOv5s algorithm model to design an acoustic rescue small target detection algorithm model. For an underwater rescue target dataset based on acoustic images with a small sample acoustic dataset, a direct fine-tuning using optical image pre-training lacks cross-domain adaptability due to the different statistical properties of optical and acoustic images. This paper therefore proposes a heterogeneous information hierarchical migration learning method. For the false detection of acoustic rescue targets in a complex underwater background, the network layer is frozen during the hierarchical migration of heterogeneous information to improve the detection accuracy. In addition, in order to be more applicable to the embedded devices carried by underwater UAVs, an underwater acoustic rescue target detection algorithm based on ShuffleNetv2 is proposed to improve the two-branch convolutional module and the backbone network of YOLOv5s algorithm, and to create a lightweight model based on hierarchical migration of heterogeneous information. Through extensive comparative experiments conducted on various acoustic images, we have thoroughly validated the feasibility and effectiveness of our method. Our approach has demonstrated state-of-the-art performance in underwater search and rescue target detection tasks. © 2024 by the authors. acoustic small target detection; deep learning; deep migration learning; lightweight network; underwater acoustic rescue target detection Autonomous underwater vehicles; Complex networks; Convolution; Convolutional neural networks; Deep learning; Geometrical optics; Image enhancement; Learning algorithms; Learning systems; Network layers; Signal detection; water; Acoustic small target detection; Deep learning; Deep migration learning; Heterogeneous information; Lightweight network; Search and rescue; Small target detection; Targets detection; Underwater acoustic rescue target detection; Underwater rescues; aged; algorithm; article; controlled study; deep learning; detection algorithm; diagnosis; drowning; echolocation; flooding; human; learning; learning algorithm; transfer of learning; visibility; Underwater acoustics";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;3;Response
399;Real-time prediction and ponding process early warning method at urban flood points based on different deep learning methods;Accurate prediction of urban floods is regarded as one of the critical means to prevent urban floods and reduce the losses caused by floods. In this study, a refined prediction and early warning method system for urban flood and waterlogging processes based on deep learning methods is proposed. The spatial autocorrelation of rain and ponding points is analyzed by Moran's I (a common used statistic for spatial autocorrelation). For each ponding point, the relationship model between the rainfall process and ponding process is constructed based on different deep learning methods, and the results are analyzed and verified by mean absolute error (MAE), root mean square error (RMSE), Nash efficiency coefficient (NSE) and correlation coefficient (CC). The results show that the gradient boosting decision tree algorithm has the highest accuracy and efficiency (with a 0.001 m RMSE of the predicted and measured ponding depth) for ponding process prediction and is regarded as the most suitable method for ponding process prediction. Finally, the real-time prediction and early warning of urban floods and waterlogging processes driven by rainfall forecast data are realized, and the results are verified by the measured data. The research results can provide theoretical support for urban flood prevention and control. © 2023 The Authors. Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd.;"deep learning; early warning; ponding point; prediction; urban flood";"autocorrelation; early warning system; flood control; machine learning; precipitation intensity; prediction; waterlogging";"Real-time prediction and ponding process early warning method at urban flood points based on different deep learning methods Accurate prediction of urban floods is regarded as one of the critical means to prevent urban floods and reduce the losses caused by floods. In this study, a refined prediction and early warning method system for urban flood and waterlogging processes based on deep learning methods is proposed. The spatial autocorrelation of rain and ponding points is analyzed by Moran's I (a common used statistic for spatial autocorrelation). For each ponding point, the relationship model between the rainfall process and ponding process is constructed based on different deep learning methods, and the results are analyzed and verified by mean absolute error (MAE), root mean square error (RMSE), Nash efficiency coefficient (NSE) and correlation coefficient (CC). The results show that the gradient boosting decision tree algorithm has the highest accuracy and efficiency (with a 0.001 m RMSE of the predicted and measured ponding depth) for ponding process prediction and is regarded as the most suitable method for ponding process prediction. Finally, the real-time prediction and early warning of urban floods and waterlogging processes driven by rainfall forecast data are realized, and the results are verified by the measured data. The research results can provide theoretical support for urban flood prevention and control. © 2023 The Authors. Journal of Flood Risk Management published by Chartered Institution of Water and Environmental Management and John Wiley & Sons Ltd. deep learning; early warning; ponding point; prediction; urban flood autocorrelation; early warning system; flood control; machine learning; precipitation intensity; prediction; waterlogging";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
400;Using road detection and OSM reference data to refine airborne camera extrinsics in the Ahr Valley flooding use case;Aerial mapping provides high-resolution spatial data over large areas and is a cornerstone in urban planning, environmental monitoring and disaster management. To ensure geo-referencing and orthorectification, accurate external camera orientations (EO) are pivotal and indispensable. Often, the position and orientation data provided by the aerial platform is not accurate enough to create accurately georeferenced imagery. With our method, we improve the EO in a post-processing chain by referencing the extracted road network with the OpenStreetMap (OSM) road network. The dataset used in this paper was collected in the aftermath of the July 2021 Ahr Valley flooding disaster along the Ahr river in Germany, employing an aircraft operating at an altitude of 3000 feet. Encompassing an expansive area of 230 sq km, it includes the diverse landscapes of the Eifel mountains, urban and rural locales, as well as the inundated and devastated regions. We implement a machine learning semantic segmentation model, namely DeepLab V3+, utilizing the RGB imagery images captured by the aircraft to identify road center lines. It is necessary to employ this rather sophisticated method because traditional road detectors were thrown off by the flooded areas, increasing false positive detections and limiting the EO optimization. We compute 3D reference points from available OSM road vector data combined with digital terrain model (DTM) elevation data. Using camera intrinsics and initial values for EOs, we project the reference points into the images and compute the distance to the detected road feature points. This distance is minimized to optimize EO parameters. Subsequently, we project the images onto an ortho-mosaic using the DTM, enhancing accuracy beyond that of the raw EO data measured by the aircraft. The resulting orthophoto map is almost free of visual artifacts and accurate in terms of geolocation, and can thus be utilized for disaster response applications. © 2024 SPIE.;"disaster response; External camera orientation; machine learning; optimization; semantic segmentation";"Adaptive boosting; Aerial photography; Aircraft accidents; Aircraft detection; Control towers; Highway administration; Image enhancement; Network security; Photointerpretation; Photomapping; Steganography; Camera orientation; Disaster-response; External camera orientation; Floodings; Machine-learning; Optimisations; Orientation data; Reference points; Road network; Semantic segmentation; Semantic Segmentation";"Using road detection and OSM reference data to refine airborne camera extrinsics in the Ahr Valley flooding use case Aerial mapping provides high-resolution spatial data over large areas and is a cornerstone in urban planning, environmental monitoring and disaster management. To ensure geo-referencing and orthorectification, accurate external camera orientations (EO) are pivotal and indispensable. Often, the position and orientation data provided by the aerial platform is not accurate enough to create accurately georeferenced imagery. With our method, we improve the EO in a post-processing chain by referencing the extracted road network with the OpenStreetMap (OSM) road network. The dataset used in this paper was collected in the aftermath of the July 2021 Ahr Valley flooding disaster along the Ahr river in Germany, employing an aircraft operating at an altitude of 3000 feet. Encompassing an expansive area of 230 sq km, it includes the diverse landscapes of the Eifel mountains, urban and rural locales, as well as the inundated and devastated regions. We implement a machine learning semantic segmentation model, namely DeepLab V3+, utilizing the RGB imagery images captured by the aircraft to identify road center lines. It is necessary to employ this rather sophisticated method because traditional road detectors were thrown off by the flooded areas, increasing false positive detections and limiting the EO optimization. We compute 3D reference points from available OSM road vector data combined with digital terrain model (DTM) elevation data. Using camera intrinsics and initial values for EOs, we project the reference points into the images and compute the distance to the detected road feature points. This distance is minimized to optimize EO parameters. Subsequently, we project the images onto an ortho-mosaic using the DTM, enhancing accuracy beyond that of the raw EO data measured by the aircraft. The resulting orthophoto map is almost free of visual artifacts and accurate in terms of geolocation, and can thus be utilized for disaster response applications. © 2024 SPIE. disaster response; External camera orientation; machine learning; optimization; semantic segmentation Adaptive boosting; Aerial photography; Aircraft accidents; Aircraft detection; Control towers; Highway administration; Image enhancement; Network security; Photointerpretation; Photomapping; Steganography; Camera orientation; Disaster-response; External camera orientation; Floodings; Machine-learning; Optimisations; Orientation data; Reference points; Road network; Semantic segmentation; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
401;Effect of the Eco-Based Flood Mitigation Projects on Urban Flooding: A Case Study in Sri Jayawardenepura Kotte;This study investigates the impact of eco-based flood mitigation projects on urban flooding in the Sri Jayewardenepura Kotte Divisional Secretariat Division (DSD). Urban flooding has become a critical issue due to rapid urbanisation and climate change, necessitating effective flood mitigation strategies. Objectives of the research are to analyse flooding patterns, map land use and land cover (LULC) changes associated with eco-based mitigation projects and assess the correlation between these changes and flooding occurrences in the study area. Additionally, a temporal analysis of rainfall variations was conducted to provide further context to the flooding events. To achieve these objectives, the study employs advanced Geographic Information System (GIS) technology. ArcGIS was utilised for detailed LULC classification with 84.4% overall accuracy, enabling the identification of temporal changes in LULC. The Inverse Distance Weighting (IDW) interpolation technique was applied to analyse rainfall data, offering insights into precipitation patterns and their relationship to flooding incidents. The analysis reveals a significant association between increased precipitation and major flood events, indicating that higher rainfall intensities contribute to urban flooding. Furthermore, the study identifies substantial LULC changes related to mitigation projects, highlighting rapid urbanisation, wetland decline, and the restoration of wetlands through eco-based mitigation efforts. It was found that the expansion of built-up areas exacerbates flood risks, while wetland recovery through eco-based flood mitigation programs acts as a natural buffer against flooding. The findings underscore the importance of targeted interventions for urban flood resilience. This research offers critical insights for sustainable urban planning and flood management, emphasising the need for integrated approaches that incorporate ecological solutions to enhance urban resilience. In conclusion, the study provides a comprehensive understanding of how eco-based flood mitigation projects, such as “Beddagana” and “Diyasaru” wetland management projects, influence urban flooding. Incorporating ecological strategies from projects such as “Beddagana” and “Diyasaru” enables cities to more effectively manage urban flooding, offering vital insights for policymakers and planners to develop resilient and sustainable urban environments. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved.;"Eco-based flood mitigation; Geographic Information System; Inverse Distance Weighting; Land Use Land Cover";"Climate change; Information management; Rain; Risk assessment; Urban planning; Eco-based flood mitigation; Flood mitigation; Floodings; Geographic information; Inverse distance weighting; Land use and land cover change; Land use/land cover; Mitigation projects; Rapid urbanizations; Urban flooding; Wetlands";"Effect of the Eco-Based Flood Mitigation Projects on Urban Flooding: A Case Study in Sri Jayawardenepura Kotte This study investigates the impact of eco-based flood mitigation projects on urban flooding in the Sri Jayewardenepura Kotte Divisional Secretariat Division (DSD). Urban flooding has become a critical issue due to rapid urbanisation and climate change, necessitating effective flood mitigation strategies. Objectives of the research are to analyse flooding patterns, map land use and land cover (LULC) changes associated with eco-based mitigation projects and assess the correlation between these changes and flooding occurrences in the study area. Additionally, a temporal analysis of rainfall variations was conducted to provide further context to the flooding events. To achieve these objectives, the study employs advanced Geographic Information System (GIS) technology. ArcGIS was utilised for detailed LULC classification with 84.4% overall accuracy, enabling the identification of temporal changes in LULC. The Inverse Distance Weighting (IDW) interpolation technique was applied to analyse rainfall data, offering insights into precipitation patterns and their relationship to flooding incidents. The analysis reveals a significant association between increased precipitation and major flood events, indicating that higher rainfall intensities contribute to urban flooding. Furthermore, the study identifies substantial LULC changes related to mitigation projects, highlighting rapid urbanisation, wetland decline, and the restoration of wetlands through eco-based mitigation efforts. It was found that the expansion of built-up areas exacerbates flood risks, while wetland recovery through eco-based flood mitigation programs acts as a natural buffer against flooding. The findings underscore the importance of targeted interventions for urban flood resilience. This research offers critical insights for sustainable urban planning and flood management, emphasising the need for integrated approaches that incorporate ecological solutions to enhance urban resilience. In conclusion, the study provides a comprehensive understanding of how eco-based flood mitigation projects, such as “Beddagana” and “Diyasaru” wetland management projects, influence urban flooding. Incorporating ecological strategies from projects such as “Beddagana” and “Diyasaru” enables cities to more effectively manage urban flooding, offering vital insights for policymakers and planners to develop resilient and sustainable urban environments. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved. Eco-based flood mitigation; Geographic Information System; Inverse Distance Weighting; Land Use Land Cover Climate change; Information management; Rain; Risk assessment; Urban planning; Eco-based flood mitigation; Flood mitigation; Floodings; Geographic information; Inverse distance weighting; Land use and land cover change; Land use/land cover; Mitigation projects; Rapid urbanizations; Urban flooding; Wetlands";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
402;Multivariate Segment Expandable Encoder-Decoder Model for Time Series Forecasting;Accurate time series forecasting is critical in a variety of fields, including transportation, weather prediction, energy management, infrastructure monitoring, and finance. Forecasting highly skewed and heavy-tailed time series, particularly in multivariate environments, is still difficult. In these cases, accurately capturing the relationships between variables is critical for successful model design. This is especially true when dealing with extreme events like droughts or floods in streamflow forecasting, which can have severe consequences on public safety and social well-being. We present the Multivariate Segment-Expandable Encoder Decoder (MSEED), a novel framework designed to address the challenges of extreme-adaptive multivariate time series forecasting. MSEED features a hierarchical encoder-decoder architecture, a short-term-enhanced subnet, and a feature assembling layer that integrates spatial and temporal information across multivariate inputs. By capturing quantile distributions across segmented subsequences at multiple scales, the model is able to detect complex patterns, enhancing both the accuracy and robustness of forecasts. Additionally, MSEED incorporates a simple vanilla encoder-decoder model for strengthening rolling predictions. The framework has been tested on four challenging real-world datasets, focusing on two critical forecasting scenarios: long-term predictions (three days ahead) and rolling predictions (every four hours) to simulate real-time decision-making in water resource management. MSEED consistently outperforms state-of-the-art models, showing improvements in forecasting accuracy ranging from 18% to 74%.  © 2013 IEEE.;"Deep learning; hydrologic prediction; LSTM; oversampling policy; representation learning; streamflow prediction; time series";"Information management; Prediction models; Resource allocation; Time series; Weather forecasting; Deep learning; Encoder-decoder; Hydrologic prediction; LSTM; Over sampling; Oversampling policy; Representation learning; Streamflow prediction; Time series forecasting; Times series; Decision making";"Multivariate Segment Expandable Encoder-Decoder Model for Time Series Forecasting Accurate time series forecasting is critical in a variety of fields, including transportation, weather prediction, energy management, infrastructure monitoring, and finance. Forecasting highly skewed and heavy-tailed time series, particularly in multivariate environments, is still difficult. In these cases, accurately capturing the relationships between variables is critical for successful model design. This is especially true when dealing with extreme events like droughts or floods in streamflow forecasting, which can have severe consequences on public safety and social well-being. We present the Multivariate Segment-Expandable Encoder Decoder (MSEED), a novel framework designed to address the challenges of extreme-adaptive multivariate time series forecasting. MSEED features a hierarchical encoder-decoder architecture, a short-term-enhanced subnet, and a feature assembling layer that integrates spatial and temporal information across multivariate inputs. By capturing quantile distributions across segmented subsequences at multiple scales, the model is able to detect complex patterns, enhancing both the accuracy and robustness of forecasts. Additionally, MSEED incorporates a simple vanilla encoder-decoder model for strengthening rolling predictions. The framework has been tested on four challenging real-world datasets, focusing on two critical forecasting scenarios: long-term predictions (three days ahead) and rolling predictions (every four hours) to simulate real-time decision-making in water resource management. MSEED consistently outperforms state-of-the-art models, showing improvements in forecasting accuracy ranging from 18% to 74%.  © 2013 IEEE. Deep learning; hydrologic prediction; LSTM; oversampling policy; representation learning; streamflow prediction; time series Information management; Prediction models; Resource allocation; Time series; Weather forecasting; Deep learning; Encoder-decoder; Hydrologic prediction; LSTM; Over sampling; Oversampling policy; Representation learning; Streamflow prediction; Time series forecasting; Times series; Decision making";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
403;Advanced Deep Learning Driven Geospatial Analysis for GLOF Risk Reduction: A Case Study from Pakistan’s Northern Mountain Ranges;Glacial Lake Outburst Floods (GLOFs) pose a severe risk to populations in high-altitude areas, particularly in Pakistan's northern regions, where glacier melt has created 3,044 glacial lakes in Gilgit-Baltistan and Khyber Pakhtunkhwa [1]. The Chitral region is especially vulnerable, highlighting the need for robust monitoring and mitigation strategies. This paper focuses on using Deep Learning (DL) models for detecting and segmenting glacial lakes to mitigate GLOF risks. Using the Glacial Lakes Detection Dataset from High-Mountain Asia, covering 2080.12 km2 and including around 30,121 glacial lakes, we selected 1,200 cloud-free true-color images with ground truth masks. Our study evaluates DL models like DeepLabV3+, UNet, and YOLOv8 for lake segmentation and classification. Experimental results demonstrated model proficiency, with DeepLabV3+ (ResNet50 backbone and Dice loss function) achieving the highest IoU score of 77.2%. These findings highlight the potential of DL-based systems for enhanced GLOF monitoring and early warning. © 2024 IEEE.;"climate change; deep neural network; geo-spatial data; Glacial Lake outburst floods (GLOF); remote sensing; satellite imagery; semantic segmentation";"Deep neural networks; Glacial geology; Glaciers; Risk analysis; Risk assessment; Tropics; Geo-spatial analysis; Geo-spatial data; Glacial lake outburst flood; Glacial lakes; Learning models; Neural-networks; Pakistan; Remote-sensing; Semantic segmentation; Semantic Segmentation";"Advanced Deep Learning Driven Geospatial Analysis for GLOF Risk Reduction: A Case Study from Pakistan’s Northern Mountain Ranges Glacial Lake Outburst Floods (GLOFs) pose a severe risk to populations in high-altitude areas, particularly in Pakistan's northern regions, where glacier melt has created 3,044 glacial lakes in Gilgit-Baltistan and Khyber Pakhtunkhwa [1]. The Chitral region is especially vulnerable, highlighting the need for robust monitoring and mitigation strategies. This paper focuses on using Deep Learning (DL) models for detecting and segmenting glacial lakes to mitigate GLOF risks. Using the Glacial Lakes Detection Dataset from High-Mountain Asia, covering 2080.12 km2 and including around 30,121 glacial lakes, we selected 1,200 cloud-free true-color images with ground truth masks. Our study evaluates DL models like DeepLabV3+, UNet, and YOLOv8 for lake segmentation and classification. Experimental results demonstrated model proficiency, with DeepLabV3+ (ResNet50 backbone and Dice loss function) achieving the highest IoU score of 77.2%. These findings highlight the potential of DL-based systems for enhanced GLOF monitoring and early warning. © 2024 IEEE. climate change; deep neural network; geo-spatial data; Glacial Lake outburst floods (GLOF); remote sensing; satellite imagery; semantic segmentation Deep neural networks; Glacial geology; Glaciers; Risk analysis; Risk assessment; Tropics; Geo-spatial analysis; Geo-spatial data; Glacial lake outburst flood; Glacial lakes; Learning models; Neural-networks; Pakistan; Remote-sensing; Semantic segmentation; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
404;Graph Learning-based Regional Heavy Rainfall Prediction Using Low-Cost Rain Gauges;Accurate and timely prediction of heavy rainfall events is crucial for effective flood risk management and disaster preparedness. By monitoring, analysing, and evaluating rainfall data at a local level, it is not only possible to take effective actions to prevent any severe climate variation but also to improve the planning of surface and underground hydrological resources. However, developing countries often lack the weather stations to collect data continuously due to the high cost of installation and maintenance. In light of this, the contribution of the present paper is twofold: first, we propose a low-cost IoT system for automatic recording, monitoring, and prediction of rainfall in rural regions. Second, we propose a novel approach to regional heavy rainfall prediction by implementing graph neural networks (GNNs), which are particularly well-suited for capturing the complex spatial dependencies inherent in rainfall patterns. The proposed approach was tested using a historical dataset spanning 72 months, with daily measurements, and experimental results demonstrated the effectiveness of the proposed method in predicting heavy rainfall events, making this approach particularly attractive for regions with limited resources or where traditional weather radar or station coverage is sparse.  © 2024 IEEE.;"Graph Neural Networks; Heavy Rainfalls; Internet of Things; Pluviometry";"Costs; Information management; Rain; Rain gages; Risk management; Weather forecasting; Disaster preparedness; Flood risk management; Graph neural networks; Heavy rainfall; Low-costs; Pluviometry; Rain gauges; Rainfall data; Rainfall event; Rainfall prediction; Graph neural networks";"Graph Learning-based Regional Heavy Rainfall Prediction Using Low-Cost Rain Gauges Accurate and timely prediction of heavy rainfall events is crucial for effective flood risk management and disaster preparedness. By monitoring, analysing, and evaluating rainfall data at a local level, it is not only possible to take effective actions to prevent any severe climate variation but also to improve the planning of surface and underground hydrological resources. However, developing countries often lack the weather stations to collect data continuously due to the high cost of installation and maintenance. In light of this, the contribution of the present paper is twofold: first, we propose a low-cost IoT system for automatic recording, monitoring, and prediction of rainfall in rural regions. Second, we propose a novel approach to regional heavy rainfall prediction by implementing graph neural networks (GNNs), which are particularly well-suited for capturing the complex spatial dependencies inherent in rainfall patterns. The proposed approach was tested using a historical dataset spanning 72 months, with daily measurements, and experimental results demonstrated the effectiveness of the proposed method in predicting heavy rainfall events, making this approach particularly attractive for regions with limited resources or where traditional weather radar or station coverage is sparse.  © 2024 IEEE. Graph Neural Networks; Heavy Rainfalls; Internet of Things; Pluviometry Costs; Information management; Rain; Rain gages; Risk management; Weather forecasting; Disaster preparedness; Flood risk management; Graph neural networks; Heavy rainfall; Low-costs; Pluviometry; Rain gauges; Rainfall data; Rainfall event; Rainfall prediction; Graph neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
405;Detection and Mitigation of DDoS Attack in SDN Using Feature Based SVM and Decision Tree Approach;Software-Defined Networking (SDN) is an innovative network architecture that separates the control plane from the data plane, enabling centralized management and dynamic network resource configuration through software. However, the centralization of the SDN controller makes it a critical target for Distributed Denial of Service (DDoS) attacks. To effectively detect and mitigate these threats, deploying a robust machine learning-based solution is essential. This study utilized Particle Swarm Optimization (PSO) and Generalized Normal Distribution Optimization (GNDO) as feature selection techniques to extract the most relevant features from the SDN-DDoS-2020 dataset, which includes ICMP, TCP, and UDP traffic (Mendeley). The selected feature subsets were used to train and evaluate the performance of Support Vector Machine (SVM) and Decision Tree (DT) classifiers. Additionally, a dedicated mitigation framework was designed to counter flood traffic generated by TCP, UDP, and ICMP-based DDoS attacks. Among the models, the PSO-Decision Tree (PSO-DT) approach demonstrated superior performance, achieving an accuracy of 98.87% and a False Alarm Rate (FAR) of 0.7702, underscoring its effectiveness in reducing false positives and enhancing detection reliability.  © 2024 IEEE.;"Decision Tree; Distributed Denial of Service; Particle Swarm Optimization; Software Defined Networking; Support Vector Optimization";"Denial-of-service attack; Feature Selection; Forward error correction; Random forests; Resource allocation; Support vector machines; Trees (mathematics); Denialof- service attacks; Distributed denial of service; Particle swarm; Particle swarm optimization; Software-defined networkings; Support vector; Support vector optimization; Support vectors machine; Swarm optimization; Vector optimizations; Particle swarm optimization (PSO)";"Detection and Mitigation of DDoS Attack in SDN Using Feature Based SVM and Decision Tree Approach Software-Defined Networking (SDN) is an innovative network architecture that separates the control plane from the data plane, enabling centralized management and dynamic network resource configuration through software. However, the centralization of the SDN controller makes it a critical target for Distributed Denial of Service (DDoS) attacks. To effectively detect and mitigate these threats, deploying a robust machine learning-based solution is essential. This study utilized Particle Swarm Optimization (PSO) and Generalized Normal Distribution Optimization (GNDO) as feature selection techniques to extract the most relevant features from the SDN-DDoS-2020 dataset, which includes ICMP, TCP, and UDP traffic (Mendeley). The selected feature subsets were used to train and evaluate the performance of Support Vector Machine (SVM) and Decision Tree (DT) classifiers. Additionally, a dedicated mitigation framework was designed to counter flood traffic generated by TCP, UDP, and ICMP-based DDoS attacks. Among the models, the PSO-Decision Tree (PSO-DT) approach demonstrated superior performance, achieving an accuracy of 98.87% and a False Alarm Rate (FAR) of 0.7702, underscoring its effectiveness in reducing false positives and enhancing detection reliability.  © 2024 IEEE. Decision Tree; Distributed Denial of Service; Particle Swarm Optimization; Software Defined Networking; Support Vector Optimization Denial-of-service attack; Feature Selection; Forward error correction; Random forests; Resource allocation; Support vector machines; Trees (mathematics); Denialof- service attacks; Distributed denial of service; Particle swarm; Particle swarm optimization; Software-defined networkings; Support vector; Support vector optimization; Support vectors machine; Swarm optimization; Vector optimizations; Particle swarm optimization (PSO)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
406;Quantum-Train Long Short-Term Memory: Application on Flood Prediction Problem;Flood prediction is a critical challenge in the context of climate change, with significant implications for ecosystem preservation, human safety, and infrastructure protection. In this study, we tackle this problem by applying the Quantum-Train (QT) technique to a forecasting Long Short-Term Memory (LSTM) model trained by Quantum Machine Learning (QML) with significant parameter reduction. The QT technique, originally successful in the 'A Matter of Taste' challenge at QHack 2024, leverages QML to reduce the number of trainable parameters to a polylogarithmic function of the number of parameters in a classical neural network (NN). This innovative framework maps classical NN weights to a Hilbert space, altering quantum state probability distributions to adjust NN parameters. Our approach directly processes classical data without the need for quantum embedding and operates independently of quantum computing resources post-training, making it highly practical and accessible for real-world flood prediction applications. This model aims to improve the efficiency of flood forecasts, ultimately contributing to better disaster preparedness and response. © 2024 IEEE.;"Long Short-Term Memory; Parameter Reduction; Quantum Machine Learning";"Long short-term memory; Quantum electronics; Weather forecasting; Classical neural networks; Critical challenges; Flood prediction; Machine-learning; Memory applications; Parameter reduction; Prediction problem; Quantum machine learning; Quantum machines; Short term memory; Quantum computers";"Quantum-Train Long Short-Term Memory: Application on Flood Prediction Problem Flood prediction is a critical challenge in the context of climate change, with significant implications for ecosystem preservation, human safety, and infrastructure protection. In this study, we tackle this problem by applying the Quantum-Train (QT) technique to a forecasting Long Short-Term Memory (LSTM) model trained by Quantum Machine Learning (QML) with significant parameter reduction. The QT technique, originally successful in the 'A Matter of Taste' challenge at QHack 2024, leverages QML to reduce the number of trainable parameters to a polylogarithmic function of the number of parameters in a classical neural network (NN). This innovative framework maps classical NN weights to a Hilbert space, altering quantum state probability distributions to adjust NN parameters. Our approach directly processes classical data without the need for quantum embedding and operates independently of quantum computing resources post-training, making it highly practical and accessible for real-world flood prediction applications. This model aims to improve the efficiency of flood forecasts, ultimately contributing to better disaster preparedness and response. © 2024 IEEE. Long Short-Term Memory; Parameter Reduction; Quantum Machine Learning Long short-term memory; Quantum electronics; Weather forecasting; Classical neural networks; Critical challenges; Flood prediction; Machine-learning; Memory applications; Parameter reduction; Prediction problem; Quantum machine learning; Quantum machines; Short term memory; Quantum computers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
407;A Machine Learning Approach for Classifying Micro-Earthquakes at Llaima Volcano;Automated systems play a key role in the development of early warning mechanisms with the objective of preserving lives and securing regions susceptible to volcanic activity. The aim of this article is to develop intelligent algorithms based on Machine Learning for the multiclass classification of micro-earthquakes originated at Llaima volcano, including tectonic earthquakes, long-period events, tremors, and volcano-tectonic earthquakes. Our method encompasses preprocessing, processing, feature extraction, feature selection, and classification stages. During the classification, we employ machine learning algorithms, specifically Decision Trees (DT), k-Nearest Neighbors (k-NN), and Support Vector Machine (SVM). The evaluation of our system performance, is assessed through the Balanced Error Rate on test data, yields significant results: 0. 1 2 for DT, 0. 1 0 for k-NN, and 0.08 for SVM. SVM algorithm presents remarkable results when applying our methodology to the feature selected matrix, which considers 29 key features, this achievement results in accuracy approaching 96% and specificity of 98%. © 2024 IEEE.;"feature extraction; feature selection; supervised classification learning; volcano monitoring system";"Adaptive boosting; Adversarial machine learning; Contrastive Learning; Earthquake effects; Feature Selection; Nearest neighbor search; Random forests; Self-supervised learning; Semi-supervised learning; Support vector machines; Classification learning; Features extraction; Features selection; Machine learning approaches; Microearthquakes; Nearest-neighbour; Supervised classification; Supervised classification learning; Support vectors machine; Volcano monitoring system; Volcanoes";"A Machine Learning Approach for Classifying Micro-Earthquakes at Llaima Volcano Automated systems play a key role in the development of early warning mechanisms with the objective of preserving lives and securing regions susceptible to volcanic activity. The aim of this article is to develop intelligent algorithms based on Machine Learning for the multiclass classification of micro-earthquakes originated at Llaima volcano, including tectonic earthquakes, long-period events, tremors, and volcano-tectonic earthquakes. Our method encompasses preprocessing, processing, feature extraction, feature selection, and classification stages. During the classification, we employ machine learning algorithms, specifically Decision Trees (DT), k-Nearest Neighbors (k-NN), and Support Vector Machine (SVM). The evaluation of our system performance, is assessed through the Balanced Error Rate on test data, yields significant results: 0. 1 2 for DT, 0. 1 0 for k-NN, and 0.08 for SVM. SVM algorithm presents remarkable results when applying our methodology to the feature selected matrix, which considers 29 key features, this achievement results in accuracy approaching 96% and specificity of 98%. © 2024 IEEE. feature extraction; feature selection; supervised classification learning; volcano monitoring system Adaptive boosting; Adversarial machine learning; Contrastive Learning; Earthquake effects; Feature Selection; Nearest neighbor search; Random forests; Self-supervised learning; Semi-supervised learning; Support vector machines; Classification learning; Features extraction; Features selection; Machine learning approaches; Microearthquakes; Nearest-neighbour; Supervised classification; Supervised classification learning; Support vectors machine; Volcano monitoring system; Volcanoes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
408;Modelling of AC Transmission Line Faults Using Machine Learning Methods;Electrical transmission lines are a link between the generation station and the consumer. Any abnormality in transmission lines may result in a prolonged total electricity shutdown if not detected and identified accurately. Malfunctioning of transmission lines contributes to economic destruction and leads to wildfires and damage to electrical equipment. The existing literature has confirmed that 70 to 80 percent of the abnormalities that usually occur in transmission lines are single line-to-ground (L-G) faults. Detecting and identifying such faults will minimize restoration time. Machine learning-based models' accuracy has never been comprehensively tested in detecting and identifying L-G faults in transmission lines under noisy conditions. Noise in power system may arise from actions such as switching of transformers, circuit breakers, fluorescent lighting, voltage produced by poor connections, contact arching, voltage notching and the presence of surrounding environmental noise. This paper tested a MATLAB machine learning-based models to detect and identify L-G faults when introducing noise on these transmission lines. Simulation research data was used to validate the model functionality and accuracy.  © 2024 IEEE.;"Detection; Identification; L-G fault; machine learning; noise; transmission lines";"Machine learning; Power distribution lines; Power transmission lines; AC transmission lines; Detection; Identification; Learning Based Models; Line-to-ground faults; Machine learning methods; Machine-learning; Noise; Transmission line faults; Transmission-line; MATLAB";"Modelling of AC Transmission Line Faults Using Machine Learning Methods Electrical transmission lines are a link between the generation station and the consumer. Any abnormality in transmission lines may result in a prolonged total electricity shutdown if not detected and identified accurately. Malfunctioning of transmission lines contributes to economic destruction and leads to wildfires and damage to electrical equipment. The existing literature has confirmed that 70 to 80 percent of the abnormalities that usually occur in transmission lines are single line-to-ground (L-G) faults. Detecting and identifying such faults will minimize restoration time. Machine learning-based models' accuracy has never been comprehensively tested in detecting and identifying L-G faults in transmission lines under noisy conditions. Noise in power system may arise from actions such as switching of transformers, circuit breakers, fluorescent lighting, voltage produced by poor connections, contact arching, voltage notching and the presence of surrounding environmental noise. This paper tested a MATLAB machine learning-based models to detect and identify L-G faults when introducing noise on these transmission lines. Simulation research data was used to validate the model functionality and accuracy.  © 2024 IEEE. Detection; Identification; L-G fault; machine learning; noise; transmission lines Machine learning; Power distribution lines; Power transmission lines; AC transmission lines; Detection; Identification; Learning Based Models; Line-to-ground faults; Machine learning methods; Machine-learning; Noise; Transmission line faults; Transmission-line; MATLAB";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
409;FDRformer: A Transformer-based electrical substation flooding disaster risk forecasting model;In this paper, we propose FDRformer model, a novel Transformer-based model designed for substation flooding disaster risk prediction. The model integrates a gated feature selection network and an encoder-decoder network to effectively reduce redundant information and capture complex nonlinear relationships among multiple variables, thereby enhancing prediction accuracy. Using real-world data from an electric power research institute in central China, the FDRformer model is trained and evaluated. Experimental results show that FDRformer outperforms existing machine learning and deep learning models, achieving a 32.87% reduction in RMSE compared to conventional deep learning methods. The proposed model is highly effective in predicting flood risks, providing valuable insights for disaster mitigation and resource allocation in substation management. © 2024 IEEE.;"deep learning; electrical substation; Flooding risk prediction; power system; Transformer architecture";"Deep learning; Disasters; Resource allocation; Transformer substations; Deep learning; Electrical substations; Flooding risk prediction; Flooding risks; Floodings; Power; Power system; Risk forecasting; Risk predictions; Transformer architecture; Prediction models";"FDRformer: A Transformer-based electrical substation flooding disaster risk forecasting model In this paper, we propose FDRformer model, a novel Transformer-based model designed for substation flooding disaster risk prediction. The model integrates a gated feature selection network and an encoder-decoder network to effectively reduce redundant information and capture complex nonlinear relationships among multiple variables, thereby enhancing prediction accuracy. Using real-world data from an electric power research institute in central China, the FDRformer model is trained and evaluated. Experimental results show that FDRformer outperforms existing machine learning and deep learning models, achieving a 32.87% reduction in RMSE compared to conventional deep learning methods. The proposed model is highly effective in predicting flood risks, providing valuable insights for disaster mitigation and resource allocation in substation management. © 2024 IEEE. deep learning; electrical substation; Flooding risk prediction; power system; Transformer architecture Deep learning; Disasters; Resource allocation; Transformer substations; Deep learning; Electrical substations; Flooding risk prediction; Flooding risks; Floodings; Power; Power system; Risk forecasting; Risk predictions; Transformer architecture; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
410;Assessment of Post-Disaster Concerns using Social Network Analysis and Machine Learning: A Study of 2018 Palu Disaster;Post-disaster needs assessment plays a critical role in developing emergency response programs and disaster preparedness. An assessment of needs identifies possible interventions or assistance and the necessary resources for emergency responses. This emergency need assessment assists stakeholders in defining possible response options, evaluating the priorities of needs of affected communities, and determining what support will be required immediately. The prevalent use of social media in Indonesia highlights its significance as a crucial means of acquiring prompt, effortless, individualized, and geo-specific information regarding the demands and requirements of communities impacted by a calamity. This study aims to analyze the tweets posted by local communities affected by the 2018 Central Sulawesi earthquakes, tsunami, and liquefaction using the Social Network Analysis (SNA) method based on machine learning and natural language processing. Through this analysis, we intend to observe the spatial interaction and relationship among the affected areas of communities for the needs and concerns after the disaster to enhance the relief operation. We found that social media users, government, and humanitarian organizations effectively shared information during the disaster. This study also underlined that the behavioral responses of individuals to disasters were not limited to the disaster-stricken areas but extended far beyond them. This action suggests that they were willing to exhibit a more empathetic, benevolent, and philanthropic demeanor in the face of such traumatic events.  © 2024 IEEE.;"concern assessment; disaster relief; natural language processing; social media; social network analysis";"Disaster prevention; Disasters; Natural language processing systems; Tweets; Concern assessment; Disaster relief; Language processing; Machine-learning; Natural language processing; Natural languages; Needs Assessment; Post disasters; Social media; Social Network Analysis";"Assessment of Post-Disaster Concerns using Social Network Analysis and Machine Learning: A Study of 2018 Palu Disaster Post-disaster needs assessment plays a critical role in developing emergency response programs and disaster preparedness. An assessment of needs identifies possible interventions or assistance and the necessary resources for emergency responses. This emergency need assessment assists stakeholders in defining possible response options, evaluating the priorities of needs of affected communities, and determining what support will be required immediately. The prevalent use of social media in Indonesia highlights its significance as a crucial means of acquiring prompt, effortless, individualized, and geo-specific information regarding the demands and requirements of communities impacted by a calamity. This study aims to analyze the tweets posted by local communities affected by the 2018 Central Sulawesi earthquakes, tsunami, and liquefaction using the Social Network Analysis (SNA) method based on machine learning and natural language processing. Through this analysis, we intend to observe the spatial interaction and relationship among the affected areas of communities for the needs and concerns after the disaster to enhance the relief operation. We found that social media users, government, and humanitarian organizations effectively shared information during the disaster. This study also underlined that the behavioral responses of individuals to disasters were not limited to the disaster-stricken areas but extended far beyond them. This action suggests that they were willing to exhibit a more empathetic, benevolent, and philanthropic demeanor in the face of such traumatic events.  © 2024 IEEE. concern assessment; disaster relief; natural language processing; social media; social network analysis Disaster prevention; Disasters; Natural language processing systems; Tweets; Concern assessment; Disaster relief; Language processing; Machine-learning; Natural language processing; Natural languages; Needs Assessment; Post disasters; Social media; Social Network Analysis";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;3;Response
411;A Water Body Extraction Network Based on Fusing SAR Images and DEM Data;Synthetic Aperture Radar (SAR) plays a crucial role in flood monitoring and early warning systems through water body extraction techniques. However, unlike optical remote sensing images, SAR images are affected by unique imaging characteristics, including shadows, noise, and complex environments, which complicate interpretation. Most existing deep learning methods rely solely on single SAR images for model training, without incorporating additional information, leading to lower accuracy in water body interpretation, especially in challenging scenarios such as mountainous areas and densely vegetated regions. To address these challenges, we propose a water body extraction algorithm that fuses SAR images with Digital Elevation Model (DEM) data to improve the accuracy and robustness in complex environments. Four different fusion strategies are explored at both the image level and feature level, incorporating DEM and slope data into an existing encoder-decoder architecture. These enhancements effectively address the aforementioned issues. Experimental results demonstrate that the fusion approach significantly improves water body extraction precision and reduces false alarm rates compared to non-fusion methods. Through a comparative analysis of the different fusion strategies, the best approach was identified as image-level fusion using DEM data, achieving a mean Intersection over Union (mIoU) of 0.95 on the GF-Floodnet dataset.  © 2024 IEEE.;"DEM; Fusion strategies; SAR; Water body extraction";"Deep learning; Flood control; Image analysis; Image coding; Image enhancement; Image fusion; Optical remote sensing; Complex environments; Digital elevation model; Digital elevation model data; Early Warning System; Flood monitoring; Fusion strategies; Network-based; Synthetic aperture radar images; Water body extraction; Waterbodies; Radar warning systems";"A Water Body Extraction Network Based on Fusing SAR Images and DEM Data Synthetic Aperture Radar (SAR) plays a crucial role in flood monitoring and early warning systems through water body extraction techniques. However, unlike optical remote sensing images, SAR images are affected by unique imaging characteristics, including shadows, noise, and complex environments, which complicate interpretation. Most existing deep learning methods rely solely on single SAR images for model training, without incorporating additional information, leading to lower accuracy in water body interpretation, especially in challenging scenarios such as mountainous areas and densely vegetated regions. To address these challenges, we propose a water body extraction algorithm that fuses SAR images with Digital Elevation Model (DEM) data to improve the accuracy and robustness in complex environments. Four different fusion strategies are explored at both the image level and feature level, incorporating DEM and slope data into an existing encoder-decoder architecture. These enhancements effectively address the aforementioned issues. Experimental results demonstrate that the fusion approach significantly improves water body extraction precision and reduces false alarm rates compared to non-fusion methods. Through a comparative analysis of the different fusion strategies, the best approach was identified as image-level fusion using DEM data, achieving a mean Intersection over Union (mIoU) of 0.95 on the GF-Floodnet dataset.  © 2024 IEEE. DEM; Fusion strategies; SAR; Water body extraction Deep learning; Flood control; Image analysis; Image coding; Image enhancement; Image fusion; Optical remote sensing; Complex environments; Digital elevation model; Digital elevation model data; Early Warning System; Flood monitoring; Fusion strategies; Network-based; Synthetic aperture radar images; Water body extraction; Waterbodies; Radar warning systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
412;Real-time Reconstruction of Flooded Areas from Low Resolution UAV Imagery;This work focuses on the estimation of flooded areas from aerial images available from optical camera powered UAVs or others lighter than air platform (LTAP) sources which are deployed for large elevated surveillance during the natural calamities related to flood. It is observed that images in such cases are mostly low in resolution. Due to limited bandwidth during crisis, UAVs will have low bandwidth to send such information to the base station. Hence, relatively low complexity modules are required in real time for such scenario which can provide possible flooded area segmentation for any given time frame. This paper incorporates the technique for real-time segmentation flooded areas into UAVs, captured during UAV navigation in crisis regions. Here, the single UAV navigation has been considered for partitioning the surveillance regions depending on the minimum hop and allowable field of view (FoV). Then we have implemented Color quantization based K-Means clustering to segment such low resolution images accurately to reconstruct the flooded locations over a vast area. Moreover, this approach would be useful in urban as well as inaccessible terrains for real time flood area modelling using low resolution images. © 2024 IEEE.;"Color Clustering; Flooded Image; GSD; Segmentation; UAV";"Aerial photography; Image reconstruction; Image segmentation; K-means clustering; Aerial images; Color clustering; Flooded areas; Flooded image; GSD; Low resolution images; Lower resolution; Real- time; Real-time reconstruction; Segmentation; Unmanned aerial vehicles (UAV)";"Real-time Reconstruction of Flooded Areas from Low Resolution UAV Imagery This work focuses on the estimation of flooded areas from aerial images available from optical camera powered UAVs or others lighter than air platform (LTAP) sources which are deployed for large elevated surveillance during the natural calamities related to flood. It is observed that images in such cases are mostly low in resolution. Due to limited bandwidth during crisis, UAVs will have low bandwidth to send such information to the base station. Hence, relatively low complexity modules are required in real time for such scenario which can provide possible flooded area segmentation for any given time frame. This paper incorporates the technique for real-time segmentation flooded areas into UAVs, captured during UAV navigation in crisis regions. Here, the single UAV navigation has been considered for partitioning the surveillance regions depending on the minimum hop and allowable field of view (FoV). Then we have implemented Color quantization based K-Means clustering to segment such low resolution images accurately to reconstruct the flooded locations over a vast area. Moreover, this approach would be useful in urban as well as inaccessible terrains for real time flood area modelling using low resolution images. © 2024 IEEE. Color Clustering; Flooded Image; GSD; Segmentation; UAV Aerial photography; Image reconstruction; Image segmentation; K-means clustering; Aerial images; Color clustering; Flooded areas; Flooded image; GSD; Low resolution images; Lower resolution; Real- time; Real-time reconstruction; Segmentation; Unmanned aerial vehicles (UAV)";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.2;Hydrological;3;Response
413;FNO Tsunami Flooding Simulation for Arbitrary Bathymetries, Shorelines, and Land Topographies;Global warming is rapidly melting glaciers, causing sea levels to rise and shorelines to encroach on communities and animal habitats. This results in coastal erosion, increased storm surges, and habitat loss, such as the breeding grounds of elephant seals. More critically, higher sea levels heighten community vulnerability to tsunamis, which can generate massive waves that overwhelm sea walls, as evidenced in Indonesia and Japan. Thus, simulating tsunami flooding is crucial. While SWASH, a tool based on shallow water equations, effectively models complex environments like ports and residential areas, it is slow, taking 4 hours on 16 CPU cores for one simulation. In contrast, we have developed neural operators that significantly speed up this process. Our neural operators, trained on hundreds of input-output image pairs, convert a 2.5D input image of the simulation environment into an output image showing the predicted flooding state, including wave height and wet-and-dry states. Benchmark tests show our neural operators complete simulations in just 1.2 seconds, compared to 7 hours for traditional finite volume solvers. This speed advantage remains regardless of input grid dimensions. While our neural operators excel at predicting the wet or dry state, further training is needed to improve wave height predictions. Figures demonstrate our neural operators' ability to predict flooding at various times given a simulation environment input. Our methodology offers a faster, efficient alternative for tsunami flooding simulations, which is crucial for enhanced disaster preparedness.  © 2024 IEEE.;"bathymetry; deep learning; flooding; neural operator; shallow water equations; tsunami";"Bathymetry; Benchmarking; Deep neural networks; Ocean habitats; Tropics; Vortex flow; Arbitrary bathymetries; Coastal erosion; Deep learning; Dry state; Floodings; Land topography; Melting glaciers; Neural operator; Shallow water equations; Simulation environment; Tsunamis";"FNO Tsunami Flooding Simulation for Arbitrary Bathymetries, Shorelines, and Land Topographies Global warming is rapidly melting glaciers, causing sea levels to rise and shorelines to encroach on communities and animal habitats. This results in coastal erosion, increased storm surges, and habitat loss, such as the breeding grounds of elephant seals. More critically, higher sea levels heighten community vulnerability to tsunamis, which can generate massive waves that overwhelm sea walls, as evidenced in Indonesia and Japan. Thus, simulating tsunami flooding is crucial. While SWASH, a tool based on shallow water equations, effectively models complex environments like ports and residential areas, it is slow, taking 4 hours on 16 CPU cores for one simulation. In contrast, we have developed neural operators that significantly speed up this process. Our neural operators, trained on hundreds of input-output image pairs, convert a 2.5D input image of the simulation environment into an output image showing the predicted flooding state, including wave height and wet-and-dry states. Benchmark tests show our neural operators complete simulations in just 1.2 seconds, compared to 7 hours for traditional finite volume solvers. This speed advantage remains regardless of input grid dimensions. While our neural operators excel at predicting the wet or dry state, further training is needed to improve wave height predictions. Figures demonstrate our neural operators' ability to predict flooding at various times given a simulation environment input. Our methodology offers a faster, efficient alternative for tsunami flooding simulations, which is crucial for enhanced disaster preparedness.  © 2024 IEEE. bathymetry; deep learning; flooding; neural operator; shallow water equations; tsunami Bathymetry; Benchmarking; Deep neural networks; Ocean habitats; Tropics; Vortex flow; Arbitrary bathymetries; Coastal erosion; Deep learning; Dry state; Floodings; Land topography; Melting glaciers; Neural operator; Shallow water equations; Simulation environment; Tsunamis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
414;Application of Machine Learning and Data Mining Techniques for Accurate Cloud Burst Prediction;"Cloudbursts are often characterized by intense rainfall over smaller areas, during catastrophic floods and landslides in vulnerable areas. Traditional models and meteorological departments struggle to predict the occurrences due to unstable weather variations. Approximate forecasting plays an important role in meteorology; it will influence the environment and people as it decreases the impact of cloudbursts. This study leverages the power of data mining and machine learning techniques to present the cloudburst prediction system, improving accuracy and providing early warnings. All the data contained is real time and factual. Machine learning techniques like neural networks, decision trees, along with the old meteorological data and satellite imagery are used to find the events and patterns of cloudbursts in the data. Data mining is used to preprocess the data, remove inconsistencies, smooth the data to find the associative rules, which tells us which combination of data items lead to cloudbursts.  © 2024 IEEE.";"Cloud burst Prediction; Data mining; Extreme weather Events; Machine Learning; Weather Forecasting";"Catastrophic floods; Catastrophic landslides; Cloud burst prediction; Data-mining techniques; Extreme weather events; Intense rainfalls; Machine data; Machine learning techniques; Machine-learning; Small area; Adversarial machine learning";"Application of Machine Learning and Data Mining Techniques for Accurate Cloud Burst Prediction Cloudbursts are often characterized by intense rainfall over smaller areas, during catastrophic floods and landslides in vulnerable areas. Traditional models and meteorological departments struggle to predict the occurrences due to unstable weather variations. Approximate forecasting plays an important role in meteorology; it will influence the environment and people as it decreases the impact of cloudbursts. This study leverages the power of data mining and machine learning techniques to present the cloudburst prediction system, improving accuracy and providing early warnings. All the data contained is real time and factual. Machine learning techniques like neural networks, decision trees, along with the old meteorological data and satellite imagery are used to find the events and patterns of cloudbursts in the data. Data mining is used to preprocess the data, remove inconsistencies, smooth the data to find the associative rules, which tells us which combination of data items lead to cloudbursts.  © 2024 IEEE. Cloud burst Prediction; Data mining; Extreme weather Events; Machine Learning; Weather Forecasting Catastrophic floods; Catastrophic landslides; Cloud burst prediction; Data-mining techniques; Extreme weather events; Intense rainfalls; Machine data; Machine learning techniques; Machine-learning; Small area; Adversarial machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
415;SAR-FM: SAR based Flood Rapid Mapping using Multi-Modal Data Fusion;Adverse climatic changes have increased the frequency of natural hazards, emphasizing the need for timely detection to support effective disaster management and mitigation efforts. Floods are among the most common weather-related natural disasters, causing significant damage to human settlements and the environment. While both Synthetic Aperture Radar (SAR) and Electro-Optical (EO) remote sensing data have been used to detect flooded areas, SAR is preferred due to its independence from weather and illumination conditions. However, detecting flood-inundated areas with SAR is challenging due to volume and double-bounce scattering, unlike permanent water bodies that are easily detected. To address this issue, we fuse SAR imagery with other multi-modal data and introduce three novel derived features: Water-Proximity Elevation Map (PEM), Water-Terrain Binary Map (WTM), and Dual-Pol SAR Map (DPM). We also present three deep learning architectures for flood mapping that effectively fuse multi-modal data. Extensive experiments and comparisons with state-of-the-art methods demonstrate that our proposed approach achieves improved performance with significant reduction in model complexity. © 2024 SPIE.;"Deep Learning; Flood Mapping; Multi-Modal Fusion; SAR";"Amplitude modulation; Banks (bodies of water); Binary images; Data fusion; Deep learning; Flood control; Flood damage; Frequency modulation; Image coding; Image enhancement; Mapping; Metadata; Multicarrier modulation; Network security; Optical remote sensing; Radar imaging; Climatic changes; Deep learning; Disaster management; Disaster mitigation; Flood mapping; Multi-modal data; Multi-modal fusion; Natural disasters; Natural hazard; Rapid mapping; Synthetic aperture radar";"SAR-FM: SAR based Flood Rapid Mapping using Multi-Modal Data Fusion Adverse climatic changes have increased the frequency of natural hazards, emphasizing the need for timely detection to support effective disaster management and mitigation efforts. Floods are among the most common weather-related natural disasters, causing significant damage to human settlements and the environment. While both Synthetic Aperture Radar (SAR) and Electro-Optical (EO) remote sensing data have been used to detect flooded areas, SAR is preferred due to its independence from weather and illumination conditions. However, detecting flood-inundated areas with SAR is challenging due to volume and double-bounce scattering, unlike permanent water bodies that are easily detected. To address this issue, we fuse SAR imagery with other multi-modal data and introduce three novel derived features: Water-Proximity Elevation Map (PEM), Water-Terrain Binary Map (WTM), and Dual-Pol SAR Map (DPM). We also present three deep learning architectures for flood mapping that effectively fuse multi-modal data. Extensive experiments and comparisons with state-of-the-art methods demonstrate that our proposed approach achieves improved performance with significant reduction in model complexity. © 2024 SPIE. Deep Learning; Flood Mapping; Multi-Modal Fusion; SAR Amplitude modulation; Banks (bodies of water); Binary images; Data fusion; Deep learning; Flood control; Flood damage; Frequency modulation; Image coding; Image enhancement; Mapping; Metadata; Multicarrier modulation; Network security; Optical remote sensing; Radar imaging; Climatic changes; Deep learning; Disaster management; Disaster mitigation; Flood mapping; Multi-modal data; Multi-modal fusion; Natural disasters; Natural hazard; Rapid mapping; Synthetic aperture radar";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
416;Spain on fire: A novel wildfire risk assessment model based on image satellite processing and atmospheric information;"Each year, wildfires destroy larger areas of Spain, threatening numerous ecosystems. Humans cause 90% of them (negligence or provoked) and the behaviour of individuals is unpredictable. However, atmospheric and environmental variables affect the spread of wildfires, and they can be analysed by using deep learning. In order to mitigate the damage of these events, we proposed the novel Wildfire Assessment Model (WAM). Our aim is to anticipate the economic and ecological impact of a wildfire, assisting managers in resource allocation and decision-making for dangerous regions in Spain, Castilla y León and Andalucía. The WAM uses a residual-style convolutional network architecture to perform regression over atmospheric variables and the greenness index, computing necessary resources, the control and extinction time, and the expected burnt surface area. It is first pre-trained with self-supervision over 100,000 examples of unlabelled data with a masked patch prediction objective and fine-tuned using a very small dataset, composed of 445 samples. The pretraining allows the model to understand situations, outclassing baselines with a 1,4%, 3,7% and 9% improvement estimating human, heavy and aerial resources; 21% and 10,2% in expected extinction and control time; and 18,8% in expected burnt area. Using the WAM we provide an example assessment map of Castilla y León, visualizing the expected resources over an entire region. © 2023";"Atmospheric variables; Autoencoder; Deep Learning; Few-shot learning; Fusion; Regression model; Wildfire risk assessment";"Antennas; Damage detection; Decision making; Deep learning; Ecology; Fires; Image processing; Learning systems; Network architecture; Regression analysis; Assessment models; Atmospheric variables; Auto encoders; Deep learning; Few-shot learning; Regression modelling; Risks assessments; Wildfire assessment; Wildfire risk assessment; Wildfire risks; Risk assessment";"Spain on fire: A novel wildfire risk assessment model based on image satellite processing and atmospheric information Each year, wildfires destroy larger areas of Spain, threatening numerous ecosystems. Humans cause 90% of them (negligence or provoked) and the behaviour of individuals is unpredictable. However, atmospheric and environmental variables affect the spread of wildfires, and they can be analysed by using deep learning. In order to mitigate the damage of these events, we proposed the novel Wildfire Assessment Model (WAM). Our aim is to anticipate the economic and ecological impact of a wildfire, assisting managers in resource allocation and decision-making for dangerous regions in Spain, Castilla y León and Andalucía. The WAM uses a residual-style convolutional network architecture to perform regression over atmospheric variables and the greenness index, computing necessary resources, the control and extinction time, and the expected burnt surface area. It is first pre-trained with self-supervision over 100,000 examples of unlabelled data with a masked patch prediction objective and fine-tuned using a very small dataset, composed of 445 samples. The pretraining allows the model to understand situations, outclassing baselines with a 1,4%, 3,7% and 9% improvement estimating human, heavy and aerial resources; 21% and 10,2% in expected extinction and control time; and 18,8% in expected burnt area. Using the WAM we provide an example assessment map of Castilla y León, visualizing the expected resources over an entire region. © 2023 Atmospheric variables; Autoencoder; Deep Learning; Few-shot learning; Fusion; Regression model; Wildfire risk assessment Antennas; Damage detection; Decision making; Deep learning; Ecology; Fires; Image processing; Learning systems; Network architecture; Regression analysis; Assessment models; Atmospheric variables; Auto encoders; Deep learning; Few-shot learning; Regression modelling; Risks assessments; Wildfire assessment; Wildfire risk assessment; Wildfire risks; Risk assessment";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
417;Research on Radar Echo Extrapolation Method Integrating Terrain Features;In recent years, severe convective weather has caused significant harm to lives and property, leading to flooding and geological disasters. Consequently, nowcasting and early warning of such events are crucial. Advances in artificial intelligence technologies have provided critical support for these efforts through radar echo extrapolation based on deep learning. However, most current deep learning methods primarily rely on historical radar reflectivity factors or vertically integrated liquid water content for extrapolation. Despite the significant influence of orographic effects on meteorological processes, the impact of terrain feature data on radar echo extrapolation remains underexplored. Therefore, this paper proposes a radar echo extrapolation model that integrates terrain feature data. By incorporating both radar and terrain feature data at the input stage, the model more effectively captures the spatiotemporal characteristics of meteorological processes, thereby enhancing its predictive capability. Additionally, a terrain-guided composite loss function is introduced to further constrain the model, leading to more accurate predictions. Experimental results demonstrate that this approach can improves prediction accuracy.  © 2024 IEEE.;"Composite loss function; Deep learning; Nowcasting; Terrain features";"Deep learning; Jurassic; Radar reflection; Tropics; Composite loss function; Convective weather; Deep learning; Extrapolation methods; Feature data; Loss functions; Nowcasting; Radar echoes; Significant harm; Terrain features; Extrapolation";"Research on Radar Echo Extrapolation Method Integrating Terrain Features In recent years, severe convective weather has caused significant harm to lives and property, leading to flooding and geological disasters. Consequently, nowcasting and early warning of such events are crucial. Advances in artificial intelligence technologies have provided critical support for these efforts through radar echo extrapolation based on deep learning. However, most current deep learning methods primarily rely on historical radar reflectivity factors or vertically integrated liquid water content for extrapolation. Despite the significant influence of orographic effects on meteorological processes, the impact of terrain feature data on radar echo extrapolation remains underexplored. Therefore, this paper proposes a radar echo extrapolation model that integrates terrain feature data. By incorporating both radar and terrain feature data at the input stage, the model more effectively captures the spatiotemporal characteristics of meteorological processes, thereby enhancing its predictive capability. Additionally, a terrain-guided composite loss function is introduced to further constrain the model, leading to more accurate predictions. Experimental results demonstrate that this approach can improves prediction accuracy.  © 2024 IEEE. Composite loss function; Deep learning; Nowcasting; Terrain features Deep learning; Jurassic; Radar reflection; Tropics; Composite loss function; Convective weather; Deep learning; Extrapolation methods; Feature data; Loss functions; Nowcasting; Radar echoes; Significant harm; Terrain features; Extrapolation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
418;Realtime Flood Forecasting: River flow analysis using Machine Learning Techniques;Kalu Ganga river basin in Sri Lanka is highly susceptible during the monsoon seasons, which frequently causes devastating floods, disrupting the lives of local communities. Addressing this critical issue, this research focusses on enhancing the accuracy of water level predictions in the Kalu Ganga river basin. Traditional methods of water level prediction have proven to be inefficient, highlighting the need for more advanced and accurate forecasting techniques. This study developed a rolling forecasting system aimed at predicting future water levels at the Ratnapura station in the Kalu Ganga using several machine learning algorithms. Data collected over a period of 10 months was utilized, with 75% allocated for training and the remainder for testing and validation. We employed four machine learning models, namely Support Vector Regression (SVR), Random Forest (RF), Artificial Neural Network (ANN), and Long Short-Term Memory (LSTM) were used for prediction. All models demonstrated high accuracy in predicting water levels, with the ANN and LSTM models marginally outperforming the SVR and RF in most cases. However, challenges were noted in accurately predicting peak water levels across all models. The limited 10-month data duration potentially constrained the models' predictive capability over extended periods. In conclusion, the rolling forecasting system developed in this study holds promise for integration into the rivernet.lk system, potentially enhancing flood management capabilities. Further research using a larger dataset spanning over multiple years is recommended to improve the accuracy of the models in predicting water levels over longer periods. This study offers insights that could advance water resource management and flood mitigation efforts in Sri Lanka. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved.;"Flood forecasting; Kalu Ganga; LSTM; Machine learning; rolling forecasting";"Information management; Long short-term memory; Prediction models; Resource allocation; Support vector regression; Weather forecasting; Flood forecasting; Forecasting system; Kalu ganga; Machine-learning; River basins; Rolling forecasting; Short term memory; Sri Lanka; Support vector regressions; Water level prediction; Rivers";"Realtime Flood Forecasting: River flow analysis using Machine Learning Techniques Kalu Ganga river basin in Sri Lanka is highly susceptible during the monsoon seasons, which frequently causes devastating floods, disrupting the lives of local communities. Addressing this critical issue, this research focusses on enhancing the accuracy of water level predictions in the Kalu Ganga river basin. Traditional methods of water level prediction have proven to be inefficient, highlighting the need for more advanced and accurate forecasting techniques. This study developed a rolling forecasting system aimed at predicting future water levels at the Ratnapura station in the Kalu Ganga using several machine learning algorithms. Data collected over a period of 10 months was utilized, with 75% allocated for training and the remainder for testing and validation. We employed four machine learning models, namely Support Vector Regression (SVR), Random Forest (RF), Artificial Neural Network (ANN), and Long Short-Term Memory (LSTM) were used for prediction. All models demonstrated high accuracy in predicting water levels, with the ANN and LSTM models marginally outperforming the SVR and RF in most cases. However, challenges were noted in accurately predicting peak water levels across all models. The limited 10-month data duration potentially constrained the models' predictive capability over extended periods. In conclusion, the rolling forecasting system developed in this study holds promise for integration into the rivernet.lk system, potentially enhancing flood management capabilities. Further research using a larger dataset spanning over multiple years is recommended to improve the accuracy of the models in predicting water levels over longer periods. This study offers insights that could advance water resource management and flood mitigation efforts in Sri Lanka. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved. Flood forecasting; Kalu Ganga; LSTM; Machine learning; rolling forecasting Information management; Long short-term memory; Prediction models; Resource allocation; Support vector regression; Weather forecasting; Flood forecasting; Forecasting system; Kalu ganga; Machine-learning; River basins; Rolling forecasting; Short term memory; Sri Lanka; Support vector regressions; Water level prediction; Rivers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
419;Seismic Waveform Recognition and Adaptive Adjustment System of Seismic Design Parameters Optimized by AI Algorithm;Seismic waveform recognition is crucial for seismic activity monitoring and early warning in the field of earthquake science and engineering, and seismic design for building seismic-waveform recognition and data is dependent on the use of artificial intelligence to achieve better earthquake detection and seismic design parameters. The development of Artificial Intelligence (AI) technology and the application of deep learning algorithms have created the conditions for automatic recognition of seismic waveforms and optimization of seismic design parameters. Based on the optimization of the artificial intelligence algorithm, this paper develops a seismic waveform recognition and seismic design parameter adaptive adjustment system. The system uses a convolutional neural network (CNN) model to recognize seismic waveforms, and enhances the recognition accuracy and processing speed of the model via various methods. The system also includes an adaptive adjustment module. Real-time seismic activity data are used to dynamically adjust the seismic design parameters. From the experimental results, the seismic waveform recognition by the system has been verified. The recognition rate of the CNN model can reach 95.2%, the processing time of the model is 578ms, and the system response is very fast. The maximum CPU utilization of the system does not exceed 28.9%, therefore the system has low resource consumption, high stability and reliability. The average time between failures indicates that the system has high reliability, and the minimum average duration of system failures is only 0.7 h. At the same time, the system exhibits good performance in adaptive adjustment of seismic design parameters, which can be quickly adjusted based on real-time data, thereby providing more accurate and effective seismic protection for buildings and infrastructure. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.;"Adaptive Adjustment System; Convolutional Neural Networks; Seismic Design; Seismic Waveform Recognition";"Adaptive control systems; Convolution; Deep learning; Earthquake effects; Integrated circuit design; Seismic design; Seismic response; Adaptive adjustment; Adaptive adjustment system; Artificial intelligence algorithms; Convolutional neural network; Design parameters; Neural network model; Optimisations; Seismic activity; Seismic waveform recognition; Seismic waveforms; Convolutional neural networks";"Seismic Waveform Recognition and Adaptive Adjustment System of Seismic Design Parameters Optimized by AI Algorithm Seismic waveform recognition is crucial for seismic activity monitoring and early warning in the field of earthquake science and engineering, and seismic design for building seismic-waveform recognition and data is dependent on the use of artificial intelligence to achieve better earthquake detection and seismic design parameters. The development of Artificial Intelligence (AI) technology and the application of deep learning algorithms have created the conditions for automatic recognition of seismic waveforms and optimization of seismic design parameters. Based on the optimization of the artificial intelligence algorithm, this paper develops a seismic waveform recognition and seismic design parameter adaptive adjustment system. The system uses a convolutional neural network (CNN) model to recognize seismic waveforms, and enhances the recognition accuracy and processing speed of the model via various methods. The system also includes an adaptive adjustment module. Real-time seismic activity data are used to dynamically adjust the seismic design parameters. From the experimental results, the seismic waveform recognition by the system has been verified. The recognition rate of the CNN model can reach 95.2%, the processing time of the model is 578ms, and the system response is very fast. The maximum CPU utilization of the system does not exceed 28.9%, therefore the system has low resource consumption, high stability and reliability. The average time between failures indicates that the system has high reliability, and the minimum average duration of system failures is only 0.7 h. At the same time, the system exhibits good performance in adaptive adjustment of seismic design parameters, which can be quickly adjusted based on real-time data, thereby providing more accurate and effective seismic protection for buildings and infrastructure. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024. Adaptive Adjustment System; Convolutional Neural Networks; Seismic Design; Seismic Waveform Recognition Adaptive control systems; Convolution; Deep learning; Earthquake effects; Integrated circuit design; Seismic design; Seismic response; Adaptive adjustment; Adaptive adjustment system; Artificial intelligence algorithms; Convolutional neural network; Design parameters; Neural network model; Optimisations; Seismic activity; Seismic waveform recognition; Seismic waveforms; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
420;Deep convolutional neural networks with Bee Collecting Pollen Algorithm (BCPA)-based landslide data balancing and spatial prediction;Predicting the landslide-prone area is critical for various applications, including emergency response, land planning, and disaster mitigation. There needs to be a thorough landslide inventory in current studies and appropriate sampling uncertainty issues. Landslide risk mapping has expanded significantly as machine learning techniques have developed. However, one of the primary issues in Landslide Prediction is data imbalance (DI). This is problematic since it is challenging or expensive to generate an accurate inventory map of landslides based on previous data. This study proposes a novel landslide prediction method using Generative Adversarial Networks (GAN) for generating the synthetic data, Synthetic Minority Oversampling Technique (SMOTE) for overcoming the data imbalance problem, and Bee Collecting Pollen Algorithm (BCPA) for feature extraction. Combining 184 landslides and ten criteria, including topographic wetness index (TWI), aspect, distance from the road, total curvature, sediment transport index (STI), height, slope, stream, lithology, and slope length, a geographical database was produced. The data was generated using GAN, a Deep Convolutional Neural Network (DCNN) technique to populate the dataset. The proposed DCNN-BCPA approach findings were merged with current machine learning methods such as Random Forests (RF), Artificial Neural Networks (ANN), k-Nearest Neighbours (k-NN), Decision Trees (DT), Support Vector Machine (SVM), logistic regression (LR). The model's accuracy, precision, recall, f-score, and RMSE were measured using the following metrics: 92.675%, 96.298%, 90.536%, 96.637%, and 45.623%. This study suggests that harmonizing landslide data may have a substantial impact on the predictive capabilities of machine learning models.  © 2024 - IOS Press. All rights reserved.;"Bee collecting pollen algorithm; data balancing; generative adversarial network; landslide susceptibility; synthetic data";"Convolutional neural networks; Deep neural networks; Forecasting; Generative adversarial networks; Learning systems; Lithology; Support vector machines; 'current; Bee collecting pollen algorithm; Convolutional neural network; Data balancing; Data imbalance; Landslide prediction; Landslide susceptibility; Landslide-prone areas; Spatial prediction; Synthetic data; Decision trees";"Deep convolutional neural networks with Bee Collecting Pollen Algorithm (BCPA)-based landslide data balancing and spatial prediction Predicting the landslide-prone area is critical for various applications, including emergency response, land planning, and disaster mitigation. There needs to be a thorough landslide inventory in current studies and appropriate sampling uncertainty issues. Landslide risk mapping has expanded significantly as machine learning techniques have developed. However, one of the primary issues in Landslide Prediction is data imbalance (DI). This is problematic since it is challenging or expensive to generate an accurate inventory map of landslides based on previous data. This study proposes a novel landslide prediction method using Generative Adversarial Networks (GAN) for generating the synthetic data, Synthetic Minority Oversampling Technique (SMOTE) for overcoming the data imbalance problem, and Bee Collecting Pollen Algorithm (BCPA) for feature extraction. Combining 184 landslides and ten criteria, including topographic wetness index (TWI), aspect, distance from the road, total curvature, sediment transport index (STI), height, slope, stream, lithology, and slope length, a geographical database was produced. The data was generated using GAN, a Deep Convolutional Neural Network (DCNN) technique to populate the dataset. The proposed DCNN-BCPA approach findings were merged with current machine learning methods such as Random Forests (RF), Artificial Neural Networks (ANN), k-Nearest Neighbours (k-NN), Decision Trees (DT), Support Vector Machine (SVM), logistic regression (LR). The model's accuracy, precision, recall, f-score, and RMSE were measured using the following metrics: 92.675%, 96.298%, 90.536%, 96.637%, and 45.623%. This study suggests that harmonizing landslide data may have a substantial impact on the predictive capabilities of machine learning models.  © 2024 - IOS Press. All rights reserved. Bee collecting pollen algorithm; data balancing; generative adversarial network; landslide susceptibility; synthetic data Convolutional neural networks; Deep neural networks; Forecasting; Generative adversarial networks; Learning systems; Lithology; Support vector machines; 'current; Bee collecting pollen algorithm; Convolutional neural network; Data balancing; Data imbalance; Landslide prediction; Landslide susceptibility; Landslide-prone areas; Spatial prediction; Synthetic data; Decision trees";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
421;Potential distribution and trends of Trachycarpus fortunei (T.f) under climate change scenarios;"Trachycarpus fortunei (T.f) is an evergreen tree of the genus Palm in the family Palmae, with high medicinal, landscape, and economic value. However, the species is sensitive to climate change, and climate change-induced weather extremes, droughts and high temperatures may threaten palms in tropical and subtropical regions. Palm oil, the raw material, is produced as biomass diesel to replace oil in many countries. This has also led to the unwarranted expansion of T.f cultivation and encroachment on more carbon-intensive forests, to the detriment of global climate governance. Predicting the distribution range of T.f in the context of climate change can provide a scientific basis and reference for the resource management and sustainable use of this species. In this paper, we analyzed the potential distribution of T.f in the world by using ArcGIS 10.8 and MaxEnt 3.4.1 software, and compared it with the Bioclim and Domain models of DIVA-GIS 7.5 software, and finally chose the model with the best prediction results. The MaxEnt model was selected as the model with the best prediction results. Then the distribution range of T.f was predicted under four climate scenarios (RCP 2.6, RCP 4.5, RCP 6.0, RCP 8.5) in the 2050s and 2070s of the 21st century. The results showed that the MaxEnt model predicted the distribution of potential habitats with the highest accuracy; Bio6, Bio11, Bio12, and Bio17 were the main climatic variables affecting the distribution of T.f; T.f is currently distributed in the gradual landward area of southeastern China's coasts, Japan's coasts, and the landward area along the coasts of the United Kingdom, France, and Germany.; Future climate change will negatively influence the distribution of its habitat, causing the habitat to shrink outward and fracture inward. The geographical features of the T.f planting are various, and it is essential to plan the process logically and effectively to prevent invading forests with high carbon density and limiting global climate governance.  © 2024 ACM.";"Climate scenarios; Machine learning model; Palm oil; Potential distribution; Suitable habitat";"Abiotic; Anthropogenic; Biotic; Climate change scenarios; Climate scenarios; Evergreen trees; Global climates; Landscape values; Machine learning models; MaxEnt models; Medicinal values; Potential distributions; Suitable habitat; Resource allocation";"Potential distribution and trends of Trachycarpus fortunei (T.f) under climate change scenarios Trachycarpus fortunei (T.f) is an evergreen tree of the genus Palm in the family Palmae, with high medicinal, landscape, and economic value. However, the species is sensitive to climate change, and climate change-induced weather extremes, droughts and high temperatures may threaten palms in tropical and subtropical regions. Palm oil, the raw material, is produced as biomass diesel to replace oil in many countries. This has also led to the unwarranted expansion of T.f cultivation and encroachment on more carbon-intensive forests, to the detriment of global climate governance. Predicting the distribution range of T.f in the context of climate change can provide a scientific basis and reference for the resource management and sustainable use of this species. In this paper, we analyzed the potential distribution of T.f in the world by using ArcGIS 10.8 and MaxEnt 3.4.1 software, and compared it with the Bioclim and Domain models of DIVA-GIS 7.5 software, and finally chose the model with the best prediction results. The MaxEnt model was selected as the model with the best prediction results. Then the distribution range of T.f was predicted under four climate scenarios (RCP 2.6, RCP 4.5, RCP 6.0, RCP 8.5) in the 2050s and 2070s of the 21st century. The results showed that the MaxEnt model predicted the distribution of potential habitats with the highest accuracy; Bio6, Bio11, Bio12, and Bio17 were the main climatic variables affecting the distribution of T.f; T.f is currently distributed in the gradual landward area of southeastern China's coasts, Japan's coasts, and the landward area along the coasts of the United Kingdom, France, and Germany.; Future climate change will negatively influence the distribution of its habitat, causing the habitat to shrink outward and fracture inward. The geographical features of the T.f planting are various, and it is essential to plan the process logically and effectively to prevent invading forests with high carbon density and limiting global climate governance.  © 2024 ACM. Climate scenarios; Machine learning model; Palm oil; Potential distribution; Suitable habitat Abiotic; Anthropogenic; Biotic; Climate change scenarios; Climate scenarios; Evergreen trees; Global climates; Landscape values; Machine learning models; MaxEnt models; Medicinal values; Potential distributions; Suitable habitat; Resource allocation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
422;A SIMULATION-BASED REAL-TIME DEEP REINFORCEMENT LEARNING APPROACH FOR FIGHTING WILDFIRES;Wildfire incidents are catastrophic events with a high level of uncertainty. Therefore, resource allocation towards its suppression is a challenging task. Our research presents a method for wildfire management that combines digital twins with a real-time deep reinforcement learning (DRL) technique. The detection of wildfires is supported by the use of real-time satellite imagery. The developed digital twin operates under two stages. Initially, a discrete simulation approach is employed to model the behavior of fires, their immediate impact on the population, and the response times of firefighting resources. Then, an Advantage Actor-Critic (A2C) DRL policy optimizes the response under different scenarios accounting for a time-based environment to adjust agent actions. The holistic approach presented in this study strives to synergize and minimize the effect of wildfires over population to create an adaptive and effective wildfire response system. © 2024 Society for Modeling & Simulation International (SCS).;"deep reinforcement learning; digital twins; real-time decision-making; resource allocation; wildfires";"Adversarial machine learning; Decision making; Digital elevation model; Premixed flames; Reinforcement learning; Resource allocation; Catastrophic event; Real time decision-making; Real- time; Real-time decision making; Reinforcement learning approach; Reinforcement learnings; Resources allocation; Uncertainty; Wildfire; Wildfire management; Deep reinforcement learning";"A SIMULATION-BASED REAL-TIME DEEP REINFORCEMENT LEARNING APPROACH FOR FIGHTING WILDFIRES Wildfire incidents are catastrophic events with a high level of uncertainty. Therefore, resource allocation towards its suppression is a challenging task. Our research presents a method for wildfire management that combines digital twins with a real-time deep reinforcement learning (DRL) technique. The detection of wildfires is supported by the use of real-time satellite imagery. The developed digital twin operates under two stages. Initially, a discrete simulation approach is employed to model the behavior of fires, their immediate impact on the population, and the response times of firefighting resources. Then, an Advantage Actor-Critic (A2C) DRL policy optimizes the response under different scenarios accounting for a time-based environment to adjust agent actions. The holistic approach presented in this study strives to synergize and minimize the effect of wildfires over population to create an adaptive and effective wildfire response system. © 2024 Society for Modeling & Simulation International (SCS). deep reinforcement learning; digital twins; real-time decision-making; resource allocation; wildfires Adversarial machine learning; Decision making; Digital elevation model; Premixed flames; Reinforcement learning; Resource allocation; Catastrophic event; Real time decision-making; Real- time; Real-time decision making; Reinforcement learning approach; Reinforcement learnings; Resources allocation; Uncertainty; Wildfire; Wildfire management; Deep reinforcement learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.4;Climatological;3;Response
423;Predictive Modeling of Earthquake Impact on Population Centers Using Random Forest Algorithm;Natural disasters like earthquakes have a profound and a wide-ranging effect on infrastructure, economic systems, and communities across the world. Accurately estimating an earthquake's 'level of impact' is essential for risk reduction, effective disaster management, and long-term planning in seismically active areas. The analysis of the effects of earthquakes has been facilitated by recent developments in information technology, including increased storage capacity and novel techniques for organizing and gathering data. This study is about using a comprehensive dataset, made up of earthquake characteristics, geographical parameters, and the degree of impact of the earthquake, to construct a model that can effectively predict the level of impact of earthquakes using machine learning regression methods. Pre-processing of the raw data, feature selection, model validation, and optimization are necessary to guarantee the robustness and generalizability of the final product. The models produced will be assessed for accuracy and performance. A variety of machine learning algorithms including linear and polynomial regression, support vector regression, decision trees, and ensemble methods like random forest regression, will be used. Urban planners, emergency response teams, and legislators can all benefit by using these models to develop and carry out leaner, more effective disaster response plans.  © 2024 IEEE.;"Earthquake Impact; magnitude; population; Prediction";"Decision trees; Earthquake effects; Random forests; Risk assessment; Risk perception; Support vector regression; Earthquake impact; Economic community; Economic system; Magnitude; Natural disasters; Population; Population centers; Predictive models; Random forest algorithm; Risks reduction; Risk management";"Predictive Modeling of Earthquake Impact on Population Centers Using Random Forest Algorithm Natural disasters like earthquakes have a profound and a wide-ranging effect on infrastructure, economic systems, and communities across the world. Accurately estimating an earthquake's 'level of impact' is essential for risk reduction, effective disaster management, and long-term planning in seismically active areas. The analysis of the effects of earthquakes has been facilitated by recent developments in information technology, including increased storage capacity and novel techniques for organizing and gathering data. This study is about using a comprehensive dataset, made up of earthquake characteristics, geographical parameters, and the degree of impact of the earthquake, to construct a model that can effectively predict the level of impact of earthquakes using machine learning regression methods. Pre-processing of the raw data, feature selection, model validation, and optimization are necessary to guarantee the robustness and generalizability of the final product. The models produced will be assessed for accuracy and performance. A variety of machine learning algorithms including linear and polynomial regression, support vector regression, decision trees, and ensemble methods like random forest regression, will be used. Urban planners, emergency response teams, and legislators can all benefit by using these models to develop and carry out leaner, more effective disaster response plans.  © 2024 IEEE. Earthquake Impact; magnitude; population; Prediction Decision trees; Earthquake effects; Random forests; Risk assessment; Risk perception; Support vector regression; Earthquake impact; Economic community; Economic system; Magnitude; Natural disasters; Population; Population centers; Predictive models; Random forest algorithm; Risks reduction; Risk management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
424;Landslide Detection and Early Warning System using Extended Kalman Filter with Convolutional Neural Networks based Long Short-Term Memory;"Nowadays, landslides are a major risk to human life and causing devastating consequences. and economic stability by resulting substantial losses annually. The effective landslide risk management relies on timely recognition of spatial changes in patterns. patterns. Traditional approaches for developing the warning system for landslide had faced challenges which includes linearity assumption and difficulty in handling non-stationary data. Therefore, this paper proposes Extended Kalman Filter-Convolutional Neural Networks-Long Short-Term Memory (EKF-CNN-LSTM) for landslide detection and early warning system. Initially, the data is collected from Fast Deploying Monitoring System (FDMS) which consists of ad-hoc router, sensors related to landslides. Then, the data is preprocessed by using Fast Fourier Transform (FFT), which effectively filtered out noise and irrelevant frequency components by improving signal quality. After that, model is trained by proposed EKF-CNN-LSTM which generated real time prediction, early warnings of potential landslides. Finally, the early warnings are predicted and give output as ""no warning"" or ""warning,"" by enabling the timely interventions. By integrating these technologies into landslide monitoring and EWS, enhanced the prediction accuracy and provided timely warnings. The proposed EKF-CNN-LSTM succeeded with high results including accuracy (0.943), precision (0.832), recall (0.786), F1-score (0.798) and MCC (0.743) when compared with existing ResNet101. © 2024 IEEE.";"convolutional neural networks; extended kalman filter; fast deploying monitoring system; landslide detection; long short-term memory";"Associative storage; Convolutional neural networks; Landslides; Long short-term memory; Risk management; Wiener filtering; Convolutional neural network; Early warning; Early Warning System; Economic stability; Fast deploying monitoring system; Human lives; Landslide detection; Monitoring system; Network-based; Short term memory; Extended Kalman filters";"Landslide Detection and Early Warning System using Extended Kalman Filter with Convolutional Neural Networks based Long Short-Term Memory Nowadays, landslides are a major risk to human life and causing devastating consequences. and economic stability by resulting substantial losses annually. The effective landslide risk management relies on timely recognition of spatial changes in patterns. patterns. Traditional approaches for developing the warning system for landslide had faced challenges which includes linearity assumption and difficulty in handling non-stationary data. Therefore, this paper proposes Extended Kalman Filter-Convolutional Neural Networks-Long Short-Term Memory (EKF-CNN-LSTM) for landslide detection and early warning system. Initially, the data is collected from Fast Deploying Monitoring System (FDMS) which consists of ad-hoc router, sensors related to landslides. Then, the data is preprocessed by using Fast Fourier Transform (FFT), which effectively filtered out noise and irrelevant frequency components by improving signal quality. After that, model is trained by proposed EKF-CNN-LSTM which generated real time prediction, early warnings of potential landslides. Finally, the early warnings are predicted and give output as ""no warning"" or ""warning,"" by enabling the timely interventions. By integrating these technologies into landslide monitoring and EWS, enhanced the prediction accuracy and provided timely warnings. The proposed EKF-CNN-LSTM succeeded with high results including accuracy (0.943), precision (0.832), recall (0.786), F1-score (0.798) and MCC (0.743) when compared with existing ResNet101. © 2024 IEEE. convolutional neural networks; extended kalman filter; fast deploying monitoring system; landslide detection; long short-term memory Associative storage; Convolutional neural networks; Landslides; Long short-term memory; Risk management; Wiener filtering; Convolutional neural network; Early warning; Early Warning System; Economic stability; Fast deploying monitoring system; Human lives; Landslide detection; Monitoring system; Network-based; Short term memory; Extended Kalman filters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
425;Automatic Near Real-Time Web-based Flood Monitoring System with Multitask Learned Water Detection Deep Learning Model;"Extreme rainfall events, characterized by substantial precipitation in a short period, have increased in frequency and intensity as climate change intensifies. Disaster response capabilities have become more crucial due to the increasing rate of unpredicted flood events. These extreme rainfall patterns necessitate the (near) real-time identification of affected areas to support decision-making and minimize damages. Synthetic Aperture Radar (SAR), unaffected by cloud cover and lighting conditions, is an optimized remote sensor for detecting flood occurrences and their extent. However, for real-time analysis of satellite images and support for disaster response, an automated flood monitoring system is required. In this study, we employed Amazon Web Services(AWS) to automatically acquire and preprocess Sentinel-1 satellite images of South Korea, followed by the use of AI deep learning models to detect water bodies in (near) real-time. Considering that flood-prone area of Korea typically occur along small streams, we adopted Multitask learning in medium-resolution (20m) Sentinel-1 images to detect fine rivers. By assigning two tasks (water body detection and the extraction of river embankment centerlines) to two decoders sharing a single encoder, we enhance the detection rate of small streams. The detected water bodies are compared against geographic information databases, such as those for river embankments and reservoir areas, to classify flood-affected regions. The satellite images and analyzed results are automatically transmitted to the web-based visualization system (Satellite Current View; SCV). SCV also provides additional spatial data, including roads, bridges, urban planning maps, and land use maps, to offer further information on disaster-affected areas. We have analyzed and provided Sentinel-1 images of actual flood events in Korea, especially 2020 and 2023 flood events. Additionally, high-resolution SAR images captured by ICEYE and Umbra are also used to analyze flood-affected areas in Korea and are visualized through SCV. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved.";"Automatic Disaster Monitoring; Flood Monitoring; Sentinel-1; Water detection";"Air quality; Conformal mapping; Deep learning; Flood damage; Metadata; Photomapping; Tropics; Affected area; Automatic disaster monitoring; Disaster monitoring; Flood event; Flood monitoring; Near-real time; Satellite images; Sentinel-1; Water detection; Waterbodies; Rain";"Automatic Near Real-Time Web-based Flood Monitoring System with Multitask Learned Water Detection Deep Learning Model Extreme rainfall events, characterized by substantial precipitation in a short period, have increased in frequency and intensity as climate change intensifies. Disaster response capabilities have become more crucial due to the increasing rate of unpredicted flood events. These extreme rainfall patterns necessitate the (near) real-time identification of affected areas to support decision-making and minimize damages. Synthetic Aperture Radar (SAR), unaffected by cloud cover and lighting conditions, is an optimized remote sensor for detecting flood occurrences and their extent. However, for real-time analysis of satellite images and support for disaster response, an automated flood monitoring system is required. In this study, we employed Amazon Web Services(AWS) to automatically acquire and preprocess Sentinel-1 satellite images of South Korea, followed by the use of AI deep learning models to detect water bodies in (near) real-time. Considering that flood-prone area of Korea typically occur along small streams, we adopted Multitask learning in medium-resolution (20m) Sentinel-1 images to detect fine rivers. By assigning two tasks (water body detection and the extraction of river embankment centerlines) to two decoders sharing a single encoder, we enhance the detection rate of small streams. The detected water bodies are compared against geographic information databases, such as those for river embankments and reservoir areas, to classify flood-affected regions. The satellite images and analyzed results are automatically transmitted to the web-based visualization system (Satellite Current View; SCV). SCV also provides additional spatial data, including roads, bridges, urban planning maps, and land use maps, to offer further information on disaster-affected areas. We have analyzed and provided Sentinel-1 images of actual flood events in Korea, especially 2020 and 2023 flood events. Additionally, high-resolution SAR images captured by ICEYE and Umbra are also used to analyze flood-affected areas in Korea and are visualized through SCV. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved. Automatic Disaster Monitoring; Flood Monitoring; Sentinel-1; Water detection Air quality; Conformal mapping; Deep learning; Flood damage; Metadata; Photomapping; Tropics; Affected area; Automatic disaster monitoring; Disaster monitoring; Flood event; Flood monitoring; Near-real time; Satellite images; Sentinel-1; Water detection; Waterbodies; Rain";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;2;Preparation
426;Leveraging Artificial Intelligence and Internet of Things for Advanced Disaster Drone Technology;Disaster management involves critical decisions where precision and accuracy are vital. Our research focuses on using artificial intelligence (AI) and Internet-of-the-Things (IoT) based drones to enhance situational awareness in disaster-stricken areas. These drones, powered by advanced deep learning algorithms, provide real-time aerial imagery, detect signs of life under rubble, and operate effectively in adverse weather conditions, including heavy rain and flooding. Equipped with gas sensors, they ensure the safety of both rescuers and survivors by detecting harmful gases. Additionally, the drones facilitate communication with survivors and support coordination during rescue operations. Using COCO, Fire datasets, and the YOLO model, our method achieved results with a fire detection precision of 97.5 %, recall of 92.3%, and an Fl-score of 95.5%. This innovative approach aims to revolutionize disaster response by making operations more effective and safer for everyone involved.  © 2024 IEEE.;"Artificial intelligence; Computer Vision; Disaster Services; Drones; Internet of things";"Aerial photography; Aircraft accidents; Deep learning; Disaster prevention; Disasters; Fire detectors; Adverse weather; Aerial imagery; Condition; Disaster management; Disaster service; Floodings; Heavy rains; Real- time; Research focus; Situational awareness; Drones";"Leveraging Artificial Intelligence and Internet of Things for Advanced Disaster Drone Technology Disaster management involves critical decisions where precision and accuracy are vital. Our research focuses on using artificial intelligence (AI) and Internet-of-the-Things (IoT) based drones to enhance situational awareness in disaster-stricken areas. These drones, powered by advanced deep learning algorithms, provide real-time aerial imagery, detect signs of life under rubble, and operate effectively in adverse weather conditions, including heavy rain and flooding. Equipped with gas sensors, they ensure the safety of both rescuers and survivors by detecting harmful gases. Additionally, the drones facilitate communication with survivors and support coordination during rescue operations. Using COCO, Fire datasets, and the YOLO model, our method achieved results with a fire detection precision of 97.5 %, recall of 92.3%, and an Fl-score of 95.5%. This innovative approach aims to revolutionize disaster response by making operations more effective and safer for everyone involved.  © 2024 IEEE. Artificial intelligence; Computer Vision; Disaster Services; Drones; Internet of things Aerial photography; Aircraft accidents; Deep learning; Disaster prevention; Disasters; Fire detectors; Adverse weather; Aerial imagery; Condition; Disaster management; Disaster service; Floodings; Heavy rains; Real- time; Research focus; Situational awareness; Drones";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;3;Response
427;Dynamic Prediction of Landslide Displacement Using Time Series GRU and Incorporating Environmental Variables;Landslide is one of the most common geological disasters globally, which has caused serious impact on human society and natural environment. High-precision prediction of landslide displacement has important effects on prevention and early warning against landslide disasters. Most existing landslide displacement prediction models focus on static model methods and focus minimally on the effects on the external environmental variables of landslide. To solve these problems, this study proposed a time series gate recurrent unit (GRU) dynamic prediction model that considers the effects of environmental variables. First, landslide displacement was decomposed into the trend and periodic term displacements by exponential smoothing. Second, a GRU model was built by considering the influences of external environmental variables on landslide displacement to predict the periodic term displacement. Third, the individual component displacements were aggregated to attain a dynamic forecast of the landslide movement. Lastly, a case study was conducted based on the landslide of Baishui River, China. The effectiveness of the proposed prediction model was verified by comparing with traditional intelligence algorithms (e.g., back propagation (BP) and extreme learning machine (ELM)). Results demonstrate that the proposed model conforms well to the evolution process of landslide displacement with consideration to the influences of external environmental variables on the fluctuation characteristics of periodic term displacement. The memory structural function of the GRU model can automatically adapt to the dynamic variation characteristics of landslide data during landslide prediction. The minimum and maximum prediction errors of the GRU model are 0.01 mm and 12 mm, respectively. The GRU model, compared with the BP and ELM static models, effectively increases the prediction accuracy (RMSE is increased by 6.7 times and the MRE is increased by 3.5 and 7.6 times, respectively). This study provides an important evidence for the prediction, early warning, prevention, and reduction of landslide disasters. © 2024 School of Science, DUTH. All rights reserved.;"Deep learning; Environmental variables; Gate recurrent unit; Landslide displacement prediction; Time series";"Deep learning; Landslides; Time series; Deep learning; Displacement prediction; Dynamic prediction; Early warning; Environmental variables; Gate recurrent unit; Landslide displacement prediction; Prediction modelling; Static modelling; Times series; Prediction models";"Dynamic Prediction of Landslide Displacement Using Time Series GRU and Incorporating Environmental Variables Landslide is one of the most common geological disasters globally, which has caused serious impact on human society and natural environment. High-precision prediction of landslide displacement has important effects on prevention and early warning against landslide disasters. Most existing landslide displacement prediction models focus on static model methods and focus minimally on the effects on the external environmental variables of landslide. To solve these problems, this study proposed a time series gate recurrent unit (GRU) dynamic prediction model that considers the effects of environmental variables. First, landslide displacement was decomposed into the trend and periodic term displacements by exponential smoothing. Second, a GRU model was built by considering the influences of external environmental variables on landslide displacement to predict the periodic term displacement. Third, the individual component displacements were aggregated to attain a dynamic forecast of the landslide movement. Lastly, a case study was conducted based on the landslide of Baishui River, China. The effectiveness of the proposed prediction model was verified by comparing with traditional intelligence algorithms (e.g., back propagation (BP) and extreme learning machine (ELM)). Results demonstrate that the proposed model conforms well to the evolution process of landslide displacement with consideration to the influences of external environmental variables on the fluctuation characteristics of periodic term displacement. The memory structural function of the GRU model can automatically adapt to the dynamic variation characteristics of landslide data during landslide prediction. The minimum and maximum prediction errors of the GRU model are 0.01 mm and 12 mm, respectively. The GRU model, compared with the BP and ELM static models, effectively increases the prediction accuracy (RMSE is increased by 6.7 times and the MRE is increased by 3.5 and 7.6 times, respectively). This study provides an important evidence for the prediction, early warning, prevention, and reduction of landslide disasters. © 2024 School of Science, DUTH. All rights reserved. Deep learning; Environmental variables; Gate recurrent unit; Landslide displacement prediction; Time series Deep learning; Landslides; Time series; Deep learning; Displacement prediction; Dynamic prediction; Early warning; Environmental variables; Gate recurrent unit; Landslide displacement prediction; Prediction modelling; Static modelling; Times series; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
428;Image Classification Based on Disaster type Using Deep Learning;The continued rise of global temperatures is causing a major climate crisis and this is leading to devastating and deadly natural disasters. People use social media platforms to capture and share real-time incidents in the form of images, videos and text. However, sharing too much information at once makes it harder for first responders to determine where exactly individuals are in need and whether they require immediate assistance. In the past, machine learning techniques were used to automatically identify and infer disaster response from images, as manually identifying disaster types is currently challenging. Therefore, in this work, deep learning models are used to investigate how well they can classify the images according to their disaster type by learning the features extracted from the input images on their own. Seven categories of disaster were considered in this study. These were: cyclones, earthquakes, floods, droughts, landslides, wildfires and urban fires. Two existing datasets namely the Comprehensive Disaster Dataset (CDD) and the Natural Disaster Dataset (NDD) were customised into a single dataset which we named as the Customised Disaster Dataset (CDD). The Customised Disaster Dataset comprises of a total of ten classes, three of which are images which are not related to disaster. These three classes are: regular images of buildings and streets, wild forest and sea. Three pre-trained deep learning models such MobileNetV2, VGG16 and InceptionV3 were used to train the datasets to allow for further comparison with existing studies. Along with that, a customised neural network model was created and trained on the datasets. Different scenarios were devised to assess the top performing models. The InceptionV3 had the best classification accuracy of 96.86% when the trainable layers were set to false. We also obtained an accuracy of 96.86% with the MobileNetV2 model but this time the trainable layers were set to true. In this study, we have demonstrated the effectiveness of CNN models as a tool for the automatic classification of disaster-related images. Most studies have used only two categories (disaster-related and non-disaster related images) or are restricted to only one type of disaster (water-related, land-related, etc.) while in our studies we have used seven categories of disasters. However, the accuracy of the models may be less if the images are taken at night or when the weather conditions are very bad. © 2024 University of Bahrain. All rights reserved.;"Convolutional Neural Networks (CNN); deep learning models; InceptionV3; MobileNetV2; Natural disasters; VGG16";NULL;"Image Classification Based on Disaster type Using Deep Learning The continued rise of global temperatures is causing a major climate crisis and this is leading to devastating and deadly natural disasters. People use social media platforms to capture and share real-time incidents in the form of images, videos and text. However, sharing too much information at once makes it harder for first responders to determine where exactly individuals are in need and whether they require immediate assistance. In the past, machine learning techniques were used to automatically identify and infer disaster response from images, as manually identifying disaster types is currently challenging. Therefore, in this work, deep learning models are used to investigate how well they can classify the images according to their disaster type by learning the features extracted from the input images on their own. Seven categories of disaster were considered in this study. These were: cyclones, earthquakes, floods, droughts, landslides, wildfires and urban fires. Two existing datasets namely the Comprehensive Disaster Dataset (CDD) and the Natural Disaster Dataset (NDD) were customised into a single dataset which we named as the Customised Disaster Dataset (CDD). The Customised Disaster Dataset comprises of a total of ten classes, three of which are images which are not related to disaster. These three classes are: regular images of buildings and streets, wild forest and sea. Three pre-trained deep learning models such MobileNetV2, VGG16 and InceptionV3 were used to train the datasets to allow for further comparison with existing studies. Along with that, a customised neural network model was created and trained on the datasets. Different scenarios were devised to assess the top performing models. The InceptionV3 had the best classification accuracy of 96.86% when the trainable layers were set to false. We also obtained an accuracy of 96.86% with the MobileNetV2 model but this time the trainable layers were set to true. In this study, we have demonstrated the effectiveness of CNN models as a tool for the automatic classification of disaster-related images. Most studies have used only two categories (disaster-related and non-disaster related images) or are restricted to only one type of disaster (water-related, land-related, etc.) while in our studies we have used seven categories of disasters. However, the accuracy of the models may be less if the images are taken at night or when the weather conditions are very bad. © 2024 University of Bahrain. All rights reserved. Convolutional Neural Networks (CNN); deep learning models; InceptionV3; MobileNetV2; Natural disasters; VGG16 NULL";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;-1;NULL;3;Response
429;Hybrid CNN-GRU Approach for Flood Prediction in Rushikulya River Basin, India;One of nature’s most destructive hazards, floods cause both human fatalities and property destruction. Numerous cities are influenced by the monsoon, therefore they frequently experience calamity. Early warning of a flood incident could help the public and the authorities plan both short-term and long-term preventive actions, prepare for evacuation and rescue efforts, and provide relief for flood victims. Without taking into account physical processes, data-driven models provide effective alternatives, but the nonstationarity that exists in observations restricts the uses of these models. As a result, a hybrid DL model for forecasting flood discharge based on the convolutional neural network–gated recurrent unit (CNN-GRU) is proposed in this paper. The CNN-GRU model performed better than CNN model, according to the results monthly flood discharge prediction when applied to the Rushikulya River of Odisha, India, with an RMSE of 5.2214 m3/s, R2 of 0.9651, and an NSE of 0.9617 m3/s for all the flood episodes throughout the testing period. Additionally, we draw the conclusion that there is a need to not only enhance technical aspects of flood forecasting but also to close the gap between hydro-meteorological model development, scientific research, and probabilistic ensemble forecasts used in real-world flood management, particularly through effective communication. © The Author(s).;"CNN; CNN-GRU; Flood; Rushikulya River";"Flood damage; Information management; Recurrent neural networks; Weather forecasting; Convolutional neural network; Convolutional neural network–gated recurrent unit; Early warning; Flood discharge; Flood prediction; Physical process; Preventive action; Property; River basins; Rushikulya river; Rivers";"Hybrid CNN-GRU Approach for Flood Prediction in Rushikulya River Basin, India One of nature’s most destructive hazards, floods cause both human fatalities and property destruction. Numerous cities are influenced by the monsoon, therefore they frequently experience calamity. Early warning of a flood incident could help the public and the authorities plan both short-term and long-term preventive actions, prepare for evacuation and rescue efforts, and provide relief for flood victims. Without taking into account physical processes, data-driven models provide effective alternatives, but the nonstationarity that exists in observations restricts the uses of these models. As a result, a hybrid DL model for forecasting flood discharge based on the convolutional neural network–gated recurrent unit (CNN-GRU) is proposed in this paper. The CNN-GRU model performed better than CNN model, according to the results monthly flood discharge prediction when applied to the Rushikulya River of Odisha, India, with an RMSE of 5.2214 m3/s, R2 of 0.9651, and an NSE of 0.9617 m3/s for all the flood episodes throughout the testing period. Additionally, we draw the conclusion that there is a need to not only enhance technical aspects of flood forecasting but also to close the gap between hydro-meteorological model development, scientific research, and probabilistic ensemble forecasts used in real-world flood management, particularly through effective communication. © The Author(s). CNN; CNN-GRU; Flood; Rushikulya River Flood damage; Information management; Recurrent neural networks; Weather forecasting; Convolutional neural network; Convolutional neural network–gated recurrent unit; Early warning; Flood discharge; Flood prediction; Physical process; Preventive action; Property; River basins; Rushikulya river; Rivers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
430;Improving VQA Counting Accuracy for Post-Flood Damage Assessment;Flood disasters cause significant damage and loss of life globally, demanding efficient disaster response mechanisms. Visual Question Answering (VQA) systems, which integrate visual and textual data to answer user queries, offer a promising solution for flood disaster management. Previous deep learning methodologies showcased limitations in answering counting related questions. To address this, we propose DiViNeCaps (DistilBERT Vision Network with CapsuleNet classifier), a novel VQA network leveraging DistilBERT as the language model and Vision Mamba for image embedding. Our approach features an innovative attention-based fusion module, combining self-attention, gated attention, and cross-attention, followed by a Capsule network classifier. Our network improves counting accuracy and overall performance compared to state-of-the-art methods. Experimental results demonstrate that our architecture improves accuracy by 2.95% for Simple Counting and 3.38% for Complex Counting questions on the FloodNet dataset.  © 2024 IEEE.;"Damage Assessment; Deep Learning; Flood Response; Image Embedding; Language Model; Multimodal Fusion; Visual Question Answering";"Image enhancement; Image fusion; Metadata; Modeling languages; Network embeddings; Visual languages; Damage assessments; Deep learning; Flood disaster; Flood response; Image embedding; Language model; Loss of life; Multi-modal fusion; Question Answering; Visual question answering; Deep learning";"Improving VQA Counting Accuracy for Post-Flood Damage Assessment Flood disasters cause significant damage and loss of life globally, demanding efficient disaster response mechanisms. Visual Question Answering (VQA) systems, which integrate visual and textual data to answer user queries, offer a promising solution for flood disaster management. Previous deep learning methodologies showcased limitations in answering counting related questions. To address this, we propose DiViNeCaps (DistilBERT Vision Network with CapsuleNet classifier), a novel VQA network leveraging DistilBERT as the language model and Vision Mamba for image embedding. Our approach features an innovative attention-based fusion module, combining self-attention, gated attention, and cross-attention, followed by a Capsule network classifier. Our network improves counting accuracy and overall performance compared to state-of-the-art methods. Experimental results demonstrate that our architecture improves accuracy by 2.95% for Simple Counting and 3.38% for Complex Counting questions on the FloodNet dataset.  © 2024 IEEE. Damage Assessment; Deep Learning; Flood Response; Image Embedding; Language Model; Multimodal Fusion; Visual Question Answering Image enhancement; Image fusion; Metadata; Modeling languages; Network embeddings; Visual languages; Damage assessments; Deep learning; Flood disaster; Flood response; Image embedding; Language model; Loss of life; Multi-modal fusion; Question Answering; Visual question answering; Deep learning";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;4;Recovery
431;Pleiades Neo satellites: new assets for emergency response and disasters management;"The use of Earth Observation satellites in support of emergency response is not new: operational since October 2020, the International Charter ""Space and Major Disasters""was the first international initiative aimed at establishing a unified system for triggering the acquisition of space data when a major disaster strikes. In Europe, the Copernicus Emergency Management Service, whose precursor service was launched in 2009, is operational since April 2012. The very-high-resolution satellites are the most interesting assets, in particular after earthquakes, hurricanes or other disasters affecting urban areas: beyond the first estimates of damages, they allow to provide detailed delineation and grading maps supporting the first responders and other users involved in crisis management. The first part of the paper is a reminder of the performances requirements and challenges of services supporting emergency response activities, from an end-to-end perspective. Launched in 2021, the two Pléiades Neo satellites are among the most recent assets worldwide providing both very high resolution and very fast revisit. The second section explains how this operational performance became a reality with significant efforts in technology development in an end-to-end perspective: imaging instrument (providing the 30-cm native imagery), satellite platform aimed at improving revisit frequency and daily coverage, network of ground stations and the operational command, control and tasking procedures, designed to optimise the end-to-end acquisition time (from tasking decision to reception of the first image of the area of interest) and, last but not least, performance of the ground processing and image distribution chain with very innovative features. In particular, it highlights the benefits of a disruptive processing solution, based on machine-learning algorithms, aimed at delivering high definition (HD15) products from native imagery at 30-cm resolution or able to automatically extract 3D building from stereo pairs. The third part of the paper shows Pléiades Neo satellites in action with recent representative examples of operational use, e. g. during the wild fires season (summer 2023), after the Earthquake in Morocco (September 2023), the deadly floods in Mexico (October 2023) and other more recent disasters. These examples illustrate the addedvalue of Pléiades Neo and the contribution if the innovative functions. The conclusion gives an outlook of future work planned by Airbus Defence to improve further performance.  Copyright © 2024 by Airbus Defence and Space.";"Copernicus; disasters management; Earth Observation; emergency response; Pléiades Neo; rapid mapping; reactive imagery";"Disaster prevention; Disasters; Geodetic satellites; Image acquisition; Image enhancement; Photomapping; Risk management; Satellite ground stations; Satellite imagery; Satellite observatories; Stereo image processing; Tropics; Copernicu; Disaster management; Earth observations; Emergency response; End to end; Performance; Pléiade neo; Rapid mapping; Reactive imagery; Very high resolution; Earthquakes";"Pleiades Neo satellites: new assets for emergency response and disasters management The use of Earth Observation satellites in support of emergency response is not new: operational since October 2020, the International Charter ""Space and Major Disasters""was the first international initiative aimed at establishing a unified system for triggering the acquisition of space data when a major disaster strikes. In Europe, the Copernicus Emergency Management Service, whose precursor service was launched in 2009, is operational since April 2012. The very-high-resolution satellites are the most interesting assets, in particular after earthquakes, hurricanes or other disasters affecting urban areas: beyond the first estimates of damages, they allow to provide detailed delineation and grading maps supporting the first responders and other users involved in crisis management. The first part of the paper is a reminder of the performances requirements and challenges of services supporting emergency response activities, from an end-to-end perspective. Launched in 2021, the two Pléiades Neo satellites are among the most recent assets worldwide providing both very high resolution and very fast revisit. The second section explains how this operational performance became a reality with significant efforts in technology development in an end-to-end perspective: imaging instrument (providing the 30-cm native imagery), satellite platform aimed at improving revisit frequency and daily coverage, network of ground stations and the operational command, control and tasking procedures, designed to optimise the end-to-end acquisition time (from tasking decision to reception of the first image of the area of interest) and, last but not least, performance of the ground processing and image distribution chain with very innovative features. In particular, it highlights the benefits of a disruptive processing solution, based on machine-learning algorithms, aimed at delivering high definition (HD15) products from native imagery at 30-cm resolution or able to automatically extract 3D building from stereo pairs. The third part of the paper shows Pléiades Neo satellites in action with recent representative examples of operational use, e. g. during the wild fires season (summer 2023), after the Earthquake in Morocco (September 2023), the deadly floods in Mexico (October 2023) and other more recent disasters. These examples illustrate the addedvalue of Pléiades Neo and the contribution if the innovative functions. The conclusion gives an outlook of future work planned by Airbus Defence to improve further performance.  Copyright © 2024 by Airbus Defence and Space. Copernicus; disasters management; Earth Observation; emergency response; Pléiades Neo; rapid mapping; reactive imagery Disaster prevention; Disasters; Geodetic satellites; Image acquisition; Image enhancement; Photomapping; Risk management; Satellite ground stations; Satellite imagery; Satellite observatories; Stereo image processing; Tropics; Copernicu; Disaster management; Earth observations; Emergency response; End to end; Performance; Pléiade neo; Rapid mapping; Reactive imagery; Very high resolution; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
432;Automating Hazard-Specific Ontology Construction: Methodological Advancements through Ontology Learning Techniques from Disaster-Related Knowledge Bases;The Ontology Learning System (OLS) aims to automate the construction of hazard-specific ontologies, with a particular focus on floods. Using ontology learning techniques and Machine Learning algorithms, OLS undergoes a series of preprocessing steps, including text cleaning, word tokenization, and the removal of stop words and irrelevant characters. Extracting frequent and relevant terms while also classifying disaster-related keywords is a critical component of automating the construction of a flood ontology. Using Bidirectional Encoder Representations from Transformers (BERT), the model achieved 89% accuracy, precision, and recall in classifying keywords as disaster-related or not. Further, the system employs Machine Learning algorithms, namely Naive Bayes, Logistic Regression, and Linear Support Vector Classifier, for multiclass classification of disaster types. The results of the multiclass classification show that the Linear Support Vector Classifier attained the highest accuracy. The identified ontology keywords can be used to construct a flood-specific ontology, playing a crucial role in organizing flood-related information. Integrating these ontologies into broader systems can significantly enhance disaster preparedness and improve overall response capabilities.  © 2024 IEEE.;"Disaster Preparedness; Flood Ontology; Hazard-Specific Ontology; Ontology Learning";"Adversarial machine learning; Disaster prevention; Knowledge based systems; Logistic regression; Ontology; Support vector regression; Disaster preparedness; Flood ontology; Hazard-specific ontology; Learning techniques; Machine learning algorithms; Multi-class classification; Ontology learning; Ontology's; Support vector classifiers; Disasters";"Automating Hazard-Specific Ontology Construction: Methodological Advancements through Ontology Learning Techniques from Disaster-Related Knowledge Bases The Ontology Learning System (OLS) aims to automate the construction of hazard-specific ontologies, with a particular focus on floods. Using ontology learning techniques and Machine Learning algorithms, OLS undergoes a series of preprocessing steps, including text cleaning, word tokenization, and the removal of stop words and irrelevant characters. Extracting frequent and relevant terms while also classifying disaster-related keywords is a critical component of automating the construction of a flood ontology. Using Bidirectional Encoder Representations from Transformers (BERT), the model achieved 89% accuracy, precision, and recall in classifying keywords as disaster-related or not. Further, the system employs Machine Learning algorithms, namely Naive Bayes, Logistic Regression, and Linear Support Vector Classifier, for multiclass classification of disaster types. The results of the multiclass classification show that the Linear Support Vector Classifier attained the highest accuracy. The identified ontology keywords can be used to construct a flood-specific ontology, playing a crucial role in organizing flood-related information. Integrating these ontologies into broader systems can significantly enhance disaster preparedness and improve overall response capabilities.  © 2024 IEEE. Disaster Preparedness; Flood Ontology; Hazard-Specific Ontology; Ontology Learning Adversarial machine learning; Disaster prevention; Knowledge based systems; Logistic regression; Ontology; Support vector regression; Disaster preparedness; Flood ontology; Hazard-specific ontology; Learning techniques; Machine learning algorithms; Multi-class classification; Ontology learning; Ontology's; Support vector classifiers; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
433;Edge-Centric Intelligent Early Warning System for Residual Soil Stability Prediction in Slope;Different geospatial and geotechnical parameters change over time and can affect the residual soil stability on a slope. Thus, it is essential to analyze the stability of slopes continuously to identify the potential landslide sections. The stability of slopes is defined by the factor of safety (FOS). To track the immediate changes in soil stability, it is essential to monitor multiple environmental parameters using Internet of Things (IoT) devices for real-Time decision making. Further, the relation between the environmental parameters and FOS is nonlinear which makes it a multivariate and complex problem. To motivate the above-mentioned challenges, in this article, we propose a new fusion-based bag-of-neural network (FuBoNN) model for predicting FOS using a set of IoT devices in edge networks. Besides that, for increasing the prediction accuracy of FOS, multiple laboratory data related to FOS are fused over the monitoring parameters and prepare a rich data set. The newly created fused data set is fed into the population-based neural network (NN) and the best NN is selected in each iteration that transfers its knowledge to the population. The fused data set is categorized into four class labels to simulate the stability issue of residual soil and fed to the input of the proposed FuBoNN model, which provides a 0.0003% of error in predicting the multiple categories of the FOS. The proposed work is compared to the standard machine learning models that demonstrate the efficiency of the proposed model and produce 2.5% improved prediction accuracy over the existing ones.  © 2014 IEEE.;"Data fusion; edge networks; factor of safety (FOS); Internet of Things (IoT); neural networks (NNs); soil slope stability";"Data fusion; Decision making; Forecasting; Interactive computer systems; Internet of things; Safety factor; Slope protection; Slope stability; Soils; EDGE Networks; Factors of safeties; Neural-networks; Predictive models; Real - Time system; Residual soil; Soil slope stability; Soil slopes; Soil stability; Terrain factors; Real time systems";"Edge-Centric Intelligent Early Warning System for Residual Soil Stability Prediction in Slope Different geospatial and geotechnical parameters change over time and can affect the residual soil stability on a slope. Thus, it is essential to analyze the stability of slopes continuously to identify the potential landslide sections. The stability of slopes is defined by the factor of safety (FOS). To track the immediate changes in soil stability, it is essential to monitor multiple environmental parameters using Internet of Things (IoT) devices for real-Time decision making. Further, the relation between the environmental parameters and FOS is nonlinear which makes it a multivariate and complex problem. To motivate the above-mentioned challenges, in this article, we propose a new fusion-based bag-of-neural network (FuBoNN) model for predicting FOS using a set of IoT devices in edge networks. Besides that, for increasing the prediction accuracy of FOS, multiple laboratory data related to FOS are fused over the monitoring parameters and prepare a rich data set. The newly created fused data set is fed into the population-based neural network (NN) and the best NN is selected in each iteration that transfers its knowledge to the population. The fused data set is categorized into four class labels to simulate the stability issue of residual soil and fed to the input of the proposed FuBoNN model, which provides a 0.0003% of error in predicting the multiple categories of the FOS. The proposed work is compared to the standard machine learning models that demonstrate the efficiency of the proposed model and produce 2.5% improved prediction accuracy over the existing ones.  © 2014 IEEE. Data fusion; edge networks; factor of safety (FOS); Internet of Things (IoT); neural networks (NNs); soil slope stability Data fusion; Decision making; Forecasting; Interactive computer systems; Internet of things; Safety factor; Slope protection; Slope stability; Soils; EDGE Networks; Factors of safeties; Neural-networks; Predictive models; Real - Time system; Residual soil; Soil slope stability; Soil slopes; Soil stability; Terrain factors; Real time systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
434;Wavelet-Based precipitation preprocessing for improved drought Forecasting: A Machine learning approach using tunable Q-factor wavelet transform and maximal overlap discrete wavelet transform;Drought forecasting plays a crucial role in mitigating the severe agricultural and social consequences caused by droughts. The fluctuating nature of droughts makes it difficult to develop an effective drought forecasting model without preprocessing the input data. This paper proposes a novel approach that introduces the tunable Q-factor wavelet transform (TQWT) with the maximal overlap discrete wavelet transform (MODWT) based Fejér–Korovkin, Coiflet, and Daubechies filters in the decomposition of precipitation data for the extended lead time forecasting of the standardized precipitation evapotranspiration index (SPEI). The decomposed datasets have been coupled with Matern Gaussian process regression (MGPR), exponential Gaussian process regression (EGPR), linear support vector machine (LSVM), and coarse Gaussian support vector machine (CGSVM), and formed hybrid models to forecast SPEI-12 and SPEI-18 for several lead times (i.e., 6, 12, 18, and 24 months). Results of the study represent that the wavelet-based hybrid models are capable of predicting SPEI-12 and SPEI-18 effectively for different lead times with promising results. Both TQWT and MODWT coupled with MGPR yielded reasonable performances for the lead time of 6 months in all stations. However, for the higher lead times, TQWT coupled with MGPR outperformed other hybrid models. The results of the TQWT-MGPR for SPEI-12 are more effective than SPEI-18 in different lead times. The study highlights that preprocessing of precipitation data using TQWT is a promising direction for drought forecasting, and the findings obtained from drought forecasting can be utilized in the areas of water and agricultural resource management to effectively mitigate and alleviate the potential impacts of future droughts. © 2024 Elsevier Ltd;"Lead time; MODWT; Precipitation; SPEI; TQWT; Wavelet decomposition";"Agriculture; Discrete wavelet transforms; Drought; Gaussian distribution; Gaussian noise (electronic); Information management; Signal reconstruction; Support vector machines; Weather forecasting; Gaussian process regression; Hybrid model; Leadtime; Maximal overlap discrete wavelet transforms; Q-factors; Standardized precipitation evapotranspiration index; Tunable Q-factor wavelet transform; Tunables; Wavelets decomposition; Wavelets transform; Wavelet decomposition";"Wavelet-Based precipitation preprocessing for improved drought Forecasting: A Machine learning approach using tunable Q-factor wavelet transform and maximal overlap discrete wavelet transform Drought forecasting plays a crucial role in mitigating the severe agricultural and social consequences caused by droughts. The fluctuating nature of droughts makes it difficult to develop an effective drought forecasting model without preprocessing the input data. This paper proposes a novel approach that introduces the tunable Q-factor wavelet transform (TQWT) with the maximal overlap discrete wavelet transform (MODWT) based Fejér–Korovkin, Coiflet, and Daubechies filters in the decomposition of precipitation data for the extended lead time forecasting of the standardized precipitation evapotranspiration index (SPEI). The decomposed datasets have been coupled with Matern Gaussian process regression (MGPR), exponential Gaussian process regression (EGPR), linear support vector machine (LSVM), and coarse Gaussian support vector machine (CGSVM), and formed hybrid models to forecast SPEI-12 and SPEI-18 for several lead times (i.e., 6, 12, 18, and 24 months). Results of the study represent that the wavelet-based hybrid models are capable of predicting SPEI-12 and SPEI-18 effectively for different lead times with promising results. Both TQWT and MODWT coupled with MGPR yielded reasonable performances for the lead time of 6 months in all stations. However, for the higher lead times, TQWT coupled with MGPR outperformed other hybrid models. The results of the TQWT-MGPR for SPEI-12 are more effective than SPEI-18 in different lead times. The study highlights that preprocessing of precipitation data using TQWT is a promising direction for drought forecasting, and the findings obtained from drought forecasting can be utilized in the areas of water and agricultural resource management to effectively mitigate and alleviate the potential impacts of future droughts. © 2024 Elsevier Ltd Lead time; MODWT; Precipitation; SPEI; TQWT; Wavelet decomposition Agriculture; Discrete wavelet transforms; Drought; Gaussian distribution; Gaussian noise (electronic); Information management; Signal reconstruction; Support vector machines; Weather forecasting; Gaussian process regression; Hybrid model; Leadtime; Maximal overlap discrete wavelet transforms; Q-factors; Standardized precipitation evapotranspiration index; Tunable Q-factor wavelet transform; Tunables; Wavelets decomposition; Wavelets transform; Wavelet decomposition";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
435;Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring;In recent years, unmanned aerial vehicles (UAVs) have played an increasingly crucial role in supporting disaster emergency response efforts by analyzing aerial images. While current deep-learning models focus on improving accuracy, they often overlook the limited computing resources of UAVs. This study recognizes the imperative for real-time data processing in disaster response scenarios and introduces a lightweight and efficient approach for aerial video understanding. Our methodology identifies redundant portions within the video through policy networks and eliminates this excess information using frame compression techniques. Additionally, we introduced the concept of a station point, which leverages future information in the sequential policy network, thereby enhancing accuracy. To validate our method, we employed the wildfire FLAME dataset. Compared to the baseline, our approach reduces computation costs by more than 10 times while improving accuracy by 3%. Moreover, our method can intelligently select salient frames from the video, refining the dataset. This feature enables sophisticated models to be effectively trained on a smaller dataset, significantly reducing the time spent during the training process. © 2024 IEEE.;NULL;"Aerial photography; Deforestation; Image compression; Unmanned aerial vehicles (UAV); Video analysis; 'current; Aerial images; Aerial vehicle; Aerial video; Efficient monitoring; Emergency response; Forest wildfires; Learning models; Policy networks; Video dataset; Premixed flames";"Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring In recent years, unmanned aerial vehicles (UAVs) have played an increasingly crucial role in supporting disaster emergency response efforts by analyzing aerial images. While current deep-learning models focus on improving accuracy, they often overlook the limited computing resources of UAVs. This study recognizes the imperative for real-time data processing in disaster response scenarios and introduces a lightweight and efficient approach for aerial video understanding. Our methodology identifies redundant portions within the video through policy networks and eliminates this excess information using frame compression techniques. Additionally, we introduced the concept of a station point, which leverages future information in the sequential policy network, thereby enhancing accuracy. To validate our method, we employed the wildfire FLAME dataset. Compared to the baseline, our approach reduces computation costs by more than 10 times while improving accuracy by 3%. Moreover, our method can intelligently select salient frames from the video, refining the dataset. This feature enables sophisticated models to be effectively trained on a smaller dataset, significantly reducing the time spent during the training process. © 2024 IEEE. NULL Aerial photography; Deforestation; Image compression; Unmanned aerial vehicles (UAV); Video analysis; 'current; Aerial images; Aerial vehicle; Aerial video; Efficient monitoring; Emergency response; Forest wildfires; Learning models; Policy networks; Video dataset; Premixed flames";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.4;Climatological;3;Response
436;Landslide Detection and Mapping using Remote Sensing U-Net Model In Leh-Ladakh Region;Landslides pose significant risks to communities and infrastructure, making detection, monitoring, and prediction crucial for effective risk management. Remote sensing techniques (RSTs) play a vital role in this regard by offering tools to observe, monitor, and predict landslides. Observation of Earth from space provides a broad perspective, allowing for the detection of changes in terrain morphology that may indicate landslide activity. Satellite imagery and data can be analyzed to identify areas prone to landslides and monitor changes over time. Laser scanning, also known as LiDAR (Light Detection and Ranging), offers high-resolution elevation data that can be used to create detailed terrain models. These models help in identifying slope instability and potential landslide risk areas. Ground-based interferometry involves the use of sensors deployed on the ground to measure ground deformation with high precision. This technique can detect subtle movements in the Earth's surface, providing early warning signs of potential landslides. By integrating data from these remote sensing techniques, scientists and authorities can develop effective strategies for landslide risk assessment, early warning systems, and disaster mitigation measures. This holistic approach is essential for safeguarding lives and infrastructure in landslide-prone areas. This paper's output provides an overview of landslip debris utilizing satellite imagery techniques and deep learning techniques based on the UNET model. © 2024 IEEE.;"hazard detection; landslide monitoring; remote sensing; UNET model";"Hydrogeology; Landslides; Risk assessment; Risk management; Tropics; Detection of changes; Hazard detection; Landslide detection; Landslide mapping; Landslide monitoring; Net model; Remote sensing techniques; Remote-sensing; Risks management; UNET model; Satellite imagery";"Landslide Detection and Mapping using Remote Sensing U-Net Model In Leh-Ladakh Region Landslides pose significant risks to communities and infrastructure, making detection, monitoring, and prediction crucial for effective risk management. Remote sensing techniques (RSTs) play a vital role in this regard by offering tools to observe, monitor, and predict landslides. Observation of Earth from space provides a broad perspective, allowing for the detection of changes in terrain morphology that may indicate landslide activity. Satellite imagery and data can be analyzed to identify areas prone to landslides and monitor changes over time. Laser scanning, also known as LiDAR (Light Detection and Ranging), offers high-resolution elevation data that can be used to create detailed terrain models. These models help in identifying slope instability and potential landslide risk areas. Ground-based interferometry involves the use of sensors deployed on the ground to measure ground deformation with high precision. This technique can detect subtle movements in the Earth's surface, providing early warning signs of potential landslides. By integrating data from these remote sensing techniques, scientists and authorities can develop effective strategies for landslide risk assessment, early warning systems, and disaster mitigation measures. This holistic approach is essential for safeguarding lives and infrastructure in landslide-prone areas. This paper's output provides an overview of landslip debris utilizing satellite imagery techniques and deep learning techniques based on the UNET model. © 2024 IEEE. hazard detection; landslide monitoring; remote sensing; UNET model Hydrogeology; Landslides; Risk assessment; Risk management; Tropics; Detection of changes; Hazard detection; Landslide detection; Landslide mapping; Landslide monitoring; Net model; Remote sensing techniques; Remote-sensing; Risks management; UNET model; Satellite imagery";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
437;A Multi-Parameter Landslide Early Warning Method;Landslides present significant risks to human safety and infrastructure, necessitating advanced early warning systems. This paper proposes a multi-parameter landslide early warning method that integrates multiple parameters, including coherence, velocity, and acceleration, with a dynamic time decay model. The proposed method emphasizes the importance of considering the time-dependent nature of landslide precursors and enhances the accuracy of warnings by dynamically adjusting the weight of these parameters over time. The feasibility of this landslide warning method has been verified in actual landslide cases.  © 2024 IEEE.;"automated anomaly detection; dynamic time decay; landslide early warning method; landslide monitoring";"Alarm systems; Anomaly detection; Anomaly detection; Automated anomaly detection; Dynamic time; Dynamic time decay; Early-warning method; Human infrastructure; Landslide early warning method; Landslide monitoring; Multiparameters; Time decay; Landslides";"A Multi-Parameter Landslide Early Warning Method Landslides present significant risks to human safety and infrastructure, necessitating advanced early warning systems. This paper proposes a multi-parameter landslide early warning method that integrates multiple parameters, including coherence, velocity, and acceleration, with a dynamic time decay model. The proposed method emphasizes the importance of considering the time-dependent nature of landslide precursors and enhances the accuracy of warnings by dynamically adjusting the weight of these parameters over time. The feasibility of this landslide warning method has been verified in actual landslide cases.  © 2024 IEEE. automated anomaly detection; dynamic time decay; landslide early warning method; landslide monitoring Alarm systems; Anomaly detection; Anomaly detection; Automated anomaly detection; Dynamic time; Dynamic time decay; Early-warning method; Human infrastructure; Landslide early warning method; Landslide monitoring; Multiparameters; Time decay; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
438;Earthquake Resilient Management in Northeast Algeria: A Multivariate Time Series Forecasting based on LSTM-Attention Neural Network;Forecasting methods are critical to earthquake management by providing early warning systems and predictive insights into potential hazards. This work proposes a prediction framework for forecasting earthquake magnitude. The methodology involves collecting historical data from the seismic catalog, identifying geophysical characteristics of northeast Algeria, specifically the Setif (Algeria) region, and proposing a multivariate time series forecasting model based on LSTM with an attention mechanism. The attention neural network learns deeply the most relevant weights representing the subduction zone characteristics that influence earthquake prediction. We validate the proposed model based on real data-sets, and the results show an accurate magnitude forecasting for the northeast Algeria zone.  © 2024 IEEE.;"Attention neural network; Earth-quake; Multivariate time series forecasting; Time series forecasting based on LSTM";"Earthquake effects; Neural networks; Time series; Algeria; Attention neural network; Earth-quake; Model-based OPC; Multivariate time series; Multivariate time series forecasting; Neural-networks; Time series forecasting; Time series forecasting based on LSTM; Prediction models";"Earthquake Resilient Management in Northeast Algeria: A Multivariate Time Series Forecasting based on LSTM-Attention Neural Network Forecasting methods are critical to earthquake management by providing early warning systems and predictive insights into potential hazards. This work proposes a prediction framework for forecasting earthquake magnitude. The methodology involves collecting historical data from the seismic catalog, identifying geophysical characteristics of northeast Algeria, specifically the Setif (Algeria) region, and proposing a multivariate time series forecasting model based on LSTM with an attention mechanism. The attention neural network learns deeply the most relevant weights representing the subduction zone characteristics that influence earthquake prediction. We validate the proposed model based on real data-sets, and the results show an accurate magnitude forecasting for the northeast Algeria zone.  © 2024 IEEE. Attention neural network; Earth-quake; Multivariate time series forecasting; Time series forecasting based on LSTM Earthquake effects; Neural networks; Time series; Algeria; Attention neural network; Earth-quake; Model-based OPC; Multivariate time series; Multivariate time series forecasting; Neural-networks; Time series forecasting; Time series forecasting based on LSTM; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
439;Flight Mission Planning Methodology in UAV Surveying;The risk associated with hydrological and geological threats, such as floods and landslides, is significant for the safety of people. The improper disposal of solid waste in the streams of the city of Cartagena and internal channels can obstruct water flow. Therefore, it is crucial to implement innovative solutions in risk management, based on new ways of characterizing these channels, such as the use of remote sensing technologies, like drone photogrammetry. The implementation of drone flight protocols is a crucial stage in UAV surveying applications for achieving accurate measurements, building reliable information, and avoiding cost overruns and delays during the data acquisition process. In this article, we execute a drone flight mission protocol that allows mapping a study area and capturing a set of images that serve as input datasets for 3D reconstruction models and Deep Learning (DL) models. The proposed approach focuses on mapping ground control points (GCPs) or targets through tests with a DJI Drone within Universidad Tecnológica de Bolívar campus in Cartagena, Colombia. A flight protocol, starting with the study and identification of the area was followed. The proposed protocol showed good detection capacity for smaller objects, like the GCPs and obstructive elements that could be found in a water channel, owing to Ground Sample Distances (GSD) in the order of milimeters. This approach allows for more precise planning and a more efficient response in planning methodologies to address the risk associated with hydrological and geological threats in the city of Cartagena. © 2024 IEEE.;"drone photogrammetry; Flight protocols; GCP; ground control point; ground sample distance; GSD; remote sensing; risk management; UAV; unmanned aerial vehicle";"Aircraft detection; Drones; Municipal solid waste; Radioactive waste disposal; Tropics; Aerial vehicle; Drone photogrammetry; Flight protocol; Ground control points; Ground sample distances; Remote-sensing; Risks management; Unmanned aerial vehicle; Risk management";"Flight Mission Planning Methodology in UAV Surveying The risk associated with hydrological and geological threats, such as floods and landslides, is significant for the safety of people. The improper disposal of solid waste in the streams of the city of Cartagena and internal channels can obstruct water flow. Therefore, it is crucial to implement innovative solutions in risk management, based on new ways of characterizing these channels, such as the use of remote sensing technologies, like drone photogrammetry. The implementation of drone flight protocols is a crucial stage in UAV surveying applications for achieving accurate measurements, building reliable information, and avoiding cost overruns and delays during the data acquisition process. In this article, we execute a drone flight mission protocol that allows mapping a study area and capturing a set of images that serve as input datasets for 3D reconstruction models and Deep Learning (DL) models. The proposed approach focuses on mapping ground control points (GCPs) or targets through tests with a DJI Drone within Universidad Tecnológica de Bolívar campus in Cartagena, Colombia. A flight protocol, starting with the study and identification of the area was followed. The proposed protocol showed good detection capacity for smaller objects, like the GCPs and obstructive elements that could be found in a water channel, owing to Ground Sample Distances (GSD) in the order of milimeters. This approach allows for more precise planning and a more efficient response in planning methodologies to address the risk associated with hydrological and geological threats in the city of Cartagena. © 2024 IEEE. drone photogrammetry; Flight protocols; GCP; ground control point; ground sample distance; GSD; remote sensing; risk management; UAV; unmanned aerial vehicle Aircraft detection; Drones; Municipal solid waste; Radioactive waste disposal; Tropics; Aerial vehicle; Drone photogrammetry; Flight protocol; Ground control points; Ground sample distances; Remote-sensing; Risks management; Unmanned aerial vehicle; Risk management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
440;REAL-TIME FLOOD RUNOFF PREDICTION FOR A SMALL-TO-MEDIUM-SCALE URBAN RIVER WATERSHED USING A BIDIRECTIONAL LONG-SHORT-TERM MEMORY MODEL (BiLSTM);Neural network-based rainfall-runoff predictive modeling (RPM) is an emerging field for implementing early warning systems to mitigate urban flooding. This research executed real-time runoff forecasting with Bidirectional Long Short-Term Memory RPM (BiLSTM-RPM) for a small-to-medium scale river basin in Tokyo. The focused basin was the Zenpukuji urban watershed, which is about 22.5 km2 in area and has minute-to-minute data. The modeling proceeded with the preceding rainfall gathered from seven rainfall locations and a preceding discharge at Aioi bridge to forecast upcoming runoffs of the same location. One hundred rainfall-runoff events were organized to train BiLSTM-RPMs designed with six target lengths from 10 to 60 minutes with an increment of 10 minutes, and the models were tested with 17 untrained test events. It was found that an equal duration of input and target lengths provided desirable accuracy. The temporal exactness and peak alignment accuracy of BiLSTM-RPMs were derived with satisfactory results for all target lengths, where the models excellently performed real-time forecasting even for an uncommon event of the test set. Upcoming hydrographs could be plotted with excellent temporal exactness and peak alignment for extensive forecasting spans by integrating each TL's last 10-minute predicted runoffs. © 2024 Japan Society of Civil Engineers. All rights reserved.;"hydrographs; neural networks; peak runoff; sliding window generators; temporal exactness";NULL;"REAL-TIME FLOOD RUNOFF PREDICTION FOR A SMALL-TO-MEDIUM-SCALE URBAN RIVER WATERSHED USING A BIDIRECTIONAL LONG-SHORT-TERM MEMORY MODEL (BiLSTM) Neural network-based rainfall-runoff predictive modeling (RPM) is an emerging field for implementing early warning systems to mitigate urban flooding. This research executed real-time runoff forecasting with Bidirectional Long Short-Term Memory RPM (BiLSTM-RPM) for a small-to-medium scale river basin in Tokyo. The focused basin was the Zenpukuji urban watershed, which is about 22.5 km2 in area and has minute-to-minute data. The modeling proceeded with the preceding rainfall gathered from seven rainfall locations and a preceding discharge at Aioi bridge to forecast upcoming runoffs of the same location. One hundred rainfall-runoff events were organized to train BiLSTM-RPMs designed with six target lengths from 10 to 60 minutes with an increment of 10 minutes, and the models were tested with 17 untrained test events. It was found that an equal duration of input and target lengths provided desirable accuracy. The temporal exactness and peak alignment accuracy of BiLSTM-RPMs were derived with satisfactory results for all target lengths, where the models excellently performed real-time forecasting even for an uncommon event of the test set. Upcoming hydrographs could be plotted with excellent temporal exactness and peak alignment for extensive forecasting spans by integrating each TL's last 10-minute predicted runoffs. © 2024 Japan Society of Civil Engineers. All rights reserved. hydrographs; neural networks; peak runoff; sliding window generators; temporal exactness NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
441;Research on Pump Condition Prediction Based on Ensemble Learning Strategies;Failures in water pumps can lead to shutdowns, production disruptions, and resource wastage, particularly in areas such as industrial production, water supply, and flood control, where the impact is significant and the losses immeasurable. Therefore, monitoring and early warning to enhance the operational efficiency and stability of water pumps is crucial. Based on research into deep learning algorithms, this paper proposes and implements a pump condition prediction model using an ensemble learning strategy. By constructing three different models: Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN), the states of the water pump within different time windows are predicted respectively. Among them, the LSTM model performs better, achieving an accuracy rate of 98.3%. Finally, an ensemble learning strategy based on voting is employed to integrate the prediction results from the LSTM, CNN, and DNN models, achieving an accuracy rate of 98.5%. This approach improves the overall prediction performance.  © 2024 IEEE.;"CNN; DNN; Ensemble Learning; LSTM; Water Pump Failure Prediction";"Contrastive Learning; Deep neural networks; Long short-term memory; Plant shutdowns; Prediction models; Water well pumps; Condition prediction; Convolutional neural network; Ensemble learning; Failures prediction; Learning strategy; Neural-networks; Pump failure; Short term memory; Water pump; Water pump failure prediction; Convolutional neural networks";"Research on Pump Condition Prediction Based on Ensemble Learning Strategies Failures in water pumps can lead to shutdowns, production disruptions, and resource wastage, particularly in areas such as industrial production, water supply, and flood control, where the impact is significant and the losses immeasurable. Therefore, monitoring and early warning to enhance the operational efficiency and stability of water pumps is crucial. Based on research into deep learning algorithms, this paper proposes and implements a pump condition prediction model using an ensemble learning strategy. By constructing three different models: Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN), the states of the water pump within different time windows are predicted respectively. Among them, the LSTM model performs better, achieving an accuracy rate of 98.3%. Finally, an ensemble learning strategy based on voting is employed to integrate the prediction results from the LSTM, CNN, and DNN models, achieving an accuracy rate of 98.5%. This approach improves the overall prediction performance.  © 2024 IEEE. CNN; DNN; Ensemble Learning; LSTM; Water Pump Failure Prediction Contrastive Learning; Deep neural networks; Long short-term memory; Plant shutdowns; Prediction models; Water well pumps; Condition prediction; Convolutional neural network; Ensemble learning; Failures prediction; Learning strategy; Neural-networks; Pump failure; Short term memory; Water pump; Water pump failure prediction; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
442;Early Detection of Forest Fire Based on Lightning Activity and Climatic Factors Using Deep Learning;Forest fires significantly threaten ecosystems, contributing to climate change and greenhouse gas emissions. Effective prediction and detection of forest fires can greatly mitigate their damage. Lightning and climate conditions are the primary factors in initiating fires. The first step in predicting forest fires involves collecting historical data on climate, lightning, and fire-prone regions. Remote sensing technology is used to gather satellite images at regular intervals. An automatic statistical learning technique is employed to develop HyperFusionNet, a deep learning classifier that predicts the severity of wildfires caused by lightning events based on lightning data. A regression model called Climate Predictor is designed to analyze climate data, incorporating factors such as temperature, humidity, and wind speed and predicting a region's climatic conditions, which can aid in predicting fire occurrence. SymbioticNet is a machine learning algorithm model for predicting the Flash Extent Density (FED) of lightning, which can help us to determine regions prone to lightning. Combining these models enables more accurate predictions of forest fires. Integrating HyperFusionNet for wildfire severity prediction, Climate Predictor for climate data, and SymbioticNet for flash extent density of lightning enhance early warning systems and control measures. This integrated method leverages historical data and advanced remote sensing to effectively predict and mitigate forest fires, reducing their environmental impact. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved.;"Deep Learning; Fire Detection; Forest Fires; Lightning-ignited Fire; SymbioticNet";"Deep learning; Fire hazards; Greenhouse gas emissions; Weather forecasting; Activity factor; Climate data; Climatic factors; Deep learning; Fire detection; Forest fires; Historical data; Lightning activity; Lightning-ignited fire; Symbioticnet; Premixed flames";"Early Detection of Forest Fire Based on Lightning Activity and Climatic Factors Using Deep Learning Forest fires significantly threaten ecosystems, contributing to climate change and greenhouse gas emissions. Effective prediction and detection of forest fires can greatly mitigate their damage. Lightning and climate conditions are the primary factors in initiating fires. The first step in predicting forest fires involves collecting historical data on climate, lightning, and fire-prone regions. Remote sensing technology is used to gather satellite images at regular intervals. An automatic statistical learning technique is employed to develop HyperFusionNet, a deep learning classifier that predicts the severity of wildfires caused by lightning events based on lightning data. A regression model called Climate Predictor is designed to analyze climate data, incorporating factors such as temperature, humidity, and wind speed and predicting a region's climatic conditions, which can aid in predicting fire occurrence. SymbioticNet is a machine learning algorithm model for predicting the Flash Extent Density (FED) of lightning, which can help us to determine regions prone to lightning. Combining these models enables more accurate predictions of forest fires. Integrating HyperFusionNet for wildfire severity prediction, Climate Predictor for climate data, and SymbioticNet for flash extent density of lightning enhance early warning systems and control measures. This integrated method leverages historical data and advanced remote sensing to effectively predict and mitigate forest fires, reducing their environmental impact. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved. Deep Learning; Fire Detection; Forest Fires; Lightning-ignited Fire; SymbioticNet Deep learning; Fire hazards; Greenhouse gas emissions; Weather forecasting; Activity factor; Climate data; Climatic factors; Deep learning; Fire detection; Forest fires; Historical data; Lightning activity; Lightning-ignited fire; Symbioticnet; Premixed flames";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
443;Flood Assessment Method for Heritage Conservation at the Site Scale: A Case Study of PuZhou Ancient City Site, China;"The concept of ""preventive conservation"" has become the focus of heritage conservation worldwide, risk assessment is one of the important elements of preventive conservation. Current flood risk assessments tend to focus on larger scales such as cities or river basins and fail to adequately refine the risk to individual buildings. This paper presents a flood risk assessment methodology tailored to the scale of cultural heritage units, to enhance the accuracy of identifying site-specific risks. The method couples hazard with vulnerability to assess flood risk. Regarding hazard, to solve the issue of the minimal contribution of slight geographical variations in water and environmental indicators at the scale of cultural heritage units to the classification of storm flood danger, we employ the Storm Water Management Model (SWMM). This model is utilized to establish rainfall scenarios and conduct numerical simulations, translating these into intuitive risk parameters like depth and extent of inundation. Additionally, a vulnerability assessment procedure for cultural heritage was designed. This procedure analyzes structural stability using finite element simulations, quantifies the sensitivity of cultural objects to floods, evaluates exposure based on potential value loss, and assesses disaster prevention and mitigation capacity considering protective measures and restoration possibilities. Flood risk can be obtained by weighing the results of hazard and vulnerability analyses. The case study of the ancient city site of Puzhou verifies the feasibility of the risk assessment methodology adopted and reveals the risk level of different areas. The results of the assessment show that the Xicheng District, except for the southern city wall, has a low flood risk and good resilience to flooding. The Drum Tower area has a damaged foundation but is on higher ground and has a lower flood risk. The Eastern District has important cultural relics, the low-lying topography makes it highly vulnerable to the double threat of flooding and human damage. Considering natural hazards and cultural heritage specifics, this integrated approach offers a targeted framework for assessing flood risks and developing prevention and mitigation strategies for heritage sites. Apply it to additional sites to further validate its effectiveness in the future. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved.";"ancient sites; cultural heritage; flood disaster risk assessment; hazard; vulnerability";"Banks (bodies of water); Disasters; Energy efficiency; Flood damage; Historic preservation; Loss prevention; Risk analysis; Risk management; Ancient site; Case-studies; Cultural heritages; Disaster risk assessments; Flood disaster; Flood disaster risk assessment; Flood risks; Heritage conservation; Preventive conservation; Vulnerability; Risk assessment";"Flood Assessment Method for Heritage Conservation at the Site Scale: A Case Study of PuZhou Ancient City Site, China The concept of ""preventive conservation"" has become the focus of heritage conservation worldwide, risk assessment is one of the important elements of preventive conservation. Current flood risk assessments tend to focus on larger scales such as cities or river basins and fail to adequately refine the risk to individual buildings. This paper presents a flood risk assessment methodology tailored to the scale of cultural heritage units, to enhance the accuracy of identifying site-specific risks. The method couples hazard with vulnerability to assess flood risk. Regarding hazard, to solve the issue of the minimal contribution of slight geographical variations in water and environmental indicators at the scale of cultural heritage units to the classification of storm flood danger, we employ the Storm Water Management Model (SWMM). This model is utilized to establish rainfall scenarios and conduct numerical simulations, translating these into intuitive risk parameters like depth and extent of inundation. Additionally, a vulnerability assessment procedure for cultural heritage was designed. This procedure analyzes structural stability using finite element simulations, quantifies the sensitivity of cultural objects to floods, evaluates exposure based on potential value loss, and assesses disaster prevention and mitigation capacity considering protective measures and restoration possibilities. Flood risk can be obtained by weighing the results of hazard and vulnerability analyses. The case study of the ancient city site of Puzhou verifies the feasibility of the risk assessment methodology adopted and reveals the risk level of different areas. The results of the assessment show that the Xicheng District, except for the southern city wall, has a low flood risk and good resilience to flooding. The Drum Tower area has a damaged foundation but is on higher ground and has a lower flood risk. The Eastern District has important cultural relics, the low-lying topography makes it highly vulnerable to the double threat of flooding and human damage. Considering natural hazards and cultural heritage specifics, this integrated approach offers a targeted framework for assessing flood risks and developing prevention and mitigation strategies for heritage sites. Apply it to additional sites to further validate its effectiveness in the future. © 2024 45th Asian Conference on Remote Sensing, ACRS 2024. All rights reserved. ancient sites; cultural heritage; flood disaster risk assessment; hazard; vulnerability Banks (bodies of water); Disasters; Energy efficiency; Flood damage; Historic preservation; Loss prevention; Risk analysis; Risk management; Ancient site; Case-studies; Cultural heritages; Disaster risk assessments; Flood disaster; Flood disaster risk assessment; Flood risks; Heritage conservation; Preventive conservation; Vulnerability; Risk assessment";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
444;Image Super-Resolution Reconstruction of Landslide Based on Real-ESRGAN;The detection and extraction of landslide remote sensing images is of great significance for disaster monitoring and forewarning, risk assessments, and disaster prevention and control. However, the collected remote sensing images are not clear enough due to equipment limitations, which affects the application of remote sensing interpretation. For the lack of high-quality landslide datasets in the process of remote sensing images interpretation, we propose a model based on Real-ESRGAN to achieve super-resolution reconstruction of low-resolution landslide images in this paper. The method we proposed embeds a second-order degradation process combined with random shuffling strategy into the ESRGAN model to simulate the degradation process of the real images, choosing a U-Net discriminator with spectrum normalization to increase the discrimination ability and stabilize training dynamics. With transfer learning algorithm, experimental verification was carried out based on aerial images of landslides around Sichuan. The experimental results show that the Real-ESRGAN model based on transfer learning achieves higher scores in peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), and achieves better results in super-resolution reconstruction.  © 2024 ACM.;"images degradation; landslide remote sensing images; super-resolution reconstruction";"Antennas; Disaster prevention; Disasters; Image reconstruction; Learning algorithms; Learning systems; Optical resolving power; Remote sensing; Risk assessment; Signal to noise ratio; Degradation process; Disaster forewarning; Disaster monitoring; Image degradation; Image super-resolution reconstruction; Landslide remote sensing image; Model-based OPC; Remote sensing images; Super-resolution reconstruction; Transfer learning; Landslides";"Image Super-Resolution Reconstruction of Landslide Based on Real-ESRGAN The detection and extraction of landslide remote sensing images is of great significance for disaster monitoring and forewarning, risk assessments, and disaster prevention and control. However, the collected remote sensing images are not clear enough due to equipment limitations, which affects the application of remote sensing interpretation. For the lack of high-quality landslide datasets in the process of remote sensing images interpretation, we propose a model based on Real-ESRGAN to achieve super-resolution reconstruction of low-resolution landslide images in this paper. The method we proposed embeds a second-order degradation process combined with random shuffling strategy into the ESRGAN model to simulate the degradation process of the real images, choosing a U-Net discriminator with spectrum normalization to increase the discrimination ability and stabilize training dynamics. With transfer learning algorithm, experimental verification was carried out based on aerial images of landslides around Sichuan. The experimental results show that the Real-ESRGAN model based on transfer learning achieves higher scores in peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), and achieves better results in super-resolution reconstruction.  © 2024 ACM. images degradation; landslide remote sensing images; super-resolution reconstruction Antennas; Disaster prevention; Disasters; Image reconstruction; Learning algorithms; Learning systems; Optical resolving power; Remote sensing; Risk assessment; Signal to noise ratio; Degradation process; Disaster forewarning; Disaster monitoring; Image degradation; Image super-resolution reconstruction; Landslide remote sensing image; Model-based OPC; Remote sensing images; Super-resolution reconstruction; Transfer learning; Landslides";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
445;Flood Discharge Cross-Domain Routing Selection Based on Resource Sharing Equilibrium for Electric Power Communication Network;In order to address the issue of inter-domain blocking phenomena in cross-domain transmission, which results in low processing capacity for burst traffic and inadequate deployment capabilities for large traffic flows, this paper proposes a flood discharge cross-domain routing selection mechanism based on resource sharing balance, referred to as FR-CD. The proposed method is structured into two layers: inter-domain routing and intra-domain routing. The inter-domain routing primarily relies on service classification, which is informed by principles from water conservancy dam management. Intra-domain routing employs an improved shortest distance calculation method, supplemented with bandwidth utilization as a weighting factor to optimize the identification of intra-domain routes. This approach contributes significantly to establishing new pathways within power systems, enhancing both operational efficiency and security within communication backbone networks and access networks. Furthermore., it supports the construction and development of modern power systems.  © 2024 IEEE.;"cross-domain routing; Electric power communication network; resource sharing; service classification";"Electric network parameters; Power electronics; Cross-domain; Cross-domain routing; Electric power communication network; Flood discharge; Interdomain Routing; Intra-domain routing; Resources sharing; Routing selection; Routings; Services classification; Resource allocation";"Flood Discharge Cross-Domain Routing Selection Based on Resource Sharing Equilibrium for Electric Power Communication Network In order to address the issue of inter-domain blocking phenomena in cross-domain transmission, which results in low processing capacity for burst traffic and inadequate deployment capabilities for large traffic flows, this paper proposes a flood discharge cross-domain routing selection mechanism based on resource sharing balance, referred to as FR-CD. The proposed method is structured into two layers: inter-domain routing and intra-domain routing. The inter-domain routing primarily relies on service classification, which is informed by principles from water conservancy dam management. Intra-domain routing employs an improved shortest distance calculation method, supplemented with bandwidth utilization as a weighting factor to optimize the identification of intra-domain routes. This approach contributes significantly to establishing new pathways within power systems, enhancing both operational efficiency and security within communication backbone networks and access networks. Furthermore., it supports the construction and development of modern power systems.  © 2024 IEEE. cross-domain routing; Electric power communication network; resource sharing; service classification Electric network parameters; Power electronics; Cross-domain; Cross-domain routing; Electric power communication network; Flood discharge; Interdomain Routing; Intra-domain routing; Resources sharing; Routing selection; Routings; Services classification; Resource allocation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
446;DL-StormCast: Deep Learning and Near-forecast of Severe Convective Precipitation;Given the rapid onset, swift evolution, and short lifespan of severe convective precipitation events, accurately forecasting these within a two-hour window presents significant challenges. In the context of meteorological big data, this study introduces an integrated learning approach that enhances the efficiency of dual-polarization radar parameters in near-term forecasting of severe convective precipitation. By combining the Support Vector Machine-Boosted Naive Trees (SVM-BNT) classification model with the Convolutional Long Short-Term Memory (ConvLSTM) prediction model, we achieve a notable improvement in forecast accuracy. The SVM-BNT model effectively transforms explanatory variables, converting prior knowledge into posterior probabilities, which enhances the classification of precipitation particles based on radar observation data. The resulting classified data then serve as refined inputs for the ConvLSTM model, enabling it to capture crucial spatiotemporal characteristics of precipitation more effectively. This integration reduces the Root Mean Square Error (RMSE) from 3.10 to 2.45 and the Mean Squared Error (MSE) from 10.65 to 7.80, demonstrating significant enhancements in prediction precision. These advancements have substantial practical implications, particularly for improving response strategies in severe weather events and optimizing resource allocation for disaster management. © 2024 IEEE.;"ConvLSTM; Deep learning; Dual polarization weather radar; Severe convective precipitation forecasting";"Deep learning; Prediction models; Support vector machines; Weather forecasting; Convective precipitation; Convolutional long short-term memory; Deep learning; Dual-polarization weather radar; Lifespans; Precipitation events; Precipitation forecasting; Severe convective precipitation forecasting; Short term memory; Support vectors machine; Resource allocation";"DL-StormCast: Deep Learning and Near-forecast of Severe Convective Precipitation Given the rapid onset, swift evolution, and short lifespan of severe convective precipitation events, accurately forecasting these within a two-hour window presents significant challenges. In the context of meteorological big data, this study introduces an integrated learning approach that enhances the efficiency of dual-polarization radar parameters in near-term forecasting of severe convective precipitation. By combining the Support Vector Machine-Boosted Naive Trees (SVM-BNT) classification model with the Convolutional Long Short-Term Memory (ConvLSTM) prediction model, we achieve a notable improvement in forecast accuracy. The SVM-BNT model effectively transforms explanatory variables, converting prior knowledge into posterior probabilities, which enhances the classification of precipitation particles based on radar observation data. The resulting classified data then serve as refined inputs for the ConvLSTM model, enabling it to capture crucial spatiotemporal characteristics of precipitation more effectively. This integration reduces the Root Mean Square Error (RMSE) from 3.10 to 2.45 and the Mean Squared Error (MSE) from 10.65 to 7.80, demonstrating significant enhancements in prediction precision. These advancements have substantial practical implications, particularly for improving response strategies in severe weather events and optimizing resource allocation for disaster management. © 2024 IEEE. ConvLSTM; Deep learning; Dual polarization weather radar; Severe convective precipitation forecasting Deep learning; Prediction models; Support vector machines; Weather forecasting; Convective precipitation; Convolutional long short-term memory; Deep learning; Dual-polarization weather radar; Lifespans; Precipitation events; Precipitation forecasting; Severe convective precipitation forecasting; Short term memory; Support vectors machine; Resource allocation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
447;Clustering Analysis of Earthquake Based on K-Means, DBSCAN, and Fuzzy C-Means in North Sumatra;"This study investigates the efficacy of three clustering algorithms-K-means, DBSCAN, and Fuzzy C-Means-in classifying seismic events in North Sumatra. Earthquake data spanning from January 2019 to December 2022, provided by the Indonesian Agency for Meteorology, Climatology, and Geophysics (BMKG), was analyzed to evaluate how well each algorithm identifies and organizes clusters based on earthquake characteristics such as longitude, latitude, magnitude, and depth. K-means excelled in producing well-separated, spherical clusters; DBSCAN effectively detected clusters of varying densities and identified noise points; Fuzzy C-Means offered insights into overlapping clusters and gradual transitions. Despite some differences in the clustering results, all methods provided similar outcomes, with DBSCAN uniquely highlighting noise points. These insights contribute to a better understanding of seismic activity in North Sumatra and could improve earthquake prediction models and disaster preparedness strategies. © 2024 IEEE.";"Clustering; DBSCAN; Fuzzy C-Means; K-means";"Cluster analysis; Disaster prevention; Disasters; Earthquake effects; Fuzzy clustering; Jurassic; Miocene; C-means; Cluster-based; Clustering analysis; Clusterings; DBSCAN; Earthquake data; Fuzzy C-mean; K-means; Seismic event; Sumatra; K-means clustering";"Clustering Analysis of Earthquake Based on K-Means, DBSCAN, and Fuzzy C-Means in North Sumatra This study investigates the efficacy of three clustering algorithms-K-means, DBSCAN, and Fuzzy C-Means-in classifying seismic events in North Sumatra. Earthquake data spanning from January 2019 to December 2022, provided by the Indonesian Agency for Meteorology, Climatology, and Geophysics (BMKG), was analyzed to evaluate how well each algorithm identifies and organizes clusters based on earthquake characteristics such as longitude, latitude, magnitude, and depth. K-means excelled in producing well-separated, spherical clusters; DBSCAN effectively detected clusters of varying densities and identified noise points; Fuzzy C-Means offered insights into overlapping clusters and gradual transitions. Despite some differences in the clustering results, all methods provided similar outcomes, with DBSCAN uniquely highlighting noise points. These insights contribute to a better understanding of seismic activity in North Sumatra and could improve earthquake prediction models and disaster preparedness strategies. © 2024 IEEE. Clustering; DBSCAN; Fuzzy C-Means; K-means Cluster analysis; Disaster prevention; Disasters; Earthquake effects; Fuzzy clustering; Jurassic; Miocene; C-means; Cluster-based; Clustering analysis; Clusterings; DBSCAN; Earthquake data; Fuzzy C-mean; K-means; Seismic event; Sumatra; K-means clustering";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
448;Enhanced Human Detection in Disaster Zones Using UAVs and Deep Learning Algorithms;Monitoring and rescuing humans and animals during disasters is essential for reducing their impact on both the environment and human populations. This paper proposes an efficient and reliable system for human detection in catastrophe scenarios using Unmanned Aerial Vehicles (UAVs) equipped with deep learning techniques. In times of natural disasters, such as earthquakes, floods, or wildfires, conventional methods of search and rescue operations often face challenges due to the complexity of the affected environment, limited accessibility, and time sensitivity. The integration of UAVs and deep learning algorithms presents a promising solution to enhance the speed, accuracy, and safety of locating and rescuing survivors in such scenarios. Traditional search and rescue efforts often encounter significant challenges in accessing remote or hazardous areas, leading to delays in locating survivors and providing timely assistance. Our proposed system combines the YOLOv8 object detection model with the Deep SORT object tracking algorithm for human detection. By integrating these advanced techniques, our approach enables accurate and efficient detection and tracking of human subjects in complex environments. The fusion of YOLOv8 and Deep SORT results in a reliable system capable of detecting and tracking moving people in low-quality videos. The combined algorithm, referred to as the Y-DS model, achieved an ultimate accuracy of 87.9% and a speed of 55.8 frames per second (FPS), demonstrating effectiveness in real-world scenarios.  © 2024 IEEE.;"Artificial Intelligence; Deep Learning; Deep SORT; Human Detection; You Only Look Once (YOLOv8)";"Adaptive boosting; Adversarial machine learning; Aircraft accidents; Aircraft detection; Contrastive Learning; Deep learning; Object tracking; Unmanned aerial vehicles (UAV); Aerial vehicle; Deep learning; Deep SORT; Disaster zones; Human detection; Human population; Learning techniques; Natural disasters; Reliable systems; You only look once (YOLOv8)";"Enhanced Human Detection in Disaster Zones Using UAVs and Deep Learning Algorithms Monitoring and rescuing humans and animals during disasters is essential for reducing their impact on both the environment and human populations. This paper proposes an efficient and reliable system for human detection in catastrophe scenarios using Unmanned Aerial Vehicles (UAVs) equipped with deep learning techniques. In times of natural disasters, such as earthquakes, floods, or wildfires, conventional methods of search and rescue operations often face challenges due to the complexity of the affected environment, limited accessibility, and time sensitivity. The integration of UAVs and deep learning algorithms presents a promising solution to enhance the speed, accuracy, and safety of locating and rescuing survivors in such scenarios. Traditional search and rescue efforts often encounter significant challenges in accessing remote or hazardous areas, leading to delays in locating survivors and providing timely assistance. Our proposed system combines the YOLOv8 object detection model with the Deep SORT object tracking algorithm for human detection. By integrating these advanced techniques, our approach enables accurate and efficient detection and tracking of human subjects in complex environments. The fusion of YOLOv8 and Deep SORT results in a reliable system capable of detecting and tracking moving people in low-quality videos. The combined algorithm, referred to as the Y-DS model, achieved an ultimate accuracy of 87.9% and a speed of 55.8 frames per second (FPS), demonstrating effectiveness in real-world scenarios.  © 2024 IEEE. Artificial Intelligence; Deep Learning; Deep SORT; Human Detection; You Only Look Once (YOLOv8) Adaptive boosting; Adversarial machine learning; Aircraft accidents; Aircraft detection; Contrastive Learning; Deep learning; Object tracking; Unmanned aerial vehicles (UAV); Aerial vehicle; Deep learning; Deep SORT; Disaster zones; Human detection; Human population; Learning techniques; Natural disasters; Reliable systems; You only look once (YOLOv8)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
449;A Dynamic Assessment and Early Warning Research for Inundation Risk of Electric Power Facilities;To solve the problem of power supply failure caused by rainstorm inundation, a dynamic assessment and early warning method for inundation risk of electric power facilities based on two-dimensional hydrodynamic model is proposed in this paper. The Chicago rainfall model and two-dimensional hydrodynamic model are used to model the rainstorm inundation of power facility nodes. The weighted comprehensive evaluation method is used to calculate the risk probability of the failure event of the flooded facilities. A short-term load prediction model combining variational mode decomposition (VMD) and long short-term memory (LSTM) is proposed to forecast the short-term load of the grid nodes during the rainstorm. A classification method of risk and warning levels is proposed to mark nodes, in order to provide scientific decision-making basis for the managers of electric power facilities. The effectiveness of the proposed method is verified by taking a regional power grid as an example. The application process of the proposed method is further illustrated by taking the power grid of a certain region as an example. The feasibility and effectiveness of the proposed method are proved. ©2024 IEEE.;"electric power facility; risk assessment; storm flooding; two-dimensional hydrodynamic model";"Chicago; Dynamic assessment; Early warning; Early-warning method; Electric power facilities; Floodings; Power-supply failure; Risks assessments; Storm flooding; Two-dimensional hydrodynamic modeling; Risk assessment";"A Dynamic Assessment and Early Warning Research for Inundation Risk of Electric Power Facilities To solve the problem of power supply failure caused by rainstorm inundation, a dynamic assessment and early warning method for inundation risk of electric power facilities based on two-dimensional hydrodynamic model is proposed in this paper. The Chicago rainfall model and two-dimensional hydrodynamic model are used to model the rainstorm inundation of power facility nodes. The weighted comprehensive evaluation method is used to calculate the risk probability of the failure event of the flooded facilities. A short-term load prediction model combining variational mode decomposition (VMD) and long short-term memory (LSTM) is proposed to forecast the short-term load of the grid nodes during the rainstorm. A classification method of risk and warning levels is proposed to mark nodes, in order to provide scientific decision-making basis for the managers of electric power facilities. The effectiveness of the proposed method is verified by taking a regional power grid as an example. The application process of the proposed method is further illustrated by taking the power grid of a certain region as an example. The feasibility and effectiveness of the proposed method are proved. ©2024 IEEE. electric power facility; risk assessment; storm flooding; two-dimensional hydrodynamic model Chicago; Dynamic assessment; Early warning; Early-warning method; Electric power facilities; Floodings; Power-supply failure; Risks assessments; Storm flooding; Two-dimensional hydrodynamic modeling; Risk assessment";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
450;Predicting Power Outages from Ice Storms Using Machine Learning Models with SHAP Interpretability;Ice storms pose significant challenges, causing extensive damage and widespread power outages. Accurate prediction of outages during such events and their impact on distribution infrastructure is critical for mitigation efforts. This study presents a machine learning-based approach to predict the likelihood of distribution line power outages due to ice storms. Utilizing a comprehensive Ice Storm dataset from the cities of Hope and Mayville of North Dakota, containing meteorological and power outage information, features such as temperature, dew point, humidity, wind gust, wind speed, wind direction and precipitation metrics are used in the dataset. Various machine learning (ML) algorithms, including Random Forest Classifier (RFC), Logistic Regression Classifier (LRC), Support Vector Machine Classifier (SVC), and Gradient Boosting Classifier (GBC), are used to develop predictive models. The preliminary results indicate that RFC has the highest precision of 1.000. Feature importance using Shapley additive explanations (SHAP) analysis highlights wind gust, and wind direction as critical variables. This work can aid utility companies and emergency management agencies, enabling proactive measures to reduce the impact of ice storms. © 2024 IEEE.;"classifier; Gradient Boosting Classifier; ice storms; Logistic Regression Classifier; power outage; power transmission lines; Random Forest Classifier; Support Vector Machine Classifier; temperature";"Adaptive boosting; Health risks; Logistic regression; Power distribution lines; Risk assessment; Static Var compensators; Support vector regression; Weather forecasting; Wind effects; Boosting classifiers; Gradient boosting; Gradient boosting classifier; Ice storm; Logistic regression classifier; Power outage; Power transmission lines; Random forest classifier; Shapley; Support vector machine classifiers; Risk management";"Predicting Power Outages from Ice Storms Using Machine Learning Models with SHAP Interpretability Ice storms pose significant challenges, causing extensive damage and widespread power outages. Accurate prediction of outages during such events and their impact on distribution infrastructure is critical for mitigation efforts. This study presents a machine learning-based approach to predict the likelihood of distribution line power outages due to ice storms. Utilizing a comprehensive Ice Storm dataset from the cities of Hope and Mayville of North Dakota, containing meteorological and power outage information, features such as temperature, dew point, humidity, wind gust, wind speed, wind direction and precipitation metrics are used in the dataset. Various machine learning (ML) algorithms, including Random Forest Classifier (RFC), Logistic Regression Classifier (LRC), Support Vector Machine Classifier (SVC), and Gradient Boosting Classifier (GBC), are used to develop predictive models. The preliminary results indicate that RFC has the highest precision of 1.000. Feature importance using Shapley additive explanations (SHAP) analysis highlights wind gust, and wind direction as critical variables. This work can aid utility companies and emergency management agencies, enabling proactive measures to reduce the impact of ice storms. © 2024 IEEE. classifier; Gradient Boosting Classifier; ice storms; Logistic Regression Classifier; power outage; power transmission lines; Random Forest Classifier; Support Vector Machine Classifier; temperature Adaptive boosting; Health risks; Logistic regression; Power distribution lines; Risk assessment; Static Var compensators; Support vector regression; Weather forecasting; Wind effects; Boosting classifiers; Gradient boosting; Gradient boosting classifier; Ice storm; Logistic regression classifier; Power outage; Power transmission lines; Random forest classifier; Shapley; Support vector machine classifiers; Risk management";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
451;A New Approach for Interferometric Phase Reconstruction over Decorrelated Regions with Deep Learning;Interferometric Synthetic Aperture Radar (InSAR), a powerful geodetic technique, can accurately provide ground deformation measurements with high spatial resolution and wide coverage. However, InSAR suffers from decorrelation due to heavy vegetation or large ground movements, especially when exploring areas with geohazard events. Ignoring or masking these incoherent areas can stabilize InSAR phase unwrapping but at the cost of losing valuable spatial information, such as the deformation near the epicenter of an earthquake. To retrieve information over the masked areas, a generative interferometric phase reconstruction (IPR) technique has been developed to handle various fringe patterns and dynamically changing decorrelated regions. In this paper, we proposed a new generative adversarial network (GAN) called PhaseNet, designed for IPR under different fringe rates and masked patterns. The model was trained using a simulated InSAR dataset under a pixel-space loss function. The structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) were used as reconstruction metrics to evaluate model performance. The model achieved an SSIM of 0.98 for the original simulated interferogram despite 15% of the pixels being masked. The largest disparity between the clear and reconstructed phases in the synthetic validation dataset remained within the range of ±1 radian in phase or sub-centimeter in deformation.  © 2024 IEEE.;"Decorrelation; Deep learning; GAN; InSAR; Interferometric phase reconstruction (IPR)";"Earthquakes; Geodesy; Hydrogeology; Image coding; Image reconstruction; Adversarial networks; De correlations; Decorrelations; Deep learning; Interferometric phase reconstruction; Interferometric phasis; Interferometric synthetic aperture radars; Phase reconstruction; Similarity indices; Structural similarity; Interferometry";"A New Approach for Interferometric Phase Reconstruction over Decorrelated Regions with Deep Learning Interferometric Synthetic Aperture Radar (InSAR), a powerful geodetic technique, can accurately provide ground deformation measurements with high spatial resolution and wide coverage. However, InSAR suffers from decorrelation due to heavy vegetation or large ground movements, especially when exploring areas with geohazard events. Ignoring or masking these incoherent areas can stabilize InSAR phase unwrapping but at the cost of losing valuable spatial information, such as the deformation near the epicenter of an earthquake. To retrieve information over the masked areas, a generative interferometric phase reconstruction (IPR) technique has been developed to handle various fringe patterns and dynamically changing decorrelated regions. In this paper, we proposed a new generative adversarial network (GAN) called PhaseNet, designed for IPR under different fringe rates and masked patterns. The model was trained using a simulated InSAR dataset under a pixel-space loss function. The structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) were used as reconstruction metrics to evaluate model performance. The model achieved an SSIM of 0.98 for the original simulated interferogram despite 15% of the pixels being masked. The largest disparity between the clear and reconstructed phases in the synthetic validation dataset remained within the range of ±1 radian in phase or sub-centimeter in deformation.  © 2024 IEEE. Decorrelation; Deep learning; GAN; InSAR; Interferometric phase reconstruction (IPR) Earthquakes; Geodesy; Hydrogeology; Image coding; Image reconstruction; Adversarial networks; De correlations; Decorrelations; Deep learning; Interferometric phase reconstruction; Interferometric phasis; Interferometric synthetic aperture radars; Phase reconstruction; Similarity indices; Structural similarity; Interferometry";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
452;An Enhanced Novel Data Visualization Based on Flood Prediction Method Using Highest and Lowest Rainfall Patterns by Comparing Decision Tree Over Logistic Regression;Accurate flood prediction is critical for effective disaster management and mitigation. This research introduces an innovative approach to data visualization for flood prediction that utilizes historical rainfall patterns. By analyzing the highest and lowest rainfall data, the study proposes a novel visualization technique that enhances the understanding of flood dynamics. We compare the effectiveness of decision tree algorithms against logistic regression in predicting flood likelihood. Our methodology centers on assessing these models’ capabilities to interpret complex rainfall data and predict potential flooding events accurately. A significant contribution of this work is the development of an intuitive visualization tool that can assist policymakers and emergency responders in making informed decisions. The results demonstrate the superior performance of decision tree models in terms of prediction accuracy and visualization efficacy. This study lays the groundwork for future research aimed at incorporating more sophisticated machine learning techniques into flood prediction models. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.;"Data visualization; Decision tree; Flood prediction; Logistic regression; Rainfall patterns";"Data accuracy; Disaster prevention; Logistic regression; Rain; Disaster management; Disaster mitigation; Flood prediction; Innovative approaches; Logistics regressions; Novel visualizations; Prediction methods; Rainfall data; Rainfall patterns; Visualization technique; Prediction models";"An Enhanced Novel Data Visualization Based on Flood Prediction Method Using Highest and Lowest Rainfall Patterns by Comparing Decision Tree Over Logistic Regression Accurate flood prediction is critical for effective disaster management and mitigation. This research introduces an innovative approach to data visualization for flood prediction that utilizes historical rainfall patterns. By analyzing the highest and lowest rainfall data, the study proposes a novel visualization technique that enhances the understanding of flood dynamics. We compare the effectiveness of decision tree algorithms against logistic regression in predicting flood likelihood. Our methodology centers on assessing these models’ capabilities to interpret complex rainfall data and predict potential flooding events accurately. A significant contribution of this work is the development of an intuitive visualization tool that can assist policymakers and emergency responders in making informed decisions. The results demonstrate the superior performance of decision tree models in terms of prediction accuracy and visualization efficacy. This study lays the groundwork for future research aimed at incorporating more sophisticated machine learning techniques into flood prediction models. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024. Data visualization; Decision tree; Flood prediction; Logistic regression; Rainfall patterns Data accuracy; Disaster prevention; Logistic regression; Rain; Disaster management; Disaster mitigation; Flood prediction; Innovative approaches; Logistics regressions; Novel visualizations; Prediction methods; Rainfall data; Rainfall patterns; Visualization technique; Prediction models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
453;Multi-Machine Learning Approach for Mapping Drought-Driven Fires in Kalimantan Using Geo-AI Data;This paper presents a comparative analysis of four machine learning algorithms-Random Forest (RF), Gradient Boosting (GTB), Support Vector Machine (SVM), and Classification and Regression Trees (CART)-for mapping drought-driven forest and land fires in Kalimantan. We utilized multi-source Geo-Artificial Intelligence data, including satellite remote sensing products and spatial data, incorporating physical, environmental, climatological, biological, and socio-economic (anthropogenic) factors as inputs. The EI Niño year of 2019 was selected as the study period for data analysis. The Global Annual Burned Area Maps (GABAM) product was refined using nighttime active fire data, which represent actual fire occurrences and served as our reference map. We applied the Graph Regularization fusion method to all the classified images to enhance mapping accuracy. Our results indicate that RF and GTB significantly outperformed SVM and CART in terms of precision, recall, Kappa, and overall accuracy, with both RF and GTB achieving an overall accuracy of 97%. The fused image yielded results similar to those of the RF and GTB images. These findings offer valuable insights for developing a national early warning system for drought-related forest and land fires, using Geo-AI data to support forest management and the goal of achieving net-zero emissions.  © 2024 IEEE.;"Drough t-driven forest/land fires; Geo-Artificial Intelligence (Geo-AI); Kalimantan; Machine learning";"Adaptive boosting; Adversarial machine learning; Decision trees; Deforestation; Forest ecology; Information management; Premixed flames; Random forests; Support vector regression; Classification trees; Drough t-driven forest/land fire; Forestlands; Geo-artificial intelligence; Gradient boosting; Kalimantan; Machine-learning; Random forests; Support vector classification; Support vectors machine; Drought";"Multi-Machine Learning Approach for Mapping Drought-Driven Fires in Kalimantan Using Geo-AI Data This paper presents a comparative analysis of four machine learning algorithms-Random Forest (RF), Gradient Boosting (GTB), Support Vector Machine (SVM), and Classification and Regression Trees (CART)-for mapping drought-driven forest and land fires in Kalimantan. We utilized multi-source Geo-Artificial Intelligence data, including satellite remote sensing products and spatial data, incorporating physical, environmental, climatological, biological, and socio-economic (anthropogenic) factors as inputs. The EI Niño year of 2019 was selected as the study period for data analysis. The Global Annual Burned Area Maps (GABAM) product was refined using nighttime active fire data, which represent actual fire occurrences and served as our reference map. We applied the Graph Regularization fusion method to all the classified images to enhance mapping accuracy. Our results indicate that RF and GTB significantly outperformed SVM and CART in terms of precision, recall, Kappa, and overall accuracy, with both RF and GTB achieving an overall accuracy of 97%. The fused image yielded results similar to those of the RF and GTB images. These findings offer valuable insights for developing a national early warning system for drought-related forest and land fires, using Geo-AI data to support forest management and the goal of achieving net-zero emissions.  © 2024 IEEE. Drough t-driven forest/land fires; Geo-Artificial Intelligence (Geo-AI); Kalimantan; Machine learning Adaptive boosting; Adversarial machine learning; Decision trees; Deforestation; Forest ecology; Information management; Premixed flames; Random forests; Support vector regression; Classification trees; Drough t-driven forest/land fire; Forestlands; Geo-artificial intelligence; Gradient boosting; Kalimantan; Machine-learning; Random forests; Support vector classification; Support vectors machine; Drought";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
454;Prediction of Flood in Jhelum River Using Hybrid SVM-PSO Approach;Flood prediction is an important aspect of disaster management, for areas prone to frequent flooding. This research endeavors to advance flood prediction capabilities in the Baramulla district of Jammu and Kashmir, an area characterized by its vulnerability to hydrological hazards. We are attempting to decipher the complicated interplay of factors that contribute to flood events in this geographically sensitive area through the extrapolation of historical data containing many hydro meteorological variables. To assess how the data-driven models can synergize to enhance the accuracy of flood forecasts on a monthly basis from 1975 to 2022, we have used an advanced computational capability from Support vector machines (SVMs), artificial neural networks (ANNs), and integration of PSO (SVM-PSO). To foster resilience and protect the population and infrastructure of Baramulla, from recurrent flooding threats, this pioneering approach, which brings together a series of inputs and leverages advanced AI algorithms to develop robust flood prediction models, underlines the importance of this research and has a significant impact in the field of hydrology and disaster risk reduction, by extending the scope of flood modeling. The model developed has shown exceptional performance through the use of a hybrid training algorithm (SVM-PSO) with a coefficient of correlation (R2) = 0.9739, Mean square error (MSE) = 4.9932, Nash–Sutcliffe Model Efficiency = 0.969. These results highlight the significant potential of development models for accurate flood forecasts. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.;"Flood; Jhelum river; SVM; SVM-PSO";"Banks (bodies of water); Disaster prevention; Population statistics; Prediction models; Recurrent neural networks; Risk assessment; Support vector machines; Weather forecasting; Disaster management; Flood forecast; Flood prediction; Floodings; Hybrid support vector machines; Jhelum river; Prediction capability; Support vector machine-PSO; Support vectors machine; Floods";"Prediction of Flood in Jhelum River Using Hybrid SVM-PSO Approach Flood prediction is an important aspect of disaster management, for areas prone to frequent flooding. This research endeavors to advance flood prediction capabilities in the Baramulla district of Jammu and Kashmir, an area characterized by its vulnerability to hydrological hazards. We are attempting to decipher the complicated interplay of factors that contribute to flood events in this geographically sensitive area through the extrapolation of historical data containing many hydro meteorological variables. To assess how the data-driven models can synergize to enhance the accuracy of flood forecasts on a monthly basis from 1975 to 2022, we have used an advanced computational capability from Support vector machines (SVMs), artificial neural networks (ANNs), and integration of PSO (SVM-PSO). To foster resilience and protect the population and infrastructure of Baramulla, from recurrent flooding threats, this pioneering approach, which brings together a series of inputs and leverages advanced AI algorithms to develop robust flood prediction models, underlines the importance of this research and has a significant impact in the field of hydrology and disaster risk reduction, by extending the scope of flood modeling. The model developed has shown exceptional performance through the use of a hybrid training algorithm (SVM-PSO) with a coefficient of correlation (R2) = 0.9739, Mean square error (MSE) = 4.9932, Nash–Sutcliffe Model Efficiency = 0.969. These results highlight the significant potential of development models for accurate flood forecasts. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024. Flood; Jhelum river; SVM; SVM-PSO Banks (bodies of water); Disaster prevention; Population statistics; Prediction models; Recurrent neural networks; Risk assessment; Support vector machines; Weather forecasting; Disaster management; Flood forecast; Flood prediction; Floodings; Hybrid support vector machines; Jhelum river; Prediction capability; Support vector machine-PSO; Support vectors machine; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
455;An Internet of Things and AI-Powered Framework for Long-Term Flood Risk Evaluation;Integrating Internet of Things (IoT) and artificial intelligence (AI) techniques have found widespread application in various fields, including smart cities, agriculture, and environmental monitoring. With the increasing availability of satellite imagery and other remote sensing data, deep learning algorithms can be used and trained to detect, classify, and segment flood regions in real time. In addition, deep learning techniques, such as convolutional neural networks (CNNs), have been successful in this field, enabling the automated analysis of vast amounts of satellite imagery. By combining AI-based flood detection with other data sources, such as meteorological forecasts and ground-based sensors, comprehensive flood monitoring systems that provide early warning of flood events and facilitate effective emergency response can be developed. In this article, we developed an image-based flood segmentation system called DeepLab that uses a deep learning algorithm to detect and segment the presence and extent of floods with high accuracy and speed. The neural network was trained on an extensive collection of satellite images, which were complemented by ground truth labels that indicated the presence of flooded areas. The trained DeepLabv3 model is applied to new satellite images during inference to forecast the likelihood of each pixel belonging to a flooded area. To do this, a binary flood map was generated from the pixel-level forecasts by incorporating a threshold into the output probabilities. The proposed system's accuracy was high compared to the state-of-the-art methods, as evidenced by segmentation and experimental results. The segmentation accuracy achieved an overall score of 87%. © 2014 IEEE.;"Artificial intelligence (AI); deep learning; flood risk assessment; Internet of Things (IoT); satellite imagery; sustainability";"Deep learning; Floods; Image segmentation; Interactive computer systems; Internet of things; Learning algorithms; Neural networks; Pixels; Real time systems; Remote sensing; Satellite imagery; Sustainable development; Artificial intelligence techniques; Deep learning; Flood risk assessments; Flood risks; Flooded areas; Images segmentations; Real - Time system; Risk evaluation; Satellite images; Risk assessment";"An Internet of Things and AI-Powered Framework for Long-Term Flood Risk Evaluation Integrating Internet of Things (IoT) and artificial intelligence (AI) techniques have found widespread application in various fields, including smart cities, agriculture, and environmental monitoring. With the increasing availability of satellite imagery and other remote sensing data, deep learning algorithms can be used and trained to detect, classify, and segment flood regions in real time. In addition, deep learning techniques, such as convolutional neural networks (CNNs), have been successful in this field, enabling the automated analysis of vast amounts of satellite imagery. By combining AI-based flood detection with other data sources, such as meteorological forecasts and ground-based sensors, comprehensive flood monitoring systems that provide early warning of flood events and facilitate effective emergency response can be developed. In this article, we developed an image-based flood segmentation system called DeepLab that uses a deep learning algorithm to detect and segment the presence and extent of floods with high accuracy and speed. The neural network was trained on an extensive collection of satellite images, which were complemented by ground truth labels that indicated the presence of flooded areas. The trained DeepLabv3 model is applied to new satellite images during inference to forecast the likelihood of each pixel belonging to a flooded area. To do this, a binary flood map was generated from the pixel-level forecasts by incorporating a threshold into the output probabilities. The proposed system's accuracy was high compared to the state-of-the-art methods, as evidenced by segmentation and experimental results. The segmentation accuracy achieved an overall score of 87%. © 2014 IEEE. Artificial intelligence (AI); deep learning; flood risk assessment; Internet of Things (IoT); satellite imagery; sustainability Deep learning; Floods; Image segmentation; Interactive computer systems; Internet of things; Learning algorithms; Neural networks; Pixels; Real time systems; Remote sensing; Satellite imagery; Sustainable development; Artificial intelligence techniques; Deep learning; Flood risk assessments; Flood risks; Flooded areas; Images segmentations; Real - Time system; Risk evaluation; Satellite images; Risk assessment";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
456;TweetSense: Navigating Disaster Signals Using Machine Learning and Deep Learning Techniques;A disaster is a sudden, catastrophic event that causes significant damage, destruction, and disruption to the normal functioning of society, often resulting in loss of life, property, and livelihoods. Disasters can be natural (earthquakes, floods, wildfires) or human-made (industrial accidents, terrorist attacks, wars). Disasters can have wide-ranging impacts on communities, economies, and the environment, requiring emergency response and recovery efforts to mitigate their effects and restore normalcy. With disasters being too frequent in the modern world, the analysis of tweets cannot be an easy task for manual analysis. To overcome this, an automated model is to be built, which can reduce the manual analysis time for emergency responders. Socialmedia, with Twitter being a prime example, acts as a primary channel for disseminating information regarding disasters. The ability to predict relevant tweets holds immense potential in the development of early warning systems, facilitating timely actions by authorities and emergency responders to mitigate the impact of disasters. The approaches followed to achieve the tweet classification are Bag of Words, Term Frequency-Inverse Document Frequency, Word2Vec, followed by machine learning & deep learning methodologies. The CNN model is notably achieving a higher accuracy of 84.1% compared to other models. © 2024 IEEE.;"disaster; emergency response; information retrieval; machine learning; natural language processing; social media; social media analytics; text classification; Twitter";"Adversarial machine learning; Deep learning; Disasters; Economic and social effects; Inverse problems; Terrorism; Emergency responders; Emergency response; Language processing; Machine-learning; Manual analysis; Natural language processing; Natural languages; Social media; Social media analytics; Text classification; Tweets";"TweetSense: Navigating Disaster Signals Using Machine Learning and Deep Learning Techniques A disaster is a sudden, catastrophic event that causes significant damage, destruction, and disruption to the normal functioning of society, often resulting in loss of life, property, and livelihoods. Disasters can be natural (earthquakes, floods, wildfires) or human-made (industrial accidents, terrorist attacks, wars). Disasters can have wide-ranging impacts on communities, economies, and the environment, requiring emergency response and recovery efforts to mitigate their effects and restore normalcy. With disasters being too frequent in the modern world, the analysis of tweets cannot be an easy task for manual analysis. To overcome this, an automated model is to be built, which can reduce the manual analysis time for emergency responders. Socialmedia, with Twitter being a prime example, acts as a primary channel for disseminating information regarding disasters. The ability to predict relevant tweets holds immense potential in the development of early warning systems, facilitating timely actions by authorities and emergency responders to mitigate the impact of disasters. The approaches followed to achieve the tweet classification are Bag of Words, Term Frequency-Inverse Document Frequency, Word2Vec, followed by machine learning & deep learning methodologies. The CNN model is notably achieving a higher accuracy of 84.1% compared to other models. © 2024 IEEE. disaster; emergency response; information retrieval; machine learning; natural language processing; social media; social media analytics; text classification; Twitter Adversarial machine learning; Deep learning; Disasters; Economic and social effects; Inverse problems; Terrorism; Emergency responders; Emergency response; Language processing; Machine-learning; Manual analysis; Natural language processing; Natural languages; Social media; Social media analytics; Text classification; Tweets";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
457;Neural Network Based Performance Analysis of Coastal Vulnerability Classification Using Social-Physical Parameters in the Special Region of Yogyakarta;Natural disasters present a serious threat to the coastal environment. Coastal disasters like erosion, floods, rising sea levels, droughts, and landslides frequently cause significant loss of life and property damage. Determining the disaster risk in these areas is crucial for providing early warnings to communities. This study employs neural network techniques to assess disaster vulnerability in coastal areas. Specifically, it demonstrates how the neural network algorithm, with varying configurations of dense layers, can effectively classify the level of vulnerability in coastal areas compared to the machine learning method. The classification of disaster vulnerability in coastal regions is segmented into five hazard categories: Very Low, Low, Medium, High, and Very High. The results of the neural network method show that multiple hidden dense layers produced an accuracy of 0,983, which is higher than that of a single hidden dense, which produced 0.977. However, the results from both applications of the neural network method are greater than those from the SVM machine learning method, which provides results of 0.97. These findings indicate that using a multiple hidden dense layer would be more appropriate for classifying vulnerability on the coast of the Special Region of Yogyakarta province using 4724 data points.  © 2024 IEEE.;"ANN; Classification; Coastal Vulnerability; CVI; Neural Network; SVM";"Coastal engineering; Coastal zones; Disasters; Machine learning; ANN; Coastal area; Coastal vulnerability; CVI; Dense layer; Disaster vulnerability; Machine learning methods; Neural network method; Neural-networks; SVM; Multilayer neural networks";"Neural Network Based Performance Analysis of Coastal Vulnerability Classification Using Social-Physical Parameters in the Special Region of Yogyakarta Natural disasters present a serious threat to the coastal environment. Coastal disasters like erosion, floods, rising sea levels, droughts, and landslides frequently cause significant loss of life and property damage. Determining the disaster risk in these areas is crucial for providing early warnings to communities. This study employs neural network techniques to assess disaster vulnerability in coastal areas. Specifically, it demonstrates how the neural network algorithm, with varying configurations of dense layers, can effectively classify the level of vulnerability in coastal areas compared to the machine learning method. The classification of disaster vulnerability in coastal regions is segmented into five hazard categories: Very Low, Low, Medium, High, and Very High. The results of the neural network method show that multiple hidden dense layers produced an accuracy of 0,983, which is higher than that of a single hidden dense, which produced 0.977. However, the results from both applications of the neural network method are greater than those from the SVM machine learning method, which provides results of 0.97. These findings indicate that using a multiple hidden dense layer would be more appropriate for classifying vulnerability on the coast of the Special Region of Yogyakarta province using 4724 data points.  © 2024 IEEE. ANN; Classification; Coastal Vulnerability; CVI; Neural Network; SVM Coastal engineering; Coastal zones; Disasters; Machine learning; ANN; Coastal area; Coastal vulnerability; CVI; Dense layer; Disaster vulnerability; Machine learning methods; Neural network method; Neural-networks; SVM; Multilayer neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
458;Satellite-Based Landslide Event Detection Leveraging EfficientDet Model;Timely detection and monitoring of landslides are essential for effective disaster preparedness and mitigation. With the notable advancement of remote sensing technology and the computational environment, enriched solutions are anticipated for landslide hazards. Hence this study evaluates a deep learning-driven approach for detecting landslides from satellite images. Particularly, EfficientDet, a popular single-staged object detection algorithm. A series of EfficientDet models namely D0, D1, D2, D3, and D4 are applied and implemented. For experimentation, publicly available Bijie landslide detection data is used. Furthermore, the model's performance is assessed using well-known indicators i.e., average precision and recall. The experimental results indicated that the accuracy of EfficientDet_D3 is best (AP@0.50=0.80) followed by EfficientDet_D4, EfficientDe_D1, EfficientDet_D2, EfficientDet_DO. The qualitative and the quantitative outcomes exhibit the power of deep learning algorithms in the studies of hazard analysis. Our findings contribute to proactive disaster management schemes, aiding in the growth of enhanced mitigation measures to minimize the effect of landslides on communities and the environment.  © 2024 IEEE.;"Deep learning; EfficientDet; landslides; satellites; YOLO";"Deep learning; Disaster prevention; Disasters; Satellite imagery; Computational environments; Deep learning; Disaster mitigation; Disaster preparedness; Efficientdet; Events detection; Landslide hazard; Remote sensing technology; Satellite images; YOLO; Landslides";"Satellite-Based Landslide Event Detection Leveraging EfficientDet Model Timely detection and monitoring of landslides are essential for effective disaster preparedness and mitigation. With the notable advancement of remote sensing technology and the computational environment, enriched solutions are anticipated for landslide hazards. Hence this study evaluates a deep learning-driven approach for detecting landslides from satellite images. Particularly, EfficientDet, a popular single-staged object detection algorithm. A series of EfficientDet models namely D0, D1, D2, D3, and D4 are applied and implemented. For experimentation, publicly available Bijie landslide detection data is used. Furthermore, the model's performance is assessed using well-known indicators i.e., average precision and recall. The experimental results indicated that the accuracy of EfficientDet_D3 is best (AP@0.50=0.80) followed by EfficientDet_D4, EfficientDe_D1, EfficientDet_D2, EfficientDet_DO. The qualitative and the quantitative outcomes exhibit the power of deep learning algorithms in the studies of hazard analysis. Our findings contribute to proactive disaster management schemes, aiding in the growth of enhanced mitigation measures to minimize the effect of landslides on communities and the environment.  © 2024 IEEE. Deep learning; EfficientDet; landslides; satellites; YOLO Deep learning; Disaster prevention; Disasters; Satellite imagery; Computational environments; Deep learning; Disaster mitigation; Disaster preparedness; Efficientdet; Events detection; Landslide hazard; Remote sensing technology; Satellite images; YOLO; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
459;Optimizing Emergency Response with UAV-Integrated Fire Safety for Real-Time Prediction and Decision-Making: A performance evaluation;This paper provides a detailed performance evaluation of integrating Unmanned Aerial Vehicles (UAVs) with cloud-based fire safety systems for real-time 3D mapping, hotspot detection, and emergency response. By leveraging UAVs’ data acquisition capabilities and applying machine learning models like Random Forest (RF) and Support Vector Machines (SVM), the study seeks to enhance situational awareness and reduce response times in emergency scenarios. Simulations were conducted for both Wildfire Monitoring and Urban Search and Rescue (USaR) missions, demonstrating the pivotal role of UAV-cloud integration in improving decision-making accuracy and operational efficiency. The performance of RF and SVM models was compared, focusing on their effectiveness in identifying fire hotspots, predicting fire spread, and supporting decision-making. The RF model consistently outperformed SVM in real-time applications, suggesting significant potential for real-world deployment in emergency settings. The paper concludes with insights into future research directions and potential optimizations for scaling the system and managing larger datasets. ©2024 IEEE.;NULL;"Fire alarm systems; Fire extinguishers; Fire hazards; Fireproofing; Premixed flames; Unmanned aerial vehicles (UAV); Aerial vehicle; Decisions makings; Emergency response; Fire safety; Performances evaluation; Prediction and decision; Random forests; Real time decisions; Real-time prediction; Support vectors machine; Support vector machines";"Optimizing Emergency Response with UAV-Integrated Fire Safety for Real-Time Prediction and Decision-Making: A performance evaluation This paper provides a detailed performance evaluation of integrating Unmanned Aerial Vehicles (UAVs) with cloud-based fire safety systems for real-time 3D mapping, hotspot detection, and emergency response. By leveraging UAVs’ data acquisition capabilities and applying machine learning models like Random Forest (RF) and Support Vector Machines (SVM), the study seeks to enhance situational awareness and reduce response times in emergency scenarios. Simulations were conducted for both Wildfire Monitoring and Urban Search and Rescue (USaR) missions, demonstrating the pivotal role of UAV-cloud integration in improving decision-making accuracy and operational efficiency. The performance of RF and SVM models was compared, focusing on their effectiveness in identifying fire hotspots, predicting fire spread, and supporting decision-making. The RF model consistently outperformed SVM in real-time applications, suggesting significant potential for real-world deployment in emergency settings. The paper concludes with insights into future research directions and potential optimizations for scaling the system and managing larger datasets. ©2024 IEEE. NULL Fire alarm systems; Fire extinguishers; Fire hazards; Fireproofing; Premixed flames; Unmanned aerial vehicles (UAV); Aerial vehicle; Decisions makings; Emergency response; Fire safety; Performances evaluation; Prediction and decision; Random forests; Real time decisions; Real-time prediction; Support vectors machine; Support vector machines";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;3;Response
460;Co-registration of PRISMA Hyperspectral Imagery for Accurate Land Cover Classification;The precise and prompt categorization of land cover types holds significant importance in the realm of land resource planning and management, as well as in risk reduction. The utilization of hyperspectral satellite imagery, such as the imagery delivered by PRISMA, plays a vital role in analyzing environmental changes. Even though PRISMA products are distributed at Preprocessing Level 2D (radiometrically and geometrically calibrated), the images may exhibit registration errors on the order of a few hundred meters. Therefore, co-registration is a crucial preprocessing step before their utilization. This study utilized a local co-registration method based on the optical flow estimation technique to coregister the PRISMA images using Sentinel-2/Landsat 8–9 as references. The results showed that a careful selection of an appropriate reference image holds immense importance in the co-registration process, and the closer the acquisition time of the reference image is to the acquisition time of the image to be co-registered, the higher the quality of the co-registration results. By integrating cutting-edge machine learning techniques, the proposed co-registration approach further enhances the usability and accuracy of PRISMA products for land cover classification, and makes them a valuable source of information for applications in land management and thematic hazard studies, including scenarios such as flood monitoring and landslide analysis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.;"Classification; Co-registration; GeFolki; Hyperspectral; Land cover; Land use; PRISMA";"Information management; Landsat; Resource allocation; Risk management; Acquisition time; Coregistration; Gefolki; Hyper-spectral imageries; HyperSpectral; Land cover; Land cover classification; Land-cover types; PRISMA; Reference image; Hyperspectral imaging";"Co-registration of PRISMA Hyperspectral Imagery for Accurate Land Cover Classification The precise and prompt categorization of land cover types holds significant importance in the realm of land resource planning and management, as well as in risk reduction. The utilization of hyperspectral satellite imagery, such as the imagery delivered by PRISMA, plays a vital role in analyzing environmental changes. Even though PRISMA products are distributed at Preprocessing Level 2D (radiometrically and geometrically calibrated), the images may exhibit registration errors on the order of a few hundred meters. Therefore, co-registration is a crucial preprocessing step before their utilization. This study utilized a local co-registration method based on the optical flow estimation technique to coregister the PRISMA images using Sentinel-2/Landsat 8–9 as references. The results showed that a careful selection of an appropriate reference image holds immense importance in the co-registration process, and the closer the acquisition time of the reference image is to the acquisition time of the image to be co-registered, the higher the quality of the co-registration results. By integrating cutting-edge machine learning techniques, the proposed co-registration approach further enhances the usability and accuracy of PRISMA products for land cover classification, and makes them a valuable source of information for applications in land management and thematic hazard studies, including scenarios such as flood monitoring and landslide analysis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024. Classification; Co-registration; GeFolki; Hyperspectral; Land cover; Land use; PRISMA Information management; Landsat; Resource allocation; Risk management; Acquisition time; Coregistration; Gefolki; Hyper-spectral imageries; HyperSpectral; Land cover; Land cover classification; Land-cover types; PRISMA; Reference image; Hyperspectral imaging";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
461;Integration of Emotional-Cognitive Architecture in ACT-R for Multi-Agent Social Dynamics for Disaster Response Simulation;Accurately simulating human behavior in disaster scenarios is crucial for effective emergency management and social response prediction. However, existing approaches lack integration of emotional factors in cognitive modeling, leading to limited fidelity in behavioral simulation. This study presents an enhanced ACT-R framework incorporating an emotional module to simulate post-disaster social dynamics, with a specific focus on earthquake scenarios. We propose a novel integration methodology that combines ACT-R with Agent-Based Modeling (ABM), where emotional states are dynamically modeled through a triple-parameter system (emotional intensity, social support level, and resource availability) that adaptively influences cognitive processing and decision thresholds. By implementing this emotion-aware framework in a case study of 20,000 agents within Xi'an city wall area, we observed distinct emotional clustering patterns and collective safety-seeking behaviors driven by emotional contagion, demonstrating the framework's capability in capturing complex social dynamics during crisis scenarios.  © 2024 IEEE.;"ACT-R; cognitive modeling; emotional influence; simulation; social effects";"Digital elevation model; Disaster prevention; Disasters; Earthquakes; Economic and social effects; Risk management; ACT-R; Agent social; Cognitive architectures; Cognitive model; Disaster-response; Emotional influence; Human behaviors; Multi agent; Simulation; Social dynamics; Multi agent systems";"Integration of Emotional-Cognitive Architecture in ACT-R for Multi-Agent Social Dynamics for Disaster Response Simulation Accurately simulating human behavior in disaster scenarios is crucial for effective emergency management and social response prediction. However, existing approaches lack integration of emotional factors in cognitive modeling, leading to limited fidelity in behavioral simulation. This study presents an enhanced ACT-R framework incorporating an emotional module to simulate post-disaster social dynamics, with a specific focus on earthquake scenarios. We propose a novel integration methodology that combines ACT-R with Agent-Based Modeling (ABM), where emotional states are dynamically modeled through a triple-parameter system (emotional intensity, social support level, and resource availability) that adaptively influences cognitive processing and decision thresholds. By implementing this emotion-aware framework in a case study of 20,000 agents within Xi'an city wall area, we observed distinct emotional clustering patterns and collective safety-seeking behaviors driven by emotional contagion, demonstrating the framework's capability in capturing complex social dynamics during crisis scenarios.  © 2024 IEEE. ACT-R; cognitive modeling; emotional influence; simulation; social effects Digital elevation model; Disaster prevention; Disasters; Earthquakes; Economic and social effects; Risk management; ACT-R; Agent social; Cognitive architectures; Cognitive model; Disaster-response; Emotional influence; Human behaviors; Multi agent; Simulation; Social dynamics; Multi agent systems";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.1;Geological;3;Response
462;Flood Risk Assesment System using Logistic Regression;One of the most frequent natural disasters in the world, flooding affects millions of people annually and causes significant economic losses as well as human tragedies, with India bearing a disproportionate amount of the burden. Reducing these effects requires effective early warning systems, yet many vulnerable communities still struggle to make use of them. Scientists and programmers are using advances in machine learning (ML) and artificial intelligence (AI) to bridge this gap by creating a trustworthy flood prediction system. This state-of-the-art system aims to offer more cost-effective choices that perform better at predicting floods caused by precipitation. This prediction system's primary job is to forecast potential flood events by utilizing previous rainfall data to create predictive models. The model looks at rainfall patterns specific to different locations to determine the likelihood of a flood event. Its growth depends on the utilization of extensive district-level rainfall information from India, which serve as the foundation for training and enhancing the prediction models. A range of machine learning (ML) techniques, such as XGBoost and K-Nearest Neighbors (KNN), are used to train predictive algorithms. These algorithms were chosen because of their ability to handle complex datasets and extract meaningful patterns that relate the amount of rainfall with the probability of floods. Through recurrent training and validation against past flood events, the models are improved to reliably determine whether a region is in danger of flooding based on measured rainfall amounts.  © 2024 IEEE.;"Flood Risk Assessment; KNearest Neighbors algorithm; Logistic Regression; Machine learning (ML) techniques; Rain fall prediction";"Flood damage; Logistic regression; Machine learning; Nearest neighbor search; Prediction models; Risk assessment; Flood event; Flood risk assessments; Flood risks; Floodings; K Nearest Neighbor (k NN) algorithm; Logistics regressions; Machine learning techniques; Prediction systems; Rain fall prediction; Rain falls; Rain";"Flood Risk Assesment System using Logistic Regression One of the most frequent natural disasters in the world, flooding affects millions of people annually and causes significant economic losses as well as human tragedies, with India bearing a disproportionate amount of the burden. Reducing these effects requires effective early warning systems, yet many vulnerable communities still struggle to make use of them. Scientists and programmers are using advances in machine learning (ML) and artificial intelligence (AI) to bridge this gap by creating a trustworthy flood prediction system. This state-of-the-art system aims to offer more cost-effective choices that perform better at predicting floods caused by precipitation. This prediction system's primary job is to forecast potential flood events by utilizing previous rainfall data to create predictive models. The model looks at rainfall patterns specific to different locations to determine the likelihood of a flood event. Its growth depends on the utilization of extensive district-level rainfall information from India, which serve as the foundation for training and enhancing the prediction models. A range of machine learning (ML) techniques, such as XGBoost and K-Nearest Neighbors (KNN), are used to train predictive algorithms. These algorithms were chosen because of their ability to handle complex datasets and extract meaningful patterns that relate the amount of rainfall with the probability of floods. Through recurrent training and validation against past flood events, the models are improved to reliably determine whether a region is in danger of flooding based on measured rainfall amounts.  © 2024 IEEE. Flood Risk Assessment; KNearest Neighbors algorithm; Logistic Regression; Machine learning (ML) techniques; Rain fall prediction Flood damage; Logistic regression; Machine learning; Nearest neighbor search; Prediction models; Risk assessment; Flood event; Flood risk assessments; Flood risks; Floodings; K Nearest Neighbor (k NN) algorithm; Logistics regressions; Machine learning techniques; Prediction systems; Rain fall prediction; Rain falls; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
463;Non-Negative Matrix Factorization (NMF) in Fire Susceptibility;In recent years, the Kurdo-Zagrosian mountains in western Iran and northern Iraq have faced numerous wildfire fires. Mapping forest fire susceptibility is crucial for several reasons, including its role in prevention and mitigation, resource allocation, ecological conservation, early warning systems, policy development, insurance and risk management, and wildfire risk mapping. Machine Learning (ML) has found numerous applications in remote sensing, including fire detection, severity assessment, fuel moisture content estimation, fire spread prediction, fire susceptibility mapping, smoke plume detection, air quality monitoring, post-fire recovery monitoring, and decision support systems for fire management. This study employs a new approach to leveraging Non-negative Matrix Factorization (NMF) for detecting fire-susceptible areas in the Kurdo-Zagrosian forests of Marivan and Sarvabad in Kurdistan Province, western Iran. The NMF is a ML method used for dimensionality reduction and feature extraction. NMF differs from traditional matrix factorization methods by enforcing non-negativity constraints on the factor matrices, making the resulting factors interpretable and often more suitable for real-world data analysis. Sentinel-2 satellite imagery, elevation, distance to the road network, and Zagros Grass Index (ZGI) have been used as the primary inputs of the model, combined with in situ data for verifying and interpreting the resulting maps. The results showed that, besides providing useful information in extracting fire susceptible areas, NMF handles wide study areas efficiently, especially for tasks like feature extraction from large-scale datasets such as satellite images or multispectral data. The results especially revealed that ZGI has specifically demonstrated improved accuracy and reliability. The resulting map also showed a very close overlap between the fired area provided by Sentinel imagery from 2021 to 2023 and the areas labeled as highly susceptible regions in 2020, especially when ZGI has been regarded between the input variables. © 2024 SPIE.;"fire susceptibility; Forest fire; Kurdo-Zagrosian forest; Machine Learning; NMF; ZGI";"Deforestation; Fire alarm systems; Fire extinguishers; Fire hazards; Forest ecology; Insurance; Matrix algebra; Photomapping; Premixed flames; Reforestation; Resource allocation; Risk assessment; Risk management; Risk perception; Smoke detectors; Features extraction; Fire susceptibility; Forest fires; Kurdo-zagrosian forest; Machine-learning; Matrix factorizations; Negative matrix factorization; Nonnegative matrix factorization; Western Iran; Zagros grass index; Non-negative matrix factorization";"Non-Negative Matrix Factorization (NMF) in Fire Susceptibility In recent years, the Kurdo-Zagrosian mountains in western Iran and northern Iraq have faced numerous wildfire fires. Mapping forest fire susceptibility is crucial for several reasons, including its role in prevention and mitigation, resource allocation, ecological conservation, early warning systems, policy development, insurance and risk management, and wildfire risk mapping. Machine Learning (ML) has found numerous applications in remote sensing, including fire detection, severity assessment, fuel moisture content estimation, fire spread prediction, fire susceptibility mapping, smoke plume detection, air quality monitoring, post-fire recovery monitoring, and decision support systems for fire management. This study employs a new approach to leveraging Non-negative Matrix Factorization (NMF) for detecting fire-susceptible areas in the Kurdo-Zagrosian forests of Marivan and Sarvabad in Kurdistan Province, western Iran. The NMF is a ML method used for dimensionality reduction and feature extraction. NMF differs from traditional matrix factorization methods by enforcing non-negativity constraints on the factor matrices, making the resulting factors interpretable and often more suitable for real-world data analysis. Sentinel-2 satellite imagery, elevation, distance to the road network, and Zagros Grass Index (ZGI) have been used as the primary inputs of the model, combined with in situ data for verifying and interpreting the resulting maps. The results showed that, besides providing useful information in extracting fire susceptible areas, NMF handles wide study areas efficiently, especially for tasks like feature extraction from large-scale datasets such as satellite images or multispectral data. The results especially revealed that ZGI has specifically demonstrated improved accuracy and reliability. The resulting map also showed a very close overlap between the fired area provided by Sentinel imagery from 2021 to 2023 and the areas labeled as highly susceptible regions in 2020, especially when ZGI has been regarded between the input variables. © 2024 SPIE. fire susceptibility; Forest fire; Kurdo-Zagrosian forest; Machine Learning; NMF; ZGI Deforestation; Fire alarm systems; Fire extinguishers; Fire hazards; Forest ecology; Insurance; Matrix algebra; Photomapping; Premixed flames; Reforestation; Resource allocation; Risk assessment; Risk management; Risk perception; Smoke detectors; Features extraction; Fire susceptibility; Forest fires; Kurdo-zagrosian forest; Machine-learning; Matrix factorizations; Negative matrix factorization; Nonnegative matrix factorization; Western Iran; Zagros grass index; Non-negative matrix factorization";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.4;Climatological;1;Prevention
464;Predicting long term regional drought pattern in Northeast India using advanced statistical technique and wavelet-machine learning approach;Understanding drought and its multifaceted challenges is crucial for safeguarding food security, promoting environmental sustainability, and fostering socio-economic well-being across the globe. As a consequence of climate change and anthropogenic factors, the occurrence and severity of drought has risen globally. In India, droughts are regular phenomenon affecting about 16% area of country each year which leads to a loss of about 0.5–1% of country’s annual GDP. Hence, the study aims to analyse and predict the meteorological drought in northeast India during 1901 to 2015 using standardised precipitation index (SPI) and analytical techniques such as Mann–Kendall test (MK), innovative trend analysis (ITA), and wavelet approach. In addition, the periodicity of the drought was estimated using Morlet wavelet technique, while discrete wavelet transform (DWT) was applied for decomposing the time series SPI-6 & SPI-12. Study shows that the northeast India experienced moderate drought conditions (SPI-6) in short term and two significant severe droughts (SPI-12) in long term between 1901 and 2015. The trend analysis shows a significant increase in SPI-6 & SPI-12 (p-value 0.01). Further, the combination of parameters i.e. approximation and levels result in the best drought prediction model with higher correlation coefficient and lower error. By using PSO-REPTtree, this study pioneers the use of decomposed parameters to detect trends and develop a drought prediction model. The study is the first step towards establishing drought early warning system that will help decision-makers and farmers to mitigate the impact of drought at the regional level. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.;"Innovative Trend Analysis; Meteorological drought pattern; Particle swarm optimization; Reduced error pruning tree; Sequential Mann–Kendall test; Standardized precipitation index";"India; climate change; drought; food security; machine learning; trend analysis; wavelet analysis";"Predicting long term regional drought pattern in Northeast India using advanced statistical technique and wavelet-machine learning approach Understanding drought and its multifaceted challenges is crucial for safeguarding food security, promoting environmental sustainability, and fostering socio-economic well-being across the globe. As a consequence of climate change and anthropogenic factors, the occurrence and severity of drought has risen globally. In India, droughts are regular phenomenon affecting about 16% area of country each year which leads to a loss of about 0.5–1% of country’s annual GDP. Hence, the study aims to analyse and predict the meteorological drought in northeast India during 1901 to 2015 using standardised precipitation index (SPI) and analytical techniques such as Mann–Kendall test (MK), innovative trend analysis (ITA), and wavelet approach. In addition, the periodicity of the drought was estimated using Morlet wavelet technique, while discrete wavelet transform (DWT) was applied for decomposing the time series SPI-6 & SPI-12. Study shows that the northeast India experienced moderate drought conditions (SPI-6) in short term and two significant severe droughts (SPI-12) in long term between 1901 and 2015. The trend analysis shows a significant increase in SPI-6 & SPI-12 (p-value 0.01). Further, the combination of parameters i.e. approximation and levels result in the best drought prediction model with higher correlation coefficient and lower error. By using PSO-REPTtree, this study pioneers the use of decomposed parameters to detect trends and develop a drought prediction model. The study is the first step towards establishing drought early warning system that will help decision-makers and farmers to mitigate the impact of drought at the regional level. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG. Innovative Trend Analysis; Meteorological drought pattern; Particle swarm optimization; Reduced error pruning tree; Sequential Mann–Kendall test; Standardized precipitation index India; climate change; drought; food security; machine learning; trend analysis; wavelet analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
465;Towards Advanced Wildfire Analysis: A Siamese Network-Based Change Detection Approach through Self-Supervised Learning;Escalating wildfire incidents necessitate improved post-disaster management practices for more effective response and recovery. This study advances the integration of Earth Observation technologies into the wildfire damage assessment phase, contributing a novel approach to augment disaster recovery efforts. Multi-temporal satellite imaging is crucial for monitoring wildfire-affected areas, and the widespread availability of multispectral images with high revisit frequencies substantially improves the comprehensive study of these changes. This paper presents an examination of deep learning techniques for change detection, employing a Siamese convolutional neural network enhanced with an Atrous Spatial Pyramid Pooling block for efficient image data processing. The model is trained and validated on the “Sentinel-2 Wildfire Change Detection Dataset” (S2-WCD), a custom-made dataset aimed at change detection methodologies. By introducing this specialized dataset and applying advanced neural network techniques, the study fills crucial research gaps, offering improvements in wildfire disaster management, particularly in the critical recovery phase following wildfire events. © 2024 IEEE.;"Change Detection; Disaster Management; Earth Observation; Self-Supervised Learning; Sentinel-2; Wildfires";"Change detection; Image enhancement; Satellite imagery; Semi-supervised learning; Tropics; Change detection; Detection approach; Disaster management; Earth Observation Technology; Earth observations; Management practises; Network-based; Post-disaster management; Sentinel-2; Wildfire; Self-supervised learning";"Towards Advanced Wildfire Analysis: A Siamese Network-Based Change Detection Approach through Self-Supervised Learning Escalating wildfire incidents necessitate improved post-disaster management practices for more effective response and recovery. This study advances the integration of Earth Observation technologies into the wildfire damage assessment phase, contributing a novel approach to augment disaster recovery efforts. Multi-temporal satellite imaging is crucial for monitoring wildfire-affected areas, and the widespread availability of multispectral images with high revisit frequencies substantially improves the comprehensive study of these changes. This paper presents an examination of deep learning techniques for change detection, employing a Siamese convolutional neural network enhanced with an Atrous Spatial Pyramid Pooling block for efficient image data processing. The model is trained and validated on the “Sentinel-2 Wildfire Change Detection Dataset” (S2-WCD), a custom-made dataset aimed at change detection methodologies. By introducing this specialized dataset and applying advanced neural network techniques, the study fills crucial research gaps, offering improvements in wildfire disaster management, particularly in the critical recovery phase following wildfire events. © 2024 IEEE. Change Detection; Disaster Management; Earth Observation; Self-Supervised Learning; Sentinel-2; Wildfires Change detection; Image enhancement; Satellite imagery; Semi-supervised learning; Tropics; Change detection; Detection approach; Disaster management; Earth Observation Technology; Earth observations; Management practises; Network-based; Post-disaster management; Sentinel-2; Wildfire; Self-supervised learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;3;Response
466;Precipitation Retrieval in Tropical Cyclones by means of TROPICS Constellation and Neural Networks;This study focuses on precipitation retrievals over oceans in tropical cyclones by means of neural networks using data from the new NASA TROPICS (Time-Resolved Observations of Precipitation structure and storm Intensity with a Constellation of Smallsats) mission. Accurate monitoring of tropical cyclones is a major concern for both the scientific community and emergency management services due to the severe damage they cause, especially when they fall on population centers and surrounding areas. The TROPICS constellation consists of four CubeSats carrying on-board passive microwave radiometers providing high revisit time measurements over the Tropics with the aim to study in detail the structure and evolution of TCs during their lifecycle. A NN architecture for precipitation retrieval is developed and trained with data from the GPM (Global Precipitation Measurement) constellation providing reference value of precipitation and it is tested on an independent dataset constituted by TROPICS observations. An automatic spatial-temporal collocation procedure between TROPICS brightness temperatures and IMERG (Integrated Multi-satellitE Retrievals for GPM) data is performed in order to set up the training dataset. In this study the tropical storm Ida is considered as test case, and the preliminary results obtained are promising showing a R2 between modeled (NN precipitation outputs) and reference target (IMERG precipitation product) above 0.8. © 2024 SPIE.;"neural networks; passive microwave sensors; precipitation retrieval; tropical cyclones; TROPICS";"Microwave measurement; Microwave sensors; Risk management; Time measurement; Tropical cyclone; Tropics; Velocity measurement; Constellation networks; Global precipitation measurements; Neural-networks; Passive microwave sensors; Precipitation retrievals; Precipitation structure; Satellite retrieval; Time-resolved; Tropical cyclone; TROPICS; NASA";"Precipitation Retrieval in Tropical Cyclones by means of TROPICS Constellation and Neural Networks This study focuses on precipitation retrievals over oceans in tropical cyclones by means of neural networks using data from the new NASA TROPICS (Time-Resolved Observations of Precipitation structure and storm Intensity with a Constellation of Smallsats) mission. Accurate monitoring of tropical cyclones is a major concern for both the scientific community and emergency management services due to the severe damage they cause, especially when they fall on population centers and surrounding areas. The TROPICS constellation consists of four CubeSats carrying on-board passive microwave radiometers providing high revisit time measurements over the Tropics with the aim to study in detail the structure and evolution of TCs during their lifecycle. A NN architecture for precipitation retrieval is developed and trained with data from the GPM (Global Precipitation Measurement) constellation providing reference value of precipitation and it is tested on an independent dataset constituted by TROPICS observations. An automatic spatial-temporal collocation procedure between TROPICS brightness temperatures and IMERG (Integrated Multi-satellitE Retrievals for GPM) data is performed in order to set up the training dataset. In this study the tropical storm Ida is considered as test case, and the preliminary results obtained are promising showing a R2 between modeled (NN precipitation outputs) and reference target (IMERG precipitation product) above 0.8. © 2024 SPIE. neural networks; passive microwave sensors; precipitation retrieval; tropical cyclones; TROPICS Microwave measurement; Microwave sensors; Risk management; Time measurement; Tropical cyclone; Tropics; Velocity measurement; Constellation networks; Global precipitation measurements; Neural-networks; Passive microwave sensors; Precipitation retrievals; Precipitation structure; Satellite retrieval; Time-resolved; Tropical cyclone; TROPICS; NASA";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
467;Artificial Intelligence based Intensity Estimation of Tropical Cyclones in North Indian Ocean: Current Status, Challenges & Future Directions;"Tropical cyclones (TCs) that develop in the Arabian Sea or the Bay of Bengal pose a serious threat to the North Indian Ocean (NIO). The main agency in charge of monitoring tropical cyclones in the NIO is the India Meteorological Department (IMD), which also offers early warnings in accordance with the TC’s Intensity. The present paper has addressed the state of intensity estimation for thermal currents (TCs) worldwide, with a focus on the NIO region. Conventionally, the Dvorak technique and its various versions are being used for satellite images to detect the intensity of TCs manually; however, a lot of efforts have been invested to develop artificial intelligence based alternatives for this. The paper examines the importance of Doppler Weather Radar (DWR) for estimating the intensity of TCs and their advantages over satellite images. The performance of the current IMD system is also highlighted in this paper. The average annual absolute intensity forecast skill for TCs in the period of 2013 to 2023 has been reported to be 74.95%. Further, the paper outlines existing challenges in the present system of intensity estimation for TCs. Finally, the paper outlines future research directions and underscores the critical need for an efficient artificial intelligence-based system for better intensity estimation of TCs. © 2024 IEEE.";"Artificial Intelligence; Cyclone Intensity; Doppler Weather Radar; Machine learning";"Hurricanes; Machine learning; Time difference of arrival; Tropical engineering; Tropics; Arabian sea; Bay of Bengal; Current status; Cyclone intensity; Doppler weather radars; Intensity estimation; Machine-learning; North indian oceans; Satellite images; Tropical cyclone; Tropical cyclone";"Artificial Intelligence based Intensity Estimation of Tropical Cyclones in North Indian Ocean: Current Status, Challenges & Future Directions Tropical cyclones (TCs) that develop in the Arabian Sea or the Bay of Bengal pose a serious threat to the North Indian Ocean (NIO). The main agency in charge of monitoring tropical cyclones in the NIO is the India Meteorological Department (IMD), which also offers early warnings in accordance with the TC’s Intensity. The present paper has addressed the state of intensity estimation for thermal currents (TCs) worldwide, with a focus on the NIO region. Conventionally, the Dvorak technique and its various versions are being used for satellite images to detect the intensity of TCs manually; however, a lot of efforts have been invested to develop artificial intelligence based alternatives for this. The paper examines the importance of Doppler Weather Radar (DWR) for estimating the intensity of TCs and their advantages over satellite images. The performance of the current IMD system is also highlighted in this paper. The average annual absolute intensity forecast skill for TCs in the period of 2013 to 2023 has been reported to be 74.95%. Further, the paper outlines existing challenges in the present system of intensity estimation for TCs. Finally, the paper outlines future research directions and underscores the critical need for an efficient artificial intelligence-based system for better intensity estimation of TCs. © 2024 IEEE. Artificial Intelligence; Cyclone Intensity; Doppler Weather Radar; Machine learning Hurricanes; Machine learning; Time difference of arrival; Tropical engineering; Tropics; Arabian sea; Bay of Bengal; Current status; Cyclone intensity; Doppler weather radars; Intensity estimation; Machine-learning; North indian oceans; Satellite images; Tropical cyclone; Tropical cyclone";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
468;Development of Flood Inundation Mapping by Using Optimized Deep Learning Model;Accurate Flood Inundation Mapping (FIM) is of paramount importance for effective disaster risk reduction in the context of climate action. This paper presents an innovative approach to FIM using optimized deep learning models. Leveraging a vast historical dataset of flood events obtained from Radiant MLHub, which includes multispectral remote sensing data consisting of 12 spectral bands categorized as 'Flood' and 'No Flood,' a deep learning model is trained to predict flood extents with high accuracy. Bayesian Optimization (BO) and the Hyperband algorithm are employed during the hyperparameter tuning process of the deep learning models. Two base models are used: a Convolutional Neural Network (CNN) and the pre-trained Visual Geometry Group 16 (VGG16) model. To achieve the best performance, four scenarios-CNN-BO, CNN-Hyperband, VGG16-BO, and VGG16-Hyperband-are investigated. The results show that VGG16-Hyperband outperforms the other scenarios, achieving 87.3% validation accuracy.  © 2024 IEEE.;"Bayesian optimization; deep learning; flood inundation mapping; hyperband; hyperparameter tuning; multispectral remote sensing";"Adversarial machine learning; Contrastive Learning; Deep learning; Mapping; Tropics; Bayesian optimization; Convolutional neural network; Deep learning; Disaster risk reductions; Flood inundation mappings; Hyper-parameter; Hyperband; Hyperparameter tuning; Learning models; Multispectral remote sensing; Convolutional neural networks";"Development of Flood Inundation Mapping by Using Optimized Deep Learning Model Accurate Flood Inundation Mapping (FIM) is of paramount importance for effective disaster risk reduction in the context of climate action. This paper presents an innovative approach to FIM using optimized deep learning models. Leveraging a vast historical dataset of flood events obtained from Radiant MLHub, which includes multispectral remote sensing data consisting of 12 spectral bands categorized as 'Flood' and 'No Flood,' a deep learning model is trained to predict flood extents with high accuracy. Bayesian Optimization (BO) and the Hyperband algorithm are employed during the hyperparameter tuning process of the deep learning models. Two base models are used: a Convolutional Neural Network (CNN) and the pre-trained Visual Geometry Group 16 (VGG16) model. To achieve the best performance, four scenarios-CNN-BO, CNN-Hyperband, VGG16-BO, and VGG16-Hyperband-are investigated. The results show that VGG16-Hyperband outperforms the other scenarios, achieving 87.3% validation accuracy.  © 2024 IEEE. Bayesian optimization; deep learning; flood inundation mapping; hyperband; hyperparameter tuning; multispectral remote sensing Adversarial machine learning; Contrastive Learning; Deep learning; Mapping; Tropics; Bayesian optimization; Convolutional neural network; Deep learning; Disaster risk reductions; Flood inundation mappings; Hyper-parameter; Hyperband; Hyperparameter tuning; Learning models; Multispectral remote sensing; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
469;The Early-Warning System to Predict Sediment Flux in the Mountain Basin During Typhoon Events;The watershed sedimentation could be determined as the severe natural disaster source. The differences between the sediment yield estimation and the real situation are large due to the inadequacy of on-site measurement data. A robust early-warning approach that encloses both the physical mechanism and statistical analysis was proposed to predict the sedimentation and improve the deficit on the current methods. This three-phase early-warning system includes, Phase I (data collection), Phase II (data generation), and Phase III (AI prediction). In Phase I, HEC-HMS was applied to transform the measured precipitation data to the flow discharge from different sub-catchments. The empirical formulas of landslide volume and soil erosion were then adopted to provide reasonable boundary conditions to Phase II. A 2D model, SRH-2D, was verified with on-site data and conducted to obtain simulated data for the temporal variation of the sediment flux with different storm events. The simulated data can be the input for Phase III to train and test the artificial neural networks. Three prediction models presented a good performance from 1-h to 6-h lead time comparing to the observation. Even though the relatively poor performance was shown in the higher sediment discharge and the difference was gradually increasing with the longer forecast lead time, the prediction models still provide the proper prediction trend, especially the forecast lead time from 1-h to 3-h. This approach shows a significant improvement on the sediment prediction due to the sufficient simulated data, and can be well-applied to better manage sediment problems in advance. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.;"ANNs; Early-warning system; Multi-phase approach; Sediment flux; Typhoon events";"Catchments; Disasters; Prediction models; Runoff; Storms; Watersheds; ANN; Early Warning System; Leadtime; Mountain basins; Multiphase approach; Phase I; Phase II; Prediction modelling; Sediment flux; Typhoon event; Hurricanes";"The Early-Warning System to Predict Sediment Flux in the Mountain Basin During Typhoon Events The watershed sedimentation could be determined as the severe natural disaster source. The differences between the sediment yield estimation and the real situation are large due to the inadequacy of on-site measurement data. A robust early-warning approach that encloses both the physical mechanism and statistical analysis was proposed to predict the sedimentation and improve the deficit on the current methods. This three-phase early-warning system includes, Phase I (data collection), Phase II (data generation), and Phase III (AI prediction). In Phase I, HEC-HMS was applied to transform the measured precipitation data to the flow discharge from different sub-catchments. The empirical formulas of landslide volume and soil erosion were then adopted to provide reasonable boundary conditions to Phase II. A 2D model, SRH-2D, was verified with on-site data and conducted to obtain simulated data for the temporal variation of the sediment flux with different storm events. The simulated data can be the input for Phase III to train and test the artificial neural networks. Three prediction models presented a good performance from 1-h to 6-h lead time comparing to the observation. Even though the relatively poor performance was shown in the higher sediment discharge and the difference was gradually increasing with the longer forecast lead time, the prediction models still provide the proper prediction trend, especially the forecast lead time from 1-h to 3-h. This approach shows a significant improvement on the sediment prediction due to the sufficient simulated data, and can be well-applied to better manage sediment problems in advance. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024. ANNs; Early-warning system; Multi-phase approach; Sediment flux; Typhoon events Catchments; Disasters; Prediction models; Runoff; Storms; Watersheds; ANN; Early Warning System; Leadtime; Mountain basins; Multiphase approach; Phase I; Phase II; Prediction modelling; Sediment flux; Typhoon event; Hurricanes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
470;Earthquake Forecasting using Time Series Data;Predicting seismic events with high accuracy remains a crucial challenge due to the complex nature of earthquake phenomena and the devastating impact they can have on human life and infrastructure. This study introduces a novel approach by integrating convolutional neural networks (CNN) with long short-term memory (LSTM) units to enhance the prediction of earthquake events. Utilizing a comprehensive dataset that includes 37,478 global earthquake instances, the model processes spatial and temporal features such as geographic location, time, magnitude, depth, and additional derived time series data. Through rigorous preprocessing, including label encoding of geographical locations and normalization of all relevant features, we prepare the dataset for effective machine learning modelling. The proposed CNN+LSTM model aims to capture both spatial dependencies through CNN layers and temporal sequences via LSTM layers, addressing the dynamic and multi-dimensional nature of seismic data. Preliminary results indicate improved accuracy in predicting the magnitude and likelihood of earthquake occurrences, offering promising implications for early warning systems and preparedness strategies. Future work will focus on refining the model through hyperparameter tuning and expanding the dataset to include more granular temporal features. © 2024 IEEE.;"Earthquake; Model; Predictions; Time Series";"Earthquakes; Long short-term memory; Multilayer neural networks; Prediction models; Seismic response; Time series; Weather forecasting; Complex nature; Convolutional neural network; Earthquake forecasting; High-accuracy; Human infrastructure; Seismic event; Short term memory; Temporal features; Time-series data; Times series; Convolutional neural networks";"Earthquake Forecasting using Time Series Data Predicting seismic events with high accuracy remains a crucial challenge due to the complex nature of earthquake phenomena and the devastating impact they can have on human life and infrastructure. This study introduces a novel approach by integrating convolutional neural networks (CNN) with long short-term memory (LSTM) units to enhance the prediction of earthquake events. Utilizing a comprehensive dataset that includes 37,478 global earthquake instances, the model processes spatial and temporal features such as geographic location, time, magnitude, depth, and additional derived time series data. Through rigorous preprocessing, including label encoding of geographical locations and normalization of all relevant features, we prepare the dataset for effective machine learning modelling. The proposed CNN+LSTM model aims to capture both spatial dependencies through CNN layers and temporal sequences via LSTM layers, addressing the dynamic and multi-dimensional nature of seismic data. Preliminary results indicate improved accuracy in predicting the magnitude and likelihood of earthquake occurrences, offering promising implications for early warning systems and preparedness strategies. Future work will focus on refining the model through hyperparameter tuning and expanding the dataset to include more granular temporal features. © 2024 IEEE. Earthquake; Model; Predictions; Time Series Earthquakes; Long short-term memory; Multilayer neural networks; Prediction models; Seismic response; Time series; Weather forecasting; Complex nature; Convolutional neural network; Earthquake forecasting; High-accuracy; Human infrastructure; Seismic event; Short term memory; Temporal features; Time-series data; Times series; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
471;Smart Flood Detection and Communication in Hazardous Zones via MANET-Based IoT and Parallely Distributed Slimmable Neural Networks;Disasters can be mitigated by an early warning signal and proper communication within the hazardous environment using the MANET technology. However, the exact prediction of disaster situation is needed for the timely disaster management. Hence in addition to MANET technology, deep learning algorithms and Inter of Things (IoT) can also be integrated into disaster detection and communication systems. The presented research developed a novel parallely distributed slimmable neural network with orchard algorithm for the accurate flood disaster prediction. The model processed the dataset containing various measurements such as temperature, humidity, precipitation, air, soil moisture and rainfall level that are collected by the IoT sensor deployed in the various location of the hazardous environment. The gathered data are initially pre-processed using the correlation coefficient min-max normalization approach. Further, the relevant characteristics of the flood disaster from the data are extracted through spike driven transformer process. These refined and dimensionality reduced data are then entered into the proposed framework for the disaster prediction. Here the integrated orchard algorithm increased the disaster monitoring accuracy on the basis of better optimized parameters. The model resulted 95% accuracy, 93% precision, 94% recall and 96% f-score. Therefore, the presented method is an effective prediction mechanism for the disaster management in IoT MANET.  © 2024 IEEE.;"communication; deep learning; feature extraction; Flood disaster prediction; orchard algorithm";"Adaptive boosting; Deep neural networks; Dimensionality reduction; Image thinning; Deep learning; Disaster management; Disaster prediction; Features extraction; Flood disaster; Flood disaster prediction; Hazardous environment; MANET's; Neural-networks; Orchard algorithm; Soil moisture";"Smart Flood Detection and Communication in Hazardous Zones via MANET-Based IoT and Parallely Distributed Slimmable Neural Networks Disasters can be mitigated by an early warning signal and proper communication within the hazardous environment using the MANET technology. However, the exact prediction of disaster situation is needed for the timely disaster management. Hence in addition to MANET technology, deep learning algorithms and Inter of Things (IoT) can also be integrated into disaster detection and communication systems. The presented research developed a novel parallely distributed slimmable neural network with orchard algorithm for the accurate flood disaster prediction. The model processed the dataset containing various measurements such as temperature, humidity, precipitation, air, soil moisture and rainfall level that are collected by the IoT sensor deployed in the various location of the hazardous environment. The gathered data are initially pre-processed using the correlation coefficient min-max normalization approach. Further, the relevant characteristics of the flood disaster from the data are extracted through spike driven transformer process. These refined and dimensionality reduced data are then entered into the proposed framework for the disaster prediction. Here the integrated orchard algorithm increased the disaster monitoring accuracy on the basis of better optimized parameters. The model resulted 95% accuracy, 93% precision, 94% recall and 96% f-score. Therefore, the presented method is an effective prediction mechanism for the disaster management in IoT MANET.  © 2024 IEEE. communication; deep learning; feature extraction; Flood disaster prediction; orchard algorithm Adaptive boosting; Deep neural networks; Dimensionality reduction; Image thinning; Deep learning; Disaster management; Disaster prediction; Features extraction; Flood disaster; Flood disaster prediction; Hazardous environment; MANET's; Neural-networks; Orchard algorithm; Soil moisture";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
472;Drone-based Human Surveillance using YOLOv5 and Multi-Features;Drone use has expanded recently to include airborne photography, human action recognition (HAR), search and rescue (SAR), and surveillance. Drones have a distinct edge in human action recognition (HAR) due to their bird's-eye vision, but this also brings challenges such as shifting backgrounds and obscured perspectives, which call for creative solutions. To address these issues, we propose a method that processes video frames using preprocessing techniques, detects human presence using YOLO, and human silhouettes are extracted using morphological erosion. We focus on key skeletal landmarks such as the head, wrists, ankles, and neck to analyze human movement patterns. We enhance feature extraction with methods like MSER and HOG. Classification is performed using Naïve Bayes algorithm, with feature optimization achieved through the Grey Wolf optimizer. Our method is tested on the Drone-Action dataset and shows itself to be rather effective, detecting human activity with an astounding 84% accuracy. © 2024 IEEE.;"Classification; Grey wolf optimization; Human action recognition; multi-feature descriptor; Naive Bayes";"Aircraft detection; Eye protection; Feature extraction; Human engineering; Airborne photography; Feature descriptors; Gray wolf optimization; Gray wolves; Human-action recognition; Multi-feature descriptor; Multifeatures; Naive bayes; Optimisations; Search and rescue; Drones";"Drone-based Human Surveillance using YOLOv5 and Multi-Features Drone use has expanded recently to include airborne photography, human action recognition (HAR), search and rescue (SAR), and surveillance. Drones have a distinct edge in human action recognition (HAR) due to their bird's-eye vision, but this also brings challenges such as shifting backgrounds and obscured perspectives, which call for creative solutions. To address these issues, we propose a method that processes video frames using preprocessing techniques, detects human presence using YOLO, and human silhouettes are extracted using morphological erosion. We focus on key skeletal landmarks such as the head, wrists, ankles, and neck to analyze human movement patterns. We enhance feature extraction with methods like MSER and HOG. Classification is performed using Naïve Bayes algorithm, with feature optimization achieved through the Grey Wolf optimizer. Our method is tested on the Drone-Action dataset and shows itself to be rather effective, detecting human activity with an astounding 84% accuracy. © 2024 IEEE. Classification; Grey wolf optimization; Human action recognition; multi-feature descriptor; Naive Bayes Aircraft detection; Eye protection; Feature extraction; Human engineering; Airborne photography; Feature descriptors; Gray wolf optimization; Gray wolves; Human-action recognition; Multi-feature descriptor; Multifeatures; Naive bayes; Optimisations; Search and rescue; Drones";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
473;Identifying the pathways of extreme rainfall in South Africa using storm trajectory analysis and unsupervised machine learning techniques;This study has utilised National Oceanic and Atmospheric Administration (NOAA) NCEP/NCAR Reanalysis 1 project meteorological data and the HYSPLIT model to extract the air parcel trajectories for selected historical extreme rainfall events in South Africa. The k-means unsupervised machine learning algorithm has been used to cluster the resulting trajectories, and from this, the spatial origin of moisture for each of the rainfall events has been determined. It has been demonstrated that rainfall events on the east coast with moisture originating from the Indian Ocean have distinctly larger average maximum daily rainfall magnitudes (279 mm) compared to those that occur on the west coast with Atlantic Ocean influences (149 mm) and those events occurring in the central plateau (150 mm) where moisture has been continentally recirculated. Further, this study has suggested new metrics by which the HYSPLIT trajectories may be assessed and demonstrated the applicability of trajectory clustering in a region not previously studied. This insight may in future facilitate improved early warning systems based on monitoring of atmospheric systems, and an understanding of rainfall magnitudes and origins can be used to improve the prediction of design floods for infrastructure design. © 2024 The Authors.;"extreme rainfall; k-means clustering; South Africa; trajectories; unsupervised learning";NULL;"Identifying the pathways of extreme rainfall in South Africa using storm trajectory analysis and unsupervised machine learning techniques This study has utilised National Oceanic and Atmospheric Administration (NOAA) NCEP/NCAR Reanalysis 1 project meteorological data and the HYSPLIT model to extract the air parcel trajectories for selected historical extreme rainfall events in South Africa. The k-means unsupervised machine learning algorithm has been used to cluster the resulting trajectories, and from this, the spatial origin of moisture for each of the rainfall events has been determined. It has been demonstrated that rainfall events on the east coast with moisture originating from the Indian Ocean have distinctly larger average maximum daily rainfall magnitudes (279 mm) compared to those that occur on the west coast with Atlantic Ocean influences (149 mm) and those events occurring in the central plateau (150 mm) where moisture has been continentally recirculated. Further, this study has suggested new metrics by which the HYSPLIT trajectories may be assessed and demonstrated the applicability of trajectory clustering in a region not previously studied. This insight may in future facilitate improved early warning systems based on monitoring of atmospheric systems, and an understanding of rainfall magnitudes and origins can be used to improve the prediction of design floods for infrastructure design. © 2024 The Authors. extreme rainfall; k-means clustering; South Africa; trajectories; unsupervised learning NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.3;Meteorological;2;Preparation
474;Feature Selection for Geomagnetic Storm Forecasting Using Particle Swarm Optimisation;Geomagnetic activity is a significant research area due to its impact on navigation, satellites, and other technologies. The geomagnetic index Kp is particularly essential for monitoring such activity. Forecasting geomagnetic storms can provide early warnings, helping to mitigate potential disasters. Various methods, including empirical models, physics-based models, and machine learning models, are employed for forecasting. This study focuses on Machine learning models, emphasising selecting appropriate features to enhance prediction accuracy. The primary aim was to improve the 3-hour Kp predictions compared to prior research. Model performance was primarily evaluated using the R2 metric. Additionally, RMSE was calculated to ensure comparability with studies that did not use R2. The model's ability to detect storms was assessed by classifying results as Kp>5 (storm) or Kp<5 (not a storm), with the F1 score used as the evaluation metric for classification. The best model achieved an R2 of 0.71, an RMSE of 0.6671, and an F1 score of 0.6. © 2024 IEEE.;"GRU; Kp index; LSTM; PCA; PSO";"Adversarial machine learning; Disasters; Feature Selection; Prediction models; Satellite navigation aids; Storms; Tropics; Weather forecasting; F1 scores; Features selection; Geomagnetic storm; GRU; Kp index; LSTM; Machine learning models; PCA; PSO; Storm forecasting; Particle swarm optimization (PSO)";"Feature Selection for Geomagnetic Storm Forecasting Using Particle Swarm Optimisation Geomagnetic activity is a significant research area due to its impact on navigation, satellites, and other technologies. The geomagnetic index Kp is particularly essential for monitoring such activity. Forecasting geomagnetic storms can provide early warnings, helping to mitigate potential disasters. Various methods, including empirical models, physics-based models, and machine learning models, are employed for forecasting. This study focuses on Machine learning models, emphasising selecting appropriate features to enhance prediction accuracy. The primary aim was to improve the 3-hour Kp predictions compared to prior research. Model performance was primarily evaluated using the R2 metric. Additionally, RMSE was calculated to ensure comparability with studies that did not use R2. The model's ability to detect storms was assessed by classifying results as Kp>5 (storm) or Kp<5 (not a storm), with the F1 score used as the evaluation metric for classification. The best model achieved an R2 of 0.71, an RMSE of 0.6671, and an F1 score of 0.6. © 2024 IEEE. GRU; Kp index; LSTM; PCA; PSO Adversarial machine learning; Disasters; Feature Selection; Prediction models; Satellite navigation aids; Storms; Tropics; Weather forecasting; F1 scores; Features selection; Geomagnetic storm; GRU; Kp index; LSTM; Machine learning models; PCA; PSO; Storm forecasting; Particle swarm optimization (PSO)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
475;Comprehensive Risk Assessment and Mapping of Multi-Hazard Vulnerabilities in Vanuatu Island using Google Earth Engine and Remote Sensing Techniques;This study aims to conduct a comprehensive multi-hazard risk assessment and mapping for Vanuatu Island, located in the Oceania region. Utilizing Google Earth Engine and multispectral satellite imagery from Landsat and Sentinel platforms from the years 2014-2024, the research employs advanced remote sensing techniques. The methodology involves meticulous data collection, pre-processing, and application of sophisticated algorithms for extracting pertinent information related to natural disasters such as earthquakes, landslides, and floods. Satellite imagery undergoes rigorous radiometric and geometric correction, ensuring accurate analysis. Advanced image processing techniques, including change detection and classification algorithms, are implemented to derive critical information on terrain stability, land cover changes, and hydrological characteristics. Integration of high-resolution elevation models further enhances the precision of landslide susceptibility assessments. The resulting spatially explicit risk maps depict the distribution of multi-hazard vulnerabilities across Vanuatu Island. These maps serve as a valuable tool for policymakers, emergency responders, and stakeholders, facilitating informed decision-making in disaster preparedness, land-use planning, infrastructure development, and resource allocation. The spatial distribution of identified risks guides the prioritization of mitigation efforts, allowing for efficient resource allocation and enhanced resilience against natural hazards. This study contributes to the advancement of scientific methodologies for multi-hazard risk assessment in geographically complex regions.  Copyright © 2024 by the International Astronautical Federation (IAF). All rights reserved.;"Advanced Image Processing Techniques; Satellite Imagery; Vanuatu Island";"Emergency services; Landsat; Network security; Photomapping; Resource allocation; Risk analysis; Risk assessment; Tropics; Advanced image processing technique; Comprehensive risks; Google earths; Hazard risk assessments; Image processing technique; Multi-hazards; Remote sensing techniques; Risk mappings; Vanuatu; Vanuatu island; Landslides";"Comprehensive Risk Assessment and Mapping of Multi-Hazard Vulnerabilities in Vanuatu Island using Google Earth Engine and Remote Sensing Techniques This study aims to conduct a comprehensive multi-hazard risk assessment and mapping for Vanuatu Island, located in the Oceania region. Utilizing Google Earth Engine and multispectral satellite imagery from Landsat and Sentinel platforms from the years 2014-2024, the research employs advanced remote sensing techniques. The methodology involves meticulous data collection, pre-processing, and application of sophisticated algorithms for extracting pertinent information related to natural disasters such as earthquakes, landslides, and floods. Satellite imagery undergoes rigorous radiometric and geometric correction, ensuring accurate analysis. Advanced image processing techniques, including change detection and classification algorithms, are implemented to derive critical information on terrain stability, land cover changes, and hydrological characteristics. Integration of high-resolution elevation models further enhances the precision of landslide susceptibility assessments. The resulting spatially explicit risk maps depict the distribution of multi-hazard vulnerabilities across Vanuatu Island. These maps serve as a valuable tool for policymakers, emergency responders, and stakeholders, facilitating informed decision-making in disaster preparedness, land-use planning, infrastructure development, and resource allocation. The spatial distribution of identified risks guides the prioritization of mitigation efforts, allowing for efficient resource allocation and enhanced resilience against natural hazards. This study contributes to the advancement of scientific methodologies for multi-hazard risk assessment in geographically complex regions.  Copyright © 2024 by the International Astronautical Federation (IAF). All rights reserved. Advanced Image Processing Techniques; Satellite Imagery; Vanuatu Island Emergency services; Landsat; Network security; Photomapping; Resource allocation; Risk analysis; Risk assessment; Tropics; Advanced image processing technique; Comprehensive risks; Google earths; Hazard risk assessments; Image processing technique; Multi-hazards; Remote sensing techniques; Risk mappings; Vanuatu; Vanuatu island; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
476;Research on Agricultural and Animal Husbandry Ecological Evolution in North China Based on Computer Machine Learning and GIS Spatial Analysis;The agro-pastoral ecotone is an ecologically fragile and sensitive area. Based on multi-source data, this research analyzes the Geo-Informatic tupu and forecasts of the ecological evolution of Ordos city in the agro-pastoral zone from 1990 to 2020. The findings reveal that: (1) Fractional vegetation cover(FVC) has been in a tendency to decrease first and then continue to increase in the last 30 years, with a total of 85% of regions with medium or higher cover by 2020.(2)The Land-Use and Land-Cover Change(LUCC)has remained relatively stable over the last 30 years, with unused land dominating the stable areas (43.38%) and unused land changing to woodland dominating the changing areas (>14% / 10 a), and ecological restoration measures being effective, with farmland forecast being turned mainly to grassland {(9. 5 6%}) and unused land to woodland (9.20%) between 2020 and 2030.(3)In the last 30 years, the average annual temperature and annual equivalent precipitation have been fluctuating upwards, and the annual equivalent precipitation line has been moving in and out widely, with extreme drought (200mm) in 2000 and 2005, and the forecast indicates that temperature and humidity will continue to rise slowly in the future. It is found that the ecological quality of Ordos has fluctuated and improved over time due to the positive and negative effects of climate change and human activities. Therefore, ecological restoration and governance play a pivotal role, and scientific measures to deal with climate change in the future are essential to prevent ecological risks. © 2024 IEEE.;"Agro-pastoral ecotone; Ecological evolution; Geo-Informatic tupu; Ordos";"Anthropogenic; Forest ecology; Livestock; Agro-pastoral ecotones; Animal husbandry; Ecological evolution; Ecological restoration; Geo-informatic tupu; Geo-informatics; Machine-learning; North China; Ordo; Spatial analysis; Abiotic";"Research on Agricultural and Animal Husbandry Ecological Evolution in North China Based on Computer Machine Learning and GIS Spatial Analysis The agro-pastoral ecotone is an ecologically fragile and sensitive area. Based on multi-source data, this research analyzes the Geo-Informatic tupu and forecasts of the ecological evolution of Ordos city in the agro-pastoral zone from 1990 to 2020. The findings reveal that: (1) Fractional vegetation cover(FVC) has been in a tendency to decrease first and then continue to increase in the last 30 years, with a total of 85% of regions with medium or higher cover by 2020.(2)The Land-Use and Land-Cover Change(LUCC)has remained relatively stable over the last 30 years, with unused land dominating the stable areas (43.38%) and unused land changing to woodland dominating the changing areas (>14% / 10 a), and ecological restoration measures being effective, with farmland forecast being turned mainly to grassland {(9. 5 6%}) and unused land to woodland (9.20%) between 2020 and 2030.(3)In the last 30 years, the average annual temperature and annual equivalent precipitation have been fluctuating upwards, and the annual equivalent precipitation line has been moving in and out widely, with extreme drought (200mm) in 2000 and 2005, and the forecast indicates that temperature and humidity will continue to rise slowly in the future. It is found that the ecological quality of Ordos has fluctuated and improved over time due to the positive and negative effects of climate change and human activities. Therefore, ecological restoration and governance play a pivotal role, and scientific measures to deal with climate change in the future are essential to prevent ecological risks. © 2024 IEEE. Agro-pastoral ecotone; Ecological evolution; Geo-Informatic tupu; Ordos Anthropogenic; Forest ecology; Livestock; Agro-pastoral ecotones; Animal husbandry; Ecological evolution; Ecological restoration; Geo-informatic tupu; Geo-informatics; Machine-learning; North China; Ordo; Spatial analysis; Abiotic";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;1;Prevention
477;Regional Heatwave Prediction Using Deep Learning Models In Bangladesh;With the growing threat of heatwaves in Bangladesh due to climate change, predicting heatwave days has become vital for effective mitigation measures. In this study, a robust dataset of 25 years, acquired through the Bangladesh Meteorological Department, is capitalized to explore Long-Short Term Memory (LSTM), Bi-directional LSTM (Bi-LSTM), and Convolutional Neural Network (CNN) models for predicting heatwave in four heatwave-prone districts of Bangladesh: Dhaka, Rajshahi, Bogra and Dinajpur. The selected features are maximum temperature, humidity, dew point temperature, pressure, wind speed and sunshine. The dataset was divided into 80% for training, 10% for validation and 10% for testing. The predictive performance of BiLSTM, Conv1D, and LSTM models was evaluated with Accuracy, F1-score, R2 score. Bi-LSTM consistently showed high accuracy, with Dhaka and Rajshahi achieving 96.96% and 97.40%, respectively, and Bogra recorded the highest accuracy at 99.57%. In Dhaka, Bi-LSTM had the highest accuracy at 96.96%, followed by Conv1D at 95.65%, and LSTM at 93.04%. For the F1-score, Bi-LSTM led with 86.27%, and in R2 score, it performed best with 77.40%. These results demonstrate the superior performance of the Bi-LSTM model in predicting heatwave days, making it a beneficial tool for early warning and preparedness efforts. Despite the predominant use of statistical models on Bangladeshi datasets, this study explores the novel application of deep learning models like Bi-LSTM and CNNs for heatwave classification, which offer better identification of complex spatiotemporal patterns and are more resistant to anomalies. ©2024 IEEE.;"Bi-LSTM; Conv1D; Heatwave Prediction; LSTM";"Convolutional neural networks; Phase locked loops; Prediction models; Time difference of arrival; Weather forecasting; Bangladesh; Bi-directional; Bi-directional LSTM; Conv1d; F1 scores; Heatwave prediction; Heatwaves; High-accuracy; Learning models; Short term memory; Long short-term memory";"Regional Heatwave Prediction Using Deep Learning Models In Bangladesh With the growing threat of heatwaves in Bangladesh due to climate change, predicting heatwave days has become vital for effective mitigation measures. In this study, a robust dataset of 25 years, acquired through the Bangladesh Meteorological Department, is capitalized to explore Long-Short Term Memory (LSTM), Bi-directional LSTM (Bi-LSTM), and Convolutional Neural Network (CNN) models for predicting heatwave in four heatwave-prone districts of Bangladesh: Dhaka, Rajshahi, Bogra and Dinajpur. The selected features are maximum temperature, humidity, dew point temperature, pressure, wind speed and sunshine. The dataset was divided into 80% for training, 10% for validation and 10% for testing. The predictive performance of BiLSTM, Conv1D, and LSTM models was evaluated with Accuracy, F1-score, R2 score. Bi-LSTM consistently showed high accuracy, with Dhaka and Rajshahi achieving 96.96% and 97.40%, respectively, and Bogra recorded the highest accuracy at 99.57%. In Dhaka, Bi-LSTM had the highest accuracy at 96.96%, followed by Conv1D at 95.65%, and LSTM at 93.04%. For the F1-score, Bi-LSTM led with 86.27%, and in R2 score, it performed best with 77.40%. These results demonstrate the superior performance of the Bi-LSTM model in predicting heatwave days, making it a beneficial tool for early warning and preparedness efforts. Despite the predominant use of statistical models on Bangladeshi datasets, this study explores the novel application of deep learning models like Bi-LSTM and CNNs for heatwave classification, which offer better identification of complex spatiotemporal patterns and are more resistant to anomalies. ©2024 IEEE. Bi-LSTM; Conv1D; Heatwave Prediction; LSTM Convolutional neural networks; Phase locked loops; Prediction models; Time difference of arrival; Weather forecasting; Bangladesh; Bi-directional; Bi-directional LSTM; Conv1d; F1 scores; Heatwave prediction; Heatwaves; High-accuracy; Learning models; Short term memory; Long short-term memory";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
478;Building Damage Assessment to Facilitate Post-Earthquake Search and Rescue Missions by Leveraging a Machine Learning Algorithm;Earthquakes have a severe impact on people's lives and infrastructure. Many emergency institutes and search and rescue missions need accurate post-earthquake response strategies, particularly in building damage assessment. Traditional methods, relying on manual inspections, are inefficient compared to Machine Learning (ML) algorithms. Thus, Random Forest (RF) algorithms stand out because they handle diverse datasets effectively and minimize overfitting. The study outlines the methodology encompassing data preparation, exploratory analysis, feature engineering, and model building, employing a preprocessing pipeline integrating numerical and categorical features. Additionally, Principal Component Analysis (PCA) is applied to reduce dimensionality. The results of the RF model showed an accuracy of 94% and the highest F1-score of 97% among all the grades, demonstrating its efficacy in predicting damage grades post-earthquake. The results can help support better disaster management plans by helping to prioritize rescue operations and allocate resources wisely. © 2024 IEEE.;"building damage assessment; damage prediction; machine learning; post-earthquake; random forest";"Adversarial machine learning; Disaster prevention; Disasters; Earthquake engineering; Machine learning; Model buildings; Building damage; Building damage assessment; Damage assessments; Damage prediction; Machine-learning; Post-earthquake; Random forests; Rescue missions; Search and rescue; Search missions; Random forests";"Building Damage Assessment to Facilitate Post-Earthquake Search and Rescue Missions by Leveraging a Machine Learning Algorithm Earthquakes have a severe impact on people's lives and infrastructure. Many emergency institutes and search and rescue missions need accurate post-earthquake response strategies, particularly in building damage assessment. Traditional methods, relying on manual inspections, are inefficient compared to Machine Learning (ML) algorithms. Thus, Random Forest (RF) algorithms stand out because they handle diverse datasets effectively and minimize overfitting. The study outlines the methodology encompassing data preparation, exploratory analysis, feature engineering, and model building, employing a preprocessing pipeline integrating numerical and categorical features. Additionally, Principal Component Analysis (PCA) is applied to reduce dimensionality. The results of the RF model showed an accuracy of 94% and the highest F1-score of 97% among all the grades, demonstrating its efficacy in predicting damage grades post-earthquake. The results can help support better disaster management plans by helping to prioritize rescue operations and allocate resources wisely. © 2024 IEEE. building damage assessment; damage prediction; machine learning; post-earthquake; random forest Adversarial machine learning; Disaster prevention; Disasters; Earthquake engineering; Machine learning; Model buildings; Building damage; Building damage assessment; Damage assessments; Damage prediction; Machine-learning; Post-earthquake; Random forests; Rescue missions; Search and rescue; Search missions; Random forests";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
479;Design of an Improved Model for IoV Security Using Bioinspired Optimization & Scalable Blockchains;"Urgent to the rising threat of attacks, such as Sybil, masquerading, and flooding attacks, whereby the integrity of the network and its performance become compromised severely, is the need for improved security and communication efficiency in IoV. This work would then propose a holistic approach integrating Blockchain-enhanced SDN and QoS slicing, bioinspired routing, machine learning-driven intrusion detection, fuzzy logic-based trust management, and deep reinforcement learning to address all these challenges in an integrated way. This proposed model utilizes Blockchain-based Secure Slicing with Smart Contracts to enhance authentication and resource management and provide a reduction of up to 30% latency along with 98% accuracy in preventing attacks. The application of ACO for routing enhancement reduces packet loss up to 10% in flooding attacks. A hybrid Intrusion Detection System integrating both machine learning and blockchain, with accuracy at 99.2% for Sybil, masquerading, and flooding attacks; Fuzzy logic-based trust management re-validates the routing by checking the trust scores at 95%; and the DRL-based mitigation system that dynamically adapts to new threats provides reduction in network downtime up to 35-40%. This multi-layered approach enhances security and performance for communication with which the IoV networks are to be resilient against evolving threats.  © 2024 IEEE.";"Ant Colony Optimization; Blockchain; Deep Reinforcement Learning; IoV Security";"Blockchain; Deep learning; Deep reinforcement learning; Network intrusion; Reinforcement learning; Resource allocation; Ant colonies; Block-chain; Colony optimization; Flooding attacks; Fuzzy-Logic; IoV security; Machine-learning; Reinforcement learnings; Sybil attack; Trust management; Ant colony optimization";"Design of an Improved Model for IoV Security Using Bioinspired Optimization & Scalable Blockchains Urgent to the rising threat of attacks, such as Sybil, masquerading, and flooding attacks, whereby the integrity of the network and its performance become compromised severely, is the need for improved security and communication efficiency in IoV. This work would then propose a holistic approach integrating Blockchain-enhanced SDN and QoS slicing, bioinspired routing, machine learning-driven intrusion detection, fuzzy logic-based trust management, and deep reinforcement learning to address all these challenges in an integrated way. This proposed model utilizes Blockchain-based Secure Slicing with Smart Contracts to enhance authentication and resource management and provide a reduction of up to 30% latency along with 98% accuracy in preventing attacks. The application of ACO for routing enhancement reduces packet loss up to 10% in flooding attacks. A hybrid Intrusion Detection System integrating both machine learning and blockchain, with accuracy at 99.2% for Sybil, masquerading, and flooding attacks; Fuzzy logic-based trust management re-validates the routing by checking the trust scores at 95%; and the DRL-based mitigation system that dynamically adapts to new threats provides reduction in network downtime up to 35-40%. This multi-layered approach enhances security and performance for communication with which the IoV networks are to be resilient against evolving threats.  © 2024 IEEE. Ant Colony Optimization; Blockchain; Deep Reinforcement Learning; IoV Security Blockchain; Deep learning; Deep reinforcement learning; Network intrusion; Reinforcement learning; Resource allocation; Ant colonies; Block-chain; Colony optimization; Flooding attacks; Fuzzy-Logic; IoV security; Machine-learning; Reinforcement learnings; Sybil attack; Trust management; Ant colony optimization";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;1;Prevention
480;Research and Application of Dunhuang Mural Image Restoration Technology Based on Deep Learning;Throughout history, the Dunhuang murals have been a shining pearl of Chinese civilization with their unparalleled artistic charm and profound cultural heritage. However, the passage of time, natural erosion, and the impact of human factors are putting this precious cultural heritage to the difficult test. The peeling, fading, and vanishing of mural surfaces is not only serious damage to the integrity of the artwork but also a reckless erasure of historical information and cultural value. This research is dedicated to exploring mural restoration methods based on deep learning, aiming to overcome the challenges using advanced computer vision techniques and develop a new approach to cultural heritage protection.  © 2024 IEEE.;"Convolutional neural network (CNN); Dunhuang murals; Generative adversarial network (GAN); Image restoration";"Convolutional neural networks; Deep learning; Restoration; Adversarial networks; Charm+; Convolutional neural network; Cultural heritages; Dunhuang mural; Generative adversarial network; Historical information; Research and application; Technology-based; Generative adversarial networks";"Research and Application of Dunhuang Mural Image Restoration Technology Based on Deep Learning Throughout history, the Dunhuang murals have been a shining pearl of Chinese civilization with their unparalleled artistic charm and profound cultural heritage. However, the passage of time, natural erosion, and the impact of human factors are putting this precious cultural heritage to the difficult test. The peeling, fading, and vanishing of mural surfaces is not only serious damage to the integrity of the artwork but also a reckless erasure of historical information and cultural value. This research is dedicated to exploring mural restoration methods based on deep learning, aiming to overcome the challenges using advanced computer vision techniques and develop a new approach to cultural heritage protection.  © 2024 IEEE. Convolutional neural network (CNN); Dunhuang murals; Generative adversarial network (GAN); Image restoration Convolutional neural networks; Deep learning; Restoration; Adversarial networks; Charm+; Convolutional neural network; Cultural heritages; Dunhuang mural; Generative adversarial network; Historical information; Research and application; Technology-based; Generative adversarial networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;-1;NULL
481;Machine Learning-Driven Earthquake Early Warning Using Optical Fiber Mesh Networks;We demonstrate interconnected meshed optical networks as sensing-localization grid for earthquake early detection. We integrate noisy polarization evolution data induced by seven earthquakes, into a Waveplate model to enhance a machine-learning algorithm that accurately detects primary waves, improves urban safety and mimic real case scenarios.  © 2024 IEEE.;"early warning; earthquakes; machine learning; optical network; polarization; sensing; waveplate model";"Adversarial machine learning; Contrastive Learning; Earthquakes; Induced polarization logging; Machine learning; Mesh generation; MESH networking; Early warning; Earthquake early warning; Fiber meshes; Localisation; Machine-learning; Meshed optical networks; MeshNetworks; Optical-; Sensing; Waveplate model; Optical fibers";"Machine Learning-Driven Earthquake Early Warning Using Optical Fiber Mesh Networks We demonstrate interconnected meshed optical networks as sensing-localization grid for earthquake early detection. We integrate noisy polarization evolution data induced by seven earthquakes, into a Waveplate model to enhance a machine-learning algorithm that accurately detects primary waves, improves urban safety and mimic real case scenarios.  © 2024 IEEE. early warning; earthquakes; machine learning; optical network; polarization; sensing; waveplate model Adversarial machine learning; Contrastive Learning; Earthquakes; Induced polarization logging; Machine learning; Mesh generation; MESH networking; Early warning; Earthquake early warning; Fiber meshes; Localisation; Machine-learning; Meshed optical networks; MeshNetworks; Optical-; Sensing; Waveplate model; Optical fibers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
482;Monitoring of ground deformation before and after an earthquake using interferometric SAR;Ground deformation can be detected by processing SAR (Synthetic Aperture Radar) phase data acquired in different periods. However, due to the characteristics of SAR, it is difficult to determine the direction of ground deformation as the distance change between the satellite and the ground surface is observed. Therefore, on-site field observation is required since SAR observation results differ from the actual amount of ground deformation. This study aims to estimate ground deformation over a wide area using satellite SAR data, understand the disaster situation quickly, and reduce secondary damage risks caused by on-site field observation. In this paper, Interferometric SAR (InSAR) analysis is applied to estimate ground deformation caused by Kumamoto earthquake in 2016 from C-band SAR data on Sentinel-1 satellite. 2.5-dimensional analysis is conducted by combining the InSAR analysis results of the ascending and descending orbits, and the direction of ground deformation caused by earthquake is visualized using displacement vectors. Furthermore, changes in land cover classification, which classifies land based on surface vegetation and geology is performed by using time-series analysis based on machine learning techniques from optical sensor images obtained from Sentinel-2. The results show that the accurate understanding of the damage situation over a wide area is very effective in terms of estimating landslides and speeding up disaster response, such as evacuation. © 2024 SPIE.;"2.5-Dimension Data Analysis; Ascending and Descending Orbits; Displacement Vectors; Earthquake; Ground Deformation; Interferometric SAR; Land Cover Classification; Synthetic Aperture Radar(SAR)";"Earthquake effects; Emergency services; Network security; Radar imaging; Risk assessment; Tropics; 2.5-dimension data analyze; Ascending and descending orbit; Dimension Data; Displacement vectors; Field observations; Ground deformations; Interferometric synthetic aperture radars; Land cover classification; Phase data; Radar data; Risk perception";"Monitoring of ground deformation before and after an earthquake using interferometric SAR Ground deformation can be detected by processing SAR (Synthetic Aperture Radar) phase data acquired in different periods. However, due to the characteristics of SAR, it is difficult to determine the direction of ground deformation as the distance change between the satellite and the ground surface is observed. Therefore, on-site field observation is required since SAR observation results differ from the actual amount of ground deformation. This study aims to estimate ground deformation over a wide area using satellite SAR data, understand the disaster situation quickly, and reduce secondary damage risks caused by on-site field observation. In this paper, Interferometric SAR (InSAR) analysis is applied to estimate ground deformation caused by Kumamoto earthquake in 2016 from C-band SAR data on Sentinel-1 satellite. 2.5-dimensional analysis is conducted by combining the InSAR analysis results of the ascending and descending orbits, and the direction of ground deformation caused by earthquake is visualized using displacement vectors. Furthermore, changes in land cover classification, which classifies land based on surface vegetation and geology is performed by using time-series analysis based on machine learning techniques from optical sensor images obtained from Sentinel-2. The results show that the accurate understanding of the damage situation over a wide area is very effective in terms of estimating landslides and speeding up disaster response, such as evacuation. © 2024 SPIE. 2.5-Dimension Data Analysis; Ascending and Descending Orbits; Displacement Vectors; Earthquake; Ground Deformation; Interferometric SAR; Land Cover Classification; Synthetic Aperture Radar(SAR) Earthquake effects; Emergency services; Network security; Radar imaging; Risk assessment; Tropics; 2.5-dimension data analyze; Ascending and descending orbit; Dimension Data; Displacement vectors; Field observations; Ground deformations; Interferometric synthetic aperture radars; Land cover classification; Phase data; Radar data; Risk perception";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
483;Early Landslide Detection Using Wireless Sensor Network;This paper aims at detecting and warning of possible landslides well in advance, preventing a significant threat to both human life and infrastructure. Hence the development of effective early detection systems is no longer an option but a necessity. Here, we performed a comprehensive study on implementing a machine learning (ML) based early landslide detection system. The primary objective is to investigate the feasibility and accuracy of an ML algorithm in predicting landslides before they occur and providing a warning using a system of wireless sensors. We considered a dataset comprising of diverse geological and meteorological features, compiled to train and evaluate the ML algorithm. Here, we also explored the application of a supervised learning algorithm, to classify regions susceptible to landslides based on historical data and update its dataset based on data from landslides which it itself predicts, to be used for future reference. Additionally, the research investigates the integration of remote sensing data, including ground-based sensors, to provide real-time inputs periodically, such as the moisture level of the soil nearby as well as any change in the inclination of the land, for the ML models. The findings contribute to the growing body of knowledge in landslide detection and offer insights into the practical implementation of ML algorithms for early warning systems. The results highlight the potentials of ML to significantly enhance the accuracy and timeliness of landslide predictions, ultimately contributing to improved disaster preparedness and risk reduction strategies.  © 2024 IEEE.;"climate conditions; geological features; random forest; real-time data analysis; supervised learning";"Adversarial machine learning; Random forests; Supervised learning; Wireless sensor networks; Climate condition; Geological features; Human infrastructure; Landslide detection; Machine learning algorithms; Machine-learning; Random forests; Real time data analysis; Sensors network; Wireless sensor; Self-supervised learning";"Early Landslide Detection Using Wireless Sensor Network This paper aims at detecting and warning of possible landslides well in advance, preventing a significant threat to both human life and infrastructure. Hence the development of effective early detection systems is no longer an option but a necessity. Here, we performed a comprehensive study on implementing a machine learning (ML) based early landslide detection system. The primary objective is to investigate the feasibility and accuracy of an ML algorithm in predicting landslides before they occur and providing a warning using a system of wireless sensors. We considered a dataset comprising of diverse geological and meteorological features, compiled to train and evaluate the ML algorithm. Here, we also explored the application of a supervised learning algorithm, to classify regions susceptible to landslides based on historical data and update its dataset based on data from landslides which it itself predicts, to be used for future reference. Additionally, the research investigates the integration of remote sensing data, including ground-based sensors, to provide real-time inputs periodically, such as the moisture level of the soil nearby as well as any change in the inclination of the land, for the ML models. The findings contribute to the growing body of knowledge in landslide detection and offer insights into the practical implementation of ML algorithms for early warning systems. The results highlight the potentials of ML to significantly enhance the accuracy and timeliness of landslide predictions, ultimately contributing to improved disaster preparedness and risk reduction strategies.  © 2024 IEEE. climate conditions; geological features; random forest; real-time data analysis; supervised learning Adversarial machine learning; Random forests; Supervised learning; Wireless sensor networks; Climate condition; Geological features; Human infrastructure; Landslide detection; Machine learning algorithms; Machine-learning; Random forests; Real time data analysis; Sensors network; Wireless sensor; Self-supervised learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
484;Prediction of Power Grid Rainfall-Induced Flood Inundation Risk Based on Machine Learning and HEC-RAS;With the continuous expansion of power grids and the frequent occurrence of extreme weather events, rainfall-induced floods pose a significant threat to the safe operation of power grids. Predicting the risk of rainfall-induced flood inundation for power grids can effectively enhance their resilience to such disasters. Taking Fangshan District, Beijing as the study area, this research designs rainfall processes for different return periods based on the rainfall intensity formula, constructs rainfall-induced flood inundation processes using HEC-RAS, employs the Random Forest algorithm for learning and training, and validates the results using rainfall data from the 'July 23' event in Beijing. The results indicate that the rainfall-induced flood inundation processes can be effectively predicted. Furthermore, by integrating power grid site data, early warnings for rainfall-induced flood disasters are implemented for the power grid. Through this approach, the prediction of power grid rainfall-induced flood inundation risk is achieved, providing a scientific basis and decision-making support for power grid planning, operation, and maintenance. © 2024 IEEE.;"HEC-RAS; Rainfall and flooding; random forest; Risk of inundation of the power grid";"Flood damage; Random forests; Floodings; HEC-RAS; On-machines; Rainfall and flooding; Rainfall-induced; Random forests; Risk of inundation; Risk of inundation of the power grid; Risk-based; Rain";"Prediction of Power Grid Rainfall-Induced Flood Inundation Risk Based on Machine Learning and HEC-RAS With the continuous expansion of power grids and the frequent occurrence of extreme weather events, rainfall-induced floods pose a significant threat to the safe operation of power grids. Predicting the risk of rainfall-induced flood inundation for power grids can effectively enhance their resilience to such disasters. Taking Fangshan District, Beijing as the study area, this research designs rainfall processes for different return periods based on the rainfall intensity formula, constructs rainfall-induced flood inundation processes using HEC-RAS, employs the Random Forest algorithm for learning and training, and validates the results using rainfall data from the 'July 23' event in Beijing. The results indicate that the rainfall-induced flood inundation processes can be effectively predicted. Furthermore, by integrating power grid site data, early warnings for rainfall-induced flood disasters are implemented for the power grid. Through this approach, the prediction of power grid rainfall-induced flood inundation risk is achieved, providing a scientific basis and decision-making support for power grid planning, operation, and maintenance. © 2024 IEEE. HEC-RAS; Rainfall and flooding; random forest; Risk of inundation of the power grid Flood damage; Random forests; Floodings; HEC-RAS; On-machines; Rainfall and flooding; Rainfall-induced; Random forests; Risk of inundation; Risk of inundation of the power grid; Risk-based; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
485;Short-term Storm Surge Prediction based on Variational Mode Decomposition and Temporal Convolutional Network;Storm surges have caused severe casualties and economic losses. Accurate prediction of storm surge levels is crucial for disaster assessment, early warning, and effective disaster management. Compared to numerical simulation methods, machine learning approaches are relatively more efficient and direct. However, traditional machine learning models often fail to significantly remove noise and effectively capture long-term sequential dependencies. Therefore, this paper proposes a hybrid storm surge prediction model that combines Variational Mode Decomposition (VMD) with Temporal Convolutional Networks (TCN). By utilizing the VMD method, the model effectively decomposes storm surge signals into intrinsic mode functions (IMF) to enhance prediction accuracy by removing noise and extracting key features. The TCN component captures long-term dependencies in time series data, making the prediction of storm surge levels more precise. Upon comparison with conventional models such as Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), Gated Recurrent Units (GRU), and standard TCN, the proposed VMD-TCN model exhibits significant advantages. The forecasting performance metrics Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Correlation Coefficient (CC) are 7.2254 cm, 9.4548 cm, and 0.9251 respectively, indicating that the model proposed in this paper performs well in predicting storm surges. © 2024 IEEE.;"Storm Surge; Temporal Convolutional 1etworks TC1; Time Series Forecasting Variational Mode Decomposition VMD";"Convolutional neural networks; Image segmentation; Information management; Long short-term memory; Prediction models; Variational mode decomposition; Variational techniques; Weather forecasting; Accurate prediction; Convolutional networks; Economic loss; Mode decomposition; Prediction-based; Storm surges; Temporal convolutional 1etwork TC1; Time series forecasting; Time series forecasting variational mode decomposition variational mode decomposition; Mean square error";"Short-term Storm Surge Prediction based on Variational Mode Decomposition and Temporal Convolutional Network Storm surges have caused severe casualties and economic losses. Accurate prediction of storm surge levels is crucial for disaster assessment, early warning, and effective disaster management. Compared to numerical simulation methods, machine learning approaches are relatively more efficient and direct. However, traditional machine learning models often fail to significantly remove noise and effectively capture long-term sequential dependencies. Therefore, this paper proposes a hybrid storm surge prediction model that combines Variational Mode Decomposition (VMD) with Temporal Convolutional Networks (TCN). By utilizing the VMD method, the model effectively decomposes storm surge signals into intrinsic mode functions (IMF) to enhance prediction accuracy by removing noise and extracting key features. The TCN component captures long-term dependencies in time series data, making the prediction of storm surge levels more precise. Upon comparison with conventional models such as Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), Gated Recurrent Units (GRU), and standard TCN, the proposed VMD-TCN model exhibits significant advantages. The forecasting performance metrics Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Correlation Coefficient (CC) are 7.2254 cm, 9.4548 cm, and 0.9251 respectively, indicating that the model proposed in this paper performs well in predicting storm surges. © 2024 IEEE. Storm Surge; Temporal Convolutional 1etworks TC1; Time Series Forecasting Variational Mode Decomposition VMD Convolutional neural networks; Image segmentation; Information management; Long short-term memory; Prediction models; Variational mode decomposition; Variational techniques; Weather forecasting; Accurate prediction; Convolutional networks; Economic loss; Mode decomposition; Prediction-based; Storm surges; Temporal convolutional 1etwork TC1; Time series forecasting; Time series forecasting variational mode decomposition variational mode decomposition; Mean square error";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
486;A PSO-CNN-BiLSTM Model for Predicting Leaf Area Index;Ecosystem restoration monitoring is a field that has been increasingly gaining popularity over the last few years. This field aims to assess the effectiveness of restoration efforts and guide adaptive management strategies. To do this, experts require several metrics to continuously monitor various ecological indicators. One of these metric is the Leaf Area Index (LAI). This metric plays a crucial role in monitoring environmental problems like land degradation and soil erosion. However, manuel large-scale measurement is difficult due to the requirements of labor and time. In this context, the purpose of this research is to obtain the LAI prediction over large areas with lower cost and shorter time by using deep learning and satellite remote sensing. This paper proposes a PSO-CNN-BiLSTM method to predict the LAI of the next several months. This method comprise Particle Swarm Optimization (PSO), Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM). CNN is used to extract the features of the remote sensing data. BiLSTM uses the extracted feature data to capture the dependencies of the time-series data with seasonal characteristic. PSO is used to optimise the hyperparameters of CNN and BiLSTM. To prove the effectiveness of the proposed method, the PSO-CNN-BiLSTM and other five methods are trained and tested on the publicly available dataset ERA5-land and GIMMS. The results show that our method outperforms the state-of-the-art methods in the prediction of LAI. © 2024 IEEE.;"LAI; PSO-CNN-BiLSTM";"Convolutional neural networks; Direct air capture; Ecosystems; Environmental monitoring; Long short-term memory; Tropics; A-particles; Adaptive Management; Convolutional neural network; Ecosystem restoration; Leaf Area Index; Memory modeling; Particle swarm; Particle swarm optimization-convolutional neural network-bidirectional long short-term memory; Short term memory; Swarm optimization; Restoration";"A PSO-CNN-BiLSTM Model for Predicting Leaf Area Index Ecosystem restoration monitoring is a field that has been increasingly gaining popularity over the last few years. This field aims to assess the effectiveness of restoration efforts and guide adaptive management strategies. To do this, experts require several metrics to continuously monitor various ecological indicators. One of these metric is the Leaf Area Index (LAI). This metric plays a crucial role in monitoring environmental problems like land degradation and soil erosion. However, manuel large-scale measurement is difficult due to the requirements of labor and time. In this context, the purpose of this research is to obtain the LAI prediction over large areas with lower cost and shorter time by using deep learning and satellite remote sensing. This paper proposes a PSO-CNN-BiLSTM method to predict the LAI of the next several months. This method comprise Particle Swarm Optimization (PSO), Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM). CNN is used to extract the features of the remote sensing data. BiLSTM uses the extracted feature data to capture the dependencies of the time-series data with seasonal characteristic. PSO is used to optimise the hyperparameters of CNN and BiLSTM. To prove the effectiveness of the proposed method, the PSO-CNN-BiLSTM and other five methods are trained and tested on the publicly available dataset ERA5-land and GIMMS. The results show that our method outperforms the state-of-the-art methods in the prediction of LAI. © 2024 IEEE. LAI; PSO-CNN-BiLSTM Convolutional neural networks; Direct air capture; Ecosystems; Environmental monitoring; Long short-term memory; Tropics; A-particles; Adaptive Management; Convolutional neural network; Ecosystem restoration; Leaf Area Index; Memory modeling; Particle swarm; Particle swarm optimization-convolutional neural network-bidirectional long short-term memory; Short term memory; Swarm optimization; Restoration";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
487;Predicting Earthquake Damage and Rehabilitation Intervention using Adaptive Fuzzy C-Means-Based Support Vector Machine;Earthquakes are the greatest rates of human loss among natural disasters in a past 20 years. Predicting rehabilitation intervention and damage grades is significant, especially in the moderate aftermath of a strong earthquake as prioritized in post-earthquake housing rescue requires data about damage extent. However, building collapses caused by earthquakes lead to huge loss of property and life. Therefore, an Adaptive Fuzzy C-means-based Support Vector Machine (AFCM-SVM) is proposed to predict earthquake damage and rehabilitation intervention using Machine Learning (ML). Initially, the xBD dataset is employed to evaluate the proposed technique and min-max normalization is established which enhances numerical stability. The AFCM is utilized to segment the building effectively and Gray-Level Co-occurrence Matrix (GLCM) is employed for extraction. Finally, SVM is performed to predict the earthquake damage and rehabilitation intervention. When compared with existing approaches like Convolutional Neural Networks (CNN) and Siamese Neural Networks, the AFCM-SVM achieves a better f1-score of 0.945 respectively. © 2024 IEEE.;"adaptive fuzzy c-means; earthquake; gray-level co-occurrence matrix; rehabilitation intervention; support vector machine";"Disasters; Earthquake effects; Fuzzy neural networks; Support vector machines; Adaptive fuzzy c-means; Adaptive-fuzzy; C-means; Earthquake damages; Gray-level co-occurrence matrix; Grey-level co-occurrence matrixes; Natural disasters; Rehabilitation intervention; Strong earthquakes; Support vectors machine; Convolutional neural networks";"Predicting Earthquake Damage and Rehabilitation Intervention using Adaptive Fuzzy C-Means-Based Support Vector Machine Earthquakes are the greatest rates of human loss among natural disasters in a past 20 years. Predicting rehabilitation intervention and damage grades is significant, especially in the moderate aftermath of a strong earthquake as prioritized in post-earthquake housing rescue requires data about damage extent. However, building collapses caused by earthquakes lead to huge loss of property and life. Therefore, an Adaptive Fuzzy C-means-based Support Vector Machine (AFCM-SVM) is proposed to predict earthquake damage and rehabilitation intervention using Machine Learning (ML). Initially, the xBD dataset is employed to evaluate the proposed technique and min-max normalization is established which enhances numerical stability. The AFCM is utilized to segment the building effectively and Gray-Level Co-occurrence Matrix (GLCM) is employed for extraction. Finally, SVM is performed to predict the earthquake damage and rehabilitation intervention. When compared with existing approaches like Convolutional Neural Networks (CNN) and Siamese Neural Networks, the AFCM-SVM achieves a better f1-score of 0.945 respectively. © 2024 IEEE. adaptive fuzzy c-means; earthquake; gray-level co-occurrence matrix; rehabilitation intervention; support vector machine Disasters; Earthquake effects; Fuzzy neural networks; Support vector machines; Adaptive fuzzy c-means; Adaptive-fuzzy; C-means; Earthquake damages; Gray-level co-occurrence matrix; Grey-level co-occurrence matrixes; Natural disasters; Rehabilitation intervention; Strong earthquakes; Support vectors machine; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
488;Accelerating Disaster Response with Deep Learning and Image Processing Techniques;This study examines the use of artificial intelligence-based image segmentation and image processing techniques in disaster management. The aim of the study is to integrate artificial intelligence and image processing techniques for fast and effective response in disaster areas to facilitate disaster management. Our hypothesis is that fast and accurate analysis can be performed with high-performance and fast AI-based segmentation models and image processing techniques on images taken from UAVs and satellites in disaster areas. In this process, important regions in the images are identified by applying segmentation processes and masks are created. Then, using these masks, numerical results can be obtained with image processing techniques with low computational cost. Thus, it is aimed to make fast and accurate decisions in disaster management. In this study, popular segmentation models were compared and analyzed using 2343 images obtained from Floodnet dataset. The results show that the SegFormer model provides detailed damage analysis and contour area or connected component analysis, which can provide both detailed and numerically accurate results for disaster management. This study reveals that the use of image processing and artificial intelligence in disaster management can improve response processes by increasing operational efficiency. ©2024 IEEE.;"deep learning; Disaster management; flood areas; image segmentation";"Image enhancement; Image segmentation; Deep learning; Disaster areas; Disaster management; Disaster-response; Flood areas; Image processing technique; Images segmentations; Intelligence processing; Segmentation models; Segmentation processing; Deep learning";"Accelerating Disaster Response with Deep Learning and Image Processing Techniques This study examines the use of artificial intelligence-based image segmentation and image processing techniques in disaster management. The aim of the study is to integrate artificial intelligence and image processing techniques for fast and effective response in disaster areas to facilitate disaster management. Our hypothesis is that fast and accurate analysis can be performed with high-performance and fast AI-based segmentation models and image processing techniques on images taken from UAVs and satellites in disaster areas. In this process, important regions in the images are identified by applying segmentation processes and masks are created. Then, using these masks, numerical results can be obtained with image processing techniques with low computational cost. Thus, it is aimed to make fast and accurate decisions in disaster management. In this study, popular segmentation models were compared and analyzed using 2343 images obtained from Floodnet dataset. The results show that the SegFormer model provides detailed damage analysis and contour area or connected component analysis, which can provide both detailed and numerically accurate results for disaster management. This study reveals that the use of image processing and artificial intelligence in disaster management can improve response processes by increasing operational efficiency. ©2024 IEEE. deep learning; Disaster management; flood areas; image segmentation Image enhancement; Image segmentation; Deep learning; Disaster areas; Disaster management; Disaster-response; Flood areas; Image processing technique; Images segmentations; Intelligence processing; Segmentation models; Segmentation processing; Deep learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
489;Exploration Robot Based On YOLOv8 Algorithm;Incidents or natural disasters, such as earthquakes, often create hazardous environments that are inaccessible to human rescuers. The extreme-condition exploration robot is the solution to it. The robot is designed to operate in these kinds of extreme conditions to assist in search and rescue operations. Equipped with sensors and utilizing machine learning capabilities, the robot can navigate through debris, detect gases, detect humans, and transmit real-time data to the rescue team. In this work, we propose a prototype extreme-condition exploration robot. The robot is equipped with an ESP32-CAM module. The video can be streamed to the server for object detection tasks. To get faster streaming times, JPEG compression is employed. Then, the YOLOv8 algorithm is employed for object detection. The YOLOv8 model is trained with 180 epochs to be able to detect three different classes. The model reached a precision of 0.951 for all classes and 0.928 mAP50. The model surpasses our target accuracy of 75% for our application. © 2024 IEEE.;NULL;"Adversarial machine learning; Disasters; Robot learning; Exploration robots; Extreme conditions; Gas detect; Hazardous environment; Learning capabilities; Machine-learning; Natural disasters; Objects detection; Real-time data; Search and rescue operations; Robots";"Exploration Robot Based On YOLOv8 Algorithm Incidents or natural disasters, such as earthquakes, often create hazardous environments that are inaccessible to human rescuers. The extreme-condition exploration robot is the solution to it. The robot is designed to operate in these kinds of extreme conditions to assist in search and rescue operations. Equipped with sensors and utilizing machine learning capabilities, the robot can navigate through debris, detect gases, detect humans, and transmit real-time data to the rescue team. In this work, we propose a prototype extreme-condition exploration robot. The robot is equipped with an ESP32-CAM module. The video can be streamed to the server for object detection tasks. To get faster streaming times, JPEG compression is employed. Then, the YOLOv8 algorithm is employed for object detection. The YOLOv8 model is trained with 180 epochs to be able to detect three different classes. The model reached a precision of 0.951 for all classes and 0.928 mAP50. The model surpasses our target accuracy of 75% for our application. © 2024 IEEE. NULL Adversarial machine learning; Disasters; Robot learning; Exploration robots; Extreme conditions; Gas detect; Hazardous environment; Learning capabilities; Machine-learning; Natural disasters; Objects detection; Real-time data; Search and rescue operations; Robots";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
490;Analysis of Seismic Activity Using Deep Learning;This research is a deep learning infrastructure designed for real-time seismic analysis and aims to optimize the precision of earthquake detection while magnitude estimations. The system gathers real-time seismic data from a network of sensors, including accelerometers, geophones, and seismometers. To analyze advanced deep learning models such as Long Short-Term Memory (LSTM) networks, Convolutional Neural Networks (CNNs), and autoencoders. CNNs are employed for feature extraction from raw seismic waveforms, capturing spatial hierarchies in the data, while LSTM networks model temporal dependencies in the seismic signals to enhance event classification over time. Furthermore, autoencoders are used for anomaly detection, identifying unusual seismic patterns that may indicate earthquake precursors. This hybrid approach enables the system to automatically classify seismic events, predict earthquake magnitudes, and differentiate between natural and anthropogenic seismicity with high precision. Integrating these models into an optimized data processing pipeline ensures that the system operates near-real-time, making it highly suitable for earthquake early warning systems. Initial results show that this approach significantly outperforms traditional methods such as signal thresholding and standard machine learning models in terms of both accuracy and computational efficiency. © 2024 IEEE.;"deep learning; Earthquake; seismic analysis";"Anomaly detection; Deep neural networks; Earthquake effects; Long short-term memory; Network coding; Pipeline processing systems; Seismic response; Spatio-temporal data; Auto encoders; Convolutional neural network; Deep learning; Earthquake detection; Magnitude estimation; Memory network; Real- time; Seismic activity; Seismic analysis; Short term memory; Convolutional neural networks";"Analysis of Seismic Activity Using Deep Learning This research is a deep learning infrastructure designed for real-time seismic analysis and aims to optimize the precision of earthquake detection while magnitude estimations. The system gathers real-time seismic data from a network of sensors, including accelerometers, geophones, and seismometers. To analyze advanced deep learning models such as Long Short-Term Memory (LSTM) networks, Convolutional Neural Networks (CNNs), and autoencoders. CNNs are employed for feature extraction from raw seismic waveforms, capturing spatial hierarchies in the data, while LSTM networks model temporal dependencies in the seismic signals to enhance event classification over time. Furthermore, autoencoders are used for anomaly detection, identifying unusual seismic patterns that may indicate earthquake precursors. This hybrid approach enables the system to automatically classify seismic events, predict earthquake magnitudes, and differentiate between natural and anthropogenic seismicity with high precision. Integrating these models into an optimized data processing pipeline ensures that the system operates near-real-time, making it highly suitable for earthquake early warning systems. Initial results show that this approach significantly outperforms traditional methods such as signal thresholding and standard machine learning models in terms of both accuracy and computational efficiency. © 2024 IEEE. deep learning; Earthquake; seismic analysis Anomaly detection; Deep neural networks; Earthquake effects; Long short-term memory; Network coding; Pipeline processing systems; Seismic response; Spatio-temporal data; Auto encoders; Convolutional neural network; Deep learning; Earthquake detection; Magnitude estimation; Memory network; Real- time; Seismic activity; Seismic analysis; Short term memory; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
491;Artificial Intelligence Driven Earthquakes Early Detection in Noisy Urban Areas;In this study, we introduce a novel approach for early earthquake detection in urban environments with high ambient noise. By using machine learning techniques to analyze the polarization alterations of light traveling along an existing traffic-carrying optical network, we demonstrate a cost-effective, secure, and efficient solution for detecting primary earthquake wave in noisy conditions. Detecting the primary wave preceding a destructive surface earthquake wave enables the rapid initiation of emergency plans, ensuring timely implementation of earthquake countermeasures. Our methodology involves collecting large dataset of polarization angular speed evolution along a fiber cable to conduct a Monte Carlo analysis, after integrating the strains induced by car passages over those induced by real earthquake ground displacement values. This dataset trains a machine learning model that leverages a deep learning architecture based on Long Short - Term Memory layers and attention mechanism. The model's training and validation show high accuracy rates, implying that additional training is unlikely to yield significant improvements, resulting in a 99% correct detection rate for multi-class classification of all events. The model demonstrates high accuracy in distinguishing between various environmental events, providing accurate early warning signals upon primary wave detection.  © 2024 IEEE.;"ambient-noise; early-warnings; earthquakes; machine-learning; optical-networks; polarization; sensing; waveplate-model";"Adversarial machine learning; Earthquake effects; Earthquake engineering; Fiber to the x; Image coding; Image thinning; Induced polarization logging; Light polarization; Seismic waves; Ambient noise; Early warning; Earthquake detection; Earthquake wave; High-accuracy; Machine-learning; Optical-; Sensing; Urban areas; Waveplate model; Deep learning";"Artificial Intelligence Driven Earthquakes Early Detection in Noisy Urban Areas In this study, we introduce a novel approach for early earthquake detection in urban environments with high ambient noise. By using machine learning techniques to analyze the polarization alterations of light traveling along an existing traffic-carrying optical network, we demonstrate a cost-effective, secure, and efficient solution for detecting primary earthquake wave in noisy conditions. Detecting the primary wave preceding a destructive surface earthquake wave enables the rapid initiation of emergency plans, ensuring timely implementation of earthquake countermeasures. Our methodology involves collecting large dataset of polarization angular speed evolution along a fiber cable to conduct a Monte Carlo analysis, after integrating the strains induced by car passages over those induced by real earthquake ground displacement values. This dataset trains a machine learning model that leverages a deep learning architecture based on Long Short - Term Memory layers and attention mechanism. The model's training and validation show high accuracy rates, implying that additional training is unlikely to yield significant improvements, resulting in a 99% correct detection rate for multi-class classification of all events. The model demonstrates high accuracy in distinguishing between various environmental events, providing accurate early warning signals upon primary wave detection.  © 2024 IEEE. ambient-noise; early-warnings; earthquakes; machine-learning; optical-networks; polarization; sensing; waveplate-model Adversarial machine learning; Earthquake effects; Earthquake engineering; Fiber to the x; Image coding; Image thinning; Induced polarization logging; Light polarization; Seismic waves; Ambient noise; Early warning; Earthquake detection; Earthquake wave; High-accuracy; Machine-learning; Optical-; Sensing; Urban areas; Waveplate model; Deep learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
492;A real-time seismic damage prediction framework based on machine learning for earthquake early warning;For implementing earthquake early warning (EEW), real-time parameters of ground motion (GM) which can be obtained at the beginning of earthquake wave arrival (i.e., P-wave arrival) are usually used to estimate the intensity measure (IM) of the following GM for supporting alarm decision. However, prior studies have proved that the estimated IM is hard to relate structural damage effectively, thus affecting the decision-making behavior and resulting in more economic losses and casualties. Although the real-time parameters may be adopted to directly predict structural damage, it is difficult to construct the accurate mathematical model between the parameters and structural damage index. To solve the above problems, a machine learning-based real-time damage prediction framework that can directly predict the structural maximum inter-story drift ratio (MIDR) after P-wave arrival is proposed for use in EEW. Within the framework of the proposed method, the database for training the prediction model come from the accurate nonlinear time-history analyses of 32 typical building types under the real GM records, where the real-time characteristic parameters of P-wave of the records are taken as the inputs. To achieve accurate and efficient prediction, a total of 32 alternative characteristic parameters of P-wave are selected and a parameter optimization approach is developed to remove the redundant and useless parameters for various type of building structure. Moreover, an interpretable approach is adopted to explain the effects of different input parameters on the prediction results and demonstrate the reliability of the proposed method. Finally, the framework is thoroughly assessed and investigated through two seismic damage cases and an occurred seismic event, which has an accuracy rate above 96.4% on average for risk-consistent early-warning trigger classification. These results show that the proposed method have the ability of continuously stable and excellent prediction power when using it for EEW. © 2023 John Wiley & Sons Ltd.;"damage prediction; earthquake early warning; information theory; interpretability of the trained model; machine learning; the optimal parameter combination";"Damage detection; Decision making; Earthquake effects; Information theory; Losses; Machine learning; Risk assessment; Seismic waves; Structural analysis; Damage prediction; Earthquake early warning; Ground-motion; Interpretability; Interpretability of the trained model; Machine-learning; Real- time; Structural damages; The optimal parameter combination; decision making; early warning system; earthquake event; ground motion; machine learning; prediction; seismic design; Forecasting";"A real-time seismic damage prediction framework based on machine learning for earthquake early warning For implementing earthquake early warning (EEW), real-time parameters of ground motion (GM) which can be obtained at the beginning of earthquake wave arrival (i.e., P-wave arrival) are usually used to estimate the intensity measure (IM) of the following GM for supporting alarm decision. However, prior studies have proved that the estimated IM is hard to relate structural damage effectively, thus affecting the decision-making behavior and resulting in more economic losses and casualties. Although the real-time parameters may be adopted to directly predict structural damage, it is difficult to construct the accurate mathematical model between the parameters and structural damage index. To solve the above problems, a machine learning-based real-time damage prediction framework that can directly predict the structural maximum inter-story drift ratio (MIDR) after P-wave arrival is proposed for use in EEW. Within the framework of the proposed method, the database for training the prediction model come from the accurate nonlinear time-history analyses of 32 typical building types under the real GM records, where the real-time characteristic parameters of P-wave of the records are taken as the inputs. To achieve accurate and efficient prediction, a total of 32 alternative characteristic parameters of P-wave are selected and a parameter optimization approach is developed to remove the redundant and useless parameters for various type of building structure. Moreover, an interpretable approach is adopted to explain the effects of different input parameters on the prediction results and demonstrate the reliability of the proposed method. Finally, the framework is thoroughly assessed and investigated through two seismic damage cases and an occurred seismic event, which has an accuracy rate above 96.4% on average for risk-consistent early-warning trigger classification. These results show that the proposed method have the ability of continuously stable and excellent prediction power when using it for EEW. © 2023 John Wiley & Sons Ltd. damage prediction; earthquake early warning; information theory; interpretability of the trained model; machine learning; the optimal parameter combination Damage detection; Decision making; Earthquake effects; Information theory; Losses; Machine learning; Risk assessment; Seismic waves; Structural analysis; Damage prediction; Earthquake early warning; Ground-motion; Interpretability; Interpretability of the trained model; Machine-learning; Real- time; Structural damages; The optimal parameter combination; decision making; early warning system; earthquake event; ground motion; machine learning; prediction; seismic design; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
493;Enhancing Human Safety through Early Detection of Floods and Landslides in Sri Lanka;In Sri Lanka, natural disasters particularly floods and landslides pose significant threats to public safety as well as the economic stability. This research introduces an innovative approach to improve disaster response by utilizing machine learning (ML) and Internet of Things (IoT) technologies. The system ensures extensive and reliable data transfer across rural areas by implementing Long Range (LoRa) technology to establish a Scalable Wireless Sensor Network. Coupled with an integrated alarm system, it markedly improves response times by providing precise, real-time predictions and detections of floods and landslides. The flood prediction model continuously improves through real-time data from IoT devices, enhancing its predictive accuracy. Meanwhile, landslide detection employs image processing techniques to analyze satellite images for early signs such as cracks, and the landslide prediction model provides early warnings. These systems are trained using specific datasets from Sri Lanka, ensuring high relevance and accuracy. Additionally, a web application delivers location-specific disaster notifications, significantly advancing disaster management and preparedness. This approach not only safeguards communities but also protects economic assets, a critical need demonstrated by the devastating 2016 landslides in Aranyaka. © 2024 IEEE.;"Advanced Warning Systems; Community Safety Systems; IoT; LoRa; Machine Learning; Real-Time Data Processing; Remote Sensing";"Landslides; Machine learning; Metadata; Proximity sensors; Wireless sensor networks; Advanced warning system; Advanced warnings; Community safety; Community safety system; Long range; Machine-learning; Prediction modelling; Real-time data processing; Remote-sensing; Sri Lanka; Remote sensing";"Enhancing Human Safety through Early Detection of Floods and Landslides in Sri Lanka In Sri Lanka, natural disasters particularly floods and landslides pose significant threats to public safety as well as the economic stability. This research introduces an innovative approach to improve disaster response by utilizing machine learning (ML) and Internet of Things (IoT) technologies. The system ensures extensive and reliable data transfer across rural areas by implementing Long Range (LoRa) technology to establish a Scalable Wireless Sensor Network. Coupled with an integrated alarm system, it markedly improves response times by providing precise, real-time predictions and detections of floods and landslides. The flood prediction model continuously improves through real-time data from IoT devices, enhancing its predictive accuracy. Meanwhile, landslide detection employs image processing techniques to analyze satellite images for early signs such as cracks, and the landslide prediction model provides early warnings. These systems are trained using specific datasets from Sri Lanka, ensuring high relevance and accuracy. Additionally, a web application delivers location-specific disaster notifications, significantly advancing disaster management and preparedness. This approach not only safeguards communities but also protects economic assets, a critical need demonstrated by the devastating 2016 landslides in Aranyaka. © 2024 IEEE. Advanced Warning Systems; Community Safety Systems; IoT; LoRa; Machine Learning; Real-Time Data Processing; Remote Sensing Landslides; Machine learning; Metadata; Proximity sensors; Wireless sensor networks; Advanced warning system; Advanced warnings; Community safety; Community safety system; Long range; Machine-learning; Prediction modelling; Real-time data processing; Remote-sensing; Sri Lanka; Remote sensing";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
494;OVERWATCH-INTEGRATED HOLOGRAPHIC CRISIS MANAGEMENT MAP: POTENTIAL IMPACT IN THE COPERNICUS EMERGENCY MANAGEMENT RAPID MAPPING MAPS PRODUCTION WORKFLOW;Climate change heightens the frequency of natural disasters, including forest fires and floods, causing severe problems such as fatalities and economic losses. The OVERWATCH project, leveraging the latest technologies of AI, drones, and Earth observation data, aims at developing an integrated system to manage disaster events. Among its objectives, the projects proposes to improve some data processing steps within the workflow of the CEMS Rapid Mapping service, by using machine learning for automatic burned and flooded area delineation. This work outlines current progresses in Earth Observation and AI components of the OVERWATCH system, focusing on automated burned area assessment. In this context, we exploit several data sources, including Copernicus EMS and EFFIS, to train a delineation model on historical wildfires, validating the results on recent large-scale CEMS RM activations. © 2024, Italian Society of Remote Sensing. All rights reserved.;"AI model; CEMS RM; OVERWATCH; Sentinel-2; wildfire delineation";NULL;"OVERWATCH-INTEGRATED HOLOGRAPHIC CRISIS MANAGEMENT MAP: POTENTIAL IMPACT IN THE COPERNICUS EMERGENCY MANAGEMENT RAPID MAPPING MAPS PRODUCTION WORKFLOW Climate change heightens the frequency of natural disasters, including forest fires and floods, causing severe problems such as fatalities and economic losses. The OVERWATCH project, leveraging the latest technologies of AI, drones, and Earth observation data, aims at developing an integrated system to manage disaster events. Among its objectives, the projects proposes to improve some data processing steps within the workflow of the CEMS Rapid Mapping service, by using machine learning for automatic burned and flooded area delineation. This work outlines current progresses in Earth Observation and AI components of the OVERWATCH system, focusing on automated burned area assessment. In this context, we exploit several data sources, including Copernicus EMS and EFFIS, to train a delineation model on historical wildfires, validating the results on recent large-scale CEMS RM activations. © 2024, Italian Society of Remote Sensing. All rights reserved. AI model; CEMS RM; OVERWATCH; Sentinel-2; wildfire delineation NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
495;Interactive AI -Based Navigation Application for Educational Mitigation of Volcanic Eruption Disasters;Volcanic eruption is a natural disaster characterized by releasing magma, gas, and volcanic ash from a volcano. Such eruptions can have significant impacts on both human populations and the environment. In Indonesia, volcanic eruptions are frequent, underscoring the importance of disaster mitigation to reduce potential losses. Effective preparedness can save lives and mitigate negative impacts on communities. Consequently, capacity-building in volcanic disaster mitigation is a top priority within Indonesia's disaster risk management strategies. The integration of machine learning and artificial intelligence (AI) into disaster mitigation offers substantial opportunities to enhance decision-making effectiveness and speed. This study develops an AI-based interactive navigation application for educational purposes in volcanic eruption disaster mitigation. The application is designed to increase public awareness of the risks posed by volcanic activity and the necessary mitigation measures. Featuring tools such as navigation, evacuation route planning, e-book access, and interactive quizzes, the application provides accurate, real-time information. Available on the Play Store, this application offers public practical access to disaster mitigation education. Leveraging AI technology, this tool aims to effectively prepare communities to face the threat of volcanic eruptions in Indonesia.  © 2024 IEEE.;"Artificial Intelligence; Disaster Mitigation; Education; Navigation Apps; Volcanic Eruption";"Decision making; Disaster prevention; Disasters; Electronic publishing; Public risks; Risk assessment; Risk management; Capacity building; Disaster mitigation; Human population; Indonesia; Natural disasters; Navigation app; Potential loss; Volcanic ash; Volcanic disasters; Volcanic eruptions; Volcanoes";"Interactive AI -Based Navigation Application for Educational Mitigation of Volcanic Eruption Disasters Volcanic eruption is a natural disaster characterized by releasing magma, gas, and volcanic ash from a volcano. Such eruptions can have significant impacts on both human populations and the environment. In Indonesia, volcanic eruptions are frequent, underscoring the importance of disaster mitigation to reduce potential losses. Effective preparedness can save lives and mitigate negative impacts on communities. Consequently, capacity-building in volcanic disaster mitigation is a top priority within Indonesia's disaster risk management strategies. The integration of machine learning and artificial intelligence (AI) into disaster mitigation offers substantial opportunities to enhance decision-making effectiveness and speed. This study develops an AI-based interactive navigation application for educational purposes in volcanic eruption disaster mitigation. The application is designed to increase public awareness of the risks posed by volcanic activity and the necessary mitigation measures. Featuring tools such as navigation, evacuation route planning, e-book access, and interactive quizzes, the application provides accurate, real-time information. Available on the Play Store, this application offers public practical access to disaster mitigation education. Leveraging AI technology, this tool aims to effectively prepare communities to face the threat of volcanic eruptions in Indonesia.  © 2024 IEEE. Artificial Intelligence; Disaster Mitigation; Education; Navigation Apps; Volcanic Eruption Decision making; Disaster prevention; Disasters; Electronic publishing; Public risks; Risk assessment; Risk management; Capacity building; Disaster mitigation; Human population; Indonesia; Natural disasters; Navigation app; Potential loss; Volcanic ash; Volcanic disasters; Volcanic eruptions; Volcanoes";-1;Não Classificado;NULL;1.1;Geological;2;Preparation
496;Earthquake Disaster Response and Management Based on Intelligent Detection System;Earthquakes are a highly hazardous natural disaster. Remote sensing is a type of image data primarily based on large-scale image data, which can assist rescue personnel in disaster assessment and rescue work. At present, earthquake damage monitoring based on multi-temporal remote sensing images before and after earthquakes and post earthquake building damage monitoring are important means of building damage monitoring in current remote sensing images. Due to the difficulty in quickly obtaining pre earthquake data, it is particularly important to use high-resolution remote sensing images only after earthquakes for damage assessment. This article proposed an intelligent detection system based on deep learning, aiming to improve the efficiency of earthquake disaster response and management. The study selected image data before and after the Kumamoto earthquake in Japan, constructed a building damage dataset, and trained and validated a building damage identification model. The experimental results showed that in the fourth round, the loss value dropped to 0.18 and the training time was also reduced. This method can effectively identify the damage to buildings after earthquakes, providing strong support for earthquake disaster response and management. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.;"Deep Learning; Earthquake Disaster Impact Assessment; Image Information Processing; Intelligent Detection System";"Earthquake effects; Deep learning; Detection system; Disaster management; Disaster-response; Earthquake disaster; Earthquake disaster impact assessment; Image information processing; Impact assessments; Intelligent detection; Intelligent detection system; Disasters";"Earthquake Disaster Response and Management Based on Intelligent Detection System Earthquakes are a highly hazardous natural disaster. Remote sensing is a type of image data primarily based on large-scale image data, which can assist rescue personnel in disaster assessment and rescue work. At present, earthquake damage monitoring based on multi-temporal remote sensing images before and after earthquakes and post earthquake building damage monitoring are important means of building damage monitoring in current remote sensing images. Due to the difficulty in quickly obtaining pre earthquake data, it is particularly important to use high-resolution remote sensing images only after earthquakes for damage assessment. This article proposed an intelligent detection system based on deep learning, aiming to improve the efficiency of earthquake disaster response and management. The study selected image data before and after the Kumamoto earthquake in Japan, constructed a building damage dataset, and trained and validated a building damage identification model. The experimental results showed that in the fourth round, the loss value dropped to 0.18 and the training time was also reduced. This method can effectively identify the damage to buildings after earthquakes, providing strong support for earthquake disaster response and management. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024. Deep Learning; Earthquake Disaster Impact Assessment; Image Information Processing; Intelligent Detection System Earthquake effects; Deep learning; Detection system; Disaster management; Disaster-response; Earthquake disaster; Earthquake disaster impact assessment; Image information processing; Impact assessments; Intelligent detection; Intelligent detection system; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
497;Estimating Earthquake Magnitude using Simple Deep Learning Methods;Rapid and precise magnitude estimation is critical for effective earthquake early warning systems and other seismic applications. This study introduces a novel deep learning-based approach for magnitude estimation that leverages extracted features from earthquake signals. The proposed model offers several advantages, including computational efficiency, adaptability to various feature sets, and the ability to operate on resource-constrained devices. Trained and evaluated on a substantial dataset, the model demonstrates robust performance across a wide range of earthquake magnitudes, making it a promising candidate for integration into seismic applications. © 2024 IEEE.;"Convolutional Neural Networks; Deep Learning Models; Machine Learning; Magnitude Estimation";"Contrastive Learning; Convolutional neural networks; Deep neural networks; Deep reinforcement learning; Earthquakes; Convolutional neural network; Deep learning model; Earthquake early warning systems; Earthquake magnitudes; Learning methods; Learning models; Machine-learning; Magnitude estimation; Seismic application; Simple++; Adversarial machine learning";"Estimating Earthquake Magnitude using Simple Deep Learning Methods Rapid and precise magnitude estimation is critical for effective earthquake early warning systems and other seismic applications. This study introduces a novel deep learning-based approach for magnitude estimation that leverages extracted features from earthquake signals. The proposed model offers several advantages, including computational efficiency, adaptability to various feature sets, and the ability to operate on resource-constrained devices. Trained and evaluated on a substantial dataset, the model demonstrates robust performance across a wide range of earthquake magnitudes, making it a promising candidate for integration into seismic applications. © 2024 IEEE. Convolutional Neural Networks; Deep Learning Models; Machine Learning; Magnitude Estimation Contrastive Learning; Convolutional neural networks; Deep neural networks; Deep reinforcement learning; Earthquakes; Convolutional neural network; Deep learning model; Earthquake early warning systems; Earthquake magnitudes; Learning methods; Learning models; Machine-learning; Magnitude estimation; Seismic application; Simple++; Adversarial machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
498;Cyclone Intensity Estimation Using Quantum Machine Learning;This research addresses the critical challenges of cyclone analysis and prediction, aiming to significantly advance the accuracy and efficiency of our understanding of cyclone behavior. Cyclones are powerful and destructive weather phe-nomena that can cause significant damage to life and property. Accurately predicting their intensity and potential impact is vital for timely evacuations and mitigation strategies. Current classical CNN models often struggle with classifying cyclones, mainly when their intensities exhibit subtle differences. Leveraging the NRLMRY (Naval Research Laboratory Monterey) Tropical Cyclone Data (TCDAT), we employ Quantum Machine Learning (QML) to enhance cyclone intensity estimation. By adopting a Quantum-Classical hybrid approach, which harnesses the power of quantum computing principles, we seek to overcome these limitations, providing more precise intensity estimations. Beyond intensity estimation, this project extends its focus to predicting the potential severity of damage that cyclones may inflict on cyclone-prone regions. Utilizing the same quantum-powered approach, we aspire to accurately classify whether a cyclone poses a significant threat to these vulnerable areas. Our holistic research endeavors to enhance the reliability of cyclone intensity estimation and damage prediction. Ultimately, integrating quantum machine learning and convolutional neural networks in cyclone analysis promises to contribute significantly to more effective disaster preparedness and response measures, safeguarding lives and property in cyclone-prone regions.  © 2024 IEEE.;"Cyclone Estimation; Quantum Feature Maps; Quantum Machine Learning; Quantum Neural Networks; Tropical Cyclone";"Convolutional neural networks; Machine learning; Quantum electronics; Quantum optics; Risk assessment; Tropical cyclone; Weather forecasting; Cyclone estimation; Feature map; Intensity estimation; Machine-learning; Quantum feature map; Quantum features; Quantum machine learning; Quantum machines; Quantum neural networks; Tropical cyclone; Quantum computers";"Cyclone Intensity Estimation Using Quantum Machine Learning This research addresses the critical challenges of cyclone analysis and prediction, aiming to significantly advance the accuracy and efficiency of our understanding of cyclone behavior. Cyclones are powerful and destructive weather phe-nomena that can cause significant damage to life and property. Accurately predicting their intensity and potential impact is vital for timely evacuations and mitigation strategies. Current classical CNN models often struggle with classifying cyclones, mainly when their intensities exhibit subtle differences. Leveraging the NRLMRY (Naval Research Laboratory Monterey) Tropical Cyclone Data (TCDAT), we employ Quantum Machine Learning (QML) to enhance cyclone intensity estimation. By adopting a Quantum-Classical hybrid approach, which harnesses the power of quantum computing principles, we seek to overcome these limitations, providing more precise intensity estimations. Beyond intensity estimation, this project extends its focus to predicting the potential severity of damage that cyclones may inflict on cyclone-prone regions. Utilizing the same quantum-powered approach, we aspire to accurately classify whether a cyclone poses a significant threat to these vulnerable areas. Our holistic research endeavors to enhance the reliability of cyclone intensity estimation and damage prediction. Ultimately, integrating quantum machine learning and convolutional neural networks in cyclone analysis promises to contribute significantly to more effective disaster preparedness and response measures, safeguarding lives and property in cyclone-prone regions.  © 2024 IEEE. Cyclone Estimation; Quantum Feature Maps; Quantum Machine Learning; Quantum Neural Networks; Tropical Cyclone Convolutional neural networks; Machine learning; Quantum electronics; Quantum optics; Risk assessment; Tropical cyclone; Weather forecasting; Cyclone estimation; Feature map; Intensity estimation; Machine-learning; Quantum feature map; Quantum features; Quantum machine learning; Quantum machines; Quantum neural networks; Tropical cyclone; Quantum computers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
499;Mapping dynamic human sentiments of heat exposure with location-based social media data;Understanding urban heat exposure dynamics is critical for public health, urban management, and climate change resilience. Near real-time analysis of urban heat enables quick decision-making and timely resource allocation, thereby enhancing the well-being of urban residents, especially during heatwaves or electricity shortages. To serve this purpose, we develop a cyberGIS framework to analyze and visualize human sentiments of heat exposure dynamically based on near real-time location-based social media (LBSM) data. Large volumes and low-cost LBSM data, together with a content analysis algorithm based on natural language processing are used effectively to generate near real-time heat exposure maps from human sentiments on social media at both city and national scales with km spatial resolution and census tract spatial unit. We conducted a case study to visualize and analyze human sentiments of heat exposure in Chicago and the United States in September 2021. Enabled with high-performance computing, dynamic visualization of heat exposure is achieved with fine spatiotemporal scales while heat exposure detected from social media data can be used to understand heat exposure from a human perspective and allow timely responses to extreme heat.HIGHLIGHTS Near real-time and high spatial resolution mapping of human sentiments of heat exposure with Twitter data An integrated cyberGIS and machine learning framework for visualizing heat exposure with Twitter data Human sentiment of heat exposure mapping in the City of Chicago and the United States. © 2024 Informa UK Limited, trading as Taylor & Francis Group.;"CyberGIS; heat exposure; location-based social media; urban heat";"Chicago; Illinois; United States; air temperature; data set; GIS; heat wave; integrated approach; Internet; machine learning; mapping method; public health; real time; social media; spatial resolution; temperature effect; urban climate; visualization";"Mapping dynamic human sentiments of heat exposure with location-based social media data Understanding urban heat exposure dynamics is critical for public health, urban management, and climate change resilience. Near real-time analysis of urban heat enables quick decision-making and timely resource allocation, thereby enhancing the well-being of urban residents, especially during heatwaves or electricity shortages. To serve this purpose, we develop a cyberGIS framework to analyze and visualize human sentiments of heat exposure dynamically based on near real-time location-based social media (LBSM) data. Large volumes and low-cost LBSM data, together with a content analysis algorithm based on natural language processing are used effectively to generate near real-time heat exposure maps from human sentiments on social media at both city and national scales with km spatial resolution and census tract spatial unit. We conducted a case study to visualize and analyze human sentiments of heat exposure in Chicago and the United States in September 2021. Enabled with high-performance computing, dynamic visualization of heat exposure is achieved with fine spatiotemporal scales while heat exposure detected from social media data can be used to understand heat exposure from a human perspective and allow timely responses to extreme heat.HIGHLIGHTS Near real-time and high spatial resolution mapping of human sentiments of heat exposure with Twitter data An integrated cyberGIS and machine learning framework for visualizing heat exposure with Twitter data Human sentiment of heat exposure mapping in the City of Chicago and the United States. © 2024 Informa UK Limited, trading as Taylor & Francis Group. CyberGIS; heat exposure; location-based social media; urban heat Chicago; Illinois; United States; air temperature; data set; GIS; heat wave; integrated approach; Internet; machine learning; mapping method; public health; real time; social media; spatial resolution; temperature effect; urban climate; visualization";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
500;Geospatial Analysis of Extreme Temperature Impacts in Agricultural Systems Using Machine Learning;"Recent higher temperatures, observed compared to previous years, have led to more frequent and intense heat waves, adversely affecting agriculture and human health. This paper explores using Inverse Distance Weighting (IDW) and Kriging to develop GIS layers for a heat-based risk index. Recent increases in global mean temperatures have led to more frequent and intense heat waves, impacting food supply, vegetation, and human health. Optimal plant growth temperatures in temperate climates range from 20°C to 30°C, with nighttime temperatures also being critical. Exceeding these temperature ranges can reduce plant immunity, cause disease, and result in poor crop yields. For example, mango cultivars need a ""safe""temperature range of 10°C to 12°C to avoid damage. The intensification of higher temperatures established July 2023 record the hottest days on Earth, highlighting the need for effective monitoring and management tools. IDW, a deterministic method, offers a straightforward and efficient approach by assuming closer points have more influence. Kriging, a probabilistic geostatistical technique, accounts for spatial autocorrelation and prediction uncertainty, making it suitable for complex datasets. The study compares these methods' performance in creating continuous surfaces representing heat-based risk. IDW is simple and efficient but less accurate in sparsely populated areas, while Kriging is more accurate and detailed but requires more computational resources and expertise. An automated workflow for data preprocessing, model training, and cross-validation ensures consistent results. The study recommends adopting IDW for initial assessments and transitioning to Kriging as expertise grows. Enhanced data collection on temperature and population density is also advised to improve GIS layer accuracy, aiding better decision-making and resource allocation to mitigate extreme heat effects on agriculture and public health. © 2024 IEEE.";"Anomaly detection; Climate change; Complex systems; Risk management; System identification";"Decision making; Fertilizers; Health risks; Inverse problems; Resource allocation; Risk assessment; Risk management; Anomaly detection; GIS layers; Heatwaves; Highest temperature; Human health; Inverse distance weighting; Kriging; Risks management; System-identification; Temperature range";"Geospatial Analysis of Extreme Temperature Impacts in Agricultural Systems Using Machine Learning Recent higher temperatures, observed compared to previous years, have led to more frequent and intense heat waves, adversely affecting agriculture and human health. This paper explores using Inverse Distance Weighting (IDW) and Kriging to develop GIS layers for a heat-based risk index. Recent increases in global mean temperatures have led to more frequent and intense heat waves, impacting food supply, vegetation, and human health. Optimal plant growth temperatures in temperate climates range from 20°C to 30°C, with nighttime temperatures also being critical. Exceeding these temperature ranges can reduce plant immunity, cause disease, and result in poor crop yields. For example, mango cultivars need a ""safe""temperature range of 10°C to 12°C to avoid damage. The intensification of higher temperatures established July 2023 record the hottest days on Earth, highlighting the need for effective monitoring and management tools. IDW, a deterministic method, offers a straightforward and efficient approach by assuming closer points have more influence. Kriging, a probabilistic geostatistical technique, accounts for spatial autocorrelation and prediction uncertainty, making it suitable for complex datasets. The study compares these methods' performance in creating continuous surfaces representing heat-based risk. IDW is simple and efficient but less accurate in sparsely populated areas, while Kriging is more accurate and detailed but requires more computational resources and expertise. An automated workflow for data preprocessing, model training, and cross-validation ensures consistent results. The study recommends adopting IDW for initial assessments and transitioning to Kriging as expertise grows. Enhanced data collection on temperature and population density is also advised to improve GIS layer accuracy, aiding better decision-making and resource allocation to mitigate extreme heat effects on agriculture and public health. © 2024 IEEE. Anomaly detection; Climate change; Complex systems; Risk management; System identification Decision making; Fertilizers; Health risks; Inverse problems; Resource allocation; Risk assessment; Risk management; Anomaly detection; GIS layers; Heatwaves; Highest temperature; Human health; Inverse distance weighting; Kriging; Risks management; System-identification; Temperature range";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
501;Damage Evaluation Following Natural Disasters Using Deep Learning;Natural catastrophes including flooding, tornadoes, earthquakes, and wildfires have been occurring more frequently over the past few decades as a result of global warming and climate change. Therefore, it is more crucial than ever to give emergency response workers accurate and timely information to enable them to respond to crises effectively. Among the many pieces of information required for disaster response and management, it is crucial that rescue workers are promptly notified of the location and extent of a building's destruction in order to maximise the effectiveness of their efforts. Nevertheless, despite significant efforts, problems with picture classification for disaster response still exist. In this study, a potential deep learning-based method is put forth for identifying damaged buildings in high-resolution satellite photos. It solves the issue of limited training data common in many remote sensing applications by using generic data augmentation. It is suggested that a pretrained model be used in conjunction with transfer learning as a fine-tuning method for the relevant task. The trials with images of Port-au-Prince, Haiti showed that the suggested strategy works well with sparse training data. With enriched training data, the Convolutional Neural Network (CNN) model can detect damaged buildings with an accuracy of 83%, compared to only 53% with the original training data. The focus of future study will be on investigating automated ways to obtain larger training datasets and model generalisation by researching more reliable data augmentation strategies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.;"Convolutional Neural Network; data augmentation; disaster response; high-resolution satellite photos; Natural catastrophes";"Convolution; Convolutional neural networks; Deep learning; Emergency services; Global warming; Human resource management; Learning systems; Neural network models; Convolutional neural network; Damage evaluation; Data augmentation; Disaster-response; High resolution satellites; High-resolution satellite photo; Natural catastrophe; Satellite photos; Training data; Workers'; Remote sensing";"Damage Evaluation Following Natural Disasters Using Deep Learning Natural catastrophes including flooding, tornadoes, earthquakes, and wildfires have been occurring more frequently over the past few decades as a result of global warming and climate change. Therefore, it is more crucial than ever to give emergency response workers accurate and timely information to enable them to respond to crises effectively. Among the many pieces of information required for disaster response and management, it is crucial that rescue workers are promptly notified of the location and extent of a building's destruction in order to maximise the effectiveness of their efforts. Nevertheless, despite significant efforts, problems with picture classification for disaster response still exist. In this study, a potential deep learning-based method is put forth for identifying damaged buildings in high-resolution satellite photos. It solves the issue of limited training data common in many remote sensing applications by using generic data augmentation. It is suggested that a pretrained model be used in conjunction with transfer learning as a fine-tuning method for the relevant task. The trials with images of Port-au-Prince, Haiti showed that the suggested strategy works well with sparse training data. With enriched training data, the Convolutional Neural Network (CNN) model can detect damaged buildings with an accuracy of 83%, compared to only 53% with the original training data. The focus of future study will be on investigating automated ways to obtain larger training datasets and model generalisation by researching more reliable data augmentation strategies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024. Convolutional Neural Network; data augmentation; disaster response; high-resolution satellite photos; Natural catastrophes Convolution; Convolutional neural networks; Deep learning; Emergency services; Global warming; Human resource management; Learning systems; Neural network models; Convolutional neural network; Damage evaluation; Data augmentation; Disaster-response; High resolution satellites; High-resolution satellite photo; Natural catastrophe; Satellite photos; Training data; Workers'; Remote sensing";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
502;A hybrid multi-step storm surge forecasting model using multiple feature selection, deep learning neural network and transfer learning;"A real-time and accurate storm surge prediction model is of great scientific value and practical significance in reducing human casualties and economic losses in coastal areas. For this purpose, a novel storm surge multi-step forecasting framework integrating time-varying filtered empirical modal decomposition (TVF-EMD), fast Fourier transform (FFT), phase space reconstruction, convolutional neural network (CNN), and long short-term memory neural network (LSTM) is proposed in this study. Among the supplementary strategies, the TVF-EMD is used to extract the fluctuation features of the storm surge data and decompose the storm surge time series into a number of IMFs; the FFT is employed to calculate the frequency values of each IMF, and the subsequences with similar frequency values are combined and reconstructed. Meanwhile, CNN is adopted to predict the preprocessed high-frequency components, while the low-frequency is predicted by LSTM. Subsequently, the ultimate prediction results of the raw storm surge are calculated by superimposing the predicted values of all components. Three datasets collected from southeastern coastal region of China and five relevant comparison models are carried out to evaluate the proposed approach, where the corresponding results demonstrate that: (1) data preprocessing strategy applying TVF-EMD and FFT can significantly improve forecasting performance; (2) the TVF-EMD decomposition method is more effective under the influence of low sampling rate and noise; (3) by observing the characteristics of the subsequence, the prediction by modules can achieve better results. In addition, in order to apply the model to engineering, the proposed model is transferred to the small data domain as a pre-trained model using a transfer learning approach. According to the prediction results of Wenzhou station in the 7821-storm surge event, it can be found the proposed model still has good robustness and generalization ability even though the new sample data is small. This also proves that the model has strong practical value in coastal storm surge warning as well as disaster prevention and mitigation. Overall, the storm surge prediction framework proposed in this study has higher prediction accuracy and transferability, and can provide scientific and reasonable theoretical guidance for the emergency management to develop disaster prevention strategies. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.";"Convolutional neural network; Long short-term memory neural network; Multi-step storm surge forecasting; Time-varying filter-based empirical mode decomposition; Transfer learning";"Brain; Convolution; Convolutional neural networks; Disaster prevention; Disasters; Fast Fourier transforms; Floods; Hurricanes; Learning systems; Long short-term memory; Losses; Storms; Convolutional neural network; Empirical Mode Decomposition; Filter-based; Long short-term memory neural network; Multi-step storm surge forecasting; Multisteps; Neural-networks; Storm surge forecasting; Time varying filters; Time-varying filter-based empirical mode decomposition; Transfer learning; Empirical mode decomposition";"A hybrid multi-step storm surge forecasting model using multiple feature selection, deep learning neural network and transfer learning A real-time and accurate storm surge prediction model is of great scientific value and practical significance in reducing human casualties and economic losses in coastal areas. For this purpose, a novel storm surge multi-step forecasting framework integrating time-varying filtered empirical modal decomposition (TVF-EMD), fast Fourier transform (FFT), phase space reconstruction, convolutional neural network (CNN), and long short-term memory neural network (LSTM) is proposed in this study. Among the supplementary strategies, the TVF-EMD is used to extract the fluctuation features of the storm surge data and decompose the storm surge time series into a number of IMFs; the FFT is employed to calculate the frequency values of each IMF, and the subsequences with similar frequency values are combined and reconstructed. Meanwhile, CNN is adopted to predict the preprocessed high-frequency components, while the low-frequency is predicted by LSTM. Subsequently, the ultimate prediction results of the raw storm surge are calculated by superimposing the predicted values of all components. Three datasets collected from southeastern coastal region of China and five relevant comparison models are carried out to evaluate the proposed approach, where the corresponding results demonstrate that: (1) data preprocessing strategy applying TVF-EMD and FFT can significantly improve forecasting performance; (2) the TVF-EMD decomposition method is more effective under the influence of low sampling rate and noise; (3) by observing the characteristics of the subsequence, the prediction by modules can achieve better results. In addition, in order to apply the model to engineering, the proposed model is transferred to the small data domain as a pre-trained model using a transfer learning approach. According to the prediction results of Wenzhou station in the 7821-storm surge event, it can be found the proposed model still has good robustness and generalization ability even though the new sample data is small. This also proves that the model has strong practical value in coastal storm surge warning as well as disaster prevention and mitigation. Overall, the storm surge prediction framework proposed in this study has higher prediction accuracy and transferability, and can provide scientific and reasonable theoretical guidance for the emergency management to develop disaster prevention strategies. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature. Convolutional neural network; Long short-term memory neural network; Multi-step storm surge forecasting; Time-varying filter-based empirical mode decomposition; Transfer learning Brain; Convolution; Convolutional neural networks; Disaster prevention; Disasters; Fast Fourier transforms; Floods; Hurricanes; Learning systems; Long short-term memory; Losses; Storms; Convolutional neural network; Empirical Mode Decomposition; Filter-based; Long short-term memory neural network; Multi-step storm surge forecasting; Multisteps; Neural-networks; Storm surge forecasting; Time varying filters; Time-varying filter-based empirical mode decomposition; Transfer learning; Empirical mode decomposition";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;2;Preparation
503;Advanced Machine Learning Strategies for Landslide Detection;This study presents an advanced machine learning framework for predicting landslides in Moio della Civitella, Italy, utilizing a comprehensive dataset from 2015-2019. Integrating Self-Supervised Learning for Anomaly Detection, Ensemble Methods, Long Short-Term Memory networks (LSTM) for Time-Series Forecasting, and Gradient Boosting Machines for Feature Importance, the research identifies critical temporal and seasonal patterns in landslide occurrences. Visual tools like Time-Series Plots and Anomaly Heatmaps highlight significant deviations and high-preparedness periods, particularly during December to February. Validation through precision and recall, alongside ROC curves, demonstrates improved prediction accuracy. Despite inherent uncertainties and dependencies on data quality, the approach significantly enhances the predictability of landslides, offering a robust tool for early warning systems and risk management strategies, thereby aiming to mitigate the human and economic toll of such natural disasters. © 2024 IEEE.;"Anomaly Detection; COSMO-SkyMed (CSK) Satellite Imagery; InSAR; Landslide Prediction; Long Short-Term Memory (LSTM) Networks; Self-Supervised Learning; Time-Series Forecasting";NULL;"Advanced Machine Learning Strategies for Landslide Detection This study presents an advanced machine learning framework for predicting landslides in Moio della Civitella, Italy, utilizing a comprehensive dataset from 2015-2019. Integrating Self-Supervised Learning for Anomaly Detection, Ensemble Methods, Long Short-Term Memory networks (LSTM) for Time-Series Forecasting, and Gradient Boosting Machines for Feature Importance, the research identifies critical temporal and seasonal patterns in landslide occurrences. Visual tools like Time-Series Plots and Anomaly Heatmaps highlight significant deviations and high-preparedness periods, particularly during December to February. Validation through precision and recall, alongside ROC curves, demonstrates improved prediction accuracy. Despite inherent uncertainties and dependencies on data quality, the approach significantly enhances the predictability of landslides, offering a robust tool for early warning systems and risk management strategies, thereby aiming to mitigate the human and economic toll of such natural disasters. © 2024 IEEE. Anomaly Detection; COSMO-SkyMed (CSK) Satellite Imagery; InSAR; Landslide Prediction; Long Short-Term Memory (LSTM) Networks; Self-Supervised Learning; Time-Series Forecasting NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
504;Fusion of Multimodal Textual and Visual Descriptors for Analyzing Disaster Response;People are using social media (SM) sites like Twitter, Facebook and Instagram more frequently to report senous catastrophes or disaster situations The scale of the event, the number of victims, and the extent of the infrastructure damage are all frequently revealed in mullimodal data published on these platforms The information can give local govemment officials and aid otganizations a comprehensive ovenfiew of the situation. It can also be used to efficiently and quickly plan relief efforts The goal of the suggested effort is to solve the difficully of locating pertinent infomwtion among the numerous published SM posts In particular, embeddings that encapsulate the relatedness of multimodal SM post in the context of disaster occurrences are created using pretrained deep learning models One dataset that offers annotations in addition to textual and visual data is the Multi-modal Datasets from Natural Disasters named as CnsisMMD captured the posts from social media, which can be used by researchers to create cnsis response systems This study has examined CnsisMMD's multi-modal data on seven significant natural disasters, including earthquakes, floods, humcanes, and fires, and developed an efficient model for categonzing social media data into useful and non-useful categones Proposed model makes use of transfer learning approach for extraction imwge features using DenseNet and transformer-based BERT model for extraction of textual features. This multimodal fusion approach has achieved the accuracy of 85.33 %,which isproved to be better than state of art techniques  © 2023 IEEE.;"BERT; DenseNet; Multimodal Fusion; Pretrained Models";"Deep learning; Disasters; Learning systems; Modal analysis; Social networking (online); BERT; Densenet; Descriptors; Disaster-response; Facebook; Multi-modal; Multi-modal fusion; Natural disasters; Pretrained model; Social media; Extraction";"Fusion of Multimodal Textual and Visual Descriptors for Analyzing Disaster Response People are using social media (SM) sites like Twitter, Facebook and Instagram more frequently to report senous catastrophes or disaster situations The scale of the event, the number of victims, and the extent of the infrastructure damage are all frequently revealed in mullimodal data published on these platforms The information can give local govemment officials and aid otganizations a comprehensive ovenfiew of the situation. It can also be used to efficiently and quickly plan relief efforts The goal of the suggested effort is to solve the difficully of locating pertinent infomwtion among the numerous published SM posts In particular, embeddings that encapsulate the relatedness of multimodal SM post in the context of disaster occurrences are created using pretrained deep learning models One dataset that offers annotations in addition to textual and visual data is the Multi-modal Datasets from Natural Disasters named as CnsisMMD captured the posts from social media, which can be used by researchers to create cnsis response systems This study has examined CnsisMMD's multi-modal data on seven significant natural disasters, including earthquakes, floods, humcanes, and fires, and developed an efficient model for categonzing social media data into useful and non-useful categones Proposed model makes use of transfer learning approach for extraction imwge features using DenseNet and transformer-based BERT model for extraction of textual features. This multimodal fusion approach has achieved the accuracy of 85.33 %,which isproved to be better than state of art techniques  © 2023 IEEE. BERT; DenseNet; Multimodal Fusion; Pretrained Models Deep learning; Disasters; Learning systems; Modal analysis; Social networking (online); BERT; Densenet; Descriptors; Disaster-response; Facebook; Multi-modal; Multi-modal fusion; Natural disasters; Pretrained model; Social media; Extraction";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;-1;NULL;3;Response
505;Uncertainty-aware structural damage warning system using deep variational composite neural networks;Structural health monitoring (SHM) is, without a doubt, one of the most important assets for building resilient communities. The vast and rapidly advancing research in data science and machine learning has provided researchers in the civil engineering community with various tools that can facilitate the processing of significant amounts of gathered data. However, deep learning models are prone to mistakes, and with the catastrophic consequences that can happen due to damage misidentification, damage diagnosis models’ predictions should not be taken for granted. In this study, we present an uncertainty-aware early-warning system that can provide near real-time SHM. The system utilizes a deep composite encoder-decoder network that combines elements from convolutional neural networks, recurrent neural networks, and variational inference (VI) to provide damage index distributions. The framework can detect anomalies in the structural system during seismic events and provide a measure of uncertainty that can be used to question the model's predictions. To assess the system's validity and practicality, we apply our proposal to three real structures, two of which suffered damage during the 1994 Northridge earthquake. We found that the early warning system delivers an accurate, yet cautious, continuous monitoring that is capable of sending warning signals when damage occurs in the course of seismic events. Source code is available at: https://github.com/keltouny/VSCAN. © 2023 John Wiley & Sons Ltd.;"anomaly detection; deep learning; recurrent neural network; structural health monitoring; uncertainty quantification; variational autoencoder";"Convolutional neural networks; Damage detection; Deep neural networks; Learning systems; Recurrent neural networks; Seismology; Structural health monitoring; Anomaly detection; Auto encoders; Deep learning; Early Warning System; Model prediction; Seismic event; Structural damages; Uncertainty; Uncertainty quantifications; Variational autoencoder; artificial neural network; earthquake damage; earthquake event; health monitoring; learning; machine learning; Anomaly detection";"Uncertainty-aware structural damage warning system using deep variational composite neural networks Structural health monitoring (SHM) is, without a doubt, one of the most important assets for building resilient communities. The vast and rapidly advancing research in data science and machine learning has provided researchers in the civil engineering community with various tools that can facilitate the processing of significant amounts of gathered data. However, deep learning models are prone to mistakes, and with the catastrophic consequences that can happen due to damage misidentification, damage diagnosis models’ predictions should not be taken for granted. In this study, we present an uncertainty-aware early-warning system that can provide near real-time SHM. The system utilizes a deep composite encoder-decoder network that combines elements from convolutional neural networks, recurrent neural networks, and variational inference (VI) to provide damage index distributions. The framework can detect anomalies in the structural system during seismic events and provide a measure of uncertainty that can be used to question the model's predictions. To assess the system's validity and practicality, we apply our proposal to three real structures, two of which suffered damage during the 1994 Northridge earthquake. We found that the early warning system delivers an accurate, yet cautious, continuous monitoring that is capable of sending warning signals when damage occurs in the course of seismic events. Source code is available at: https://github.com/keltouny/VSCAN. © 2023 John Wiley & Sons Ltd. anomaly detection; deep learning; recurrent neural network; structural health monitoring; uncertainty quantification; variational autoencoder Convolutional neural networks; Damage detection; Deep neural networks; Learning systems; Recurrent neural networks; Seismology; Structural health monitoring; Anomaly detection; Auto encoders; Deep learning; Early Warning System; Model prediction; Seismic event; Structural damages; Uncertainty; Uncertainty quantifications; Variational autoencoder; artificial neural network; earthquake damage; earthquake event; health monitoring; learning; machine learning; Anomaly detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
506;Firefly Algorithm in detection of TEC seismo-ionospheric anomalies;Anomaly detection in time series of different earthquake precursors is an essential introduction to create an early warning system with an allowable uncertainty. Since these time series are more often non linear, complex and massive, therefore the applied predictor method should be able to detect the discord patterns from a large data in a short time. This study acknowledges Firefly Algorithm (FA) as a simple and robust predictor to detect the TEC (Total Electron Content) seismo-ionospheric anomalies around the time of the some powerful earthquakes including Chile (27 February 2010), Varzeghan (11 August 2012) and Saravan (16 April 2013). Outstanding anomalies were observed 7 and 5 days before the Chile and Varzeghan earthquakes, respectively and also 3 and 8 days prior to the Saravan earthquake. © 2015 COSPAR. Published by Elsevier Ltd. All rights reserved.;"Anomaly; Ionosphere; Precursor; TEC";"Bioluminescence; Ionosphere; Optimization; Time series; Anomaly; Anomaly detection; Early Warning System; Earthquake precursors; Firefly algorithms; Ionospheric anomalies; Precursor; Total electron content; Earthquakes";"Firefly Algorithm in detection of TEC seismo-ionospheric anomalies Anomaly detection in time series of different earthquake precursors is an essential introduction to create an early warning system with an allowable uncertainty. Since these time series are more often non linear, complex and massive, therefore the applied predictor method should be able to detect the discord patterns from a large data in a short time. This study acknowledges Firefly Algorithm (FA) as a simple and robust predictor to detect the TEC (Total Electron Content) seismo-ionospheric anomalies around the time of the some powerful earthquakes including Chile (27 February 2010), Varzeghan (11 August 2012) and Saravan (16 April 2013). Outstanding anomalies were observed 7 and 5 days before the Chile and Varzeghan earthquakes, respectively and also 3 and 8 days prior to the Saravan earthquake. © 2015 COSPAR. Published by Elsevier Ltd. All rights reserved. Anomaly; Ionosphere; Precursor; TEC Bioluminescence; Ionosphere; Optimization; Time series; Anomaly; Anomaly detection; Early Warning System; Earthquake precursors; Firefly algorithms; Ionospheric anomalies; Precursor; Total electron content; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
507;Drought Prediction Based on Feature-Based Transfer Learning and Time Series Imaging;Drought is an extreme climate phenomenon that has a great impact on the economy, tourism, agriculture, and water resources. Drought prediction can provide an early warning of the occurrence of drought and reduce losses. In this article, the standard precipitation evapotranspiration index (SPEI) on four time scales: SPEI-3, SPEI-6, SPEI-9, and SPEI-12 are used to measure and predict drought. Unlike the general methods of directly modeling the SPEI index, time-series imaging and feature-based transfer learning are used to extract the features of the SPEI sequence and use the extracted features for prediction. First, we use Gramian Angular Summation/Difference Field (GASF/GADF), Markov Transition Field (MTF), and Recurrence Plot (RP) as the time series imaging techniques to encode SPEI sequences into images. Secondly, we utilize imaging data sets and convolutional neural networks (CNNs) such as residual network (ResNet) and VGG to train the feature extraction network. Finally, the following four regressors: Random Forest (RF), Long and Short-Term Memory network (LSTM), Wavelet Neural Network (WNN), Support Vector Regression (SVR) are used to model the extracted features and drought prediction. To verify the effectiveness of the method proposed in this article, we use the SPEI of four time scales at eight stations in the Haihe River Basin for prediction. Compared with the existing methods, the prediction results of different time scales and stations are improved. For example, after feature extraction, LSTM can reach MAPE = 0.5400, SMAPE = 0.4452, MAE = 0.2150, MSE = 0.0853 and R{2} = 0.8960 in the SPEI-12 prediction of the Beijing site, and other results show that the proposed method is not sensitive to the time scale of drought prediction.  © 2013 IEEE.;"deep learning; Drought prediction; imaging; transfer learning";"Agricultural robots; Convolutional neural networks; Decision trees; Drought; Extraction; Feature extraction; Forecasting; Imaging techniques; Support vector regression; Time measurement; Time series; Transfer learning; Water resources; Different time scale; Extreme climates; Haihe River basin; Long and short term memory; Prediction-based; Support vector regression (SVR); Transition fields; Wavelet neural networks; Long short-term memory";"Drought Prediction Based on Feature-Based Transfer Learning and Time Series Imaging Drought is an extreme climate phenomenon that has a great impact on the economy, tourism, agriculture, and water resources. Drought prediction can provide an early warning of the occurrence of drought and reduce losses. In this article, the standard precipitation evapotranspiration index (SPEI) on four time scales: SPEI-3, SPEI-6, SPEI-9, and SPEI-12 are used to measure and predict drought. Unlike the general methods of directly modeling the SPEI index, time-series imaging and feature-based transfer learning are used to extract the features of the SPEI sequence and use the extracted features for prediction. First, we use Gramian Angular Summation/Difference Field (GASF/GADF), Markov Transition Field (MTF), and Recurrence Plot (RP) as the time series imaging techniques to encode SPEI sequences into images. Secondly, we utilize imaging data sets and convolutional neural networks (CNNs) such as residual network (ResNet) and VGG to train the feature extraction network. Finally, the following four regressors: Random Forest (RF), Long and Short-Term Memory network (LSTM), Wavelet Neural Network (WNN), Support Vector Regression (SVR) are used to model the extracted features and drought prediction. To verify the effectiveness of the method proposed in this article, we use the SPEI of four time scales at eight stations in the Haihe River Basin for prediction. Compared with the existing methods, the prediction results of different time scales and stations are improved. For example, after feature extraction, LSTM can reach MAPE = 0.5400, SMAPE = 0.4452, MAE = 0.2150, MSE = 0.0853 and R{2} = 0.8960 in the SPEI-12 prediction of the Beijing site, and other results show that the proposed method is not sensitive to the time scale of drought prediction.  © 2013 IEEE. deep learning; Drought prediction; imaging; transfer learning Agricultural robots; Convolutional neural networks; Decision trees; Drought; Extraction; Feature extraction; Forecasting; Imaging techniques; Support vector regression; Time measurement; Time series; Transfer learning; Water resources; Different time scale; Extreme climates; Haihe River basin; Long and short term memory; Prediction-based; Support vector regression (SVR); Transition fields; Wavelet neural networks; Long short-term memory";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.4;Climatological;2;Preparation
508;Detection and Semantic Segmentation of Disaster Damage in UAV Footage;In the aftermath of large-scale disasters, such as hurricanes, floods, or earthquakes, preliminarily damage assessment (PDA) is carried out to determine the impact and magnitude of damage and meet the needs of affected individuals, businesses, and communities. Traditionally, site evaluation and consensus-based assessment techniques are used to estimate the extent of the damage. More recently, given their low-cost, ease of operation, and ability to be deployed ondemand, unmanned aerial vehicles (UAVs) are increasingly used for disaster response and mitigation. However, the resulting large volume of visual data collected by and shared among first responders and volunteer groups is not used effectively because current practices of processing such data are heavily human-dependent, extremely resource-intensive, and significantly slow compared to the fast-evolving nature and progression of disaster impact. This paper contributes to the core body of knowledge by presenting a fully annotated dataset (with the object classes people, flooded area, and damaged and undamaged building roof, car, debris, vegetation, road, and boat) and a host of convolutional neural network (CNN) models for detecting and segmenting critical objects in the aerial footage of disaster sites. For best results, two CNN-based image segmentation architectures, namely, Mask-RCNN and Pyramid Scene Parsing Network (PSPNet), are adopted (through transfer learning), trained, validated, and tested on annotated videos to detect countable and bulk objects. The paper further introduces a targeted data augmentation technique to preserve data balance, as well as a data-driven approach to splitting highly mismatched classes for better model performance. Through these improvements, the best performing Mask-RCNN model generates pixel-level segmentations of countable objects with a 51.54% mean average precision (mAP). Additionally, the best performing PSPNet models can achieve mean intersection over union (mIoU) as high as 32.17% and accuracy as high as 77.01% on bulk objects.  © 2020 American Society of Civil Engineers.;"Convolutional neural network (CNN); Data augmentation; Deep learning; Disaster management; Preliminarily damage assessment (PDA); Semantic segmentation; Unmanned aerial vehicle (UAV)";"Aircraft detection; Antennas; Convolutional neural networks; Data handling; Disasters; Earthquakes; Floods; Image segmentation; Object detection; Semantics; Transfer learning; Unmanned aerial vehicles (UAV); Assessment technique; Current practices; Damage assessments; Data augmentation; Data-driven approach; Large scale disasters; Model performance; Semantic segmentation; Damage detection";"Detection and Semantic Segmentation of Disaster Damage in UAV Footage In the aftermath of large-scale disasters, such as hurricanes, floods, or earthquakes, preliminarily damage assessment (PDA) is carried out to determine the impact and magnitude of damage and meet the needs of affected individuals, businesses, and communities. Traditionally, site evaluation and consensus-based assessment techniques are used to estimate the extent of the damage. More recently, given their low-cost, ease of operation, and ability to be deployed ondemand, unmanned aerial vehicles (UAVs) are increasingly used for disaster response and mitigation. However, the resulting large volume of visual data collected by and shared among first responders and volunteer groups is not used effectively because current practices of processing such data are heavily human-dependent, extremely resource-intensive, and significantly slow compared to the fast-evolving nature and progression of disaster impact. This paper contributes to the core body of knowledge by presenting a fully annotated dataset (with the object classes people, flooded area, and damaged and undamaged building roof, car, debris, vegetation, road, and boat) and a host of convolutional neural network (CNN) models for detecting and segmenting critical objects in the aerial footage of disaster sites. For best results, two CNN-based image segmentation architectures, namely, Mask-RCNN and Pyramid Scene Parsing Network (PSPNet), are adopted (through transfer learning), trained, validated, and tested on annotated videos to detect countable and bulk objects. The paper further introduces a targeted data augmentation technique to preserve data balance, as well as a data-driven approach to splitting highly mismatched classes for better model performance. Through these improvements, the best performing Mask-RCNN model generates pixel-level segmentations of countable objects with a 51.54% mean average precision (mAP). Additionally, the best performing PSPNet models can achieve mean intersection over union (mIoU) as high as 32.17% and accuracy as high as 77.01% on bulk objects.  © 2020 American Society of Civil Engineers. Convolutional neural network (CNN); Data augmentation; Deep learning; Disaster management; Preliminarily damage assessment (PDA); Semantic segmentation; Unmanned aerial vehicle (UAV) Aircraft detection; Antennas; Convolutional neural networks; Data handling; Disasters; Earthquakes; Floods; Image segmentation; Object detection; Semantics; Transfer learning; Unmanned aerial vehicles (UAV); Assessment technique; Current practices; Damage assessments; Data augmentation; Data-driven approach; Large scale disasters; Model performance; Semantic segmentation; Damage detection";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;-1;NULL;3;Response
509;CrackEmbed: Point feature embedding for crack segmentation from disaster site point clouds with anomaly detection;Laser-scanned point clouds can be used to represent the 3D as-damaged condition of building structures in a post-disaster scenario. Performing crack detection from the acquired point clouds is a critical component of disaster relief tasks such as structural damage assessment and risk assessment. Crack detection methods based on intensity or normals commonly result in noisy detections. On the other hand, deep learning methods can achieve higher accuracy but require a large dataset of annotated cracks. This research proposes an unsupervised learning framework based on anomaly detection to segment out cracked regions from disaster site point clouds. First, building components of interest are extracted from the point cloud scene using region growing segmentation. Next, a point-based deep neural network is used to extract discriminative point features using the geometry of the local point neighborhood. The neural network embedding, CrackEmbed, is trained using the triplet loss function on the S3DIS dataset. Then, an anomaly detection algorithm is used to separate out the points belonging to cracked regions based on the distribution of these point features. The proposed method was evaluated on laser-scanned point clouds from the 2015 Nepal earthquake as well as a disaster response training facility in the U.S. Evaluation results based on the point-level precision and recall metrics showed that CrackEmbed in conjunction with the isolation forest algorithm resulted in the best performance overall. © 2022 Elsevier Ltd;"Anomaly detection; Crack; Disaster site; Point cloud; Segmentation";"Anomaly detection; Crack detection; Damage detection; Disaster prevention; Disasters; Embeddings; Feature extraction; Large dataset; Petroleum reservoir evaluation; Risk assessment; Anomaly detection; Building structure; Condition; Crack segmentations; Disaster site; Feature embedding; Laser scanned; Point features; Point-clouds; Segmentation; Deep neural networks";"CrackEmbed: Point feature embedding for crack segmentation from disaster site point clouds with anomaly detection Laser-scanned point clouds can be used to represent the 3D as-damaged condition of building structures in a post-disaster scenario. Performing crack detection from the acquired point clouds is a critical component of disaster relief tasks such as structural damage assessment and risk assessment. Crack detection methods based on intensity or normals commonly result in noisy detections. On the other hand, deep learning methods can achieve higher accuracy but require a large dataset of annotated cracks. This research proposes an unsupervised learning framework based on anomaly detection to segment out cracked regions from disaster site point clouds. First, building components of interest are extracted from the point cloud scene using region growing segmentation. Next, a point-based deep neural network is used to extract discriminative point features using the geometry of the local point neighborhood. The neural network embedding, CrackEmbed, is trained using the triplet loss function on the S3DIS dataset. Then, an anomaly detection algorithm is used to separate out the points belonging to cracked regions based on the distribution of these point features. The proposed method was evaluated on laser-scanned point clouds from the 2015 Nepal earthquake as well as a disaster response training facility in the U.S. Evaluation results based on the point-level precision and recall metrics showed that CrackEmbed in conjunction with the isolation forest algorithm resulted in the best performance overall. © 2022 Elsevier Ltd Anomaly detection; Crack; Disaster site; Point cloud; Segmentation Anomaly detection; Crack detection; Damage detection; Disaster prevention; Disasters; Embeddings; Feature extraction; Large dataset; Petroleum reservoir evaluation; Risk assessment; Anomaly detection; Building structure; Condition; Crack segmentations; Disaster site; Feature embedding; Laser scanned; Point features; Point-clouds; Segmentation; Deep neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
510;Advances in Seismo-LAI anomalies detection within Google Earth Engine (GEE) cloud platform;Nowadays, satellite data is an appropriate and undeniable source for studying earthquake precursors due to their diversity, wide coverage, being up to date and low cost. Time series analysis of satellite data plays an important role in the process of detecting seismic anomalies in earthquake warning systems. But in order to reduce uncertainty during the seismic anomalies detection, it is necessary the use a variety of satellite data, although it leads to increase of data size and processing time. This paper aims to explain the role of Google Earth Engine (GEE) cloud platform in considerable progress of seismo-Lithospheric Atmospheric Ionospheric (LAI) anomalies detection in earthquake early warning systems. Among the different studied earthquakes, for example two recent powerful earthquakes in Japan (13 February and 20 March 2021) have been discussed. Deduced time series of three precursors (i.e. Aerosol Optical Thickness (AOT), Chlorophyll and Ozone) from GEE platform were investigated using Median method and a Long Short-Term Memory (LSTM) neural network to detect potentially seismo-LAI anomalies. Our satisfactory results show that with the addition of other various satellite data and also known predictors intelligent algorithms such as deep learning to GEE platform, we will see a significant leap forward in studies of earthquake precursors. © 2022 COSPAR;"Earthquake precursor; Google Earth Engine (GEE); Lithospheric Atmospheric Ionospheric anomalies; Satellites data";"Anomaly detection; Data handling; Earthquakes; Long short-term memory; Satellites; Time series analysis; Anomaly detection; Cloud platforms; Earthquake precursors; Google earth engine; Google earths; Ionospheric anomalies; Lithospheric; Lithospheric atmospheric ionospheric anomaly; Low-costs; Satellite data; Engines";"Advances in Seismo-LAI anomalies detection within Google Earth Engine (GEE) cloud platform Nowadays, satellite data is an appropriate and undeniable source for studying earthquake precursors due to their diversity, wide coverage, being up to date and low cost. Time series analysis of satellite data plays an important role in the process of detecting seismic anomalies in earthquake warning systems. But in order to reduce uncertainty during the seismic anomalies detection, it is necessary the use a variety of satellite data, although it leads to increase of data size and processing time. This paper aims to explain the role of Google Earth Engine (GEE) cloud platform in considerable progress of seismo-Lithospheric Atmospheric Ionospheric (LAI) anomalies detection in earthquake early warning systems. Among the different studied earthquakes, for example two recent powerful earthquakes in Japan (13 February and 20 March 2021) have been discussed. Deduced time series of three precursors (i.e. Aerosol Optical Thickness (AOT), Chlorophyll and Ozone) from GEE platform were investigated using Median method and a Long Short-Term Memory (LSTM) neural network to detect potentially seismo-LAI anomalies. Our satisfactory results show that with the addition of other various satellite data and also known predictors intelligent algorithms such as deep learning to GEE platform, we will see a significant leap forward in studies of earthquake precursors. © 2022 COSPAR Earthquake precursor; Google Earth Engine (GEE); Lithospheric Atmospheric Ionospheric anomalies; Satellites data Anomaly detection; Data handling; Earthquakes; Long short-term memory; Satellites; Time series analysis; Anomaly detection; Cloud platforms; Earthquake precursors; Google earth engine; Google earths; Ionospheric anomalies; Lithospheric; Lithospheric atmospheric ionospheric anomaly; Low-costs; Satellite data; Engines";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
511;Unsupervised active–transfer learning for automated landslide mapping;Detailed landslide inventories are required for multiple purposes including disaster damage assessments, susceptibility mapping for spatial planning, and disaster risk reduction. Active learning is an artificial intelligence strategy that can achieve good performances in landslide mapping by training a machine-learning model with a reduced number of landslide/non-landslide observations, which can save time and effort in labeling training instances. Nevertheless, active-learning models are unstable at the beginning of sample selection due to the limited initial knowledge of landslide distribution. Transfer learning can help make the learner robust by transferring a landslide model trained on an existing landslide inventory from a different, but geographically similar source area, to the unseen target area. In order to adjust a transferred machine-learning model to the possibly unique environmental characteristics of the unseen area, we proposed a new framework called Unsupervised Active-Transfer Learning (UATL). This framework used a weight function to combine the landslide model transferred from the source area, with a model trained on a small, but increasing number of landslide/non-landslide observations from the target area to efficiently build a more robust learner. We examined two methods, adaptive UATL and regular UATL, which differed in the way they assign weights to the combined learners. We evaluated our proposed new methods by comparing them with three benchmark methods (active learning only, model transfer only, and the model trained in the unseen area itself) by means of the partial area under the receiver operating characteristic (ROC) curve (AUROC) as the evaluation criterion. The results showed that the new methods, and especially adaptive UATL, can achieve good predictive performances. With only about 235 training instances from the target area, the partial AUROC obtained from adaptive UATL was only 2% lower than that obtained from the model trained in the target area itself, and consistently outperformed the other two benchmarks. Overall, we suggest that the framework proposed can be applied to the natural hazards management workflow for assisting in emergency response, especially in data-scarce regions (e.g., mountainous areas and developing countries). © 2023 The Authors;"Active learning; GeoAI; Landslide mapping; Transfer learning";"Damage detection; Developing countries; Disasters; Learning systems; Machine learning; Mapping; Risk assessment; Active Learning; Damage assessments; GeoAI; Landslide inventories; Landslide mapping; Machine learning models; Source area; Spatial planning; Susceptibility mapping; Transfer learning; disaster management; hazard assessment; hazard management; landslide; machine learning; mapping; natural hazard; spatial planning; Landslides";"Unsupervised active–transfer learning for automated landslide mapping Detailed landslide inventories are required for multiple purposes including disaster damage assessments, susceptibility mapping for spatial planning, and disaster risk reduction. Active learning is an artificial intelligence strategy that can achieve good performances in landslide mapping by training a machine-learning model with a reduced number of landslide/non-landslide observations, which can save time and effort in labeling training instances. Nevertheless, active-learning models are unstable at the beginning of sample selection due to the limited initial knowledge of landslide distribution. Transfer learning can help make the learner robust by transferring a landslide model trained on an existing landslide inventory from a different, but geographically similar source area, to the unseen target area. In order to adjust a transferred machine-learning model to the possibly unique environmental characteristics of the unseen area, we proposed a new framework called Unsupervised Active-Transfer Learning (UATL). This framework used a weight function to combine the landslide model transferred from the source area, with a model trained on a small, but increasing number of landslide/non-landslide observations from the target area to efficiently build a more robust learner. We examined two methods, adaptive UATL and regular UATL, which differed in the way they assign weights to the combined learners. We evaluated our proposed new methods by comparing them with three benchmark methods (active learning only, model transfer only, and the model trained in the unseen area itself) by means of the partial area under the receiver operating characteristic (ROC) curve (AUROC) as the evaluation criterion. The results showed that the new methods, and especially adaptive UATL, can achieve good predictive performances. With only about 235 training instances from the target area, the partial AUROC obtained from adaptive UATL was only 2% lower than that obtained from the model trained in the target area itself, and consistently outperformed the other two benchmarks. Overall, we suggest that the framework proposed can be applied to the natural hazards management workflow for assisting in emergency response, especially in data-scarce regions (e.g., mountainous areas and developing countries). © 2023 The Authors Active learning; GeoAI; Landslide mapping; Transfer learning Damage detection; Developing countries; Disasters; Learning systems; Machine learning; Mapping; Risk assessment; Active Learning; Damage assessments; GeoAI; Landslide inventories; Landslide mapping; Machine learning models; Source area; Spatial planning; Susceptibility mapping; Transfer learning; disaster management; hazard assessment; hazard management; landslide; machine learning; mapping; natural hazard; spatial planning; Landslides";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
512;Drone network for early warning of forest fire and dynamic fire quenching plan generation;Wildfires are one of the most frequent natural disasters which significantly harm the environment, society, and the economy. Transfer learning algorithms and modern machine learning tools can help in early forest fire prediction, detection, and dynamic fire quenching. A group of drones that has high-definition image processing and decision-making capabilities are used to detect the forest fires in the very early stage. The proposed system generates a fire quenching plan using particle swarm optimization technique and alerts the fire and rescue department for a quick action, thereby stop the forest fire at an early stage. Also, the drone network plays a major role to track the live status of forest fire and quenches the fire. ResNet, VGGNet, MobileNet, AlexNet, and GoogLeNet are used to detect the forest fire hazards. The experimental results prove that the proposed technique GoogLeNet-TL provides 96% accuracy and 97% F1 score in comparison with the state-of-the-art deep learning models. © 2023, The Author(s).;"Convolutional neural networks; Drone network; Early warning; Forest fire; GoogLeNet; Transfer learning";"Behavioral research; Decision making; Deep learning; Deforestation; Disasters; Drones; Fire hazards; Learning algorithms; Learning systems; Particle swarm optimization (PSO); Quenching; Transfer learning; Convolutional neural network; Drone network; Early warning; Forest dynamics; Forest fires; Googlenet; Modern machines; Natural disasters; Plan generation; Transfer learning; Fires";"Drone network for early warning of forest fire and dynamic fire quenching plan generation Wildfires are one of the most frequent natural disasters which significantly harm the environment, society, and the economy. Transfer learning algorithms and modern machine learning tools can help in early forest fire prediction, detection, and dynamic fire quenching. A group of drones that has high-definition image processing and decision-making capabilities are used to detect the forest fires in the very early stage. The proposed system generates a fire quenching plan using particle swarm optimization technique and alerts the fire and rescue department for a quick action, thereby stop the forest fire at an early stage. Also, the drone network plays a major role to track the live status of forest fire and quenches the fire. ResNet, VGGNet, MobileNet, AlexNet, and GoogLeNet are used to detect the forest fire hazards. The experimental results prove that the proposed technique GoogLeNet-TL provides 96% accuracy and 97% F1 score in comparison with the state-of-the-art deep learning models. © 2023, The Author(s). Convolutional neural networks; Drone network; Early warning; Forest fire; GoogLeNet; Transfer learning Behavioral research; Decision making; Deep learning; Deforestation; Disasters; Drones; Fire hazards; Learning algorithms; Learning systems; Particle swarm optimization (PSO); Quenching; Transfer learning; Convolutional neural network; Drone network; Early warning; Forest dynamics; Forest fires; Googlenet; Modern machines; Natural disasters; Plan generation; Transfer learning; Fires";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.4;Climatological;2;Preparation
513;From monitoring of seismic fields to the automatic forecasting of earthquakes;"Purpose: The purpose of this paper is to offer two Web-based platforms for systematic analysis of seismic processes. Both platforms are designed to analyze and forecast the state of the environment and, in particular, the level of seismic hazard. The first platform analyzes the fields representing the properties of the seismic process; the second platform forecasts strong earthquakes. Earthquake forecasting is based on a new one-class classification method. Design/methodology/approach: The paper suggests an approach to systematic forecasting of earthquakes and examines the results of tests. This approach is based on a new method of machine learning, called the method of the minimum area of alarm. The method allows to construct a forecast rule that optimizes the probability of detecting target earthquakes in a learning sample set, provided that the area of the alarm zone does not exceed a predetermined one. Findings: The paper presents two platforms alongside the method of analysis. It was shown that these platforms can be used for systematic analysis of seismic process. By testing of the earthquake forecasting method in several regions, it was shown that the method of the minimum area of alarm has satisfactory forecast quality. Originality/value: The described technology has two advantages: simplicity of configuration for a new problem area and a combination of interactive easy analysis supported by intuitive operations and a simplified user interface with a detailed, comprehensive analysis of spatio-temporal processes intended for specialists. The method of the minimum area of alarm solves the problem of one-class classification. The method is original. It uses in training the precedents of anomalous objects and statistically takes into account normal objects. © 2019, Emerald Publishing Limited.";"Earthquake forecasting; Method of the minimum area of alarm; One-class classification method; Seismic hazard monitoring technology; Spatio-temporal data analysis; Web-based GIS platform";"Alarm systems; Earthquakes; Hazards; Learning systems; Seismic response; User interfaces; Websites; Earthquake forecasting; Method of the minimum area of alarm; One-class Classification; Seismic hazards; Spatio-temporal data; Web-based GIS; Forecasting";"From monitoring of seismic fields to the automatic forecasting of earthquakes Purpose: The purpose of this paper is to offer two Web-based platforms for systematic analysis of seismic processes. Both platforms are designed to analyze and forecast the state of the environment and, in particular, the level of seismic hazard. The first platform analyzes the fields representing the properties of the seismic process; the second platform forecasts strong earthquakes. Earthquake forecasting is based on a new one-class classification method. Design/methodology/approach: The paper suggests an approach to systematic forecasting of earthquakes and examines the results of tests. This approach is based on a new method of machine learning, called the method of the minimum area of alarm. The method allows to construct a forecast rule that optimizes the probability of detecting target earthquakes in a learning sample set, provided that the area of the alarm zone does not exceed a predetermined one. Findings: The paper presents two platforms alongside the method of analysis. It was shown that these platforms can be used for systematic analysis of seismic process. By testing of the earthquake forecasting method in several regions, it was shown that the method of the minimum area of alarm has satisfactory forecast quality. Originality/value: The described technology has two advantages: simplicity of configuration for a new problem area and a combination of interactive easy analysis supported by intuitive operations and a simplified user interface with a detailed, comprehensive analysis of spatio-temporal processes intended for specialists. The method of the minimum area of alarm solves the problem of one-class classification. The method is original. It uses in training the precedents of anomalous objects and statistically takes into account normal objects. © 2019, Emerald Publishing Limited. Earthquake forecasting; Method of the minimum area of alarm; One-class classification method; Seismic hazard monitoring technology; Spatio-temporal data analysis; Web-based GIS platform Alarm systems; Earthquakes; Hazards; Learning systems; Seismic response; User interfaces; Websites; Earthquake forecasting; Method of the minimum area of alarm; One-class Classification; Seismic hazards; Spatio-temporal data; Web-based GIS; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
514;Near-Real-Time Seismic Human Fatality Information Retrieval from Social Media with Few-Shot Large-Language Models;Real-time disaster-induced human fatality information is critical for rapid and accurate disaster impact and loss estimation and effective emergency response. Systems like PAGER incorporate online reported death tolls and loss projection models trained on significant historical earthquake events and ground shaking data to provide projected final seismic loss estimations. However, the input reported death toll data are mainly retrieved from news platforms manually, which is time-consuming and may have a large time bias. In recent years, platforms such as Facebook and Twitter have become hot spots for witness reporting and communication during disaster events, producing large volumes of immediate fatality information without the hindrances of official channels. Though lucrative, social media data is very noisy both in syntax and accuracy, necessitating robust solutions. In this work, we design and deploy a new online system that automatically extracts near-realtime multi-lingual human fatality information including death tolls and injury tolls, from a variety of information sources immediately after an earthquake occurs. Past studies have proposed to use popular machine learning methods such as SVMs, CNNs and Logistic Regression in conjunction with word embeddings to classify the relevancy of each social media message. However, these techniques suffer from impeding requirements of annotated data, which are unavailable at the onset of natural disasters, and cannot directly extract disaster information, instead relying on statistical analysis on their classification results. To address such challenges, we propose a Large Language Model-based approach that leverages its robust language understanding and few-shot learning abilities. In combination with our novel multilingual Hierarchical Event Classifier, another contribution, we achieve effective automatic earthquake casualty information retrieval from social media, which we test by deploying our framework to two recent earthquakes.  © 2022 ACM.;"earthquake; few-shot learning; large language models; near-realtime information retrieval; rapid disaster response";"Classification (of information); Computational linguistics; Emergency services; Information retrieval; Learning systems; Logistic regression; Natural language processing systems; Online systems; Social networking (online); Death tolls; Disaster-response; Few-shot learning; Language model; Large language model; Near-realtime information retrieval; Rapid disaster response; Real- time; Real-time information; Social media; Earthquakes";"Near-Real-Time Seismic Human Fatality Information Retrieval from Social Media with Few-Shot Large-Language Models Real-time disaster-induced human fatality information is critical for rapid and accurate disaster impact and loss estimation and effective emergency response. Systems like PAGER incorporate online reported death tolls and loss projection models trained on significant historical earthquake events and ground shaking data to provide projected final seismic loss estimations. However, the input reported death toll data are mainly retrieved from news platforms manually, which is time-consuming and may have a large time bias. In recent years, platforms such as Facebook and Twitter have become hot spots for witness reporting and communication during disaster events, producing large volumes of immediate fatality information without the hindrances of official channels. Though lucrative, social media data is very noisy both in syntax and accuracy, necessitating robust solutions. In this work, we design and deploy a new online system that automatically extracts near-realtime multi-lingual human fatality information including death tolls and injury tolls, from a variety of information sources immediately after an earthquake occurs. Past studies have proposed to use popular machine learning methods such as SVMs, CNNs and Logistic Regression in conjunction with word embeddings to classify the relevancy of each social media message. However, these techniques suffer from impeding requirements of annotated data, which are unavailable at the onset of natural disasters, and cannot directly extract disaster information, instead relying on statistical analysis on their classification results. To address such challenges, we propose a Large Language Model-based approach that leverages its robust language understanding and few-shot learning abilities. In combination with our novel multilingual Hierarchical Event Classifier, another contribution, we achieve effective automatic earthquake casualty information retrieval from social media, which we test by deploying our framework to two recent earthquakes.  © 2022 ACM. earthquake; few-shot learning; large language models; near-realtime information retrieval; rapid disaster response Classification (of information); Computational linguistics; Emergency services; Information retrieval; Learning systems; Logistic regression; Natural language processing systems; Online systems; Social networking (online); Death tolls; Disaster-response; Few-shot learning; Language model; Large language model; Near-realtime information retrieval; Rapid disaster response; Real- time; Real-time information; Social media; Earthquakes";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;3;Response
515;Tsunami impact prediction system based on tsunawi inundation data;It is very important for tsunami early warning systems to provide inundation predictions within a short period of time. Inundation is one of the factors that directly cause destruction and damage from tsunamis. This research proposes a tsunami impact prediction system based on inundation data analysis. The inundation data used in this analysis were obtained from the tsunami modeling called TsunAWI. The inundation data analysis refers to the coastal forecast zones for each city/regency that are currently used in the Indonesia Tsunami Early Warning System (InaTEWS). The data analysis process comprises data collection, data transformation, data analysis (through GIS analysis, predictive analysis, and simple statistical analysis), and data integration, ultimately producing a pre-calculated inundation database for inundation prediction and tsunami impact prediction. As the outcome, the tsunami impact prediction system provides estimations of the flow depth and inundation distance for each city/regency incorporated into generated tsunami warning bulletins and impact predictions based on the Integrated Tsunami Intensity Scale (ITIS-2012). In addition, the system provides automatic sea level anomaly detection from tide gauge sensors by applying a tsunami detection algorithm. Finally, the contribution of this research is expected to bring enhancements to the tsunami warning products of InaTEWS. © 2021 Published by IRCS-ITB.;"Early warning system; Inundation data analysis; Tsunami impact prediction system; Tsunami modeling; Tsunami warning information";NULL;"Tsunami impact prediction system based on tsunawi inundation data It is very important for tsunami early warning systems to provide inundation predictions within a short period of time. Inundation is one of the factors that directly cause destruction and damage from tsunamis. This research proposes a tsunami impact prediction system based on inundation data analysis. The inundation data used in this analysis were obtained from the tsunami modeling called TsunAWI. The inundation data analysis refers to the coastal forecast zones for each city/regency that are currently used in the Indonesia Tsunami Early Warning System (InaTEWS). The data analysis process comprises data collection, data transformation, data analysis (through GIS analysis, predictive analysis, and simple statistical analysis), and data integration, ultimately producing a pre-calculated inundation database for inundation prediction and tsunami impact prediction. As the outcome, the tsunami impact prediction system provides estimations of the flow depth and inundation distance for each city/regency incorporated into generated tsunami warning bulletins and impact predictions based on the Integrated Tsunami Intensity Scale (ITIS-2012). In addition, the system provides automatic sea level anomaly detection from tide gauge sensors by applying a tsunami detection algorithm. Finally, the contribution of this research is expected to bring enhancements to the tsunami warning products of InaTEWS. © 2021 Published by IRCS-ITB. Early warning system; Inundation data analysis; Tsunami impact prediction system; Tsunami modeling; Tsunami warning information NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
516;Knowledge transfer between buildings for post-earthquake damage diagnosis without historical data;Automated structural damage diagnosis is important for improving the efficiency of disaster responses and rehabilitation. In conventional data-driven frameworks using machine learning or statistical models, structural damage diagnosis models are often constructed in a manner of supervised learning. The supervised learning utilizes collected historical structural sensing data and respective damage states (i.e., labels) for each building to learn the building-specific mapping between them. However, in post-earthquake scenarios, historical data with labels are often not available for many buildings in the affected area, which makes it difficult to construct a damage diagnosis model. Directly using the historical data from other buildings to construct a damage diagnosis model for the target building would lead to inaccurate results. This is because data-driven models assume that the data distribution of training dataset (from other buildings) is consistent with that of test dataset (from the target building), while in practice each building has unique physical properties and thus unique data distribution. To this end, we introduce a new modeling framework to transfer the knowledge learned from other buildings to diagnose structural damage states in the target building. This framework is based on an adversarial domain adaptation approach that extracts domain-invariant feature representations of data from different buildings. The feature extraction function is trained in an adversarial way, which ensures the extracted feature distributions are robust to changes in structures while being predictive of the damage states. With the extracted domain-invariant feature representations, the data distributions become consistent across different buildings. We finally adopt the damage diagnosis model learned from other buildings datasets to infer the damage state of the target building. We evaluate our proposed framework using data collected from 5 different buildings subjected to 40 different earthquakes. We transfer the model learned from multiple buildings to diagnose a new building subjected to new earthquakes. The results show an up to 33.8% improvement in damage detection accuracy compared to conventional methods. © International Workshop on Structural Health Monitoring. All rights reserved.;NULL;"Buildings; Damage detection; Data mining; Earthquakes; Internet of things; Knowledge management; Life cycle; Machine learning; Statistical tests; Structural health monitoring; Supervised learning; Conventional methods; Earthquake damages; Earthquake scenario; Feature distribution; Invariant features; Multiple buildings; Structural damage diagnosis; Structural damages; Structural analysis";"Knowledge transfer between buildings for post-earthquake damage diagnosis without historical data Automated structural damage diagnosis is important for improving the efficiency of disaster responses and rehabilitation. In conventional data-driven frameworks using machine learning or statistical models, structural damage diagnosis models are often constructed in a manner of supervised learning. The supervised learning utilizes collected historical structural sensing data and respective damage states (i.e., labels) for each building to learn the building-specific mapping between them. However, in post-earthquake scenarios, historical data with labels are often not available for many buildings in the affected area, which makes it difficult to construct a damage diagnosis model. Directly using the historical data from other buildings to construct a damage diagnosis model for the target building would lead to inaccurate results. This is because data-driven models assume that the data distribution of training dataset (from other buildings) is consistent with that of test dataset (from the target building), while in practice each building has unique physical properties and thus unique data distribution. To this end, we introduce a new modeling framework to transfer the knowledge learned from other buildings to diagnose structural damage states in the target building. This framework is based on an adversarial domain adaptation approach that extracts domain-invariant feature representations of data from different buildings. The feature extraction function is trained in an adversarial way, which ensures the extracted feature distributions are robust to changes in structures while being predictive of the damage states. With the extracted domain-invariant feature representations, the data distributions become consistent across different buildings. We finally adopt the damage diagnosis model learned from other buildings datasets to infer the damage state of the target building. We evaluate our proposed framework using data collected from 5 different buildings subjected to 40 different earthquakes. We transfer the model learned from multiple buildings to diagnose a new building subjected to new earthquakes. The results show an up to 33.8% improvement in damage detection accuracy compared to conventional methods. © International Workshop on Structural Health Monitoring. All rights reserved. NULL Buildings; Damage detection; Data mining; Earthquakes; Internet of things; Knowledge management; Life cycle; Machine learning; Statistical tests; Structural health monitoring; Supervised learning; Conventional methods; Earthquake damages; Earthquake scenario; Feature distribution; Invariant features; Multiple buildings; Structural damage diagnosis; Structural damages; Structural analysis";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;4;Recovery
517;A Novel Transfer Learning based CNN Model for Wildfire Susceptibility Prediction;Wildfires are one of the most commonly occurring natural disasters in the world, posing significant threats to ecosystems and human settlements alike. One of the most important risk mitigation strategies is to implement early warning systems by identifying the regions more susceptible to wildfires. The development of remote sensing technologies combined with the increasing success of deep learning algorithms has greatly accelerated the development of such systems. Significant research has been done so far for wildfire detection in ground level imagery using neural network classifiers, but there is a lack of research focusing on satellite imagery. This paper proposes a method of wildfire risk assessment of large land regions focusing on remote sensing satellite imagery. The dataset used consists of 31280 satellite images each of size 350 x 350 pixels in jpg format from the Quebec region and was built using wildfire data from Canada's Open Government Portal website to identify regions where wildfires have occurred, and satellite images of those regions before or during occurrence have been used as the wildfire class in the binary classification. We implemented different CNN based classification models, namely VGG16, ResNet50, Xception and InceptionV3 as well as four VGG-16 based transfer learning models. All the models were simulated on 5640 test images and their performance was compared. Our proposed transfer learning model having a three layered pyramid structure with Batch Normalization and Dropouts yielded the best results, with an accuracy of 0.9650, 0.9715 precision and an F-1 score of 0.9648 outperforming all of the traditional as well as the basic transfer learning models by a notable margin. © 2024 IEEE.;"CNN; Computer vision; Deep Learning; Transfer Learning; VGG16; Wildfire";"Classification (of information); Deep learning; Disasters; Fires; Learning algorithms; Learning systems; Remote sensing; Risk assessment; Satellite imagery; CNN models; Deep learning; Human settlements; Learning models; Natural disasters; Satellite images; Transfer learning; VGG16; Wildfire; Wildfire susceptibilities; Computer vision";"A Novel Transfer Learning based CNN Model for Wildfire Susceptibility Prediction Wildfires are one of the most commonly occurring natural disasters in the world, posing significant threats to ecosystems and human settlements alike. One of the most important risk mitigation strategies is to implement early warning systems by identifying the regions more susceptible to wildfires. The development of remote sensing technologies combined with the increasing success of deep learning algorithms has greatly accelerated the development of such systems. Significant research has been done so far for wildfire detection in ground level imagery using neural network classifiers, but there is a lack of research focusing on satellite imagery. This paper proposes a method of wildfire risk assessment of large land regions focusing on remote sensing satellite imagery. The dataset used consists of 31280 satellite images each of size 350 x 350 pixels in jpg format from the Quebec region and was built using wildfire data from Canada's Open Government Portal website to identify regions where wildfires have occurred, and satellite images of those regions before or during occurrence have been used as the wildfire class in the binary classification. We implemented different CNN based classification models, namely VGG16, ResNet50, Xception and InceptionV3 as well as four VGG-16 based transfer learning models. All the models were simulated on 5640 test images and their performance was compared. Our proposed transfer learning model having a three layered pyramid structure with Batch Normalization and Dropouts yielded the best results, with an accuracy of 0.9650, 0.9715 precision and an F-1 score of 0.9648 outperforming all of the traditional as well as the basic transfer learning models by a notable margin. © 2024 IEEE. CNN; Computer vision; Deep Learning; Transfer Learning; VGG16; Wildfire Classification (of information); Deep learning; Disasters; Fires; Learning algorithms; Learning systems; Remote sensing; Risk assessment; Satellite imagery; CNN models; Deep learning; Human settlements; Learning models; Natural disasters; Satellite images; Transfer learning; VGG16; Wildfire; Wildfire susceptibilities; Computer vision";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.4;Climatological;1;Prevention
518;Convolutional neural networks for object detection in aerial imagery for disaster response and recovery;Accurate and timely access to data describing disaster impact and extent of damage is key to successful disaster management (a process that includes prevention, mitigation, preparedness, response, and recovery). Airborne data acquisition using helicopter and unmanned aerial vehicle (UAV) helps obtain a bird's-eye view of disaster-affected areas. However, a major challenge to this approach is robustly processing a large amount of data to identify and map objects of interest on the ground in real-time. The current process is resource-intensive (must be carried out manually) and requires offline computing (through post-processing of aerial videos). This research introduces and evaluates a series of convolutional neural network (CNN) models for ground object detection from aerial views of disaster's aftermath. These models are capable of recognizing critical ground assets including building roofs (both damaged and undamaged), vehicles, vegetation, debris, and flooded areas. The CNN models are trained on an in-house aerial video dataset (named Volan2018) that is created using web mining techniques. Volan2018 contains eight annotated aerial videos (65,580 frames) collected by drone or helicopter from eight different locations in various hurricanes that struck the United States in 2017–2018. Eight CNN models based on You-Only-Look-Once (YOLO) algorithm are trained by transfer learning, i.e., pre-trained on the COCO/VOC dataset and re-trained on Volan2018 dataset, and achieve 80.69% mAP for high altitude (helicopter footage) and 74.48% for low altitude (drone footage), respectively. This paper also presents a thorough investigation of the effect of camera altitude, data balance, and pre-trained weights on model performance, and finds that models trained and tested on videos taken from similar altitude outperform those trained and tested on videos taken from different altitudes. Moreover, the CNN model pre-trained on the VOC dataset and re-trained on balanced drone video yields the best result in significantly shorter training time. © 2019 Elsevier Ltd;"Aerial reconnaissance; Convolutional neural network (CNN); Deep learning; Disaster management; Unmanned aerial vehicle (UAV)";"Aerial photography; Computer system recovery; Convolution; Data acquisition; Deep learning; Deep neural networks; Disaster prevention; Drones; Emergency services; Neural networks; Object detection; Object recognition; Reconnaissance aircraft; Unmanned aerial vehicles (UAV); Aerial reconnaissance; Airborne data acquisition; Convolutional neural network; Disaster management; Disaster response; Model performance; Off-line computing; Transfer learning; Antennas";"Convolutional neural networks for object detection in aerial imagery for disaster response and recovery Accurate and timely access to data describing disaster impact and extent of damage is key to successful disaster management (a process that includes prevention, mitigation, preparedness, response, and recovery). Airborne data acquisition using helicopter and unmanned aerial vehicle (UAV) helps obtain a bird's-eye view of disaster-affected areas. However, a major challenge to this approach is robustly processing a large amount of data to identify and map objects of interest on the ground in real-time. The current process is resource-intensive (must be carried out manually) and requires offline computing (through post-processing of aerial videos). This research introduces and evaluates a series of convolutional neural network (CNN) models for ground object detection from aerial views of disaster's aftermath. These models are capable of recognizing critical ground assets including building roofs (both damaged and undamaged), vehicles, vegetation, debris, and flooded areas. The CNN models are trained on an in-house aerial video dataset (named Volan2018) that is created using web mining techniques. Volan2018 contains eight annotated aerial videos (65,580 frames) collected by drone or helicopter from eight different locations in various hurricanes that struck the United States in 2017–2018. Eight CNN models based on You-Only-Look-Once (YOLO) algorithm are trained by transfer learning, i.e., pre-trained on the COCO/VOC dataset and re-trained on Volan2018 dataset, and achieve 80.69% mAP for high altitude (helicopter footage) and 74.48% for low altitude (drone footage), respectively. This paper also presents a thorough investigation of the effect of camera altitude, data balance, and pre-trained weights on model performance, and finds that models trained and tested on videos taken from similar altitude outperform those trained and tested on videos taken from different altitudes. Moreover, the CNN model pre-trained on the VOC dataset and re-trained on balanced drone video yields the best result in significantly shorter training time. © 2019 Elsevier Ltd Aerial reconnaissance; Convolutional neural network (CNN); Deep learning; Disaster management; Unmanned aerial vehicle (UAV) Aerial photography; Computer system recovery; Convolution; Data acquisition; Deep learning; Deep neural networks; Disaster prevention; Drones; Emergency services; Neural networks; Object detection; Object recognition; Reconnaissance aircraft; Unmanned aerial vehicles (UAV); Aerial reconnaissance; Airborne data acquisition; Convolutional neural network; Disaster management; Disaster response; Model performance; Off-line computing; Transfer learning; Antennas";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;-1;NULL;3;Response
519;Detection of Damaged Structures From Satellite Imagery Processed by Autoencoder With Boruta Feature Selection Method;Many worldwide changing events, including meteorology, weather forecasting, disaster response, and environmental monitoring, are tracked by states or companies via satellite imagery. Early response to disasters is critical for human life. In these cases, artificial intelligence applications are also used to make rapid determinations about large geographical region. In this study, satellite images of flooded and undamaged structures in Hurricane Harvey were used. An autoencoder process has been applied to this dataset to reduce the noise in satellite imagery. AlexNet and VGG16 deep learning (DL) models are used to extract features from both datasets. The most effective features selected by the Boruta feature selection algorithm were classified with the support vector machine, and the highest classification accuracy of 99.35% was obtained. Since disasters involve the evaluation of very big datasets from large geographic areas, presenting the data with the smallest possible feature will facilitate the process. For this reason, by applying dimensionality reduction to the selected attributes, a 98.29% success was achieved in the classification with only 90 attributes. The proposed approach shows that DL and feature engineering are very effective methods to quickly respond to disaster areas using satellite imagery. © 2023 Istanbul University. All rights reserved.;"Autoencoder; Boruta; dimensionality reduction; transfer learning";"Disasters; Feature Selection; Large dataset; Learning systems; Satellite imagery; Support vector machines; Weather forecasting; Auto encoders; Borutum; Damaged structures; Dimensionality reduction; Disaster-response; Environmental Monitoring; Feature selection methods; Human lives; Satellite images; Transfer learning; Deep learning";"Detection of Damaged Structures From Satellite Imagery Processed by Autoencoder With Boruta Feature Selection Method Many worldwide changing events, including meteorology, weather forecasting, disaster response, and environmental monitoring, are tracked by states or companies via satellite imagery. Early response to disasters is critical for human life. In these cases, artificial intelligence applications are also used to make rapid determinations about large geographical region. In this study, satellite images of flooded and undamaged structures in Hurricane Harvey were used. An autoencoder process has been applied to this dataset to reduce the noise in satellite imagery. AlexNet and VGG16 deep learning (DL) models are used to extract features from both datasets. The most effective features selected by the Boruta feature selection algorithm were classified with the support vector machine, and the highest classification accuracy of 99.35% was obtained. Since disasters involve the evaluation of very big datasets from large geographic areas, presenting the data with the smallest possible feature will facilitate the process. For this reason, by applying dimensionality reduction to the selected attributes, a 98.29% success was achieved in the classification with only 90 attributes. The proposed approach shows that DL and feature engineering are very effective methods to quickly respond to disaster areas using satellite imagery. © 2023 Istanbul University. All rights reserved. Autoencoder; Boruta; dimensionality reduction; transfer learning Disasters; Feature Selection; Large dataset; Learning systems; Satellite imagery; Support vector machines; Weather forecasting; Auto encoders; Borutum; Damaged structures; Dimensionality reduction; Disaster-response; Environmental Monitoring; Feature selection methods; Human lives; Satellite images; Transfer learning; Deep learning";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;3;Response
520;A Multi-Modal Wireless Sensor System for River Monitoring: A Case for Kikuletwa River Floods in Tanzania;Reliable and accurate flood prediction in poorly gauged basins is challenging due to data scarcity, especially in developing countries where many rivers remain insufficiently monitored. This hinders the design and development of advanced flood prediction models and early warning systems. This paper introduces a multi-modal, sensor-based, near-real-time river monitoring system that produces a multi-feature data set for the Kikuletwa River in Northern Tanzania, an area frequently affected by floods. The system improves upon existing literature by collecting six parameters relevant to weather and river flood detection: current hour rainfall (mm), previous hour rainfall (mm/h), previous day rainfall (mm/day), river level (cm), wind speed (km/h), and wind direction. These data complement the existing local weather station functionalities and can be used for river monitoring and extreme weather prediction. Tanzanian river basins currently lack reliable mechanisms for accurately establishing river thresholds for anomaly detection, which is essential for flood prediction models. The proposed monitoring system addresses this issue by gathering information about river depth levels and weather conditions at multiple locations. This broadens the ground truth of river characteristics, ultimately improving the accuracy of flood predictions. We provide details on the monitoring system used to gather the data, as well as report on the methodology and the nature of the data. The discussion then focuses on the relevance of the data set in the context of flood prediction, the most suitable AI/ML-based forecasting approaches, and highlights potential applications beyond flood warning systems. © 2023 by the authors.;"flood detection; machine learning; multi-featured data; river floods; wireless sensors";"Anomaly detection; Developing countries; Floods; Machine learning; Rain; Weather forecasting; Wind; Featured data; Flood detections; Flood prediction; Machine-learning; Monitoring system; Multi-featured data; River floods; River monitoring; Tanzania; Wireless sensor; Rivers";"A Multi-Modal Wireless Sensor System for River Monitoring: A Case for Kikuletwa River Floods in Tanzania Reliable and accurate flood prediction in poorly gauged basins is challenging due to data scarcity, especially in developing countries where many rivers remain insufficiently monitored. This hinders the design and development of advanced flood prediction models and early warning systems. This paper introduces a multi-modal, sensor-based, near-real-time river monitoring system that produces a multi-feature data set for the Kikuletwa River in Northern Tanzania, an area frequently affected by floods. The system improves upon existing literature by collecting six parameters relevant to weather and river flood detection: current hour rainfall (mm), previous hour rainfall (mm/h), previous day rainfall (mm/day), river level (cm), wind speed (km/h), and wind direction. These data complement the existing local weather station functionalities and can be used for river monitoring and extreme weather prediction. Tanzanian river basins currently lack reliable mechanisms for accurately establishing river thresholds for anomaly detection, which is essential for flood prediction models. The proposed monitoring system addresses this issue by gathering information about river depth levels and weather conditions at multiple locations. This broadens the ground truth of river characteristics, ultimately improving the accuracy of flood predictions. We provide details on the monitoring system used to gather the data, as well as report on the methodology and the nature of the data. The discussion then focuses on the relevance of the data set in the context of flood prediction, the most suitable AI/ML-based forecasting approaches, and highlights potential applications beyond flood warning systems. © 2023 by the authors. flood detection; machine learning; multi-featured data; river floods; wireless sensors Anomaly detection; Developing countries; Floods; Machine learning; Rain; Weather forecasting; Wind; Featured data; Flood detections; Flood prediction; Machine-learning; Monitoring system; Multi-featured data; River floods; River monitoring; Tanzania; Wireless sensor; Rivers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
521;FFHIApp: An Application for Flash Flood Hotspots Identification Using Real-Time Images;Extreme climate changes have become the new norm in today’s world. As a result, flash flood disasters continue to increase. It has become imperative to devise a quick disaster response system for minimizing the magnitude of damage and reducing the difficulties of human life. In this paper, we propose an application using android technology that provides real-time updates and prompt flow of authentic information of flood-ravaged areas to the rescue personnel or common people. The application accepts images belonging to flood-affected regions from rescue personnel, volunteers, etc. It authenticates and then filters those images using deep learning techniques. The severity of floods is estimated and plotted on a map using the associated location information of the images. The data is analyzed by using clustering techniques and visualized on the map. Subsequently, the affected areas of the flash flood are identified. Their peripheries are mapped so that these hotspots can be targeted for immediate relief operations. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.;"Clustering; Convolutional neural network; Flood hotspots identification; Flood-image detection; Transfer learning; VGG19";"Climate change; Deep learning; Disasters; Floods; Personnel; Android technologies; Clustering techniques; Disaster response systems; Extreme climates; Learning techniques; Location information; Real-time updates; Relief operations; Learning systems";"FFHIApp: An Application for Flash Flood Hotspots Identification Using Real-Time Images Extreme climate changes have become the new norm in today’s world. As a result, flash flood disasters continue to increase. It has become imperative to devise a quick disaster response system for minimizing the magnitude of damage and reducing the difficulties of human life. In this paper, we propose an application using android technology that provides real-time updates and prompt flow of authentic information of flood-ravaged areas to the rescue personnel or common people. The application accepts images belonging to flood-affected regions from rescue personnel, volunteers, etc. It authenticates and then filters those images using deep learning techniques. The severity of floods is estimated and plotted on a map using the associated location information of the images. The data is analyzed by using clustering techniques and visualized on the map. Subsequently, the affected areas of the flash flood are identified. Their peripheries are mapped so that these hotspots can be targeted for immediate relief operations. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Clustering; Convolutional neural network; Flood hotspots identification; Flood-image detection; Transfer learning; VGG19 Climate change; Deep learning; Disasters; Floods; Personnel; Android technologies; Clustering techniques; Disaster response systems; Extreme climates; Learning techniques; Location information; Real-time updates; Relief operations; Learning systems";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;3;Response
522;Wildfire detection using transfer learning on augmented datasets;Wildfire detection is a time-critical application as the difficulty to pinpoint ignition locations in a short time-frame often leads to the escalation of the severity of fire events. This problem has motivated considerable interest from expert systems research to develop accurate early-warning applications and the breakthroughs in deep learning in complex visual understanding tasks open novel research opportunities. However, despite the improvements in performance demonstrated in the current literature, a comprehensive study of the challenges and limitations of this approach is still a gap in the state-of-the-art. To address this issue, the contributions of this work are threefold. First, we overview recent works to identify common difficulties and shortcomings of these approaches, and assess issues related to the quality of the databases. Second, to overcome data limitations, this work proposes a transfer learning approach coupled with data augmentation techniques tested under a tenfold cross-validation scheme. The proposed framework enables leveraging an open-source dataset featuring images from more than 35 real fire events, which unlike video-based works offers higher variability between samples, allowing evaluating the approach in an extensive set of real scenarios. Third, this article presents an in-depth study of the limitations, providing a comprehensive analysis of the patterns causing misclassifications. The key insights gained in this analysis provide relevant takeaways to guide future research towards the implementation of expert systems in decision support systems in firefighting and civil protection operations. © 2019 Elsevier Ltd;"Data augmentation; Deep learning; Fire detection; Transfer learning; Wildfires datasets; Wildland-urban-interface";"Artificial intelligence; Decision support systems; Expert systems; Fire hazards; Fires; Data augmentation; Fire detection; Transfer learning; Wildfires datasets; Wildland urban interface; Deep learning";"Wildfire detection using transfer learning on augmented datasets Wildfire detection is a time-critical application as the difficulty to pinpoint ignition locations in a short time-frame often leads to the escalation of the severity of fire events. This problem has motivated considerable interest from expert systems research to develop accurate early-warning applications and the breakthroughs in deep learning in complex visual understanding tasks open novel research opportunities. However, despite the improvements in performance demonstrated in the current literature, a comprehensive study of the challenges and limitations of this approach is still a gap in the state-of-the-art. To address this issue, the contributions of this work are threefold. First, we overview recent works to identify common difficulties and shortcomings of these approaches, and assess issues related to the quality of the databases. Second, to overcome data limitations, this work proposes a transfer learning approach coupled with data augmentation techniques tested under a tenfold cross-validation scheme. The proposed framework enables leveraging an open-source dataset featuring images from more than 35 real fire events, which unlike video-based works offers higher variability between samples, allowing evaluating the approach in an extensive set of real scenarios. Third, this article presents an in-depth study of the limitations, providing a comprehensive analysis of the patterns causing misclassifications. The key insights gained in this analysis provide relevant takeaways to guide future research towards the implementation of expert systems in decision support systems in firefighting and civil protection operations. © 2019 Elsevier Ltd Data augmentation; Deep learning; Fire detection; Transfer learning; Wildfires datasets; Wildland-urban-interface Artificial intelligence; Decision support systems; Expert systems; Fire hazards; Fires; Data augmentation; Fire detection; Transfer learning; Wildfires datasets; Wildland urban interface; Deep learning";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.4;Climatological;2;Preparation
523;Fighting for information credibility: An end-to-end framework to identify fake news during natural disasters;Fast-spreading fake news has become an epidemic in the post-truth world of politics, the stock market, or even during natural disasters. A large amount of unverified information may reach a vast audience quickly via social media. The effect of misinformation (false) and disinformation (deliberately false) is more severe during the critical time of natural disasters such as flooding, hurricanes, or earthquakes. This can lead to disruptions in rescue missions and recovery activities, costing human lives and delaying the time needed for affected communities to return to normal. In this paper, we designed a comprehensive framework which is capable of developing a training set and trains a deep learning model for detecting fake news events occurring during disasters. Our proposed framework includes infrastructure to collect Twitter posts which spread false information. In our model implementation, we utilized the Transfer Learning scheme to transfer knowledge gained from a large and general fake news dataset to relatively smaller fake news events occurring during disasters as a means of overcoming the limited size of our training dataset. Our detection model was able to achieve an accuracy of 91.47% and F1 score of 90.89 when it was trained with the first 28 hours of Twitter data. Our vision for this study is to help emergency managers during disaster response with our framework so that they may perform their rescue and recovery actions effectively and efficiently without being distracted by false information. © 2020 Information Systems for Crisis Response and Management, ISCRAM. All rights reserved.;"Deep Learning; Fake News; Natural Disaster; Neural Networks; Social Network";"Deep neural networks; Disasters; Information systems; Information use; Large dataset; Personnel training; Social networking (online); Critical time; Deep learning; End to end; Fake news; Information credibilities; Large amounts; Natural disasters; Neural-networks; Social media; Social network; Information management";"Fighting for information credibility: An end-to-end framework to identify fake news during natural disasters Fast-spreading fake news has become an epidemic in the post-truth world of politics, the stock market, or even during natural disasters. A large amount of unverified information may reach a vast audience quickly via social media. The effect of misinformation (false) and disinformation (deliberately false) is more severe during the critical time of natural disasters such as flooding, hurricanes, or earthquakes. This can lead to disruptions in rescue missions and recovery activities, costing human lives and delaying the time needed for affected communities to return to normal. In this paper, we designed a comprehensive framework which is capable of developing a training set and trains a deep learning model for detecting fake news events occurring during disasters. Our proposed framework includes infrastructure to collect Twitter posts which spread false information. In our model implementation, we utilized the Transfer Learning scheme to transfer knowledge gained from a large and general fake news dataset to relatively smaller fake news events occurring during disasters as a means of overcoming the limited size of our training dataset. Our detection model was able to achieve an accuracy of 91.47% and F1 score of 90.89 when it was trained with the first 28 hours of Twitter data. Our vision for this study is to help emergency managers during disaster response with our framework so that they may perform their rescue and recovery actions effectively and efficiently without being distracted by false information. © 2020 Information Systems for Crisis Response and Management, ISCRAM. All rights reserved. Deep Learning; Fake News; Natural Disaster; Neural Networks; Social Network Deep neural networks; Disasters; Information systems; Information use; Large dataset; Personnel training; Social networking (online); Critical time; Deep learning; End to end; Fake news; Information credibilities; Large amounts; Natural disasters; Neural-networks; Social media; Social network; Information management";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;-1;NULL;3;Response
524;The Resilience to Emergencies and Disasters Index: Applying big data to benchmark and validate neighborhood resilience capacity;Resilience planning and emergency management require policymakers and agency leaders to make difficult decisions regarding which at-risk populations should be given priority in the allocation of limited resources. Our work focuses on benchmarking neighborhood resilience by developing a unified, multi-factor index of local and regional resilience capacity: the Resilience to Emergencies and Disasters Index (REDI). The strength of the REDI methodology is the integration of measures of physical, natural, and social systems – operationalized through the collection and analysis of large-scale, heterogeneous, and high resolution urban data – to classify and rank the relative resilience capacity embedded in localized urban systems. Feature selection methodologies are discussed to justify the selection of included indicator variables. Hurricane Sandy is used to validate the REDI scores by measuring the recovery periods for neighborhoods directly impacted by the storm. Using over 12,000,000 records for New York City's 311 service request system, we develop a proxy for neighborhood activity, both pre- and post-event. Hurricane Sandy had a significant and immediate impact on neighborhoods classified as least resilient based on the calculated REDI scores, while the most resilient neighborhoods were shown to better withstand disruption to normal activity patterns and more quickly recover to pre-event functional capacity. © 2017 Elsevier Ltd;"Anomaly detection; Big data; Community resilience; Disaster recovery; Emergency management; Urban resilience";"Civil defense; Disasters; Embedded systems; Hurricanes; Recovery; Risk management; Anomaly detection; Community resiliences; Disaster recovery; Emergency management; Urban resilience; Big data";"The Resilience to Emergencies and Disasters Index: Applying big data to benchmark and validate neighborhood resilience capacity Resilience planning and emergency management require policymakers and agency leaders to make difficult decisions regarding which at-risk populations should be given priority in the allocation of limited resources. Our work focuses on benchmarking neighborhood resilience by developing a unified, multi-factor index of local and regional resilience capacity: the Resilience to Emergencies and Disasters Index (REDI). The strength of the REDI methodology is the integration of measures of physical, natural, and social systems – operationalized through the collection and analysis of large-scale, heterogeneous, and high resolution urban data – to classify and rank the relative resilience capacity embedded in localized urban systems. Feature selection methodologies are discussed to justify the selection of included indicator variables. Hurricane Sandy is used to validate the REDI scores by measuring the recovery periods for neighborhoods directly impacted by the storm. Using over 12,000,000 records for New York City's 311 service request system, we develop a proxy for neighborhood activity, both pre- and post-event. Hurricane Sandy had a significant and immediate impact on neighborhoods classified as least resilient based on the calculated REDI scores, while the most resilient neighborhoods were shown to better withstand disruption to normal activity patterns and more quickly recover to pre-event functional capacity. © 2017 Elsevier Ltd Anomaly detection; Big data; Community resilience; Disaster recovery; Emergency management; Urban resilience Civil defense; Disasters; Embedded systems; Hurricanes; Recovery; Risk management; Anomaly detection; Community resiliences; Disaster recovery; Emergency management; Urban resilience; Big data";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.3;Meteorological;1;Prevention
525;Early detection of internal erosion in earth dams: combining seismic monitoring and convolutional AutoEncoders;Levees/earth dams are critical infrastructures for supplementing clean water, flood management, and energy production, prone to progressive failures due to internal erosion. Current inspection methods are unable to detect internal erosion until its exterior manifestation when it is too late to prevent the often-catastrophic failures. Therefore, finding innovative methods for the early detection of internal erosion is crucial. Despite the knowledge about the general mechanism of internal erosion, its early detection (and prevention) has remained a gap. This study introduces a novel artificial intelligence (AI) method to identify the temporal patterns within the passive seismic monitoring data, which can be associated with internal erosion initiation in earth dams. The proposed approach implements Convolutional AutoEncoders, an emerging deep-learning algorithm for anomaly detection in time-series data. Through an unsupervised learning framework, the AutoEncoders are trained using passive seismic monitoring data collected from a full-scale test embankment. In addition to the approximate initiation time, this algorithm can evaluate the initiation location by identifying the first sensors demonstrating internal erosion signs. The proposed deep learning framework combined with continuous seismic monitoring can serve as a basis for developing advanced early warning systems for internal erosion in earth dams. © 2023 Informa UK Limited, trading as Taylor & Francis Group.;"Anomaly detection; Artificial intelligence; Convolutional autoencoder; Earth dams; Internal erosion; Passive seismic data";"algorithm; artificial intelligence; early warning system; earth dam; erosion; levee; seismic data";"Early detection of internal erosion in earth dams: combining seismic monitoring and convolutional AutoEncoders Levees/earth dams are critical infrastructures for supplementing clean water, flood management, and energy production, prone to progressive failures due to internal erosion. Current inspection methods are unable to detect internal erosion until its exterior manifestation when it is too late to prevent the often-catastrophic failures. Therefore, finding innovative methods for the early detection of internal erosion is crucial. Despite the knowledge about the general mechanism of internal erosion, its early detection (and prevention) has remained a gap. This study introduces a novel artificial intelligence (AI) method to identify the temporal patterns within the passive seismic monitoring data, which can be associated with internal erosion initiation in earth dams. The proposed approach implements Convolutional AutoEncoders, an emerging deep-learning algorithm for anomaly detection in time-series data. Through an unsupervised learning framework, the AutoEncoders are trained using passive seismic monitoring data collected from a full-scale test embankment. In addition to the approximate initiation time, this algorithm can evaluate the initiation location by identifying the first sensors demonstrating internal erosion signs. The proposed deep learning framework combined with continuous seismic monitoring can serve as a basis for developing advanced early warning systems for internal erosion in earth dams. © 2023 Informa UK Limited, trading as Taylor & Francis Group. Anomaly detection; Artificial intelligence; Convolutional autoencoder; Earth dams; Internal erosion; Passive seismic data algorithm; artificial intelligence; early warning system; earth dam; erosion; levee; seismic data";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
526;Fine-Tuned EfficientNetB4 Transfer Learning Model for Weather Classification;In meteorology and climate research, weather classification is known as weather classification, identifying the present weather patterns based on readings and observations of numerous environmental characteristics, such as temperature, humidity, pressure, and precipitation. This study presents a deep learning model for classifying the weather that uses the Adam optimizer and the EfficientNetB4 architecture with fine-tuning layers. A collection of 6862 photos with 11 annotated classes, including dew, fog/smog, frost, glazing, hail, lightning, rain, rainbow, rime, sandstorm, and snow, served as the basis for training and testing the model. The dataset was randomly divided into three different sets for training (80%), validation (10%), and testing (10%). Data augmentation was then employed on the training set to remove class imbalances. The proposed model's accuracy at predicting the weather was 92%, proving its effectiveness. The model adapted to the specific assignment of classifying the weather thanks to the fine-tuning layers, and the EfficientNetB4 architecture offered a solid and effective feature extractor. The model was trained using the Adam optimizer, which adjusted the rate of learning for each of the parameters separately and assisted the model to converge more quickly and consistently. The suggested model may benefit various applications, including transportation, agriculture, aviation, and emergency management, that call for precise and fast meteorological data. Additionally, it can enhance machine learning and digital image processing. Future studies may further investigate other architectures, hyperparameters, and regularization methods to boost the model's speed and resilience. © 2023 IEEE.;"Changing Weather Patterns; Climate Change Management; Climate Early Warning; EfficientNetB4; Fine-tuning; Weather Classification";"Architecture; Climate models; Deep learning; Image enhancement; Learning systems; Precipitation (meteorology); Risk assessment; Risk management; Statistical tests; Storms; Change management; Changing weather pattern; Climate change management; Climate early warning; Early warning; Efficientnetb4; Fine tuning; Learning models; Weather classification; Weather patterns; Climate change";"Fine-Tuned EfficientNetB4 Transfer Learning Model for Weather Classification In meteorology and climate research, weather classification is known as weather classification, identifying the present weather patterns based on readings and observations of numerous environmental characteristics, such as temperature, humidity, pressure, and precipitation. This study presents a deep learning model for classifying the weather that uses the Adam optimizer and the EfficientNetB4 architecture with fine-tuning layers. A collection of 6862 photos with 11 annotated classes, including dew, fog/smog, frost, glazing, hail, lightning, rain, rainbow, rime, sandstorm, and snow, served as the basis for training and testing the model. The dataset was randomly divided into three different sets for training (80%), validation (10%), and testing (10%). Data augmentation was then employed on the training set to remove class imbalances. The proposed model's accuracy at predicting the weather was 92%, proving its effectiveness. The model adapted to the specific assignment of classifying the weather thanks to the fine-tuning layers, and the EfficientNetB4 architecture offered a solid and effective feature extractor. The model was trained using the Adam optimizer, which adjusted the rate of learning for each of the parameters separately and assisted the model to converge more quickly and consistently. The suggested model may benefit various applications, including transportation, agriculture, aviation, and emergency management, that call for precise and fast meteorological data. Additionally, it can enhance machine learning and digital image processing. Future studies may further investigate other architectures, hyperparameters, and regularization methods to boost the model's speed and resilience. © 2023 IEEE. Changing Weather Patterns; Climate Change Management; Climate Early Warning; EfficientNetB4; Fine-tuning; Weather Classification Architecture; Climate models; Deep learning; Image enhancement; Learning systems; Precipitation (meteorology); Risk assessment; Risk management; Statistical tests; Storms; Change management; Changing weather pattern; Climate change management; Climate early warning; Early warning; Efficientnetb4; Fine tuning; Learning models; Weather classification; Weather patterns; Climate change";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;2;Preparation
527;Marine Seismic P-Phase Picking Network for Floating Seismographs Using Transfer Learning;P-phase picking is especially crucial in marine earthquake because the P-wave is the first-arrival wave, which can be used for early warning and seafloor geological imaging. However, due to the scarcity of data, the performance of P-phase picking in marine earthquake is not satisfactory, especially in floating seismographs. In this paper, we use land seismic data to guide floating seismographs data, propose a transfer-learned method for P-phase picking in marine earthquake, and propose a ID axial attention module, significantly improving the performance of marine seismic P-phase picking under limited data. Experimental results show that our model has a higher Fl-score of 97.4% compared to other methods.  © 2024 IEEE.;"floating seismograph; P-phase picking; transfer learning";"Earthquake effects; Hydrogeology; Seismic response; Seismic waves; Seismographs; Early warning; First arrival; Floating seismograph; Limited data; P waves; P-phase picking; Performance; Phase pickings; Seafloors; Transfer learning; Transfer learning";"Marine Seismic P-Phase Picking Network for Floating Seismographs Using Transfer Learning P-phase picking is especially crucial in marine earthquake because the P-wave is the first-arrival wave, which can be used for early warning and seafloor geological imaging. However, due to the scarcity of data, the performance of P-phase picking in marine earthquake is not satisfactory, especially in floating seismographs. In this paper, we use land seismic data to guide floating seismographs data, propose a transfer-learned method for P-phase picking in marine earthquake, and propose a ID axial attention module, significantly improving the performance of marine seismic P-phase picking under limited data. Experimental results show that our model has a higher Fl-score of 97.4% compared to other methods.  © 2024 IEEE. floating seismograph; P-phase picking; transfer learning Earthquake effects; Hydrogeology; Seismic response; Seismic waves; Seismographs; Early warning; First arrival; Floating seismograph; Limited data; P waves; P-phase picking; Performance; Phase pickings; Seafloors; Transfer learning; Transfer learning";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;2;Preparation
528;Satellite Imagery Driven Flood Detection using Deep Learning;Flood detection is crucial for effective dis-aster response and management, enabling early warning systems and targeted relief efforts. This paper proposes a deep learning-oriented approach for flood detection in Convolutional neural networks (CNNs) and transfer learning applied to satellite pictures techniques. Satellite imagery provides wide coverage and high spatial resolution, making it a valuable resource for flood monitoring. However, the complexity of flood patterns requires advanced computational techniques for accurate analysis. CNNs deep learning models have demonstrated impressive achievements in the realm of image classification, showcasing an ability to autonomously acquire significant features from visual data. The proposed methodology aims to train a robust flood detection model capable of discerning flooded and non-flooded regions within satellite images. transfer learning enhances the model's performance by adapting pre-trained models to t Transfer learning uses pre-trained models to improve the performance of the model. he flood detection domain, even in regions with limited historical flood data. the output of this work have the ability make revolution for flood detection systems and improve catastrophe handling techniques, leading to reduced vulnerability in flood-prone areas. by leveraging deep learning methodologies, we can advance our understanding of floods and build more resilient communities to confront this escalating threat. The paper introduces an innovative method for flood detection that combines CNNs and transfer learning in an effort to create a reliable and accurate flood detection model. The work's outcomes contribute to the advancement of flood monitoring systems, improving disaster response and reducing vulnerability in flood-prone areas © 2024 IEEE.;"Computer Vision & AI; Convolutional Neural Networks (CNNs); Deep Learning; Flood Detection; NN; Pooling; Satellite Imagery; Transfer learning";"Convolutional neural networks; Deep neural networks; Image enhancement; Metadata; Network security; Transfer learning; Computer vision & AI; Convolutional neural network; Deep learning; Flood detections; Flood monitoring; Neural network learning; NN; Pooling; Transfer learning; Satellite imagery";"Satellite Imagery Driven Flood Detection using Deep Learning Flood detection is crucial for effective dis-aster response and management, enabling early warning systems and targeted relief efforts. This paper proposes a deep learning-oriented approach for flood detection in Convolutional neural networks (CNNs) and transfer learning applied to satellite pictures techniques. Satellite imagery provides wide coverage and high spatial resolution, making it a valuable resource for flood monitoring. However, the complexity of flood patterns requires advanced computational techniques for accurate analysis. CNNs deep learning models have demonstrated impressive achievements in the realm of image classification, showcasing an ability to autonomously acquire significant features from visual data. The proposed methodology aims to train a robust flood detection model capable of discerning flooded and non-flooded regions within satellite images. transfer learning enhances the model's performance by adapting pre-trained models to t Transfer learning uses pre-trained models to improve the performance of the model. he flood detection domain, even in regions with limited historical flood data. the output of this work have the ability make revolution for flood detection systems and improve catastrophe handling techniques, leading to reduced vulnerability in flood-prone areas. by leveraging deep learning methodologies, we can advance our understanding of floods and build more resilient communities to confront this escalating threat. The paper introduces an innovative method for flood detection that combines CNNs and transfer learning in an effort to create a reliable and accurate flood detection model. The work's outcomes contribute to the advancement of flood monitoring systems, improving disaster response and reducing vulnerability in flood-prone areas © 2024 IEEE. Computer Vision & AI; Convolutional Neural Networks (CNNs); Deep Learning; Flood Detection; NN; Pooling; Satellite Imagery; Transfer learning Convolutional neural networks; Deep neural networks; Image enhancement; Metadata; Network security; Transfer learning; Computer vision & AI; Convolutional neural network; Deep learning; Flood detections; Flood monitoring; Neural network learning; NN; Pooling; Transfer learning; Satellite imagery";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;2;Preparation
529;Intelligent monitor for typhoon in IoT system of smart city;"Accidents often occur in the earth—typhoons, floods, earthquakes, traffic accidents and so on. Whether these accidents can be timely and effectively responded to has been an important indicator to judge whether a region is advanced or not. IoT provide a possibility to solve such emergent problems by intelligent monitoring, diagnosis and repair. For example, coastal cities are often attacked by typhoons, if typhoon meteorological identification and early warning can be effectively carried out, many unnecessary property and personnel losses can be reduced. Accurate typhoon prediction has very important practical significance. However, current typhoon monitoring and prediction are mainly based on simulation with meteorological data; the accuracy still needs to be improved. Nowadays, the technology of Internet of Things (IoT) and remote sensing technology become more and more closely linked; many IoT systems in smart cities’ can obtain high-resolution remote sensing image data. Therefore, it is possible to use urban IoT system to realize the early warning of typhoon. In this paper, we propose a deep learning method for typhoon cloud recognition and typhoon center location, and design a general algorithm framework, including data preprocessing, model training and parameter selection, test and result analysis. Besides, we implement a typhoon early warning demo system. The experimental results show that our algorithm is better than the traditional methods in recognition accuracy. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.";"Fast R-CNN; Fine-tuning; IoT; Transfer learning; Typhoon recognition";"Accidents; Deep learning; Hurricanes; Learning systems; Remote sensing; Smart city; Algorithm framework; High resolution remote sensing images; Intelligent monitoring; Internet of Things (IOT); Meteorological data; Recognition accuracy; Remote sensing technology; Typhoon center locations; Internet of things";"Intelligent monitor for typhoon in IoT system of smart city Accidents often occur in the earth—typhoons, floods, earthquakes, traffic accidents and so on. Whether these accidents can be timely and effectively responded to has been an important indicator to judge whether a region is advanced or not. IoT provide a possibility to solve such emergent problems by intelligent monitoring, diagnosis and repair. For example, coastal cities are often attacked by typhoons, if typhoon meteorological identification and early warning can be effectively carried out, many unnecessary property and personnel losses can be reduced. Accurate typhoon prediction has very important practical significance. However, current typhoon monitoring and prediction are mainly based on simulation with meteorological data; the accuracy still needs to be improved. Nowadays, the technology of Internet of Things (IoT) and remote sensing technology become more and more closely linked; many IoT systems in smart cities’ can obtain high-resolution remote sensing image data. Therefore, it is possible to use urban IoT system to realize the early warning of typhoon. In this paper, we propose a deep learning method for typhoon cloud recognition and typhoon center location, and design a general algorithm framework, including data preprocessing, model training and parameter selection, test and result analysis. Besides, we implement a typhoon early warning demo system. The experimental results show that our algorithm is better than the traditional methods in recognition accuracy. © 2020, Springer Science+Business Media, LLC, part of Springer Nature. Fast R-CNN; Fine-tuning; IoT; Transfer learning; Typhoon recognition Accidents; Deep learning; Hurricanes; Learning systems; Remote sensing; Smart city; Algorithm framework; High resolution remote sensing images; Intelligent monitoring; Internet of Things (IOT); Meteorological data; Recognition accuracy; Remote sensing technology; Typhoon center locations; Internet of things";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;2;Preparation
530;Automatic Analysis of Potential Hazard Events Using Unmanned Aerial Vehicles;This paper is motivated by the possibility of developing a wide variety of applications and domains in which Unmanned Aerial Vehicles (UAVs) can be used globally for various purposes. UAVs are currently used by public administrations and security forces such as police, fire brigades, civil protection, research institutions, construction, and agriculture entities. The purpose of this paper is to facilitate the handling of UAVs to retrieve various data from the environment. The drone (UAV) visits some points to collect data (image and/or video input) from sensors like GPS, camera, gyroscope, and accelerometer. GPS sensor coordinates are used to compare the data taken with subsequent results through processing with specialized software. The drone is used as an access gate with built-in sensors. Certain hazard events (fires, floods, avalanches, landslides) are not limited to narrow geographical areas, but can impact the environment by triggering negative chain events. 3D modeling offers a wide range of possibilities to prevent potential hazard events, or, if such an event has occurred, makes it possible to monitor the affected area and assess the damage by comparing the area in the pre-event configuration with the after-event one. After image processing and data acquisition, a report is generated that includes the map and the 3D model of the analyzed object. A hazard is an agent that has the potential to cause damage to a particular target. Terms such as risk or danger can be used in similar contexts. TensorFlow is an open source software library in high-performance computing. Flexible architecture allows easy deployment of computing on a variety of platforms (CPU, GPU, TPU), from desktop to server or mobile devices. We used the learning transfer: at first we used a model that was already prepared for another problem, and then we re-qualified it on a similar problem. Deep learning from scratch can take several days, but learning transfer can be done shortly. We applied Python along with TensorFlow to train an image classifier and classify images with it. We formed a consistent set of training pictures, using three labels: fire, flood (detectable hazards) and nature (non-hazard images). We then re-qualified an efficient, small-sized neural network by (re)training the image set in order to get the best results in the hazards prediction selection process with a progressive higher accuracy as (re) training evolves at optimal rating. With Python and OpenCV technologies, we used four decision algorithms to generate prediction of hazard: Support Vector Machine, Naive Bayes, Logistic Regression, and Decision Tree Classifier. Each generated report includes precision, recall, f1-score, and support indices, depending on the class and intervals used. We also used the confusion matrix as an alternative method to evaluate the classification accuracy. Analyzing the 4 algorithms we noticed that they behave differently. Training using TensorFlow generated better results than the other methods. For the main classes tested hazard is recognized up to 99%. © 2019 IEEE.;"Classifiers; Decision; Hazard; Prediction; UAV";"3D modeling; Agricultural robots; Data acquisition; Data handling; Decision trees; Deep learning; Drones; Fire protection; Floods; Global positioning system; Hazards; High level languages; Image classification; Logistic regression; Mobile antennas; Open source software; Open systems; Public administration; Support vector machines; Support vector regression; Automatic analysis; Classification accuracy; Decision algorithms; Decision tree classifiers; Flexible architectures; High performance computing; Research institutions; Specialized software; Transfer learning";"Automatic Analysis of Potential Hazard Events Using Unmanned Aerial Vehicles This paper is motivated by the possibility of developing a wide variety of applications and domains in which Unmanned Aerial Vehicles (UAVs) can be used globally for various purposes. UAVs are currently used by public administrations and security forces such as police, fire brigades, civil protection, research institutions, construction, and agriculture entities. The purpose of this paper is to facilitate the handling of UAVs to retrieve various data from the environment. The drone (UAV) visits some points to collect data (image and/or video input) from sensors like GPS, camera, gyroscope, and accelerometer. GPS sensor coordinates are used to compare the data taken with subsequent results through processing with specialized software. The drone is used as an access gate with built-in sensors. Certain hazard events (fires, floods, avalanches, landslides) are not limited to narrow geographical areas, but can impact the environment by triggering negative chain events. 3D modeling offers a wide range of possibilities to prevent potential hazard events, or, if such an event has occurred, makes it possible to monitor the affected area and assess the damage by comparing the area in the pre-event configuration with the after-event one. After image processing and data acquisition, a report is generated that includes the map and the 3D model of the analyzed object. A hazard is an agent that has the potential to cause damage to a particular target. Terms such as risk or danger can be used in similar contexts. TensorFlow is an open source software library in high-performance computing. Flexible architecture allows easy deployment of computing on a variety of platforms (CPU, GPU, TPU), from desktop to server or mobile devices. We used the learning transfer: at first we used a model that was already prepared for another problem, and then we re-qualified it on a similar problem. Deep learning from scratch can take several days, but learning transfer can be done shortly. We applied Python along with TensorFlow to train an image classifier and classify images with it. We formed a consistent set of training pictures, using three labels: fire, flood (detectable hazards) and nature (non-hazard images). We then re-qualified an efficient, small-sized neural network by (re)training the image set in order to get the best results in the hazards prediction selection process with a progressive higher accuracy as (re) training evolves at optimal rating. With Python and OpenCV technologies, we used four decision algorithms to generate prediction of hazard: Support Vector Machine, Naive Bayes, Logistic Regression, and Decision Tree Classifier. Each generated report includes precision, recall, f1-score, and support indices, depending on the class and intervals used. We also used the confusion matrix as an alternative method to evaluate the classification accuracy. Analyzing the 4 algorithms we noticed that they behave differently. Training using TensorFlow generated better results than the other methods. For the main classes tested hazard is recognized up to 99%. © 2019 IEEE. Classifiers; Decision; Hazard; Prediction; UAV 3D modeling; Agricultural robots; Data acquisition; Data handling; Decision trees; Deep learning; Drones; Fire protection; Floods; Global positioning system; Hazards; High level languages; Image classification; Logistic regression; Mobile antennas; Open source software; Open systems; Public administration; Support vector machines; Support vector regression; Automatic analysis; Classification accuracy; Decision algorithms; Decision tree classifiers; Flexible architectures; High performance computing; Research institutions; Specialized software; Transfer learning";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;-1;NULL;2;Preparation
531;WSN-Based Smart Landslide Monitoring Device;In this article, we present a smart monitoring wireless sensor networking (WSN) node device, referred to as 'SMARTMODE. ' The indigenously developed SMARTMODE is energy-efficient, context-adaptive, reliable, and accurate. We have custom-designed it to monitor landslide movement and causative parameters and to ensure the system's longevity, reliability, and robustness in harsh environmental conditions. When in sleep mode, the SMARTMODE is designed in such a way that it consumes less power (i.e., 0.034 mA at 4.2 V) while is still able to detect minor slope variations. This is achieved by enabling low-power interrupt-based sensing in the SMARTMODE. The SMARTMODE is also capable of automatically varying its data acquisition frequency based on the severity of the environmental context, thereby making it context-adaptive. Furthermore, we designed the SMARTMODE in such a way that it can detect and filter faults such as noise and outliers in the acquired sensor's data. In addition, to enhance the reliability, SMARTMODE comprises energy harvesting and backup approach to deal with power outage issues. This approach enables automatic recharging of SMARTMODE's battery even during nighttime to ensure reliable and continuous landslide monitoring. Finally, we evaluated the performance of the proposed SMARTMODE by deploying it in an actual landslide location.  © 1963-2012 IEEE.;"Data fault detection; landslide; landslide early warning system (LEWS); noise suppression; outlier detection; sensor nodes";"Anomaly detection; Data acquisition; Data handling; Energy efficiency; Energy harvesting; Fault detection; Landslides; Outages; Statistics; Data faults; Data-fault detection; Faults detection; Hardware; Landslide monitoring; LEWS; Noise suppression; Outlier Detection; Software; Terrain factors; Sensor nodes";"WSN-Based Smart Landslide Monitoring Device In this article, we present a smart monitoring wireless sensor networking (WSN) node device, referred to as 'SMARTMODE. ' The indigenously developed SMARTMODE is energy-efficient, context-adaptive, reliable, and accurate. We have custom-designed it to monitor landslide movement and causative parameters and to ensure the system's longevity, reliability, and robustness in harsh environmental conditions. When in sleep mode, the SMARTMODE is designed in such a way that it consumes less power (i.e., 0.034 mA at 4.2 V) while is still able to detect minor slope variations. This is achieved by enabling low-power interrupt-based sensing in the SMARTMODE. The SMARTMODE is also capable of automatically varying its data acquisition frequency based on the severity of the environmental context, thereby making it context-adaptive. Furthermore, we designed the SMARTMODE in such a way that it can detect and filter faults such as noise and outliers in the acquired sensor's data. In addition, to enhance the reliability, SMARTMODE comprises energy harvesting and backup approach to deal with power outage issues. This approach enables automatic recharging of SMARTMODE's battery even during nighttime to ensure reliable and continuous landslide monitoring. Finally, we evaluated the performance of the proposed SMARTMODE by deploying it in an actual landslide location.  © 1963-2012 IEEE. Data fault detection; landslide; landslide early warning system (LEWS); noise suppression; outlier detection; sensor nodes Anomaly detection; Data acquisition; Data handling; Energy efficiency; Energy harvesting; Fault detection; Landslides; Outages; Statistics; Data faults; Data-fault detection; Faults detection; Hardware; Landslide monitoring; LEWS; Noise suppression; Outlier Detection; Software; Terrain factors; Sensor nodes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
532;PhyMDAN: Physics-informed knowledge transfer between buildings for seismic damage diagnosis through adversarial learning;Automated structural damage diagnosis after earthquakes is important for improving efficiency of disaster response and city rehabilitation. In conventional data-driven frameworks which use machine learning or statistical models, structural damage diagnosis models are often constructed using supervised learning. Supervised learning requires historical structural response data and corresponding damage states (i.e., labels) for each building to learn the building-specific damage diagnosis model. However, in post-earthquake scenarios, historical data with labels are often not available for many buildings in the affected area. This makes it difficult to construct a damage diagnosis model. Further, directly using the historical data from other buildings to construct a damage diagnosis model for the target building would lead to inaccurate results. This is because each building has unique physical properties and thus unique data distribution. To this end, we introduce a new framework, Physics-Informed Multi-source Domain Adversarial Networks (PhyMDAN), to transfer the model learned from other buildings to diagnose structural damage states in the target building without any labels. This framework is based on an adversarial domain adaptation approach that extracts domain-invariant feature representations of data from different buildings. The feature extraction function is trained in an adversarial way, which ensures that extracted feature distributions are robust to variations of structural properties. The feature extraction function is simultaneously jointly trained with damage prediction function to ensure extracted features being predictive for structural damage states. With extracted domain-invariant feature representations, data distributions become consistent across different buildings. We evaluate our framework on both numerical simulation and field data collected from multiple building structures. The results show up to 90.13% damage detection accuracy and 84.47% damage quantification accuracy on simulation data, and up to 100% damage detection accuracy and 69.93% 5-class damage quantification accuracy when transferring from numerical simulation data to real-world experimental data, which outperforms the state-of-the-art benchmark methods. © 2020 Elsevier Ltd;"Domain adaptation; Statistical signal processing; Structural damage diagnosis; Transfer learning";"Buildings; Damage detection; Data mining; Earthquakes; Extraction; Feature extraction; Knowledge management; Learning systems; Numerical methods; Numerical models; Supervised learning; Adversarial learning; Adversarial networks; Damage quantification; Earthquake scenario; Feature distribution; Improving efficiency; Structural damage diagnosis; Structural response; Structural analysis";"PhyMDAN: Physics-informed knowledge transfer between buildings for seismic damage diagnosis through adversarial learning Automated structural damage diagnosis after earthquakes is important for improving efficiency of disaster response and city rehabilitation. In conventional data-driven frameworks which use machine learning or statistical models, structural damage diagnosis models are often constructed using supervised learning. Supervised learning requires historical structural response data and corresponding damage states (i.e., labels) for each building to learn the building-specific damage diagnosis model. However, in post-earthquake scenarios, historical data with labels are often not available for many buildings in the affected area. This makes it difficult to construct a damage diagnosis model. Further, directly using the historical data from other buildings to construct a damage diagnosis model for the target building would lead to inaccurate results. This is because each building has unique physical properties and thus unique data distribution. To this end, we introduce a new framework, Physics-Informed Multi-source Domain Adversarial Networks (PhyMDAN), to transfer the model learned from other buildings to diagnose structural damage states in the target building without any labels. This framework is based on an adversarial domain adaptation approach that extracts domain-invariant feature representations of data from different buildings. The feature extraction function is trained in an adversarial way, which ensures that extracted feature distributions are robust to variations of structural properties. The feature extraction function is simultaneously jointly trained with damage prediction function to ensure extracted features being predictive for structural damage states. With extracted domain-invariant feature representations, data distributions become consistent across different buildings. We evaluate our framework on both numerical simulation and field data collected from multiple building structures. The results show up to 90.13% damage detection accuracy and 84.47% damage quantification accuracy on simulation data, and up to 100% damage detection accuracy and 69.93% 5-class damage quantification accuracy when transferring from numerical simulation data to real-world experimental data, which outperforms the state-of-the-art benchmark methods. © 2020 Elsevier Ltd Domain adaptation; Statistical signal processing; Structural damage diagnosis; Transfer learning Buildings; Damage detection; Data mining; Earthquakes; Extraction; Feature extraction; Knowledge management; Learning systems; Numerical methods; Numerical models; Supervised learning; Adversarial learning; Adversarial networks; Damage quantification; Earthquake scenario; Feature distribution; Improving efficiency; Structural damage diagnosis; Structural response; Structural analysis";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;4;Recovery
533;A structural response logger with anomaly detection using a MEMS – based system;Due to the fact that Philippines is located along the Pacific ring of fire, visual inspection to assess the integrity of a structure is commonly used in the country. Other methods also exist but require destruction of a portion of the structure or may need expensive tools. Both give reasons for the proponents to think of a solution in risk reduction among structures such as buildings. In this study, a cost-efficient system will produce a solution to the time consuming, expensive and inaccurate approximation of the integrity of a structure. Using MEMS accelerometers incorporated with microcontroller capable of sending data over the internet, a structural response logger that can detect anomaly was produced. Its accuracy was evaluated by a vibration laboratory tests while its functionality such as the alarm was tested using the Government’s earthquake simulator. © 2018, UK Simulation Society. All rights reserved.;"Acceleration; Earthquake; Fire; Grounding; Intensity";NULL;"A structural response logger with anomaly detection using a MEMS – based system Due to the fact that Philippines is located along the Pacific ring of fire, visual inspection to assess the integrity of a structure is commonly used in the country. Other methods also exist but require destruction of a portion of the structure or may need expensive tools. Both give reasons for the proponents to think of a solution in risk reduction among structures such as buildings. In this study, a cost-efficient system will produce a solution to the time consuming, expensive and inaccurate approximation of the integrity of a structure. Using MEMS accelerometers incorporated with microcontroller capable of sending data over the internet, a structural response logger that can detect anomaly was produced. Its accuracy was evaluated by a vibration laboratory tests while its functionality such as the alarm was tested using the Government’s earthquake simulator. © 2018, UK Simulation Society. All rights reserved. Acceleration; Earthquake; Fire; Grounding; Intensity NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
534;Knowledge Discovery of Global Landslides Using Automated Machine Learning Algorithms;Understanding the complex dynamics of global landslides is essential for disaster planners to make timely and effective decisions that save lives and reduce the economic impacts on society. Using NASA's inventory of global landslide data, we developed a new machine learning (ML)-based system for town planners, disaster recovery strategists, and landslide researchers. Our system revealed hidden knowledge about a range of complex scenarios created from five landslide feature attributes. Users of our system can select from a list of 1.295× 1064 possible global landslide scenarios to discover valuable knowledge and predictions about the selected scenario in an interactive manner. Three ML algorithms - anomaly detection, decomposition analysis, and automated regression analysis - are used to elicit detailed knowledge about 25 scenarios selected from 14,532 global landslide records covering 12,220 injuries and 63,573 fatalities across 157 countries. Anomaly detection, logistic regression, and decomposition analysis performed well for all scenarios under study, with the area under the curve averaging 0.951, 0.911, and 0.896, respectively. Moreover, the prediction accuracy of linear regression had a mean absolute percentage error of 0.255. To the best of our knowledge, our scenario-based ML knowledge discovery system is the first of its kind to provide a comprehensive understanding of global landslide data.  © 2013 IEEE.;"anomaly detection; decomposition analysis; knowledge discovery; machine learning; regression analysis; Strategic decision support tool for landslides";"Anomaly detection; Disasters; Landslides; Learning algorithms; Machine learning; NASA; Planning; Regression analysis; Anomaly detection; Automated machines; Complex dynamics; Decomposition analysis; Disaster recovery; Economic impacts; Hidden knowledge; Machine learning algorithms; Strategic decision support tool for landslide; Strategic decisions; Decision support systems";"Knowledge Discovery of Global Landslides Using Automated Machine Learning Algorithms Understanding the complex dynamics of global landslides is essential for disaster planners to make timely and effective decisions that save lives and reduce the economic impacts on society. Using NASA's inventory of global landslide data, we developed a new machine learning (ML)-based system for town planners, disaster recovery strategists, and landslide researchers. Our system revealed hidden knowledge about a range of complex scenarios created from five landslide feature attributes. Users of our system can select from a list of 1.295× 1064 possible global landslide scenarios to discover valuable knowledge and predictions about the selected scenario in an interactive manner. Three ML algorithms - anomaly detection, decomposition analysis, and automated regression analysis - are used to elicit detailed knowledge about 25 scenarios selected from 14,532 global landslide records covering 12,220 injuries and 63,573 fatalities across 157 countries. Anomaly detection, logistic regression, and decomposition analysis performed well for all scenarios under study, with the area under the curve averaging 0.951, 0.911, and 0.896, respectively. Moreover, the prediction accuracy of linear regression had a mean absolute percentage error of 0.255. To the best of our knowledge, our scenario-based ML knowledge discovery system is the first of its kind to provide a comprehensive understanding of global landslide data.  © 2013 IEEE. anomaly detection; decomposition analysis; knowledge discovery; machine learning; regression analysis; Strategic decision support tool for landslides Anomaly detection; Disasters; Landslides; Learning algorithms; Machine learning; NASA; Planning; Regression analysis; Anomaly detection; Automated machines; Complex dynamics; Decomposition analysis; Disaster recovery; Economic impacts; Hidden knowledge; Machine learning algorithms; Strategic decision support tool for landslide; Strategic decisions; Decision support systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
535;Time-frequency methods for structural health monitoring;"Detection of early warning signals for the imminent failure of large and complex engineered structures is a daunting challenge with many open research questions. In this paper we report on novel ways to perform Structural Health Monitoring (SHM) of flood protection systems (levees, earthen dikes and concrete dams) using sensor data. We present a robust data-driven anomaly detection method that combines time-frequency feature extraction, using wavelet analysis and phase shift, with one-sided classification techniques to identify the onset of failure anomalies in real-time sensor measurements. The methodology has been successfully tested at three operational levees. We detected a dam leakage in the retaining dam (Germany) and ""strange"" behaviour of sensors installed in a Boston levee (UK) and a Rhine levee (Germany). © 2014 by the authors; licensee MDPI, Basel, Switzerland.";"Anomaly detection; Flood protection systems; Leakage detection; Levee monitoring; One-side classification; Sensors; Structural health monitoring; Time-frequency analysis";"Feature extraction; Flood control; Floods; Sensors; Structural health monitoring; Wavelet analysis; Anomaly detection; Classification technique; Flood protection; Leakage detection; Structural health monitoring (SHM); Time frequency analysis; Time frequency features; Time-frequency methods; Levees";"Time-frequency methods for structural health monitoring Detection of early warning signals for the imminent failure of large and complex engineered structures is a daunting challenge with many open research questions. In this paper we report on novel ways to perform Structural Health Monitoring (SHM) of flood protection systems (levees, earthen dikes and concrete dams) using sensor data. We present a robust data-driven anomaly detection method that combines time-frequency feature extraction, using wavelet analysis and phase shift, with one-sided classification techniques to identify the onset of failure anomalies in real-time sensor measurements. The methodology has been successfully tested at three operational levees. We detected a dam leakage in the retaining dam (Germany) and ""strange"" behaviour of sensors installed in a Boston levee (UK) and a Rhine levee (Germany). © 2014 by the authors; licensee MDPI, Basel, Switzerland. Anomaly detection; Flood protection systems; Leakage detection; Levee monitoring; One-side classification; Sensors; Structural health monitoring; Time-frequency analysis Feature extraction; Flood control; Floods; Sensors; Structural health monitoring; Wavelet analysis; Anomaly detection; Classification technique; Flood protection; Leakage detection; Structural health monitoring (SHM); Time frequency analysis; Time frequency features; Time-frequency methods; Levees";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
536;Disaster damage detection through synergistic use of deep learning and 3D point cloud features derived from very high resolution oblique aerial images, and multiple-kernel-learning;Oblique aerial images offer views of both building roofs and façades, and thus have been recognized as a potential source to detect severe building damages caused by destructive disaster events such as earthquakes. Therefore, they represent an important source of information for first responders or other stakeholders involved in the post-disaster response process. Several automated methods based on supervised learning have already been demonstrated for damage detection using oblique airborne images. However, they often do not generalize well when data from new unseen sites need to be processed, hampering their practical use. Reasons for this limitation include image and scene characteristics, though the most prominent one relates to the image features being used for training the classifier. Recently features based on deep learning approaches, such as convolutional neural networks (CNNs), have been shown to be more effective than conventional hand-crafted features, and have become the state-of-the-art in many domains, including remote sensing. Moreover, often oblique images are captured with high block overlap, facilitating the generation of dense 3D point clouds – an ideal source to derive geometric characteristics. We hypothesized that the use of CNN features, either independently or in combination with 3D point cloud features, would yield improved performance in damage detection. To this end we used CNN and 3D features, both independently and in combination, using images from manned and unmanned aerial platforms over several geographic locations that vary significantly in terms of image and scene characteristics. A multiple-kernel-learning framework, an effective way for integrating features from different modalities, was used for combining the two sets of features for classification. The results are encouraging: while CNN features produced an average classification accuracy of about 91%, the integration of 3D point cloud features led to an additional improvement of about 3% (i.e. an average classification accuracy of 94%). The significance of 3D point cloud features becomes more evident in the model transferability scenario (i.e., training and testing samples from different sites that vary slightly in the aforementioned characteristics), where the integration of CNN and 3D point cloud features significantly improved the model transferability accuracy up to a maximum of 7% compared with the accuracy achieved by CNN features alone. Overall, an average accuracy of 85% was achieved for the model transferability scenario across all experiments. Our main conclusion is that such an approach qualifies for practical use. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS);"3D point cloud features; CNN features; Model transferability; Multiple-kernel-learning; Oblique images; Structural damage detections; Transfer learning; UAV";"Classification (of information); Damage detection; Deep learning; Deep neural networks; Disasters; Integration testing; Neural networks; Remote sensing; Structural analysis; Unmanned aerial vehicles (UAV); 3D point cloud; CNN features; Model transferabilities; Multiple Kernel Learning; Oblique images; Structural damage detection; Transfer learning; accuracy assessment; aerial photography; artificial neural network; disaster; earthquake; image analysis; model validation; stakeholder; supervised learning; three-dimensional modeling; training; Feature extraction";"Disaster damage detection through synergistic use of deep learning and 3D point cloud features derived from very high resolution oblique aerial images, and multiple-kernel-learning Oblique aerial images offer views of both building roofs and façades, and thus have been recognized as a potential source to detect severe building damages caused by destructive disaster events such as earthquakes. Therefore, they represent an important source of information for first responders or other stakeholders involved in the post-disaster response process. Several automated methods based on supervised learning have already been demonstrated for damage detection using oblique airborne images. However, they often do not generalize well when data from new unseen sites need to be processed, hampering their practical use. Reasons for this limitation include image and scene characteristics, though the most prominent one relates to the image features being used for training the classifier. Recently features based on deep learning approaches, such as convolutional neural networks (CNNs), have been shown to be more effective than conventional hand-crafted features, and have become the state-of-the-art in many domains, including remote sensing. Moreover, often oblique images are captured with high block overlap, facilitating the generation of dense 3D point clouds – an ideal source to derive geometric characteristics. We hypothesized that the use of CNN features, either independently or in combination with 3D point cloud features, would yield improved performance in damage detection. To this end we used CNN and 3D features, both independently and in combination, using images from manned and unmanned aerial platforms over several geographic locations that vary significantly in terms of image and scene characteristics. A multiple-kernel-learning framework, an effective way for integrating features from different modalities, was used for combining the two sets of features for classification. The results are encouraging: while CNN features produced an average classification accuracy of about 91%, the integration of 3D point cloud features led to an additional improvement of about 3% (i.e. an average classification accuracy of 94%). The significance of 3D point cloud features becomes more evident in the model transferability scenario (i.e., training and testing samples from different sites that vary slightly in the aforementioned characteristics), where the integration of CNN and 3D point cloud features significantly improved the model transferability accuracy up to a maximum of 7% compared with the accuracy achieved by CNN features alone. Overall, an average accuracy of 85% was achieved for the model transferability scenario across all experiments. Our main conclusion is that such an approach qualifies for practical use. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS) 3D point cloud features; CNN features; Model transferability; Multiple-kernel-learning; Oblique images; Structural damage detections; Transfer learning; UAV Classification (of information); Damage detection; Deep learning; Deep neural networks; Disasters; Integration testing; Neural networks; Remote sensing; Structural analysis; Unmanned aerial vehicles (UAV); 3D point cloud; CNN features; Model transferabilities; Multiple Kernel Learning; Oblique images; Structural damage detection; Transfer learning; accuracy assessment; aerial photography; artificial neural network; disaster; earthquake; image analysis; model validation; stakeholder; supervised learning; three-dimensional modeling; training; Feature extraction";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;3;Response
537;Combining data-driven methods with finite element analysis for flood early warning systems;We developed a robust approach for real-time levee condition monitoring based on combination of data-driven methods (one-side classification) and finite element analysis. It was implemented within a flood early warning system and validated on a series of full-scale levee failure experiments organised by the IJkdijk consortium in August-September 2012 in the Netherlands. Our approach has detected anomalies and predicted levee failures several days before the actual collapse. This approach was used in the UrbanFlood decision support system for routine levee quality assessment and for critical situations of a potential levee breach and inundation. In case of emergency, the system generates an alarm, warns dike managers and city authorities, and launches advanced urgent simulations of levee stability and flood dynamics, thus helping to make informed decisions on preventive measures, to evaluate the risks and to alleviate adverse effects of a flood. © The Authors. Published by Elsevier B.V.;"Anomaly detection; Finite element modelling; IJkdijk; Levee monitoring; One-side classification";"Alarm systems; Artificial intelligence; Condition monitoring; Decision support systems; Embankments; Floods; Hydraulic structures; Levees; Risk assessment; Anomaly detection; Data-driven methods; Early Warning System; Finite element modelling; IJkdijk; Informed decision; Preventive measures; Quality assessment; Ijkdijk; Finite element method";"Combining data-driven methods with finite element analysis for flood early warning systems We developed a robust approach for real-time levee condition monitoring based on combination of data-driven methods (one-side classification) and finite element analysis. It was implemented within a flood early warning system and validated on a series of full-scale levee failure experiments organised by the IJkdijk consortium in August-September 2012 in the Netherlands. Our approach has detected anomalies and predicted levee failures several days before the actual collapse. This approach was used in the UrbanFlood decision support system for routine levee quality assessment and for critical situations of a potential levee breach and inundation. In case of emergency, the system generates an alarm, warns dike managers and city authorities, and launches advanced urgent simulations of levee stability and flood dynamics, thus helping to make informed decisions on preventive measures, to evaluate the risks and to alleviate adverse effects of a flood. © The Authors. Published by Elsevier B.V. Anomaly detection; Finite element modelling; IJkdijk; Levee monitoring; One-side classification Alarm systems; Artificial intelligence; Condition monitoring; Decision support systems; Embankments; Floods; Hydraulic structures; Levees; Risk assessment; Anomaly detection; Data-driven methods; Early Warning System; Finite element modelling; IJkdijk; Informed decision; Preventive measures; Quality assessment; Ijkdijk; Finite element method";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
538;CNN Based Approach for Post Disaster Damage Assessment;After any disaster, the Government rehabilitates the victims based on the severity of the damage caused to their properties. Since a huge number of rehabilitation claims flow in after the disaster, it takes up a lot of manual labor in inspecting and validating the claims along with deciding the amount of rehabilitation to be granted. Moreover, such manual inspection leads to a lack of transparency. In recent years, social media posts, text, and images have become a rich source of post-disaster situational information that may be useful in assessing damage at a low cost. Most of the existing research explores the use of social media text for extracting situational information useful for disaster response. The usage of social media images to assess disaster damage is limited. In this paper, we propose a convolutional neural network-based approach to locate damage in a disaster image and to quantify the degree of the damage. The proposed damage assessment system categorizes images of earthquake-affected buildings and decides the severity of the damage caused by the earthquake. Our proposed approach enables the use of social media images for post-disaster damage assessment and provides an inexpensive and feasible alternative to the more expensive GIS approach. Our approach exhibits high accuracy in classifying earthquake-affected buildings and determining the severity of damage at a negligible loss. © 2020 ACM.;"Damage Assessment; ImageNet; Transfer Learning; VGG16; VGG19";"Convolutional neural networks; Disasters; Earthquakes; Social networking (online); Damage assessments; Disaster response; Feasible alternatives; High-accuracy; Manual inspection; Manual labors; Post disasters; Social media; Damage detection";"CNN Based Approach for Post Disaster Damage Assessment After any disaster, the Government rehabilitates the victims based on the severity of the damage caused to their properties. Since a huge number of rehabilitation claims flow in after the disaster, it takes up a lot of manual labor in inspecting and validating the claims along with deciding the amount of rehabilitation to be granted. Moreover, such manual inspection leads to a lack of transparency. In recent years, social media posts, text, and images have become a rich source of post-disaster situational information that may be useful in assessing damage at a low cost. Most of the existing research explores the use of social media text for extracting situational information useful for disaster response. The usage of social media images to assess disaster damage is limited. In this paper, we propose a convolutional neural network-based approach to locate damage in a disaster image and to quantify the degree of the damage. The proposed damage assessment system categorizes images of earthquake-affected buildings and decides the severity of the damage caused by the earthquake. Our proposed approach enables the use of social media images for post-disaster damage assessment and provides an inexpensive and feasible alternative to the more expensive GIS approach. Our approach exhibits high accuracy in classifying earthquake-affected buildings and determining the severity of damage at a negligible loss. © 2020 ACM. Damage Assessment; ImageNet; Transfer Learning; VGG16; VGG19 Convolutional neural networks; Disasters; Earthquakes; Social networking (online); Damage assessments; Disaster response; Feasible alternatives; High-accuracy; Manual inspection; Manual labors; Post disasters; Social media; Damage detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
539;Building A Deep Learning Model for Multi-Label Classification of Natural Disasters;Natural disasters, such as earthquakes, hurricanes/typhoons and wildfires, usually cause severe damage. Disaster response and management is a great challenge to the authority. Current studies usually focus on a single disaster identification using social media data. In reality, there are relationships among different types of disasters. And several disasters may happen simultaneously. In this study, we explore the role of the deep learning model in multi-label disaster classification. We build a deep CNN model for multi-label classification with the instruction of a high-order strategy. We train and validate our model using a professional low-altitude disaster dataset, LADI. We find our proposed deep learning model with the transfer learning method outperforms many other machine learning models in the previous study. © 2023 IEEE.;"deep learning; disaster classification; multi-label learning";"Classification (of information); Deep learning; Learning systems; Deep learning; Disaster classification; Disaster management; Disaster-response; Hurricanes/typhoons; Learning models; Management IS; Multi-label classifications; Multi-label learning; Natural disasters; Disasters";"Building A Deep Learning Model for Multi-Label Classification of Natural Disasters Natural disasters, such as earthquakes, hurricanes/typhoons and wildfires, usually cause severe damage. Disaster response and management is a great challenge to the authority. Current studies usually focus on a single disaster identification using social media data. In reality, there are relationships among different types of disasters. And several disasters may happen simultaneously. In this study, we explore the role of the deep learning model in multi-label disaster classification. We build a deep CNN model for multi-label classification with the instruction of a high-order strategy. We train and validate our model using a professional low-altitude disaster dataset, LADI. We find our proposed deep learning model with the transfer learning method outperforms many other machine learning models in the previous study. © 2023 IEEE. deep learning; disaster classification; multi-label learning Classification (of information); Deep learning; Learning systems; Deep learning; Disaster classification; Disaster management; Disaster-response; Hurricanes/typhoons; Learning models; Management IS; Multi-label classifications; Multi-label learning; Natural disasters; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
540;"Real-time anomaly detection and analysis of time series data for crack gauge in landslides; [滑坡裂缝计时序数据实时异常检测分析]";In order to address the issue of effectively identifying abnormal data in real-time monitoring of landslide cracks，occasional abnormal thresholds based on the various deformation stages of landslides are established. Additionally，a real-time anomaly detection method for time series data，utilizing interval prediction，is proposed. This method takes into consideration the temporal logic relationship between data and the correlated information of landslide deformation stages. Firstly，the time series characteristics of cumulative displacement of crack gauge are extracted using the autoregressive integrated moving average(ARIMA) model. Subsequently，an interval prediction model is constructed，and the sliding window algorithm is employed to predict sub-sequences. Secondly，to determine prospective abnormal points，a modified confidence interval(with α = 0.05) is utilized，and occasional abnormal thresholds are established for different deformation stages of landslides. Finally，exceptional information is obtained through combined anomaly recognition. The research results indicate that this method accurately identifies abnormal data values and demonstrates universal applicability in real-time anomaly detection of time series data. By comparing the predicted interval of the model with the abnormal values，the real-time possibility of data abnormalities can be obtained. Furthermore，this method provides valuable data reference for intelligent decision-making in landslide monitoring and early warning. © 2024 Academia Sinica. All rights reserved.;"early warning; landslide; monitoring; real-time anomaly detection; slope engineering; time series model";"Anomaly detection; Decision making; Forecasting; Time series; Abnormal data; Anomaly analysis; Deformation stages; Early warning; Interval prediction; Real time monitoring; Real-time anomaly detections; Slope engineering; Time-series data; Times series models; Landslides";"Real-time anomaly detection and analysis of time series data for crack gauge in landslides; [滑坡裂缝计时序数据实时异常检测分析] In order to address the issue of effectively identifying abnormal data in real-time monitoring of landslide cracks，occasional abnormal thresholds based on the various deformation stages of landslides are established. Additionally，a real-time anomaly detection method for time series data，utilizing interval prediction，is proposed. This method takes into consideration the temporal logic relationship between data and the correlated information of landslide deformation stages. Firstly，the time series characteristics of cumulative displacement of crack gauge are extracted using the autoregressive integrated moving average(ARIMA) model. Subsequently，an interval prediction model is constructed，and the sliding window algorithm is employed to predict sub-sequences. Secondly，to determine prospective abnormal points，a modified confidence interval(with α = 0.05) is utilized，and occasional abnormal thresholds are established for different deformation stages of landslides. Finally，exceptional information is obtained through combined anomaly recognition. The research results indicate that this method accurately identifies abnormal data values and demonstrates universal applicability in real-time anomaly detection of time series data. By comparing the predicted interval of the model with the abnormal values，the real-time possibility of data abnormalities can be obtained. Furthermore，this method provides valuable data reference for intelligent decision-making in landslide monitoring and early warning. © 2024 Academia Sinica. All rights reserved. early warning; landslide; monitoring; real-time anomaly detection; slope engineering; time series model Anomaly detection; Decision making; Forecasting; Time series; Abnormal data; Anomaly analysis; Deformation stages; Early warning; Interval prediction; Real time monitoring; Real-time anomaly detections; Slope engineering; Time-series data; Times series models; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
541;Seismic Surveillance of Vrancea Active Region in Romania by Time Series Satellite Data Anomalies;"The Vrancea zone in Romania located at the bending of the South-Eastern Carpathians is one of the high-risk seismic zones in Europe, characterized by a high occurrence of intermediate-depth earthquakes, confined in a 60-200 km depth lithospheric volume. For continuous surveillance of the Vrancea seismic active area in Romania, this study developed and implemented an advanced integrated methodology of multi-field time series satellite- and ground-based observational data of seismic precursors and lithosphere-atmosphere coupling modeling, for new seismic increased activity indicators design. Based on the seismic records in synergy with atmospheric and land surface pre-seismic anomalies detection from Land Surface Temperature (LST) from the time series MODIS Terra/Aqua and NOAA AVHRR along with Air Temperature at 2m height (AT), this study found significant correlations with moderate seismic events of moment magnitude Mw ≥ 5 on Richter scale for 2012-2023 period. It was observed also a high correlation between air temperature AT at 2m height and land surface temperature LST, the Spearman rank correlation coefficient was (r= 0.95; p<0.01). Also, there is a high correlation between the moment magnitude of the six moderated earthquakes recorded in the Vrancea area from 2012 to 2023 years and LST values (r=0.65; p<0.01). The findings of this study aim to improve, by cross-validating, the methodologies for seismic hazard assessment in Romania due to the Vrancea source and detect preparatory seismic phases and precursors. Early detection and monitoring of induced geophysical anomalies can help decision-makers mitigate the impact and improve disaster response efforts. This will contribute to promoting an EOS for Romania in the frame of ESA Copernicus. The investigation of the seismo-associated phenomena from space is a challenge for Earth Observation and earthquake forecasting, with a high impact on SDGs as well as the Natural Hazard Directive in the EU. © 2024 SPIE.";"Romania; seismic precursors; time series satellite data; Vrancea geotectonic active area";"Earthquake effects; Geochronology; Hydrogeology; Induced Seismicity; Lithology; Seismic design; Seismic response; Tropics; Active area; Geotectonics; Land surface temperature; Romania; Satellite data; Seismic precursors; Time series satellite data; Times series; Vrancea; Vranceum geotectonic active area; Atmospheric temperature";"Seismic Surveillance of Vrancea Active Region in Romania by Time Series Satellite Data Anomalies The Vrancea zone in Romania located at the bending of the South-Eastern Carpathians is one of the high-risk seismic zones in Europe, characterized by a high occurrence of intermediate-depth earthquakes, confined in a 60-200 km depth lithospheric volume. For continuous surveillance of the Vrancea seismic active area in Romania, this study developed and implemented an advanced integrated methodology of multi-field time series satellite- and ground-based observational data of seismic precursors and lithosphere-atmosphere coupling modeling, for new seismic increased activity indicators design. Based on the seismic records in synergy with atmospheric and land surface pre-seismic anomalies detection from Land Surface Temperature (LST) from the time series MODIS Terra/Aqua and NOAA AVHRR along with Air Temperature at 2m height (AT), this study found significant correlations with moderate seismic events of moment magnitude Mw ≥ 5 on Richter scale for 2012-2023 period. It was observed also a high correlation between air temperature AT at 2m height and land surface temperature LST, the Spearman rank correlation coefficient was (r= 0.95; p<0.01). Also, there is a high correlation between the moment magnitude of the six moderated earthquakes recorded in the Vrancea area from 2012 to 2023 years and LST values (r=0.65; p<0.01). The findings of this study aim to improve, by cross-validating, the methodologies for seismic hazard assessment in Romania due to the Vrancea source and detect preparatory seismic phases and precursors. Early detection and monitoring of induced geophysical anomalies can help decision-makers mitigate the impact and improve disaster response efforts. This will contribute to promoting an EOS for Romania in the frame of ESA Copernicus. The investigation of the seismo-associated phenomena from space is a challenge for Earth Observation and earthquake forecasting, with a high impact on SDGs as well as the Natural Hazard Directive in the EU. © 2024 SPIE. Romania; seismic precursors; time series satellite data; Vrancea geotectonic active area Earthquake effects; Geochronology; Hydrogeology; Induced Seismicity; Lithology; Seismic design; Seismic response; Tropics; Active area; Geotectonics; Land surface temperature; Romania; Satellite data; Seismic precursors; Time series satellite data; Times series; Vrancea; Vranceum geotectonic active area; Atmospheric temperature";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
542;Signal analysis and anomaly detection for flood early warning systems;We describe the detection methods and the results of anomalous conditions in dikes (earthen dams/levees) based on a simultaneous processing of several data streams originating from sensors installed in these dikes. Applied methods are especially valuable in cases where lack of information or computational resources prohibit computing the state of the dike with finite element and other mathematical models. The data-driven methods are part of the artificial intelligence (AI) component of the 'Urbanflood' early warning system. This AI component includes pre-processing (e.g., gap filling and measurements synchronization procedures) of data streams, feature extraction and anomaly detection by one-side (also known as one-class) classification methods. Our approach has been successfully validated during a non-destructive piping experiment at the Zeeland dike (The Netherlands). © IWA Publishing 2014.;"'Urbanflood'; Anomaly detection; Levee monitoring; One-side classification";NULL;"Signal analysis and anomaly detection for flood early warning systems We describe the detection methods and the results of anomalous conditions in dikes (earthen dams/levees) based on a simultaneous processing of several data streams originating from sensors installed in these dikes. Applied methods are especially valuable in cases where lack of information or computational resources prohibit computing the state of the dike with finite element and other mathematical models. The data-driven methods are part of the artificial intelligence (AI) component of the 'Urbanflood' early warning system. This AI component includes pre-processing (e.g., gap filling and measurements synchronization procedures) of data streams, feature extraction and anomaly detection by one-side (also known as one-class) classification methods. Our approach has been successfully validated during a non-destructive piping experiment at the Zeeland dike (The Netherlands). © IWA Publishing 2014. 'Urbanflood'; Anomaly detection; Levee monitoring; One-side classification NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
543;Regional landslide mapping model developed by a deep transfer learning framework using post-event optical imagery;Landslides are major natural disasters in mountainous areas, often caused by earthquakes and heavy rainfalls. Traditional manual delineation methods for identifying landslide features using optical imagery are inefficient, highlighting the need for automated detection techniques. Deep Convolutional Neural Networks (CNNs) have emerged as advanced solutions in computer vision for this purpose. Despite the reliance on pre-event and post-event imagery or various data sources like digital elevation models (DEMs), the success of deep learning models largely depends on the quality and availability of training data. This poses a challenge for their immediate application after a landslide. This study explores the transferability of a CNN model trained on data from the 2016 Kumamoto Earthquakes for detecting landslides in different events, specifically the 2018 Hokkaido earthquake and the 2017 Asakura Rainfall in Japan. These cases were chosen for their geographical similarities. The proposed deep transfer learning model, based on a DeepLabV3 + architecture built on a pre-trained ResNet50, automatically identifies landslide features without needing specific training data or model adjustments for each event. It achieved high accuracy in both cases, demonstrating CNNs’ potential for broad application in landslide detection and enhancing disaster response efforts. © 2024 Informa UK Limited, trading as Taylor & Francis Group.;"cross-event transferability; data augmentation; Deep transfer learning; regional landslide mapping; semantic image segmentation";"data set; extreme event; image analysis; landslide; machine learning; natural disaster; optical property; segmentation";"Regional landslide mapping model developed by a deep transfer learning framework using post-event optical imagery Landslides are major natural disasters in mountainous areas, often caused by earthquakes and heavy rainfalls. Traditional manual delineation methods for identifying landslide features using optical imagery are inefficient, highlighting the need for automated detection techniques. Deep Convolutional Neural Networks (CNNs) have emerged as advanced solutions in computer vision for this purpose. Despite the reliance on pre-event and post-event imagery or various data sources like digital elevation models (DEMs), the success of deep learning models largely depends on the quality and availability of training data. This poses a challenge for their immediate application after a landslide. This study explores the transferability of a CNN model trained on data from the 2016 Kumamoto Earthquakes for detecting landslides in different events, specifically the 2018 Hokkaido earthquake and the 2017 Asakura Rainfall in Japan. These cases were chosen for their geographical similarities. The proposed deep transfer learning model, based on a DeepLabV3 + architecture built on a pre-trained ResNet50, automatically identifies landslide features without needing specific training data or model adjustments for each event. It achieved high accuracy in both cases, demonstrating CNNs’ potential for broad application in landslide detection and enhancing disaster response efforts. © 2024 Informa UK Limited, trading as Taylor & Francis Group. cross-event transferability; data augmentation; Deep transfer learning; regional landslide mapping; semantic image segmentation data set; extreme event; image analysis; landslide; machine learning; natural disaster; optical property; segmentation";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
544;Landslide Displacement Prediction Based on Transfer Learning and Bi-GRU;Predicting slope deformation prediction is crucial for early warning of slope failure, preventing damage to properties, and saving human lives. However, in practice, equipment maintenance causes discontinuity in the displacement data, and the traditional prediction models based on deep networks do not perform well in this case. To solve the problem of prediction accuracy in case of discontinuous and inadequate data, we propose a combined displacement prediction model that integrates the bidirectional gated recurrent unit (Bi-GRU), attention mechanism, and transfer learning. The Bi-GRU is employed to extract the forward and backward characteristics of displacement series, and the attention mechanism is utilized to give different weights to the extracted information so as to highlight the critical information. Transfer learning is used to guarantee prediction accuracy in case of discontinuous and limited data. The model is then employed to predict the slope displacement of the JinYu Cement Plant in China. Finally, the modeling results excellently agree with measured displacement, especially in case of insufficient sample data.  © 2022 Haiqing Zheng et al.;NULL;"Attention mechanisms; Deformation's predictions; Discontinuous data; Displacement prediction; Early warning; Prediction accuracy; Prediction modelling; Prediction-based; Slope deformation; Transfer learning; Forecasting";"Landslide Displacement Prediction Based on Transfer Learning and Bi-GRU Predicting slope deformation prediction is crucial for early warning of slope failure, preventing damage to properties, and saving human lives. However, in practice, equipment maintenance causes discontinuity in the displacement data, and the traditional prediction models based on deep networks do not perform well in this case. To solve the problem of prediction accuracy in case of discontinuous and inadequate data, we propose a combined displacement prediction model that integrates the bidirectional gated recurrent unit (Bi-GRU), attention mechanism, and transfer learning. The Bi-GRU is employed to extract the forward and backward characteristics of displacement series, and the attention mechanism is utilized to give different weights to the extracted information so as to highlight the critical information. Transfer learning is used to guarantee prediction accuracy in case of discontinuous and limited data. The model is then employed to predict the slope displacement of the JinYu Cement Plant in China. Finally, the modeling results excellently agree with measured displacement, especially in case of insufficient sample data.  © 2022 Haiqing Zheng et al. NULL Attention mechanisms; Deformation's predictions; Discontinuous data; Displacement prediction; Early warning; Prediction accuracy; Prediction modelling; Prediction-based; Slope deformation; Transfer learning; Forecasting";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
545;The next big one: Detecting earthquakes and other rare events from community-based sensors;Can one use cell phones for earthquake early warning? Detecting rare, disruptive events using community-held sensors is a promising opportunity, but also presents difficult challenges. Rare events are often difficult or impossible to model and characterize a priori, yet we wish to maximize detection performance. Further, heterogeneous, community-operated sensors may differ widely in quality and communication constraints. In this paper, we present a principled approach towards detecting rare events that learns sensor-specific decision thresholds online, in a distributed way. It maximizes anomaly detection performance at a fusion center, under constraints on the false alarm rate and number of messages per sensor. We then present an implementation of our approach in the Community Seismic Network (CSN), a community sensing system with the goal of rapidly detecting earthquakes using cell phone accelerometers, consumer USB devices and cloud-computing based sensor fusion. We experimentally evaluate our approach based on a pilot deployment of the CSN system. Our results, including data from shake table experiments, indicate the effectiveness of our approach in distinguishing seismic motion from accelerations due to normal daily manipulation. They also provide evidence of the feasibility of earthquake early warning using a dense network of cell phones. © 2011 ACM.;"community sensing; distributed anomaly detection; Sensor networks";"Accelerometers; Cellular telephone systems; Cloud computing; Data processing; Earthquakes; Mobile phones; Sensor networks; Telecommunication equipment; Telephone; Telephone circuits; Telephone sets; Uncertainty analysis; Anomaly detection; Cell phone; Communication constraints; community sensing; Decision threshold; Dense network; Detection performance; Disruptive event; Distributed anomaly detection; Earthquake early warning; False alarm rate; Fusion center; Rare event; Seismic motions; Seismic networks; Sensing systems; Sensor fusion; Shake table experiments; USB devices; Sensors";"The next big one: Detecting earthquakes and other rare events from community-based sensors Can one use cell phones for earthquake early warning? Detecting rare, disruptive events using community-held sensors is a promising opportunity, but also presents difficult challenges. Rare events are often difficult or impossible to model and characterize a priori, yet we wish to maximize detection performance. Further, heterogeneous, community-operated sensors may differ widely in quality and communication constraints. In this paper, we present a principled approach towards detecting rare events that learns sensor-specific decision thresholds online, in a distributed way. It maximizes anomaly detection performance at a fusion center, under constraints on the false alarm rate and number of messages per sensor. We then present an implementation of our approach in the Community Seismic Network (CSN), a community sensing system with the goal of rapidly detecting earthquakes using cell phone accelerometers, consumer USB devices and cloud-computing based sensor fusion. We experimentally evaluate our approach based on a pilot deployment of the CSN system. Our results, including data from shake table experiments, indicate the effectiveness of our approach in distinguishing seismic motion from accelerations due to normal daily manipulation. They also provide evidence of the feasibility of earthquake early warning using a dense network of cell phones. © 2011 ACM. community sensing; distributed anomaly detection; Sensor networks Accelerometers; Cellular telephone systems; Cloud computing; Data processing; Earthquakes; Mobile phones; Sensor networks; Telecommunication equipment; Telephone; Telephone circuits; Telephone sets; Uncertainty analysis; Anomaly detection; Cell phone; Communication constraints; community sensing; Decision threshold; Dense network; Detection performance; Disruptive event; Distributed anomaly detection; Earthquake early warning; False alarm rate; Fusion center; Rare event; Seismic motions; Seismic networks; Sensing systems; Sensor fusion; Shake table experiments; USB devices; Sensors";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
546;Anomaly Detection for Population Dynamics using Autoencoder Leveraging Periodic Residual Component in Disaster Situations;It is important for local governments to grasp the evacuation situation upon the occurrence of a large-scale disaster because administrative support can be provided, such as opening temporary accommodation facilities or distributing relief supplies. Early identification of locations where population transitions differ from normal conditions will help local governments to identify areas for disaster response. In this paper, we propose a simple yet effective anomaly detection method for population transition using an autoencoder that leverages periodic residual components using real-time population dynamics generated from operation data of cellular phone networks. We evaluated the effectiveness of our proposed method using data from three real earthquakes in Japan. The results demonstrated that our approach, which leverages periodic residual components, outperforms a baseline method that directly uses population transition. Moreover, the experimental results showed that the standard deviation of recall is smaller than that of the baseline method, indicating that utilizing the residual component has a stabilizing effect on accuracy.  © 2023 ACM.;"Anomaly Detection; Deep Learning; Disaster Response; Mobile Phone-Based Population";"Anomaly detection; Deep learning; Emergency services; Learning systems; Population dynamics; Population statistics; Anomaly detection; Auto encoders; Baseline methods; Deep learning; Disaster situations; Disaster-response; Local government; Mobile phone-based population; Phone-based; Residual components; Cellular telephones";"Anomaly Detection for Population Dynamics using Autoencoder Leveraging Periodic Residual Component in Disaster Situations It is important for local governments to grasp the evacuation situation upon the occurrence of a large-scale disaster because administrative support can be provided, such as opening temporary accommodation facilities or distributing relief supplies. Early identification of locations where population transitions differ from normal conditions will help local governments to identify areas for disaster response. In this paper, we propose a simple yet effective anomaly detection method for population transition using an autoencoder that leverages periodic residual components using real-time population dynamics generated from operation data of cellular phone networks. We evaluated the effectiveness of our proposed method using data from three real earthquakes in Japan. The results demonstrated that our approach, which leverages periodic residual components, outperforms a baseline method that directly uses population transition. Moreover, the experimental results showed that the standard deviation of recall is smaller than that of the baseline method, indicating that utilizing the residual component has a stabilizing effect on accuracy.  © 2023 ACM. Anomaly Detection; Deep Learning; Disaster Response; Mobile Phone-Based Population Anomaly detection; Deep learning; Emergency services; Learning systems; Population dynamics; Population statistics; Anomaly detection; Auto encoders; Baseline methods; Deep learning; Disaster situations; Disaster-response; Local government; Mobile phone-based population; Phone-based; Residual components; Cellular telephones";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
547;Multiclass Damage Detection for Autonomous Post-disaster Reconnaissance Using Quantum Convolutional Neural Network;Timely assessment of earthquake-induced building damage is critical for ensuring life safety, mitigating financial losses, expediting the rehabilitation process, and improving structural resilience. Due to the exponential growth in computer power and intrinsic capacity to address problems with manual inspections, the use of artificial intelligence (AI) in post-earthquake inspections and reconnaissance has drawn a lot of attention in recent years. With recent advancements in non-contact sensing technologies such as cameras, unmanned aerial and ground vehicles, the structural health monitoring (SHM) community has seen a significant increase in deep learning-based condition assessment methodologies of structural system. These deep learning algorithms mostly rely on convolutional neural networks (CNNs), which has risen its popularity for many machines learning (ML) applications, particularly in the field of image recognition. However, machine learning algorithms experience computational bottlenecks due to the curse of dimensionality. This study presents the adoption of Quantum Convolutional Neural Network (QCNN) to classify the building damages into five damage grades (1) Negligible to slight damage (2) Moderate damage (3) Heavy damage (4) Very heavy damage and (5) Collapse. The reinforced concrete building damage images collected from past-earthquake events are used to train the models and their performance is evaluated based on the testing images, neither of which are included to the training dataset. Furthermore, the comparison is made between the results obtained from QCNN and various CNNs architectures. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.;"convolutional neural network; damage detection; earthquake damage images; quantum convolutional neural network";"Antennas; Convolution; Convolutional neural networks; Deep learning; Earthquakes; Image recognition; Learning algorithms; Learning systems; Losses; Reinforced concrete; Statistical tests; Structural health monitoring; Building damage; Convolutional neural network; Damage images; Earthquake damage image; Earthquake damages; Heavy damage; Life safety; Post disaster reconnaissance; Quantum convolutional neural network; Damage detection";"Multiclass Damage Detection for Autonomous Post-disaster Reconnaissance Using Quantum Convolutional Neural Network Timely assessment of earthquake-induced building damage is critical for ensuring life safety, mitigating financial losses, expediting the rehabilitation process, and improving structural resilience. Due to the exponential growth in computer power and intrinsic capacity to address problems with manual inspections, the use of artificial intelligence (AI) in post-earthquake inspections and reconnaissance has drawn a lot of attention in recent years. With recent advancements in non-contact sensing technologies such as cameras, unmanned aerial and ground vehicles, the structural health monitoring (SHM) community has seen a significant increase in deep learning-based condition assessment methodologies of structural system. These deep learning algorithms mostly rely on convolutional neural networks (CNNs), which has risen its popularity for many machines learning (ML) applications, particularly in the field of image recognition. However, machine learning algorithms experience computational bottlenecks due to the curse of dimensionality. This study presents the adoption of Quantum Convolutional Neural Network (QCNN) to classify the building damages into five damage grades (1) Negligible to slight damage (2) Moderate damage (3) Heavy damage (4) Very heavy damage and (5) Collapse. The reinforced concrete building damage images collected from past-earthquake events are used to train the models and their performance is evaluated based on the testing images, neither of which are included to the training dataset. Furthermore, the comparison is made between the results obtained from QCNN and various CNNs architectures. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. convolutional neural network; damage detection; earthquake damage images; quantum convolutional neural network Antennas; Convolution; Convolutional neural networks; Deep learning; Earthquakes; Image recognition; Learning algorithms; Learning systems; Losses; Reinforced concrete; Statistical tests; Structural health monitoring; Building damage; Convolutional neural network; Damage images; Earthquake damage image; Earthquake damages; Heavy damage; Life safety; Post disaster reconnaissance; Quantum convolutional neural network; Damage detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
548;Estimating the nankai trough megathrust earthquake’s anticipated fiscal impact on Japanese governments;This study estimates the fiscal impact of the anticipated Nankai Trough Megathrust Earthquake on both the national and local Japanese governments to iden-tify their sovereign risk. First, we estimate the impact of the Great East Japan Earthquake on local public finance using panel data regressions on 2008–2015 fiscal data. Second, based on the anticipated damage data – seismic intensity and area of inundation – of the Nankai earthquake and the coefficients derived from the first step, we estimate the amounts of fiscal revenue and expenditures that would be required by every local government for the anticipated Nankai earthquake. Finally, we estimate the fiscal expenditure of the national government in proportion to the estimated local ones. We find that first, the estimated fiscal requirements in the two years after the earthquake are about JPY 161 trillion, 5.9 times those of the 2011 Great East Japan Earthquake. Second, the financial disparity between affected and non-affected local governments is large because the Nankai earthquake would affect more municipalities than the Great East Japan Earthquake. The fiscal burden of non-affected municipalities would be relatively higher. These findings indicate that the Nankai earthquake will not only be a local disaster but also a national catastrophe. © 2020, Fuji Technology Press. All rights reserved.;"Disaster recovery; Disaster-related contingent liabil-ity; Fiscal risk; Great East Japan Earthquake; Nankai Trough Megathrust Earth-quake";"Disasters; Fiscal expenditures; Great east japan earthquakes; Local government; Megathrust earthquakes; Nankai earthquake; National governments; Seismic intensity; Sovereign risks; Earthquakes";"Estimating the nankai trough megathrust earthquake’s anticipated fiscal impact on Japanese governments This study estimates the fiscal impact of the anticipated Nankai Trough Megathrust Earthquake on both the national and local Japanese governments to iden-tify their sovereign risk. First, we estimate the impact of the Great East Japan Earthquake on local public finance using panel data regressions on 2008–2015 fiscal data. Second, based on the anticipated damage data – seismic intensity and area of inundation – of the Nankai earthquake and the coefficients derived from the first step, we estimate the amounts of fiscal revenue and expenditures that would be required by every local government for the anticipated Nankai earthquake. Finally, we estimate the fiscal expenditure of the national government in proportion to the estimated local ones. We find that first, the estimated fiscal requirements in the two years after the earthquake are about JPY 161 trillion, 5.9 times those of the 2011 Great East Japan Earthquake. Second, the financial disparity between affected and non-affected local governments is large because the Nankai earthquake would affect more municipalities than the Great East Japan Earthquake. The fiscal burden of non-affected municipalities would be relatively higher. These findings indicate that the Nankai earthquake will not only be a local disaster but also a national catastrophe. © 2020, Fuji Technology Press. All rights reserved. Disaster recovery; Disaster-related contingent liabil-ity; Fiscal risk; Great East Japan Earthquake; Nankai Trough Megathrust Earth-quake Disasters; Fiscal expenditures; Great east japan earthquakes; Local government; Megathrust earthquakes; Nankai earthquake; National governments; Seismic intensity; Sovereign risks; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
549;Social-Based Physical Reconstruction Planning in Case of Natural Disaster: A Machine Learning Approach;Natural disasters have several adverse effects on human lives. It is challenging for the governments to tackle these events and to reconstruct damaged areas with minimal budget and time, but still guaranteeing social benefits to the affected population. This article presents an approach of decision-support system for post-disaster re-construction planning of buildings damaged by a natural disaster. The proposed framework determines a set of alternative plans which satisfy all constraints, accommodate political priorities, and guarantee social benefits for the affected population. The determined plans are then provided to public servants that select the plan to implement. The approach is generic and it can be applied to areas of any extension as long as the decision makers share the same goals. We will demonstrate the approach on the L’Aquila city destroyed by an earthquake in 2009. © 2020, Springer Nature Switzerland AG.;"Decision-support system; Natural disaster; Political priority; Reconstruction planning; Social benefits";"Budget control; Decision making; Decision support systems; Disasters; Machine learning; Decision makers; Machine learning approaches; Natural disasters; Physical reconstruction; Political priorities; Post disasters; Public servants; Social benefits; Economic and social effects";"Social-Based Physical Reconstruction Planning in Case of Natural Disaster: A Machine Learning Approach Natural disasters have several adverse effects on human lives. It is challenging for the governments to tackle these events and to reconstruct damaged areas with minimal budget and time, but still guaranteeing social benefits to the affected population. This article presents an approach of decision-support system for post-disaster re-construction planning of buildings damaged by a natural disaster. The proposed framework determines a set of alternative plans which satisfy all constraints, accommodate political priorities, and guarantee social benefits for the affected population. The determined plans are then provided to public servants that select the plan to implement. The approach is generic and it can be applied to areas of any extension as long as the decision makers share the same goals. We will demonstrate the approach on the L’Aquila city destroyed by an earthquake in 2009. © 2020, Springer Nature Switzerland AG. Decision-support system; Natural disaster; Political priority; Reconstruction planning; Social benefits Budget control; Decision making; Decision support systems; Disasters; Machine learning; Decision makers; Machine learning approaches; Natural disasters; Physical reconstruction; Political priorities; Post disasters; Public servants; Social benefits; Economic and social effects";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
550;Damage identification and assessment using image processing on post-disaster satellite imagery;Natural disasters such as earthquakes and tsunamis often have a devastating effect on human life and cause noticeable damage to infrastructure. Active research has been ongoing to mitigate the impact of these catastrophes and preclude the economic losses. The existing methods that utilize pre-event and post-event images not only require the immediate and guaranteed availability of the appropriate data set but are also encumbered by manual mapping of the images, necessitating the indication of corresponding control points in the two images. This paper highlights the use of only post-event imagery in the absence of reference data to achieve a more timely delivery to produce damage maps as the output. This eliminates the need for manual georeferencing of images. Our method incorporates simple linear iterative clustering (SLIC) for segmenting the images into uniform superpixels and extraction of 62 features for each superpixel. We used various classifiers of which Random Forest classifier was found to give a comparatively high accuracy of 90.4% over others. To enumerate the accuracy of the method proposed, we used 1500 data regions of which 20% were used for testing, and 80% were used for training. The aerial images taken by GeoEye1 after the 2011 Christchurch earthquake and 2011 Japan earthquake and tsunami are utilized in this study to detect building damage. In the case of availability of ground truth, we compare the histograms of the pre- and post-imagery to quantify similarity as the SSD (Sum of Squared Distances) value and thus, our approach produces an assessment as an output map displaying the extent of damage in the area covered by each superpixel. We consider 6 levels of damage ranging from 1 to 6, where 1 signifies no damage, and 6, maximum damage. © 2017 IEEE.;"Damage Assessment; Image Processing; Random Forest Classifier; Segmentation; Texture Features";"Antennas; Decision trees; Disasters; Earthquakes; Image processing; Image segmentation; Iterative methods; Losses; Pixels; Satellite imagery; Superpixels; Tsunamis; 2011 Christchurch earthquakes; Corresponding control points; Damage assessments; Damage Identification; Devastating effects; Random forest classifier; Simple Linear Iterative Clustering; Texture features; Damage detection";"Damage identification and assessment using image processing on post-disaster satellite imagery Natural disasters such as earthquakes and tsunamis often have a devastating effect on human life and cause noticeable damage to infrastructure. Active research has been ongoing to mitigate the impact of these catastrophes and preclude the economic losses. The existing methods that utilize pre-event and post-event images not only require the immediate and guaranteed availability of the appropriate data set but are also encumbered by manual mapping of the images, necessitating the indication of corresponding control points in the two images. This paper highlights the use of only post-event imagery in the absence of reference data to achieve a more timely delivery to produce damage maps as the output. This eliminates the need for manual georeferencing of images. Our method incorporates simple linear iterative clustering (SLIC) for segmenting the images into uniform superpixels and extraction of 62 features for each superpixel. We used various classifiers of which Random Forest classifier was found to give a comparatively high accuracy of 90.4% over others. To enumerate the accuracy of the method proposed, we used 1500 data regions of which 20% were used for testing, and 80% were used for training. The aerial images taken by GeoEye1 after the 2011 Christchurch earthquake and 2011 Japan earthquake and tsunami are utilized in this study to detect building damage. In the case of availability of ground truth, we compare the histograms of the pre- and post-imagery to quantify similarity as the SSD (Sum of Squared Distances) value and thus, our approach produces an assessment as an output map displaying the extent of damage in the area covered by each superpixel. We consider 6 levels of damage ranging from 1 to 6, where 1 signifies no damage, and 6, maximum damage. © 2017 IEEE. Damage Assessment; Image Processing; Random Forest Classifier; Segmentation; Texture Features Antennas; Decision trees; Disasters; Earthquakes; Image processing; Image segmentation; Iterative methods; Losses; Pixels; Satellite imagery; Superpixels; Tsunamis; 2011 Christchurch earthquakes; Corresponding control points; Damage assessments; Damage Identification; Devastating effects; Random forest classifier; Simple Linear Iterative Clustering; Texture features; Damage detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
551;Multi3Net: Segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery;We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate medium- and long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segmentation as well as to alternative fusion approaches for the segmentation of flooded buildings and find that our model performs best on both tasks. We also demonstrate that our model produces highly accurate segmentation maps of flooded buildings using only publicly available medium-resolution data instead of significantly more detailed but sparsely available very high-resolution data. We release the first open-source dataset of fully preprocessed and labeled multiresolution, multispectral, and multitemporal satellite images of disaster sites along with our source code. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.;NULL;"Buildings; Damage detection; Floods; Image segmentation; Neural networks; Open source software; Open systems; Satellite imagery; Convolutional neural network; First responders; Flood maps; Flooded buildings; Multiresolution; Multisensor and multitemporal; Multisensor satellites; Multitemporal satellite imagery; Rapid segmentations; Segmentation map; Disasters";"Multi3Net: Segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate medium- and long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segmentation as well as to alternative fusion approaches for the segmentation of flooded buildings and find that our model performs best on both tasks. We also demonstrate that our model produces highly accurate segmentation maps of flooded buildings using only publicly available medium-resolution data instead of significantly more detailed but sparsely available very high-resolution data. We release the first open-source dataset of fully preprocessed and labeled multiresolution, multispectral, and multitemporal satellite images of disaster sites along with our source code. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. NULL Buildings; Damage detection; Floods; Image segmentation; Neural networks; Open source software; Open systems; Satellite imagery; Convolutional neural network; First responders; Flood maps; Flooded buildings; Multiresolution; Multisensor and multitemporal; Multisensor satellites; Multitemporal satellite imagery; Rapid segmentations; Segmentation map; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
552;Generating Diverse Optimal Road Management Plans in Post-Disaster by Applying Envelope Multi-Objective Deep Reinforcement Learning;The authors used a data-driven reinforcement learning model for the post-disaster rapid recovery of human mobility, considering human-mobility recovery rate, road connectivity, and travel cost as the recovery components, to generate the reward framework. Each component has relative importance with respect to the others. However, if the preference is different from the original one, the optimal policy may not always be identified. This limitation must be addressed to enhance the robustness and generalizability of the proposed deep Q-network model. Therefore, a set of optimal policies were identified over a predetermined preference space, and the underlying importance was evaluated by applying envelope multiobjective reinforcement learning. The agent used in this study could distinguish the importance of each damaged road based on a given relative preference and derive a road-recovery policy suitable for each criterion. Furthermore, the authors provided the guidelines for constructing the optimal road-management plan. Based on the generalized policy network, the government can access diverse restoration strategies and select the most appropriate one depending on the disaster situation. © Fuji Technology Press Ltd.;"multi-objective reinforcement learning; relative importance; road restoration; western Japan flooding";"Computer system recovery; Deep learning; Disasters; Highway administration; Highway planning; Recovery; Reinforcement learning; Roads and streets; Floodings; Management plans; Multi objective; Multi-objective reinforcement learning; Reinforcement learnings; Relative importance; Road management; Road restoration; Western Japan; Western japan flooding; Restoration";"Generating Diverse Optimal Road Management Plans in Post-Disaster by Applying Envelope Multi-Objective Deep Reinforcement Learning The authors used a data-driven reinforcement learning model for the post-disaster rapid recovery of human mobility, considering human-mobility recovery rate, road connectivity, and travel cost as the recovery components, to generate the reward framework. Each component has relative importance with respect to the others. However, if the preference is different from the original one, the optimal policy may not always be identified. This limitation must be addressed to enhance the robustness and generalizability of the proposed deep Q-network model. Therefore, a set of optimal policies were identified over a predetermined preference space, and the underlying importance was evaluated by applying envelope multiobjective reinforcement learning. The agent used in this study could distinguish the importance of each damaged road based on a given relative preference and derive a road-recovery policy suitable for each criterion. Furthermore, the authors provided the guidelines for constructing the optimal road-management plan. Based on the generalized policy network, the government can access diverse restoration strategies and select the most appropriate one depending on the disaster situation. © Fuji Technology Press Ltd. multi-objective reinforcement learning; relative importance; road restoration; western Japan flooding Computer system recovery; Deep learning; Disasters; Highway administration; Highway planning; Recovery; Reinforcement learning; Roads and streets; Floodings; Management plans; Multi objective; Multi-objective reinforcement learning; Reinforcement learnings; Relative importance; Road management; Road restoration; Western Japan; Western japan flooding; Restoration";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.2;Hydrological;4;Recovery
553;Temporal city modeling using street level imagery;Estimation of the temporal changes to a city is useful for city management, disaster recovery operations, and understanding natural phenomena. When several types of data are available for this task, the optimal type should be chosen depending on the changes that need to be detected. However, data of the desired type are not always available, particularly historical data. In this study, we propose two methods for detecting changes in a city, which can be used in complement to process available data types and detect changes in selected targets. The first method estimates the presence of buildings by comparing street-level images and a 2D city map of buildings created at different points in time. This method uses the Structure from Motion (SfM) technique to reconstruct a point cloud of the structures of the city, and matches the point cloud with the 3D building structures recovered from its 2D map. While 2D city maps are available for most cities, most are not very accurate. Therefore, this method is designed to overcome these inaccuracies and thus is widely applicable. On the other hand, the method cannot detect the following types of scene change: wall paintings, buildings that were reconstructed and closely restored to their previous shape, pedestrians, cars, and vegetation. The second method uses a pair of street-level images that are roughly aligned with GPS data collected at different points in time to detect such scene changes. This method uses the features of a convolutional neural network (CNN) in combination with superpixel segmentation to address inaccurate image alignment and it also enables change detection with pixel-level accuracy. Additionally, the second method is scalable for large-scale estimation because it can quickly detect scene changes by merely using an image pair without performing large-scale SfM. The authors consider the proper use of these two methods to enable temporal city modeling in various situations. We experimentally apply these methods to cities damaged by the tsunami that struck Japan in 2011 and the results show their effectiveness. © 2017 Elsevier Inc.;"Change detection; City map; City-scale; CNN; SfM; Vehicular imagery";"Image segmentation; Neural networks; Pixels; Signal detection; Change detection; City map; City scale; Convolutional neural network; Natural phenomena; Structure from motion; Superpixel segmentations; Vehicular imagery; Global positioning system";"Temporal city modeling using street level imagery Estimation of the temporal changes to a city is useful for city management, disaster recovery operations, and understanding natural phenomena. When several types of data are available for this task, the optimal type should be chosen depending on the changes that need to be detected. However, data of the desired type are not always available, particularly historical data. In this study, we propose two methods for detecting changes in a city, which can be used in complement to process available data types and detect changes in selected targets. The first method estimates the presence of buildings by comparing street-level images and a 2D city map of buildings created at different points in time. This method uses the Structure from Motion (SfM) technique to reconstruct a point cloud of the structures of the city, and matches the point cloud with the 3D building structures recovered from its 2D map. While 2D city maps are available for most cities, most are not very accurate. Therefore, this method is designed to overcome these inaccuracies and thus is widely applicable. On the other hand, the method cannot detect the following types of scene change: wall paintings, buildings that were reconstructed and closely restored to their previous shape, pedestrians, cars, and vegetation. The second method uses a pair of street-level images that are roughly aligned with GPS data collected at different points in time to detect such scene changes. This method uses the features of a convolutional neural network (CNN) in combination with superpixel segmentation to address inaccurate image alignment and it also enables change detection with pixel-level accuracy. Additionally, the second method is scalable for large-scale estimation because it can quickly detect scene changes by merely using an image pair without performing large-scale SfM. The authors consider the proper use of these two methods to enable temporal city modeling in various situations. We experimentally apply these methods to cities damaged by the tsunami that struck Japan in 2011 and the results show their effectiveness. © 2017 Elsevier Inc. Change detection; City map; City-scale; CNN; SfM; Vehicular imagery Image segmentation; Neural networks; Pixels; Signal detection; Change detection; City map; City scale; Convolutional neural network; Natural phenomena; Structure from motion; Superpixel segmentations; Vehicular imagery; Global positioning system";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
554;AI-Landslide: Software for acquiring hidden insights from global landslide data using Artificial Intelligence;AI-Landslide is a decision support software for a city planner, disaster recovery strategist or landslide researcher. The software can be deployed in a number of platforms including mobile phones (Android or iOS), tablets or desktop environments. AI-Landslide uses AI based algorithms like automated regression analysis, decomposition analysis and anomaly detection to find out hidden insights on landslide features. The user of AI-landslide does not need to have prior knowledge on AI algorithms, since the right algorithms are executed in the background and the hidden insights on the landslide data are presented in a natural language. AI-Landslide is available at https://github.com/DrSufi/GlobalLandslide. © 2021 The Author(s);"AI Landslide; Artificial intelligence on global landslide; Discovery of hidden insight from landslide data; Intelligent knowledge acquisition; Landslide decision support system";NULL;"AI-Landslide: Software for acquiring hidden insights from global landslide data using Artificial Intelligence AI-Landslide is a decision support software for a city planner, disaster recovery strategist or landslide researcher. The software can be deployed in a number of platforms including mobile phones (Android or iOS), tablets or desktop environments. AI-Landslide uses AI based algorithms like automated regression analysis, decomposition analysis and anomaly detection to find out hidden insights on landslide features. The user of AI-landslide does not need to have prior knowledge on AI algorithms, since the right algorithms are executed in the background and the hidden insights on the landslide data are presented in a natural language. AI-Landslide is available at https://github.com/DrSufi/GlobalLandslide. © 2021 The Author(s) AI Landslide; Artificial intelligence on global landslide; Discovery of hidden insight from landslide data; Intelligent knowledge acquisition; Landslide decision support system NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
555;Spatial model for predictive recovery monitoring based on hazard, built environment, and population features and their spillover effects;The ability to proactively monitor the trajectory of post-disaster recovery is valuable for resource allocation prioritization. Existing knowledge, however, lacks models and insights for quantifying and proactively monitoring post-disaster community recovery. This study examines models that could predict population activity recovery at the scale of the census block group (CBG). Population activity recovery is measured by using location-based human mobility visitation patterns to essential points-of-interest (POIs) in the context of the 2017 Hurricane Harvey in Harris County, Texas. The study examined the association between the population activity recovery duration and 32 features split into four categories: (1) physical vulnerability and access, (2) hazard exposure and impact, (3) proactive actions and (4) population features. Several types of spatial regression models were evaluated to determine their ability to capture this relationship. The Spatial Durbin Model was identified as the best fit for assessing direct, spillover, and total effects of features on population activity recovery at the CBG level. The results show the extent of physical vulnerability, measured by road network density, prolongs the duration of population activity recovery by a combination of direct and spillover effects. Also, the extent of access to essential facilities, measured based on the number of POIs, shortens the duration of population activity recovery. Correspondingly, the extent of flooding is not a significant feature in explaining the population recovery duration in CBGs. The results show that better preparedness, measured by extent of POIs visitations prior to hurricane landing, is associated with faster population activity recovery. In terms of population attributes, the total number of people, the percentage of minorities, and the percentage of Black and Asian subpopulations are significant features in the model for predicting the duration of population activity recovery. The study outcome offers data-driven insights for understanding the determinants of population activity recovery and provides a new model tool for predictive recovery monitoring based on evaluating the direct, spillover, and total effects of features. These findings can identify areas with slower or more rapid recovery to inform emergency managers and public officials in ensuring equitable resource allocation prioritization. © The Author(s) 2023.;"big data; Community recovery; data-driven approaches; essential activity recovery; mobility based recovery";"Harris County [Texas]; Texas; United States; census; hazard assessment; modeling; population dynamics; prediction; spatial analysis; spillover effect; urban population";"Spatial model for predictive recovery monitoring based on hazard, built environment, and population features and their spillover effects The ability to proactively monitor the trajectory of post-disaster recovery is valuable for resource allocation prioritization. Existing knowledge, however, lacks models and insights for quantifying and proactively monitoring post-disaster community recovery. This study examines models that could predict population activity recovery at the scale of the census block group (CBG). Population activity recovery is measured by using location-based human mobility visitation patterns to essential points-of-interest (POIs) in the context of the 2017 Hurricane Harvey in Harris County, Texas. The study examined the association between the population activity recovery duration and 32 features split into four categories: (1) physical vulnerability and access, (2) hazard exposure and impact, (3) proactive actions and (4) population features. Several types of spatial regression models were evaluated to determine their ability to capture this relationship. The Spatial Durbin Model was identified as the best fit for assessing direct, spillover, and total effects of features on population activity recovery at the CBG level. The results show the extent of physical vulnerability, measured by road network density, prolongs the duration of population activity recovery by a combination of direct and spillover effects. Also, the extent of access to essential facilities, measured based on the number of POIs, shortens the duration of population activity recovery. Correspondingly, the extent of flooding is not a significant feature in explaining the population recovery duration in CBGs. The results show that better preparedness, measured by extent of POIs visitations prior to hurricane landing, is associated with faster population activity recovery. In terms of population attributes, the total number of people, the percentage of minorities, and the percentage of Black and Asian subpopulations are significant features in the model for predicting the duration of population activity recovery. The study outcome offers data-driven insights for understanding the determinants of population activity recovery and provides a new model tool for predictive recovery monitoring based on evaluating the direct, spillover, and total effects of features. These findings can identify areas with slower or more rapid recovery to inform emergency managers and public officials in ensuring equitable resource allocation prioritization. © The Author(s) 2023. big data; Community recovery; data-driven approaches; essential activity recovery; mobility based recovery Harris County [Texas]; Texas; United States; census; hazard assessment; modeling; population dynamics; prediction; spatial analysis; spillover effect; urban population";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;4;Recovery
556;SAR and LIDAR datasets for building damage evaluation based on support vector machine and random forest algorithms—A case study of Kumamoto earthquake, Japan;The evaluation of buildings damage following disasters from natural hazards is a crucial step in determining the extent of the damage and measuring renovation needs. In this study, a combination of the synthetic aperture radar (SAR) and light detection and ranging (LIDAR) data before and after the earthquake were used to assess the damage to buildings caused by the Kumamoto earthquake. For damage assessment, three variables including elevation difference (ELD) and texture difference (TD) in pre-and post-event LIDAR images and coherence difference (CD) in SAR images before and after the event were considered and their results were extracted. Machine learning algorithms including random forest (RDF) and the support vector machine (SVM) were used to classify and predict the rate of damage. The results showed that ELD parameter played a key role in identifying the damaged buildings. The SVM algorithm using the ELD parameter and considering three damage rates, including D0 and D1 (Negligible to slight damages), D2, D3 and D4 (Moderate to Heavy damages) and D5 and D6 (Collapsed buildings) provided an overall accuracy of about 87.1%. In addition, for four damage rates, the overall accuracy was about 78.1%. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.;"Airborne SAR; Damage assessment; Earthquake; Kumamoto; LIDAR";NULL;"SAR and LIDAR datasets for building damage evaluation based on support vector machine and random forest algorithms—A case study of Kumamoto earthquake, Japan The evaluation of buildings damage following disasters from natural hazards is a crucial step in determining the extent of the damage and measuring renovation needs. In this study, a combination of the synthetic aperture radar (SAR) and light detection and ranging (LIDAR) data before and after the earthquake were used to assess the damage to buildings caused by the Kumamoto earthquake. For damage assessment, three variables including elevation difference (ELD) and texture difference (TD) in pre-and post-event LIDAR images and coherence difference (CD) in SAR images before and after the event were considered and their results were extracted. Machine learning algorithms including random forest (RDF) and the support vector machine (SVM) were used to classify and predict the rate of damage. The results showed that ELD parameter played a key role in identifying the damaged buildings. The SVM algorithm using the ELD parameter and considering three damage rates, including D0 and D1 (Negligible to slight damages), D2, D3 and D4 (Moderate to Heavy damages) and D5 and D6 (Collapsed buildings) provided an overall accuracy of about 87.1%. In addition, for four damage rates, the overall accuracy was about 78.1%. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. Airborne SAR; Damage assessment; Earthquake; Kumamoto; LIDAR NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
557;Integrative modeling of housing recovery as a physical, economic, and social process;This paper presents a novel approach to modeling housing recovery through the formulation of recovery-based fragility functions built on empirical data collected longitudinally after a recent flood disaster. Previous community resilience frameworks have not addressed social and economic considerations in engineering-based recovery modeling. In doing so, this work takes an important step forward, advancing the use of probability and statistics in civil engineering applications and facilitating their role in interdisciplinary analysis of post-disaster recovery. To address community housing recovery after a flood event, two recovery-based limit states were analyzed: repair completion and re-occupancy. Two least squares regression models identified the variables most strongly associated with each limit state. These variables included household race and ethnicity, whether the household received post-disaster financial recovery assistance, and physical damage to the home. The analyses provide evidence of the simultaneous and interconnected social, economic, and physical processes that take place in a community and influence recovery progress, further demonstrating the need for multidisciplinary teams and analytic approaches in modeling resilience and recovery. © 13th International Conference on Applications of Statistics and Probability in Civil Engineering, ICASP 2019. All rights reserved.;NULL;"Disasters; Floods; Housing; Probability; Regression analysis; Civil engineering applications; Community resiliences; Economic considerations; Integrative modeling; Interdisciplinary analysis; Least squares regression; Multi-disciplinary teams; Probability and statistics; Recovery";"Integrative modeling of housing recovery as a physical, economic, and social process This paper presents a novel approach to modeling housing recovery through the formulation of recovery-based fragility functions built on empirical data collected longitudinally after a recent flood disaster. Previous community resilience frameworks have not addressed social and economic considerations in engineering-based recovery modeling. In doing so, this work takes an important step forward, advancing the use of probability and statistics in civil engineering applications and facilitating their role in interdisciplinary analysis of post-disaster recovery. To address community housing recovery after a flood event, two recovery-based limit states were analyzed: repair completion and re-occupancy. Two least squares regression models identified the variables most strongly associated with each limit state. These variables included household race and ethnicity, whether the household received post-disaster financial recovery assistance, and physical damage to the home. The analyses provide evidence of the simultaneous and interconnected social, economic, and physical processes that take place in a community and influence recovery progress, further demonstrating the need for multidisciplinary teams and analytic approaches in modeling resilience and recovery. © 13th International Conference on Applications of Statistics and Probability in Civil Engineering, ICASP 2019. All rights reserved. NULL Disasters; Floods; Housing; Probability; Regression analysis; Civil engineering applications; Community resiliences; Economic considerations; Integrative modeling; Interdisciplinary analysis; Least squares regression; Multi-disciplinary teams; Probability and statistics; Recovery";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;4;Recovery
558;A Case-Based Reasoning Framework Augmented with Causal Graph Bayesian Networks for Multi-Hazard Assessment of Earthquake Impacts;Earthquakes often lead to significant secondary hazards such as landslides, liquefaction, and aftershocks, which in turn cause great damage to buildings and seriously jeopardize socio-ecological welfare. The prevailing models for post-earthquake damage assessment predominantly utilize deep learning methods and InSAR-based Damage Proxy Maps. However, these approaches require data of high quality, with both multi-temporal and spatiotemporal resolution, and are heavily reliant on supervised learning, limiting their applicability on a broader scale. This paper presents a Case-Based Reasoning Framework Augmented with Causal Graph Bayesian Networks for Multi-Hazard Impact Assessment. This method demonstrates strong adaptability to noisy data, making it an innovative tool in the field of earthquake damage estimation. We applied this framework to analyze the catastrophic earthquakes that struck Turkey and Japan in 2023 and 2024, respectively, using them as bases for our case-based reasoning process. For the Turkey case, our model achieved a precision of 99.9%, a recall of 40.2%, and an F1 score of 57.4% in detecting landslides—significantly surpassing the performance of the USGS a priori model. In detecting liquefaction, the model showed a recall of 95.9% and an F1 score of 70.6%, both substantial improvements over the preliminary model. For the 2024 Noto Peninsula earthquake, our method enhanced the Area Under the Curve (AUC) index from 0.73 to 0.77, further validating the effectiveness of our approach. This study offers a highly precise, scalable, and unsupervised learning method for estimating earthquake disaster damage, providing a valuable asset for optimizing post-disaster resource allocation, reducing economic losses and accurately repairing the environment. © 2024 Copyright for this paper by its authors.;"Case-Based Reasoning; Causal Graph Bayesian Networks; Earthquake; Multi-Hazard Assessment";"Bayesian networks; Case based reasoning; Damage detection; Disasters; Earthquakes; Hazards; Landslides; Learning systems; Liquefaction; Losses; Unsupervised learning; Bayesia n networks; Casebased reasonings (CBR); Causal graph; Causal graph bayesian network; Earthquake damages; F1 scores; Hazard Assessment; Multi-hazard assessment; Multi-hazards; Reasoning framework; Deep learning";"A Case-Based Reasoning Framework Augmented with Causal Graph Bayesian Networks for Multi-Hazard Assessment of Earthquake Impacts Earthquakes often lead to significant secondary hazards such as landslides, liquefaction, and aftershocks, which in turn cause great damage to buildings and seriously jeopardize socio-ecological welfare. The prevailing models for post-earthquake damage assessment predominantly utilize deep learning methods and InSAR-based Damage Proxy Maps. However, these approaches require data of high quality, with both multi-temporal and spatiotemporal resolution, and are heavily reliant on supervised learning, limiting their applicability on a broader scale. This paper presents a Case-Based Reasoning Framework Augmented with Causal Graph Bayesian Networks for Multi-Hazard Impact Assessment. This method demonstrates strong adaptability to noisy data, making it an innovative tool in the field of earthquake damage estimation. We applied this framework to analyze the catastrophic earthquakes that struck Turkey and Japan in 2023 and 2024, respectively, using them as bases for our case-based reasoning process. For the Turkey case, our model achieved a precision of 99.9%, a recall of 40.2%, and an F1 score of 57.4% in detecting landslides—significantly surpassing the performance of the USGS a priori model. In detecting liquefaction, the model showed a recall of 95.9% and an F1 score of 70.6%, both substantial improvements over the preliminary model. For the 2024 Noto Peninsula earthquake, our method enhanced the Area Under the Curve (AUC) index from 0.73 to 0.77, further validating the effectiveness of our approach. This study offers a highly precise, scalable, and unsupervised learning method for estimating earthquake disaster damage, providing a valuable asset for optimizing post-disaster resource allocation, reducing economic losses and accurately repairing the environment. © 2024 Copyright for this paper by its authors. Case-Based Reasoning; Causal Graph Bayesian Networks; Earthquake; Multi-Hazard Assessment Bayesian networks; Case based reasoning; Damage detection; Disasters; Earthquakes; Hazards; Landslides; Learning systems; Liquefaction; Losses; Unsupervised learning; Bayesia n networks; Casebased reasonings (CBR); Causal graph; Causal graph bayesian network; Earthquake damages; F1 scores; Hazard Assessment; Multi-hazard assessment; Multi-hazards; Reasoning framework; Deep learning";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;3;Response
559;Rapid post-earthquake damage assessment of building portfolios through deep learning-based component-level image recognition;Frequent seismic events significantly heighten the likelihood of the building structure being impacted throughout its operational lifecycle. Seismic effects on these structures result in a notable reduction in structural safety and serviceability. Enabling the post-earthquake recovery of building structures necessitates a precise and swift assessment of earthquake-induced damage. The conventional method for assessing building damage involves collecting data through close manual observation or contact inspection. Although it can obtain relatively accurate results by combining evaluation specifications, it is time-consuming and labour-intensive. Deep learning (DL) is data-driven and employs computational methods for data processing. The image classification function of convolutional neural network (CNN) in DL brings great convenience to image data processing. It has been applied to various aspects of structural health monitoring/post-disaster assessment. However, most of the current studies focus on the component level and lack of a comprehensive perspective on the whole structure, which is not conducive to the judgment of the overall condition. Consequently, this paper proposes a rapid assessment method of the overall damage level of post-earthquake buildings based on component images and DL. Two component-level image classification models, component type and damage level, are firstly trained. Then, the images of the buildings to be evaluated after the earthquake are collected and classified. Combined with the weights of different component types and damage levels proposed, the overall condition scores and grades of the structures can be obtained by weighted calculation. The proposed method realizes an overall evaluation of the structural damage condition after seismic events, and further extends to the building portfolios, providing actionable guidance for subsequent personnel and fund allocation, rescue, and maintenance measures. Due to limitations in the dataset, such as the potential biases in the training dataset, the trained model is not perfect and faces challenges in distinguishing between minor and moderate damage. In the future, the dataset should update and ideally cover a wide range of building types, component types, and damage levels to ensure the model's robustness and applicability to various real-world scenarios. © 2024 Elsevier Ltd;"Component image; Deep learning; Global damage level; Image classification; Structural assessment";"Buildings; Building structure; Component image; Component levels; Damage level; Deep learning; Global damage; Global damage level; Images classification; Seismic event; Structural assessments; Earthquake effects";"Rapid post-earthquake damage assessment of building portfolios through deep learning-based component-level image recognition Frequent seismic events significantly heighten the likelihood of the building structure being impacted throughout its operational lifecycle. Seismic effects on these structures result in a notable reduction in structural safety and serviceability. Enabling the post-earthquake recovery of building structures necessitates a precise and swift assessment of earthquake-induced damage. The conventional method for assessing building damage involves collecting data through close manual observation or contact inspection. Although it can obtain relatively accurate results by combining evaluation specifications, it is time-consuming and labour-intensive. Deep learning (DL) is data-driven and employs computational methods for data processing. The image classification function of convolutional neural network (CNN) in DL brings great convenience to image data processing. It has been applied to various aspects of structural health monitoring/post-disaster assessment. However, most of the current studies focus on the component level and lack of a comprehensive perspective on the whole structure, which is not conducive to the judgment of the overall condition. Consequently, this paper proposes a rapid assessment method of the overall damage level of post-earthquake buildings based on component images and DL. Two component-level image classification models, component type and damage level, are firstly trained. Then, the images of the buildings to be evaluated after the earthquake are collected and classified. Combined with the weights of different component types and damage levels proposed, the overall condition scores and grades of the structures can be obtained by weighted calculation. The proposed method realizes an overall evaluation of the structural damage condition after seismic events, and further extends to the building portfolios, providing actionable guidance for subsequent personnel and fund allocation, rescue, and maintenance measures. Due to limitations in the dataset, such as the potential biases in the training dataset, the trained model is not perfect and faces challenges in distinguishing between minor and moderate damage. In the future, the dataset should update and ideally cover a wide range of building types, component types, and damage levels to ensure the model's robustness and applicability to various real-world scenarios. © 2024 Elsevier Ltd Component image; Deep learning; Global damage level; Image classification; Structural assessment Buildings; Building structure; Component image; Component levels; Damage level; Deep learning; Global damage; Global damage level; Images classification; Seismic event; Structural assessments; Earthquake effects";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
560;Infrastructure recovery curve estimation using Gaussian process regression on expert elicited data;The U.S. National Institute of Standards and Technology (NIST)’s Community Resilience Planning Guide uses recovery times of infrastructure functions as key metrics for disaster resilience. The existing literature also widely uses the recovery curve and the area under it to measure infrastructure resilience. Therefore, infrastructure recovery curve estimation is critical to understanding and improving disaster resilience. Unfortunately, this process is challenging in the pre-event planning context due to lack of historical data. To bridge this gap, we consider a situation where infrastructure experts are asked to estimate the time for different infrastructure systems to recover to certain functionality levels after a scenario hazard event. We propose a methodological framework to use expert-elicited data to estimate the expected recovery time curve of a particular infrastructure system. This framework uses the Gaussian process regression (GPR) to capture the experts’ estimation-uncertainty and satisfy known physical constraints of recovery processes. The framework is designed to find a balance between the data collection cost of expert elicitation and the prediction accuracy of GPR. We evaluate the framework on simulated expert-elicited data concerning two case study events, the 1995 Great Hanshin-Awaji Earthquake and the 2011 Great East Japan Earthquake. It is shown that the framework is robust against different configurations such as the number of experts, how the quantities of interest are elicited, and uncertainty in the experts’ estimates. © 2021 Elsevier Ltd;"Disaster recovery; Expert elicitation; Gaussian process regression; Infrastructure restoration; Resilience planning";"Bridges; Disasters; Earthquakes; Gaussian distribution; Recovery; Regression analysis; Uncertainty analysis; Curve estimation; Disaster recovery; Disaster resiliences; Expert elicitation; Gaussian process regression; Infrastructure restorations; Infrastructure systems; Recovery curves; Recovery time; Resilience planning; Gaussian noise (electronic)";"Infrastructure recovery curve estimation using Gaussian process regression on expert elicited data The U.S. National Institute of Standards and Technology (NIST)’s Community Resilience Planning Guide uses recovery times of infrastructure functions as key metrics for disaster resilience. The existing literature also widely uses the recovery curve and the area under it to measure infrastructure resilience. Therefore, infrastructure recovery curve estimation is critical to understanding and improving disaster resilience. Unfortunately, this process is challenging in the pre-event planning context due to lack of historical data. To bridge this gap, we consider a situation where infrastructure experts are asked to estimate the time for different infrastructure systems to recover to certain functionality levels after a scenario hazard event. We propose a methodological framework to use expert-elicited data to estimate the expected recovery time curve of a particular infrastructure system. This framework uses the Gaussian process regression (GPR) to capture the experts’ estimation-uncertainty and satisfy known physical constraints of recovery processes. The framework is designed to find a balance between the data collection cost of expert elicitation and the prediction accuracy of GPR. We evaluate the framework on simulated expert-elicited data concerning two case study events, the 1995 Great Hanshin-Awaji Earthquake and the 2011 Great East Japan Earthquake. It is shown that the framework is robust against different configurations such as the number of experts, how the quantities of interest are elicited, and uncertainty in the experts’ estimates. © 2021 Elsevier Ltd Disaster recovery; Expert elicitation; Gaussian process regression; Infrastructure restoration; Resilience planning Bridges; Disasters; Earthquakes; Gaussian distribution; Recovery; Regression analysis; Uncertainty analysis; Curve estimation; Disaster recovery; Disaster resiliences; Expert elicitation; Gaussian process regression; Infrastructure restorations; Infrastructure systems; Recovery curves; Recovery time; Resilience planning; Gaussian noise (electronic)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
561;Semantic Segmentation of Historical Landslide Based on Improved U-Net;Landslide disasters are extremely destructive. Accurate identification of landslides plays an important role in disaster assessment, loss control and post-disaster reconstruction. This paper proposes a semantic segmentation landslide identification method based on improved U-Net. The deep convolution neural network and jump connection method is used for end-to-end semantic segmentation to achieve deep feature extraction and fusion of different receptive fields, thus enriching feature information. SENet modules are adopted to enhance the ability of the model to extract important features, so as to further improve the accuracy of model recognition. Extensive experiments show that our improved U-Net achieves better performance than the original algorithm on our landslide datasets. The results of Iou are improved by 4.12% which demonstrates our work is of great significance for the research of landslide area identification. Finally, the model is deployed to the web and applied to the geological hazard intelligent monitoring system to realize the landslide identification task. © 2023 Technical Committee on Control Theory, Chinese Association of Automation.;"improved U-Net; Landslide identification; semantic segmentation; web deployment";"Disasters; Risk management; Semantic Segmentation; Semantic Web; Semantics; Convolution neural network; Identification method; Improved U-net; Jump connections; Landslide identification; Loss control; Network connection; Post-disaster reconstruction; Semantic segmentation; Web deployment; Landslides";"Semantic Segmentation of Historical Landslide Based on Improved U-Net Landslide disasters are extremely destructive. Accurate identification of landslides plays an important role in disaster assessment, loss control and post-disaster reconstruction. This paper proposes a semantic segmentation landslide identification method based on improved U-Net. The deep convolution neural network and jump connection method is used for end-to-end semantic segmentation to achieve deep feature extraction and fusion of different receptive fields, thus enriching feature information. SENet modules are adopted to enhance the ability of the model to extract important features, so as to further improve the accuracy of model recognition. Extensive experiments show that our improved U-Net achieves better performance than the original algorithm on our landslide datasets. The results of Iou are improved by 4.12% which demonstrates our work is of great significance for the research of landslide area identification. Finally, the model is deployed to the web and applied to the geological hazard intelligent monitoring system to realize the landslide identification task. © 2023 Technical Committee on Control Theory, Chinese Association of Automation. improved U-Net; Landslide identification; semantic segmentation; web deployment Disasters; Risk management; Semantic Segmentation; Semantic Web; Semantics; Convolution neural network; Identification method; Improved U-net; Jump connections; Landslide identification; Loss control; Network connection; Post-disaster reconstruction; Semantic segmentation; Web deployment; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
562;Developing a damage assessment model for bridge surroundings: a study of the disaster caused by Typhoon Morakot in Taiwan;Typhoon Morakot has been the most severe typhoon disaster to strike Taiwan in recent decades causing tremendous damage to bridge surroundings in 2009. However, we still lack a means of assessing post-typhoon damage for follow-up rebuilding. This paper presents an integrated model that automatically measures changes in rivers, areas of damage to bridge surroundings, and changes in vegetation. The proposed model is based on a neurofuzzy mechanism enhanced by the self-organising map optimisation algorithm and also includes the particular functions of dilation, erosion, and skeletonisation to deal with river imagery. High resolution FORMOSAT-2 satellite imagery from before and after the invasion period is adopted. A bridge is randomly selected from the 129 destroyed due to the typhoon for applications of the model. The recognition results show that the river average width has increased 66% with a maximum increase of over 200%. The ruined segment of the bridge is located exactly in the most scoured region. There has also been a nearly 10% reduction in the vegetation coverage. The results yielded by the proposed model demonstrate a pinpoint accuracy rate of 99.94%. This study successfully develops a tool for large-scale damage assessment as well as for precise measurement after disasters. © 2013 Taylor & Francis.;"ANN; bridge; damage assessment; fuzzy; optimisation; remote sensing imagery; SOM; typhoon disaster";"Taiwan; Algorithms; Bridges; Damage detection; Disasters; Rivers; Satellite imagery; Vegetation; ANN; Damage assessments; fuzzy; Optimisations; Remote sensing imagery; SOM; artificial neural network; assessment method; bridge; FORMOSAT-2; fuzzy mathematics; natural disaster; optimization; remote sensing; satellite imagery; storm damage; typhoon; Hurricanes";"Developing a damage assessment model for bridge surroundings: a study of the disaster caused by Typhoon Morakot in Taiwan Typhoon Morakot has been the most severe typhoon disaster to strike Taiwan in recent decades causing tremendous damage to bridge surroundings in 2009. However, we still lack a means of assessing post-typhoon damage for follow-up rebuilding. This paper presents an integrated model that automatically measures changes in rivers, areas of damage to bridge surroundings, and changes in vegetation. The proposed model is based on a neurofuzzy mechanism enhanced by the self-organising map optimisation algorithm and also includes the particular functions of dilation, erosion, and skeletonisation to deal with river imagery. High resolution FORMOSAT-2 satellite imagery from before and after the invasion period is adopted. A bridge is randomly selected from the 129 destroyed due to the typhoon for applications of the model. The recognition results show that the river average width has increased 66% with a maximum increase of over 200%. The ruined segment of the bridge is located exactly in the most scoured region. There has also been a nearly 10% reduction in the vegetation coverage. The results yielded by the proposed model demonstrate a pinpoint accuracy rate of 99.94%. This study successfully develops a tool for large-scale damage assessment as well as for precise measurement after disasters. © 2013 Taylor & Francis. ANN; bridge; damage assessment; fuzzy; optimisation; remote sensing imagery; SOM; typhoon disaster Taiwan; Algorithms; Bridges; Damage detection; Disasters; Rivers; Satellite imagery; Vegetation; ANN; Damage assessments; fuzzy; Optimisations; Remote sensing imagery; SOM; artificial neural network; assessment method; bridge; FORMOSAT-2; fuzzy mathematics; natural disaster; optimization; remote sensing; satellite imagery; storm damage; typhoon; Hurricanes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;4;Recovery
563;Evaluation of Seismic Landslide Susceptibility by Integrating Statistical Learning Model and Newmark Model-A Case Study of the Wenchuan Earthquake;Whether it can quickly and effectively predict the susceptibility of regional earthquake landslides to achieve rapid rescue, loss assessment and post-disaster reconstruction has always been a difficult problem. However, the traditional high-precision evaluation of seismic landslide susceptibility often relies heavily on the complete or incomplete landslide inventory, which is poor in timeliness and cannot effectively evaluate the target area before or shortly after the earthquake. In most cases, the Newmark model relies on experts' experience to select model parameters, therefore the evaluation result of this method is unstable and it lacks strong generalization ability. A fused model is proposed to classify the positive and negative training samples of the study area through the evaluation results of the Newmark model under the slope units, and it applies a variety of statistical learning models to evaluate the landslide susceptibility of the Wenchuan earthquake based on the classification results of the Newmark model. The results show that the evaluation of the statistical learning model fused with the Newmark model has higher accuracy. This method can overcome the inherent shortcomings of a single Newmark model to obtain better evaluation results without relying on obtaining the complete landslide inventory. Meanwhile, the model can be applied to quickly obtain the evaluation results of regional landslide susceptibility before or shortly after the earthquake, thereby effectively reducing human and economic losses caused by earthquake landslides. © 2021 The authors and IOS Press.;"Earthquake landslide; landslide susceptibility; newmark model; slope units; statistical learning model";"Earthquakes; Learning systems; Losses; Earthquake landslide; Evaluation results; Landslide susceptibility; Learning models; Newmark; Newmark model; Seismic landslides; Slope unit; Statistical learning; Statistical learning model; Landslides";"Evaluation of Seismic Landslide Susceptibility by Integrating Statistical Learning Model and Newmark Model-A Case Study of the Wenchuan Earthquake Whether it can quickly and effectively predict the susceptibility of regional earthquake landslides to achieve rapid rescue, loss assessment and post-disaster reconstruction has always been a difficult problem. However, the traditional high-precision evaluation of seismic landslide susceptibility often relies heavily on the complete or incomplete landslide inventory, which is poor in timeliness and cannot effectively evaluate the target area before or shortly after the earthquake. In most cases, the Newmark model relies on experts' experience to select model parameters, therefore the evaluation result of this method is unstable and it lacks strong generalization ability. A fused model is proposed to classify the positive and negative training samples of the study area through the evaluation results of the Newmark model under the slope units, and it applies a variety of statistical learning models to evaluate the landslide susceptibility of the Wenchuan earthquake based on the classification results of the Newmark model. The results show that the evaluation of the statistical learning model fused with the Newmark model has higher accuracy. This method can overcome the inherent shortcomings of a single Newmark model to obtain better evaluation results without relying on obtaining the complete landslide inventory. Meanwhile, the model can be applied to quickly obtain the evaluation results of regional landslide susceptibility before or shortly after the earthquake, thereby effectively reducing human and economic losses caused by earthquake landslides. © 2021 The authors and IOS Press. Earthquake landslide; landslide susceptibility; newmark model; slope units; statistical learning model Earthquakes; Learning systems; Losses; Earthquake landslide; Evaluation results; Landslide susceptibility; Learning models; Newmark; Newmark model; Seismic landslides; Slope unit; Statistical learning; Statistical learning model; Landslides";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
564;Water Extraction in PolSAR Image Based on Superpixel and Graph Convolutional Network;The timely detection and mapping of surface water bodies from Polarimetric Synthetic Aperture Radar (PolSAR) images are of great significance for emergency management and post-disaster restoration tasks. Though various methods have been proposed in previous years, there are still some inherent flaws. Thus, this paper proposes a new surface water extraction method based on superpixels and Graph Convolutional Networks (GCN). First, the PolSAR images are segmented to generate superpixels as the basic unit of classification, and the graph structure data are established according to their connection to superpixels. Then, the features of each superpixel are extracted. Finally, a GCN is used to classify each superpixel unit using node features and their relationships. This study conducted experiments on a sudden flooding event due to heavy rain and a lake in the city. Detailed verification was carried out. Compared to traditional methods, the recall was improved by 3% while maintaining almost 100% accuracy in complex flood areas. The results show that the proposed method of surface water extraction from PolSAR images has great advantages, acquiring higher accuracy and better boundary adherence in cases of fewer samples. This paper also illustrates the advantage of using GCN to mine the contextual information of classification objects. © 2023 by the authors.;"graph convolutional networks (GCN); object-based image analysis (OBIA); superpixel; synthetic aperture radar (SAR); threshold segmentation; water extraction";NULL;"Water Extraction in PolSAR Image Based on Superpixel and Graph Convolutional Network The timely detection and mapping of surface water bodies from Polarimetric Synthetic Aperture Radar (PolSAR) images are of great significance for emergency management and post-disaster restoration tasks. Though various methods have been proposed in previous years, there are still some inherent flaws. Thus, this paper proposes a new surface water extraction method based on superpixels and Graph Convolutional Networks (GCN). First, the PolSAR images are segmented to generate superpixels as the basic unit of classification, and the graph structure data are established according to their connection to superpixels. Then, the features of each superpixel are extracted. Finally, a GCN is used to classify each superpixel unit using node features and their relationships. This study conducted experiments on a sudden flooding event due to heavy rain and a lake in the city. Detailed verification was carried out. Compared to traditional methods, the recall was improved by 3% while maintaining almost 100% accuracy in complex flood areas. The results show that the proposed method of surface water extraction from PolSAR images has great advantages, acquiring higher accuracy and better boundary adherence in cases of fewer samples. This paper also illustrates the advantage of using GCN to mine the contextual information of classification objects. © 2023 by the authors. graph convolutional networks (GCN); object-based image analysis (OBIA); superpixel; synthetic aperture radar (SAR); threshold segmentation; water extraction NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
565;CNN-Based Semantic Change Detection in Satellite Imagery;Timely disaster risk management requires accurate road maps and prompt damage assessment. Currently, this is done by volunteers manually marking satellite imagery of affected areas but this process is slow and often error-prone. Segmentation algorithms can be applied to satellite images to detect road networks. However, existing methods are unsuitable for disaster-struck areas as they make assumptions about the road network topology which may no longer be valid in these scenarios. Herein, we propose a CNN-based framework for identifying accessible roads in post-disaster imagery by detecting changes from pre-disaster imagery. Graph theory is combined with the CNN output for detecting semantic changes in road networks with OpenStreetMap data. Our results are validated with data of a tsunami-affected region in Palu, Indonesia acquired from DigitalGlobe. © Springer Nature Switzerland AG 2019.;"Convolutional Neural Networks; Graph theory; Satellite imagery; Semantic segmentation";"Damage detection; Disasters; Graph theory; Highway planning; Image segmentation; Motor transportation; Neural networks; Risk assessment; Risk management; Roads and streets; Semantics; Affected area; Change detection; Convolutional neural network; Damage assessments; Post disasters; Satellite images; Segmentation algorithms; Semantic segmentation; Satellite imagery";"CNN-Based Semantic Change Detection in Satellite Imagery Timely disaster risk management requires accurate road maps and prompt damage assessment. Currently, this is done by volunteers manually marking satellite imagery of affected areas but this process is slow and often error-prone. Segmentation algorithms can be applied to satellite images to detect road networks. However, existing methods are unsuitable for disaster-struck areas as they make assumptions about the road network topology which may no longer be valid in these scenarios. Herein, we propose a CNN-based framework for identifying accessible roads in post-disaster imagery by detecting changes from pre-disaster imagery. Graph theory is combined with the CNN output for detecting semantic changes in road networks with OpenStreetMap data. Our results are validated with data of a tsunami-affected region in Palu, Indonesia acquired from DigitalGlobe. © Springer Nature Switzerland AG 2019. Convolutional Neural Networks; Graph theory; Satellite imagery; Semantic segmentation Damage detection; Disasters; Graph theory; Highway planning; Image segmentation; Motor transportation; Neural networks; Risk assessment; Risk management; Roads and streets; Semantics; Affected area; Change detection; Convolutional neural network; Damage assessments; Post disasters; Satellite images; Segmentation algorithms; Semantic segmentation; Satellite imagery";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
566;Diverse approaches to the preservation of built vernacular heritage: case study of post-disaster reconstruction of the Xijie Historic District in Dujiangyan City, China;"Preservation of the built vernacular heritage contributes to maintaining a ‘sense of place' and cultural diversity; yet, it is often ignored in preservation practices that favour high styled architectures and monumental sites. In China, although the understanding of the value of vernacular expression has shown some progress, technical and methodological efforts are still necessary to address the diversity and complexity of vernacular heritage. In this paper, the Xijie Historic District in Dujiangyan City in China provides an example for the preservation of the built vernacular heritage in the context of neighbourhood revitalization during a post-earthquake reconstruction project. Five types of intervention are examined in this paper, including the repair and restoration of the monuments, restoration of historic buildings, rehabilitation of traditional houses, contextual design of new buildings, and demolition to provide public space and facilities. In particular, the measures implemented to meet the residents' needs while maintaining the diversity of the built vernacular heritage are inspected. This study concludes with three recommendations: the classification of vernacular environments and employment of diverse measures to each type; the adaptation of the vernacular environment to meet residents’ expectations and aspirations; and recognition of the development and reasonable control of the changes. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.";"Built vernacular heritage; China; community; diversity; historic district; historic preservation";NULL;"Diverse approaches to the preservation of built vernacular heritage: case study of post-disaster reconstruction of the Xijie Historic District in Dujiangyan City, China Preservation of the built vernacular heritage contributes to maintaining a ‘sense of place' and cultural diversity; yet, it is often ignored in preservation practices that favour high styled architectures and monumental sites. In China, although the understanding of the value of vernacular expression has shown some progress, technical and methodological efforts are still necessary to address the diversity and complexity of vernacular heritage. In this paper, the Xijie Historic District in Dujiangyan City in China provides an example for the preservation of the built vernacular heritage in the context of neighbourhood revitalization during a post-earthquake reconstruction project. Five types of intervention are examined in this paper, including the repair and restoration of the monuments, restoration of historic buildings, rehabilitation of traditional houses, contextual design of new buildings, and demolition to provide public space and facilities. In particular, the measures implemented to meet the residents' needs while maintaining the diversity of the built vernacular heritage are inspected. This study concludes with three recommendations: the classification of vernacular environments and employment of diverse measures to each type; the adaptation of the vernacular environment to meet residents’ expectations and aspirations; and recognition of the development and reasonable control of the changes. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group. Built vernacular heritage; China; community; diversity; historic district; historic preservation NULL";-1;Não Classificado;NULL;1.1;Geological;4;Recovery
567;UAV remote sensing image processing and classification in Wenchuan earthquake district;"Remote sensing image presented as one of the most effective methods to acquire the disaster information in the Wenchuan earthquake. For its technical advantages such as rapid and flexible takeoff, the Unmanned Aerial Vehicles (UAV) played an important role in the ""5.12"" Wenchuan earthquake emergency relief. The UAV remote sensing image has some special characteristics contrasting with other RS image, so how to process quickly the images is the key issue. In this paper, a fast method is presented for moisacing the remote sensing images from the UAV without ground control points. The method grounds on two principles, parallel computing and digital photogrammetry. The experimental results justify the needs of uncontrolled mosaic images production for the requirements of post-disaster emergency. The mosaic images without correction by ground control points provide no geographic information, but they can provide panoramic and macro-information of earthquake-stricken areas. Then the geometric correction technique was employed to correct the panoramic mosaic images to obtain the high-precise digital orthophoto based on the pre-disaster information such as topographic maps. By using the object-oriented method for the information extraction and rapid automatic classification of the UVA remote sensing image can achieve the anticipative goals. © 2009 Copyright SPIE - The International Society for Optical Engineering.";"Non-standard image processing; Remote sensing image classification; Unmanned-Aircraft Vehicle (UAV); Wenchuan earthquake";"Aircraft accidents; Automatic indexing; Data processing; Earthquakes; Geographic information systems; Image analysis; Image classification; Image reconstruction; Imaging systems; Maps; Parallel architectures; Remote sensing; Remotely operated vehicles; Rock mechanics; Unmanned aerial vehicles (UAV); Aircraft vehicles; Automatic classification; Digital photogrammetry; Disaster Information; Emergency relief; Fast methods; Geographic information; Geometric correction; Ground control points; High-precise; Information Extraction; Key issues; Mosaic images; Object oriented method; Panoramic mosaic image; Parallel Computing; Remote sensing image classification; Remote sensing images; RS image; Standard images; Topographic map; Wenchuan Earthquake; Object recognition";"UAV remote sensing image processing and classification in Wenchuan earthquake district Remote sensing image presented as one of the most effective methods to acquire the disaster information in the Wenchuan earthquake. For its technical advantages such as rapid and flexible takeoff, the Unmanned Aerial Vehicles (UAV) played an important role in the ""5.12"" Wenchuan earthquake emergency relief. The UAV remote sensing image has some special characteristics contrasting with other RS image, so how to process quickly the images is the key issue. In this paper, a fast method is presented for moisacing the remote sensing images from the UAV without ground control points. The method grounds on two principles, parallel computing and digital photogrammetry. The experimental results justify the needs of uncontrolled mosaic images production for the requirements of post-disaster emergency. The mosaic images without correction by ground control points provide no geographic information, but they can provide panoramic and macro-information of earthquake-stricken areas. Then the geometric correction technique was employed to correct the panoramic mosaic images to obtain the high-precise digital orthophoto based on the pre-disaster information such as topographic maps. By using the object-oriented method for the information extraction and rapid automatic classification of the UVA remote sensing image can achieve the anticipative goals. © 2009 Copyright SPIE - The International Society for Optical Engineering. Non-standard image processing; Remote sensing image classification; Unmanned-Aircraft Vehicle (UAV); Wenchuan earthquake Aircraft accidents; Automatic indexing; Data processing; Earthquakes; Geographic information systems; Image analysis; Image classification; Image reconstruction; Imaging systems; Maps; Parallel architectures; Remote sensing; Remotely operated vehicles; Rock mechanics; Unmanned aerial vehicles (UAV); Aircraft vehicles; Automatic classification; Digital photogrammetry; Disaster Information; Emergency relief; Fast methods; Geographic information; Geometric correction; Ground control points; High-precise; Information Extraction; Key issues; Mosaic images; Object oriented method; Panoramic mosaic image; Parallel Computing; Remote sensing image classification; Remote sensing images; RS image; Standard images; Topographic map; Wenchuan Earthquake; Object recognition";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
568;Nazr-CNN: Fine-Grained classification of UAV imagery for damage assessment;We propose Nazr-CNN1, a deep learning pipeline for object detection and fine-grained classification in images acquired from Unmanned Aerial Vehicles (UAVs) for damage assessment and monitoring. Nazr-CNN consists of two components. The function of the first component is to localize objects (e.g. houses or infrastructure) in an image by carrying out a pixel-level classification. In the second component, a hidden layer of a Convolutional Neural Network (CNN) is used to encode Fisher Vectors (FV) of the segments generated from the first component in order to help discriminate between different levels of damage. To showcase our approach we use data from UAVs that were deployed to assess the level of damage in the aftermath of a devastating cyclone that hit the island of Vanuatu in 2015. The collected images were labeled by a crowdsourcing effort and the labeling categories consisted of fine-grained levels of damage to built structures. Since our data set is relatively small, a pre-trained network for pixel-level classification and FV encoding was used. Nazr-CNN attains promising results both for object detection and damage assessment suggesting that the integrated pipeline is robust in the face of small data sets and labeling errors by annotators. While the focus of Nazr-CNN is on assessment of UAV images in a post-disaster scenario, our solution is general and can be applied in many diverse settings. We show one such case of transfer learning to assess the level of damage in aerial images collected after a typhoon in Philippines. © 2017 IEEE.;NULL;"Advanced Analytics; Aircraft detection; Antennas; Classification (of information); Deep learning; Encoding (symbols); Image classification; Multilayer neural networks; Object detection; Object recognition; Pipelines; Pixels; Sodium compounds; Storms; Structures (built objects); Unmanned aerial vehicles (UAV); Convolutional neural network; Damage assessments; Fisher vectors; Hidden layers; Post disasters; Small data set; Transfer learning; Two-component; Damage detection";"Nazr-CNN: Fine-Grained classification of UAV imagery for damage assessment We propose Nazr-CNN1, a deep learning pipeline for object detection and fine-grained classification in images acquired from Unmanned Aerial Vehicles (UAVs) for damage assessment and monitoring. Nazr-CNN consists of two components. The function of the first component is to localize objects (e.g. houses or infrastructure) in an image by carrying out a pixel-level classification. In the second component, a hidden layer of a Convolutional Neural Network (CNN) is used to encode Fisher Vectors (FV) of the segments generated from the first component in order to help discriminate between different levels of damage. To showcase our approach we use data from UAVs that were deployed to assess the level of damage in the aftermath of a devastating cyclone that hit the island of Vanuatu in 2015. The collected images were labeled by a crowdsourcing effort and the labeling categories consisted of fine-grained levels of damage to built structures. Since our data set is relatively small, a pre-trained network for pixel-level classification and FV encoding was used. Nazr-CNN attains promising results both for object detection and damage assessment suggesting that the integrated pipeline is robust in the face of small data sets and labeling errors by annotators. While the focus of Nazr-CNN is on assessment of UAV images in a post-disaster scenario, our solution is general and can be applied in many diverse settings. We show one such case of transfer learning to assess the level of damage in aerial images collected after a typhoon in Philippines. © 2017 IEEE. NULL Advanced Analytics; Aircraft detection; Antennas; Classification (of information); Deep learning; Encoding (symbols); Image classification; Multilayer neural networks; Object detection; Object recognition; Pipelines; Pixels; Sodium compounds; Storms; Structures (built objects); Unmanned aerial vehicles (UAV); Convolutional neural network; Damage assessments; Fisher vectors; Hidden layers; Post disasters; Small data set; Transfer learning; Two-component; Damage detection";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;3;Response
569;The study on the influencing factors on the tourism intention to postdisaster reconstruction tourist areas;The tourism resource in post-disaster reconstruction tourist areas is very unique, containing not only traditional natural and cultural tourism resources, but also dark tourism resources. The existing theories could not explain the tourism intention to post-disaster reconstruction tourist areas. The paper discusses the important factors influencing tourism intention to Wenchuan, based on the Tourism Value Chain and Theory of Planned Behavior. The authors conclude that, the factors that influence tourists intention are earthquake remains, the lifestyle in the disaster area, spirit of earthquake relief work and big love, time travel time and safety concerns. © 2014 WIT Press.;"Path analysis; Post-disaster reconstruction tourist areas; Tourism intention; TPB";"Earthquakes; Information technology; Regression analysis; Disaster areas; Path analysis; Post-disaster reconstruction; Safety concerns; Theory of Planned Behavior; Tourism intention; Tourist areas; TPB; Disasters";"The study on the influencing factors on the tourism intention to postdisaster reconstruction tourist areas The tourism resource in post-disaster reconstruction tourist areas is very unique, containing not only traditional natural and cultural tourism resources, but also dark tourism resources. The existing theories could not explain the tourism intention to post-disaster reconstruction tourist areas. The paper discusses the important factors influencing tourism intention to Wenchuan, based on the Tourism Value Chain and Theory of Planned Behavior. The authors conclude that, the factors that influence tourists intention are earthquake remains, the lifestyle in the disaster area, spirit of earthquake relief work and big love, time travel time and safety concerns. © 2014 WIT Press. Path analysis; Post-disaster reconstruction tourist areas; Tourism intention; TPB Earthquakes; Information technology; Regression analysis; Disaster areas; Path analysis; Post-disaster reconstruction; Safety concerns; Theory of Planned Behavior; Tourism intention; Tourist areas; TPB; Disasters";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
570;Spatio-Temporal Analysis of Post-Disaster Built-up Expansion in Banda Aceh City, Indonesia;The 2004 tsunami destroyed most of the built-up areas in the city of Banda Aceh, Indonesia. The physical development recoveries were immediately taken place to restore urban function. This study aims to identify spatio-temporal patterns of built-up expansion in the city of Banda Aceh during the recovery (2005-2009) and post-recovery period (2009-2019). Urban land cover was classified from Landsat imagery using the supervised classification method with maximum likelihood. The results indicate that the built-up area has increased from 1426 ha in 2005 to 3321 ha in 2019. The intensity of growth in the initial period was high, while the intensity in the following period has decreased significantly. Urban recovery had contributed greatly to the built-up expansion in a relatively short period of time. This study also proved the difference in the direction of expansion at the micro-scale (sub-district level) in the two periods. © 2020 IEEE.;"Banda Aceh; Landsat; reconstruction; spatio-temporal pattern; Urban expansion";"Maximum likelihood; Remote sensing; Banda Aceh; Built-up areas; Indonesia; LANDSAT; Physical development; Post disasters; Reconstruction; Spatiotemporal analysis; Spatiotemporal patterns; Urban expansion; Recovery";"Spatio-Temporal Analysis of Post-Disaster Built-up Expansion in Banda Aceh City, Indonesia The 2004 tsunami destroyed most of the built-up areas in the city of Banda Aceh, Indonesia. The physical development recoveries were immediately taken place to restore urban function. This study aims to identify spatio-temporal patterns of built-up expansion in the city of Banda Aceh during the recovery (2005-2009) and post-recovery period (2009-2019). Urban land cover was classified from Landsat imagery using the supervised classification method with maximum likelihood. The results indicate that the built-up area has increased from 1426 ha in 2005 to 3321 ha in 2019. The intensity of growth in the initial period was high, while the intensity in the following period has decreased significantly. Urban recovery had contributed greatly to the built-up expansion in a relatively short period of time. This study also proved the difference in the direction of expansion at the micro-scale (sub-district level) in the two periods. © 2020 IEEE. Banda Aceh; Landsat; reconstruction; spatio-temporal pattern; Urban expansion Maximum likelihood; Remote sensing; Banda Aceh; Built-up areas; Indonesia; LANDSAT; Physical development; Post disasters; Reconstruction; Spatiotemporal analysis; Spatiotemporal patterns; Urban expansion; Recovery";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
571;Multiagent Q-Learning for Multicrew Dynamic Scheduling and Routing in Road Network Restoration;Road network restoration is an important issue in the post-disaster disposal and rescue, especially when extraordinarily serious natural disasters (e.g., floods and earthquakes) occur. Central to this endeavour is the problem of determining how to reasonably schedule and route the repair crew to quickly restore the damaged road network and establish reliable supply lines from supply nodes to demand nodes. However, most existing work focuses on the activities of the single repair crew in the static road network, which is unable to adapt to the dynamic changes of the road network caused by secondary disasters, such as rock slides and debris flows. Consequently, this work is concentrated on multicrew dynamic scheduling and routing in road network restoration. Specifically, a model of multicrew dynamic scheduling and routing is first presented. Next, the Markov decision process is adopted to construct a decision model for repair crews, based on which a multiagent Q-learning algorithm is developed to find the best scheduling and routing strategy. Finally, experimental results demonstrate that the proposed method can make repair crews adjust their scheduling and routing strategies according to the dynamic changes of the damaged road network and provides a useful attempt to restore the damaged road network in complex, dynamic emergency scenarios of post-disaster. © 2022 IEEE.;"Dynamic Scheduling and Routing; Multiagent Q-Learning; Multiple Repair Crews; Road Network Restoration";"Disasters; Markov processes; Motor transportation; Multi agent systems; Reinforcement learning; Restoration; Roads and streets; Routing algorithms; Dynamic routing; Dynamic scheduling; Multi agent; Multiagent Q-learning; Multiple repair crew; Network restoration; Q-learning; Road network; Road network restoration; Scheduling and routing; Learning algorithms";"Multiagent Q-Learning for Multicrew Dynamic Scheduling and Routing in Road Network Restoration Road network restoration is an important issue in the post-disaster disposal and rescue, especially when extraordinarily serious natural disasters (e.g., floods and earthquakes) occur. Central to this endeavour is the problem of determining how to reasonably schedule and route the repair crew to quickly restore the damaged road network and establish reliable supply lines from supply nodes to demand nodes. However, most existing work focuses on the activities of the single repair crew in the static road network, which is unable to adapt to the dynamic changes of the road network caused by secondary disasters, such as rock slides and debris flows. Consequently, this work is concentrated on multicrew dynamic scheduling and routing in road network restoration. Specifically, a model of multicrew dynamic scheduling and routing is first presented. Next, the Markov decision process is adopted to construct a decision model for repair crews, based on which a multiagent Q-learning algorithm is developed to find the best scheduling and routing strategy. Finally, experimental results demonstrate that the proposed method can make repair crews adjust their scheduling and routing strategies according to the dynamic changes of the damaged road network and provides a useful attempt to restore the damaged road network in complex, dynamic emergency scenarios of post-disaster. © 2022 IEEE. Dynamic Scheduling and Routing; Multiagent Q-Learning; Multiple Repair Crews; Road Network Restoration Disasters; Markov processes; Motor transportation; Multi agent systems; Reinforcement learning; Restoration; Roads and streets; Routing algorithms; Dynamic routing; Dynamic scheduling; Multi agent; Multiagent Q-learning; Multiple repair crew; Network restoration; Q-learning; Road network; Road network restoration; Scheduling and routing; Learning algorithms";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;4;Recovery
572;FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding;Visual scene understanding is the core task in making any crucial decision in any computer vision system. Although popular computer vision datasets like Cityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g. image classification, segmentation, object detection), these datasets are hardly suitable for post disaster damage assessments. On the other hand, existing natural disaster datasets include mainly satellite imagery which has low spatial resolution and a high revisit period. Therefore, they do not have a scope to provide quick and efficient damage assessment tasks. Unmanned Aerial Vehicle (UAV) can effortlessly access difficult places during any disaster and collect high resolution imagery that is required for aforementioned tasks of computer vision. To address these issues we present a high resolution UAV imagery, FloodNet, captured after the hurricane Harvey. This dataset demonstrates the post flooded damages of the affected areas. The images are labeled pixel-wise for semantic segmentation task and questions are produced for the task of visual question answering. FloodNet poses several challenges including detection of flooded roads and buildings and distinguishing between natural water and flooded water. With the advancement of deep learning algorithms, we can analyze the impact of any disaster which can make a precise understanding of the affected areas. In this paper, we compare and contrast the performances of baseline methods for image classification, semantic segmentation, and visual question answering on our dataset. FloodNet dataset can be downloaded from here: https://github.com/BinaLab/FloodNet-Supervised_v1.0. © 2013 IEEE.;"Artificial intelligence; deep learning; hurricane Harvey; image classification; machine learning; natural disaster dataset; remote sensing; semantic segmentation; unmanned aerial vehicle (UAV); visual question answering";"Aerial photography; Antennas; Classification (of information); Computer vision; Damage detection; Deep learning; Disasters; Hurricanes; Image classification; Image segmentation; Learning algorithms; Object detection; Remote sensing; Satellite imagery; Semantics; Unmanned aerial vehicles (UAV); Deep learning; Hurricane harvey; Images classification; Natural disaster dataset; Natural disasters; Question Answering; Remote-sensing; Semantic segmentation; Unmanned aerial vehicle; Visual question answering; Semantic Segmentation";"FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding Visual scene understanding is the core task in making any crucial decision in any computer vision system. Although popular computer vision datasets like Cityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g. image classification, segmentation, object detection), these datasets are hardly suitable for post disaster damage assessments. On the other hand, existing natural disaster datasets include mainly satellite imagery which has low spatial resolution and a high revisit period. Therefore, they do not have a scope to provide quick and efficient damage assessment tasks. Unmanned Aerial Vehicle (UAV) can effortlessly access difficult places during any disaster and collect high resolution imagery that is required for aforementioned tasks of computer vision. To address these issues we present a high resolution UAV imagery, FloodNet, captured after the hurricane Harvey. This dataset demonstrates the post flooded damages of the affected areas. The images are labeled pixel-wise for semantic segmentation task and questions are produced for the task of visual question answering. FloodNet poses several challenges including detection of flooded roads and buildings and distinguishing between natural water and flooded water. With the advancement of deep learning algorithms, we can analyze the impact of any disaster which can make a precise understanding of the affected areas. In this paper, we compare and contrast the performances of baseline methods for image classification, semantic segmentation, and visual question answering on our dataset. FloodNet dataset can be downloaded from here: https://github.com/BinaLab/FloodNet-Supervised_v1.0. © 2013 IEEE. Artificial intelligence; deep learning; hurricane Harvey; image classification; machine learning; natural disaster dataset; remote sensing; semantic segmentation; unmanned aerial vehicle (UAV); visual question answering Aerial photography; Antennas; Classification (of information); Computer vision; Damage detection; Deep learning; Disasters; Hurricanes; Image classification; Image segmentation; Learning algorithms; Object detection; Remote sensing; Satellite imagery; Semantics; Unmanned aerial vehicles (UAV); Deep learning; Hurricane harvey; Images classification; Natural disaster dataset; Natural disasters; Question Answering; Remote-sensing; Semantic segmentation; Unmanned aerial vehicle; Visual question answering; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
573;A Scenario-Based Case Study: Using AI to Analyze Casualties from Landslides in Chittagong Metropolitan Area, Bangladesh;Understanding the complex dynamics of landslides is crucial for disaster planners to make timely and effective decisions that save lives and reduce the economic impact on society. Using the landslide inventory of the Chittagong Metropolitan Area (CMA), we have created a new artificial intelligence (AI)-based insight system for the town planners and senior disaster recovery strategists of Chittagong, Bangladesh. Our system generates dynamic AI-based insights for a range of complex scenarios created from 7 different landslide feature attributes. The users of our system can select a particular kind of scenario out of the exhaustive list of 1.054 × 1041 possible scenario sets, and our AI-based system will immediately predict how many casualties are likely to occur based on the selected kind of scenario. Moreover, an AI-based system shows how landslide attributes (e.g., rainfall, area of mass, elevation, etc.) correlate with landslide casualty by drawing detailed trend lines by performing both linear and logistic regressions. According to the literature and the best of our knowledge, our CMA scenario-based AI insight system is the first of its kind, providing the most comprehensive understanding of landslide scenarios and associated deaths and damages in the CMA. The system was deployed on a wide range of platforms including Android, iOS, and Windows systems so that it could be easily adapted for strategic disaster planners. The deployed solutions were handed down to 12 landslide strategists and disaster planners for evaluations, whereby 91.67% of users found the solution easy to use, effective, and self-explanatory while using it via mobile. © 2023 by the authors.;"AI; causalities; hazards; landslides";"Bangladesh; Chittagong [Bangladesh]; artificial intelligence; disaster management; hazard assessment; landslide; metropolitan area; natural disaster; scenario analysis";"A Scenario-Based Case Study: Using AI to Analyze Casualties from Landslides in Chittagong Metropolitan Area, Bangladesh Understanding the complex dynamics of landslides is crucial for disaster planners to make timely and effective decisions that save lives and reduce the economic impact on society. Using the landslide inventory of the Chittagong Metropolitan Area (CMA), we have created a new artificial intelligence (AI)-based insight system for the town planners and senior disaster recovery strategists of Chittagong, Bangladesh. Our system generates dynamic AI-based insights for a range of complex scenarios created from 7 different landslide feature attributes. The users of our system can select a particular kind of scenario out of the exhaustive list of 1.054 × 1041 possible scenario sets, and our AI-based system will immediately predict how many casualties are likely to occur based on the selected kind of scenario. Moreover, an AI-based system shows how landslide attributes (e.g., rainfall, area of mass, elevation, etc.) correlate with landslide casualty by drawing detailed trend lines by performing both linear and logistic regressions. According to the literature and the best of our knowledge, our CMA scenario-based AI insight system is the first of its kind, providing the most comprehensive understanding of landslide scenarios and associated deaths and damages in the CMA. The system was deployed on a wide range of platforms including Android, iOS, and Windows systems so that it could be easily adapted for strategic disaster planners. The deployed solutions were handed down to 12 landslide strategists and disaster planners for evaluations, whereby 91.67% of users found the solution easy to use, effective, and self-explanatory while using it via mobile. © 2023 by the authors. AI; causalities; hazards; landslides Bangladesh; Chittagong [Bangladesh]; artificial intelligence; disaster management; hazard assessment; landslide; metropolitan area; natural disaster; scenario analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
574;EarthquakeNet: A High-Resolution UAV-Based Dataset for Earthquake Damage Assessment;Advancements in computer vision and deep learning have significantly propelled progress in scene understanding, aiding rescue teams in accurately assessing damage after natural disasters. In this paper, we introduce EarthquakeNet, a meticulously curated high-resolution post-earthquake dataset featuring detailed classification and semantic segmentation annotations, designed to enhance comprehensive scene understanding following natural disasters. EarthquakeNet comprises post-disaster images captured using unmanned aerial vehicles (UAVs) from multiple affected areas after an earthquake. The uniqueness of EarthquakeNet lies in providing high-resolution post-disaster imagery, each with exhaustive annotations. Unlike existing datasets that offer annotations for specific scene elements like buildings, EarthquakeNet provides pixel-level annotations for a broader range of categories, including roads, houses, and tents. We also demonstrate the utility of the dataset by implementing state-of-the-art segmentation models on EarthquakeNet, showcasing its value in improving existing methods for natural disaster damage assessment. © 2024 IEEE.;"Land use detection; Post-earthquake assesment; Semantic segmentation";"Aircraft accidents; Deep learning; Disasters; Earthquake effects; Image annotation; Unmanned aerial vehicles (UAV); Aerial vehicle; Assesment; Damage assessments; High resolution; Land use detection; Natural disasters; Post disasters; Post-earthquake assesment; Scene understanding; Semantic segmentation; Semantic Segmentation";"EarthquakeNet: A High-Resolution UAV-Based Dataset for Earthquake Damage Assessment Advancements in computer vision and deep learning have significantly propelled progress in scene understanding, aiding rescue teams in accurately assessing damage after natural disasters. In this paper, we introduce EarthquakeNet, a meticulously curated high-resolution post-earthquake dataset featuring detailed classification and semantic segmentation annotations, designed to enhance comprehensive scene understanding following natural disasters. EarthquakeNet comprises post-disaster images captured using unmanned aerial vehicles (UAVs) from multiple affected areas after an earthquake. The uniqueness of EarthquakeNet lies in providing high-resolution post-disaster imagery, each with exhaustive annotations. Unlike existing datasets that offer annotations for specific scene elements like buildings, EarthquakeNet provides pixel-level annotations for a broader range of categories, including roads, houses, and tents. We also demonstrate the utility of the dataset by implementing state-of-the-art segmentation models on EarthquakeNet, showcasing its value in improving existing methods for natural disaster damage assessment. © 2024 IEEE. Land use detection; Post-earthquake assesment; Semantic segmentation Aircraft accidents; Deep learning; Disasters; Earthquake effects; Image annotation; Unmanned aerial vehicles (UAV); Aerial vehicle; Assesment; Damage assessments; High resolution; Land use detection; Natural disasters; Post disasters; Post-earthquake assesment; Scene understanding; Semantic segmentation; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
575;The impact of urban form on disaster resiliency: A case study of Brisbane and Ipswich, Australia;Purpose: This paper examines the impact of urban form on disaster resiliency. The literature shows a complex relationship between urban form factors such as density and diversity and disaster recovery. The empirical analysis in this paper tests the impact of land use mix, population density, building type and diversity on the reconstruction progress in three, six and nine months after the 2010 flood in Brisbane and Ipswich as proxies of disaster resilience. Considerable debate exists on whether urban form factors are the causal incentive or are they mediating other non-urban form causal factors such as income level. In view of this, the effects of a series of established non-urban form factors such as income and tenure, already known as effective factors on disaster resilience, are controlled in the analysis. Design/methodology/approach: The structure of this paper is based on a two-phase research approach. In the first phase, for identification of hypothetical relationships between urban form and disaster resiliency, information was gathered from different sources on the basis of theory and past research findings. Then in phase two, a database was developed to test these hypothetical relationships, employing statistical techniques (including multivariate regression and correlation analysis) in which disaster recovery was compared among 76 suburbs of Brisbane and Ipswich with differing levels of population density and land use mix. Findings: The results indicate that population density is positively related to disaster resilience, even when controlling for contextual variables such as income level and home ownership. The association between population density and disaster reconstruction is non-linear. The progress of reconstruction to population density ratio increases from low, medium to high densities, while in very low and very high density areas the reconstruction progress does not show the same behavior, which suggests that medium-high density is the most resilient. Originality/value: The originality of this paper is in extracting hypothetical relationships between urban form and resiliency and testing them with real world data. The results confirmed the contribution of density to recovery process in this case study. This illustrates the importance of attention to disaster resiliency measures from the early stages of design and planning in development of resilient urban communities. © 2016, © Emerald Group Publishing Limited.;"Brisbane and Ipswich; Built environment; Density; Disaster resiliency; Reconstruction; Urban form";"Australia; Brisbane; England; Ipswich; Queensland; Suffolk; United Kingdom; anthropogenic effect; correlation; database; disaster management; empirical analysis; hypothesis testing; income; land use; population density; reconstruction; research work; statistical analysis; suburban area";"The impact of urban form on disaster resiliency: A case study of Brisbane and Ipswich, Australia Purpose: This paper examines the impact of urban form on disaster resiliency. The literature shows a complex relationship between urban form factors such as density and diversity and disaster recovery. The empirical analysis in this paper tests the impact of land use mix, population density, building type and diversity on the reconstruction progress in three, six and nine months after the 2010 flood in Brisbane and Ipswich as proxies of disaster resilience. Considerable debate exists on whether urban form factors are the causal incentive or are they mediating other non-urban form causal factors such as income level. In view of this, the effects of a series of established non-urban form factors such as income and tenure, already known as effective factors on disaster resilience, are controlled in the analysis. Design/methodology/approach: The structure of this paper is based on a two-phase research approach. In the first phase, for identification of hypothetical relationships between urban form and disaster resiliency, information was gathered from different sources on the basis of theory and past research findings. Then in phase two, a database was developed to test these hypothetical relationships, employing statistical techniques (including multivariate regression and correlation analysis) in which disaster recovery was compared among 76 suburbs of Brisbane and Ipswich with differing levels of population density and land use mix. Findings: The results indicate that population density is positively related to disaster resilience, even when controlling for contextual variables such as income level and home ownership. The association between population density and disaster reconstruction is non-linear. The progress of reconstruction to population density ratio increases from low, medium to high densities, while in very low and very high density areas the reconstruction progress does not show the same behavior, which suggests that medium-high density is the most resilient. Originality/value: The originality of this paper is in extracting hypothetical relationships between urban form and resiliency and testing them with real world data. The results confirmed the contribution of density to recovery process in this case study. This illustrates the importance of attention to disaster resiliency measures from the early stages of design and planning in development of resilient urban communities. © 2016, © Emerald Group Publishing Limited. Brisbane and Ipswich; Built environment; Density; Disaster resiliency; Reconstruction; Urban form Australia; Brisbane; England; Ipswich; Queensland; Suffolk; United Kingdom; anthropogenic effect; correlation; database; disaster management; empirical analysis; hypothesis testing; income; land use; population density; reconstruction; research work; statistical analysis; suburban area";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;4;Recovery
576;Using Artificial Neural Network Models to Assess Hurricane Damage through Transfer Learning;"Coastal hazard events such as hurricanes pose a significant threat to coastal communities. Disaster relief is essential to mitigating damage from these catastrophes; therefore, accurate and efficient damage assessment is key to evaluating the extent of damage inflicted on coastal cities and structures. Historically, this process has been carried out by human task forces that manually take post-disaster images and identify the damaged areas. While this method has been well established, current digital tools used for computer vision tasks such as artificial intelligence and machine learning put forth a more efficient and reliable method for assessing post-disaster damage. Using transfer learning on three advanced neural networks, ResNet, MobileNet, and EfficientNet, we applied techniques for damage classification and damaged object detection to our post-hurricane image dataset comprised of damaged buildings from the coastal region of the southeastern United States. Our dataset included 1000 images for the classification model with a binary classification structure containing classes of floods and non-floods and 800 images for the object detection model with four damaged object classes damaged roof, damaged wall, flood damage, and structural damage. Our damage classification model achieved 76% overall accuracy for ResNet and 87% overall accuracy for MobileNet. The F1 score for MobileNet was also 9% higher than the F1 score of ResNet at 0.88. Our damaged object detection model achieved predominant predictions of the four damaged object classes, with MobileNet attaining the highest overall confidence score of 97.58% in its predictions. The object detection results highlight the model’s ability to successfully identify damaged areas of buildings and structures from images in a time span of seconds, which is necessary for more efficient damage assessment. Thus, we show that this level of accuracy for our damage assessment using artificial intelligence is akin to the accuracy of manual damage assessments while also completing the assessment in a drastically shorter time span. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.";"Artificial intelligence; Building damage; Damage classification; Damage detection; Hurricane; Transfer learning";NULL;"Using Artificial Neural Network Models to Assess Hurricane Damage through Transfer Learning Coastal hazard events such as hurricanes pose a significant threat to coastal communities. Disaster relief is essential to mitigating damage from these catastrophes; therefore, accurate and efficient damage assessment is key to evaluating the extent of damage inflicted on coastal cities and structures. Historically, this process has been carried out by human task forces that manually take post-disaster images and identify the damaged areas. While this method has been well established, current digital tools used for computer vision tasks such as artificial intelligence and machine learning put forth a more efficient and reliable method for assessing post-disaster damage. Using transfer learning on three advanced neural networks, ResNet, MobileNet, and EfficientNet, we applied techniques for damage classification and damaged object detection to our post-hurricane image dataset comprised of damaged buildings from the coastal region of the southeastern United States. Our dataset included 1000 images for the classification model with a binary classification structure containing classes of floods and non-floods and 800 images for the object detection model with four damaged object classes damaged roof, damaged wall, flood damage, and structural damage. Our damage classification model achieved 76% overall accuracy for ResNet and 87% overall accuracy for MobileNet. The F1 score for MobileNet was also 9% higher than the F1 score of ResNet at 0.88. Our damaged object detection model achieved predominant predictions of the four damaged object classes, with MobileNet attaining the highest overall confidence score of 97.58% in its predictions. The object detection results highlight the model’s ability to successfully identify damaged areas of buildings and structures from images in a time span of seconds, which is necessary for more efficient damage assessment. Thus, we show that this level of accuracy for our damage assessment using artificial intelligence is akin to the accuracy of manual damage assessments while also completing the assessment in a drastically shorter time span. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. Artificial intelligence; Building damage; Damage classification; Damage detection; Hurricane; Transfer learning NULL";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.3;Meteorological;3;Response
577;DSFA: cross-scene domain style and feature adaptation for landslide detection from high spatial resolution images;Rapid and accurate landslide inventory mapping is significant for emergency rescue and post-disaster reconstruction. Nowadays, deep learning methods exhibit excellent performance in supervised landslide detection. However, due to differences between cross-scene images, the performance of existing methods is significantly degraded when directly applied to another scene, which limits the application of rapid landslide inventory mapping. In this study, we propose a novel Domain Style and Feature Adaptation (DSFA) method for cross-scene landslide detection from high spatial resolution images, which can leverage labeled source domain images and unlabeled target domain images to mine robust landslide representations for different scenes. Specifically, we mitigate the large discrepancy between domains at the dataset level and feature level. At the dataset level, we introduce a domain style adaptation strategy to shift landslide styles, which not only bridges the domain gap, but also increases the diversity of landslide samples. At the feature level, adversarial learning and domain distance minimization are integrated to narrow large feature distribution discrepancies for learning domain-invariant information. In addition, to avoid information omission, we improve the U-Net3+ model. Extensive experimental results demonstrate that DSFA has superior detection capability and outperforms other methods, showing its great application potential in unsupervised landslide domain detection. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.;"deep learning; domain adaptation; high spatial resolution; Landslide detection; remote sensing";"Deep learning; Feature extraction; Image resolution; Landslides; Large dataset; Learning systems; Mapping; Deep learning; Domain adaptation; Feature adaptation; Feature level; High spatial resolution; High spatial resolution images; Landslide detection; Landslide inventories; Performance; Remote-sensing; detection method; image resolution; landslide; machine learning; remote sensing; spatial resolution; Remote sensing";"DSFA: cross-scene domain style and feature adaptation for landslide detection from high spatial resolution images Rapid and accurate landslide inventory mapping is significant for emergency rescue and post-disaster reconstruction. Nowadays, deep learning methods exhibit excellent performance in supervised landslide detection. However, due to differences between cross-scene images, the performance of existing methods is significantly degraded when directly applied to another scene, which limits the application of rapid landslide inventory mapping. In this study, we propose a novel Domain Style and Feature Adaptation (DSFA) method for cross-scene landslide detection from high spatial resolution images, which can leverage labeled source domain images and unlabeled target domain images to mine robust landslide representations for different scenes. Specifically, we mitigate the large discrepancy between domains at the dataset level and feature level. At the dataset level, we introduce a domain style adaptation strategy to shift landslide styles, which not only bridges the domain gap, but also increases the diversity of landslide samples. At the feature level, adversarial learning and domain distance minimization are integrated to narrow large feature distribution discrepancies for learning domain-invariant information. In addition, to avoid information omission, we improve the U-Net3+ model. Extensive experimental results demonstrate that DSFA has superior detection capability and outperforms other methods, showing its great application potential in unsupervised landslide domain detection. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. deep learning; domain adaptation; high spatial resolution; Landslide detection; remote sensing Deep learning; Feature extraction; Image resolution; Landslides; Large dataset; Learning systems; Mapping; Deep learning; Domain adaptation; Feature adaptation; Feature level; High spatial resolution; High spatial resolution images; Landslide detection; Landslide inventories; Performance; Remote-sensing; detection method; image resolution; landslide; machine learning; remote sensing; spatial resolution; Remote sensing";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.1;Geological;1;Prevention
578;Deep learning-based multi-class damage detection for autonomous post-disaster reconnaissance;Timely assessment of damages induced to buildings due to an earthquake is critical for ensuring life safety, mitigating financial losses, and expediting the rehabilitation process as well as enhancing the structural resilience where resilience is measured by an infrastructure's capacity to restore full functionality post extreme events. Since manual inspection is expensive, time consuming and risky, low-cost unmanned aerial vehicles or robots can be leveraged as a viable alternative for quick reconnaissance. Visual data captured by the sensors mounted on the robots can be analyzed, and the damages can be detected and classified autonomously. The present study proposes the use of deep learning-based approaches to this end. Region-based convolutional neural network (Faster RCNN) is exploited to detect four different damage types, namely, surface crack, spalling (which includes façade spalling and concrete spalling), and severe damage with exposed rebars and severely buckled rebars. The performance of the proposed approach is evaluated on manually annotated image data collected from reinforced concrete buildings damaged under several past earthquakes such as Nepal (2015), Taiwan (2016), Ecuador (2016), Erzincan (1992), Duzce (1999), Bingol (2003), Peru (2007), Wenchuan (2008), and Haiti (2010). Several experiments are presented in the paper to illustrate the capabilities, as well as the limitations, of the proposed approach for earthquake reconnaissance. It was observed that Inception-ResNet-v2 significantly outperforms the other networks considered in this study. The research outcome is a stepping stone forward to facilitate the autonomous assessment of buildings where this can be potentially useful for insurance companies, government agencies, and property owners. © 2020 John Wiley & Sons, Ltd.;"damage detection; deep learning; earthquake reconnaissance; Faster RCNN; UAV";"Aircraft detection; Antennas; Concrete buildings; Costs; Damage detection; Earthquakes; Insurance; Losses; Neural networks; Reinforced concrete; Spalling; Unmanned aerial vehicles (UAV); Concrete spalling; Convolutional neural network; Earthquake reconnaissance; Faster RCNN; Government agencies; Insurance companies; Learning-based approach; Post disaster reconnaissance; Deep learning";"Deep learning-based multi-class damage detection for autonomous post-disaster reconnaissance Timely assessment of damages induced to buildings due to an earthquake is critical for ensuring life safety, mitigating financial losses, and expediting the rehabilitation process as well as enhancing the structural resilience where resilience is measured by an infrastructure's capacity to restore full functionality post extreme events. Since manual inspection is expensive, time consuming and risky, low-cost unmanned aerial vehicles or robots can be leveraged as a viable alternative for quick reconnaissance. Visual data captured by the sensors mounted on the robots can be analyzed, and the damages can be detected and classified autonomously. The present study proposes the use of deep learning-based approaches to this end. Region-based convolutional neural network (Faster RCNN) is exploited to detect four different damage types, namely, surface crack, spalling (which includes façade spalling and concrete spalling), and severe damage with exposed rebars and severely buckled rebars. The performance of the proposed approach is evaluated on manually annotated image data collected from reinforced concrete buildings damaged under several past earthquakes such as Nepal (2015), Taiwan (2016), Ecuador (2016), Erzincan (1992), Duzce (1999), Bingol (2003), Peru (2007), Wenchuan (2008), and Haiti (2010). Several experiments are presented in the paper to illustrate the capabilities, as well as the limitations, of the proposed approach for earthquake reconnaissance. It was observed that Inception-ResNet-v2 significantly outperforms the other networks considered in this study. The research outcome is a stepping stone forward to facilitate the autonomous assessment of buildings where this can be potentially useful for insurance companies, government agencies, and property owners. © 2020 John Wiley & Sons, Ltd. damage detection; deep learning; earthquake reconnaissance; Faster RCNN; UAV Aircraft detection; Antennas; Concrete buildings; Costs; Damage detection; Earthquakes; Insurance; Losses; Neural networks; Reinforced concrete; Spalling; Unmanned aerial vehicles (UAV); Concrete spalling; Convolutional neural network; Earthquake reconnaissance; Faster RCNN; Government agencies; Insurance companies; Learning-based approach; Post disaster reconnaissance; Deep learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
579;UAS-based Multispectral Remote Sensing and NDVI Calculation for Post Disaster Assessment;This paper focuses on the use of unmanned aircraft system (UAS) for multispectral remote sensing in post disaster situation awareness and damage assessment applications. The KHawk UAS was developed as a test bed for multispectral aerial video acquisition. Efficient UAS flight planning, data acquisition, and post flight orthomosaic generation workflows are created to ensure the robustness of the overall remote sensing system. The generated 2D orthomaps and the accuracy analysis are presented for validation purposes. A robust normalized difference vegetation index (NDVI) calibration procedure using ground reflectance targets and a pixel-level linear regression algorithm is proposed. The developed KHawk UAS and the proposed procedure is validated in a post tornado (EF2) damage assessment mission. © 2018 IEEE.;"aerial image othorectification; disaster analysis; multispectral remote sensing; NDVI; Unmanned aircraft systems";"Aircraft accidents; Antennas; Damage detection; Data acquisition; Disasters; Unmanned aerial vehicles (UAV); Aerial images; Disaster analysis; Multispectral remote sensing; NDVI; Unmanned aircraft system; Remote sensing";"UAS-based Multispectral Remote Sensing and NDVI Calculation for Post Disaster Assessment This paper focuses on the use of unmanned aircraft system (UAS) for multispectral remote sensing in post disaster situation awareness and damage assessment applications. The KHawk UAS was developed as a test bed for multispectral aerial video acquisition. Efficient UAS flight planning, data acquisition, and post flight orthomosaic generation workflows are created to ensure the robustness of the overall remote sensing system. The generated 2D orthomaps and the accuracy analysis are presented for validation purposes. A robust normalized difference vegetation index (NDVI) calibration procedure using ground reflectance targets and a pixel-level linear regression algorithm is proposed. The developed KHawk UAS and the proposed procedure is validated in a post tornado (EF2) damage assessment mission. © 2018 IEEE. aerial image othorectification; disaster analysis; multispectral remote sensing; NDVI; Unmanned aircraft systems Aircraft accidents; Antennas; Damage detection; Data acquisition; Disasters; Unmanned aerial vehicles (UAV); Aerial images; Disaster analysis; Multispectral remote sensing; NDVI; Unmanned aircraft system; Remote sensing";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;3;Response
580;Continuous assessing for the recovery and reconstruction of post-tsunami disaster of rikuzentakata city in RIA coast by using time series high resolution satellite images;It is important to monitor the recovery and reconstruction process after the disaster for the long term for the following revival plans. Especially, a time series mapping of the change of condition and shape of land surface assessed in the high resolution is hoped for the reconstruction of post-disaster. Extensive damage occurred because of an earthquake on March 11, 2011 in eastern Japan, which generated a huge tsunami. Every municipal district town and village in the coastal regions of Iwate, Miyagi, and Fukushima Prefecture were damaged due to this tsunami. Especially, the town located in the ria coast fell into a catastrophic situation. The inundation was assessed so far by a lot of satellite and airborne sensing. Moreover, various struck situations immediately after disaster was investigated in detail. However, the case continuously assessed is few the recovery and reconstruction of the tsunami-devastated areas by using the high resolution satellite images in time series. In this study, the change in land cover for one and a half years after disaster were assessed by the high resolution satellite images in Rikuzentakata City located in the ria coast. The image data of multi spectral and the panchromatic images that had been observed in the time series by Worldview-2 and Quickbird satellite were used. A time series changes process of inundated area were pursued from the classification of the land cover and the operation between images. Especially, the change on the land surface after the inundation had disappeared was assessed in detail. Additionally, the recovery process of the shape of a public road and block has been quantitatively extracted by operating between images and filtering of edge detection. In addition, the changes in distribution of tsunami debris were shown by assessing the shape and roughness of the images. As a result, the recovery process was effectively made a map in Rikuzentakata City for one and a half years after tsunami disaster. Copyright © (2013) by the American Society for Photogrammetry & Remote Sensing.;NULL;"Disasters; Edge detection; Floods; Photogrammetry; Recovery; Remote sensing; Satellites; Surface measurement; Time series; Tsunamis; Coastal regions; High resolution; High resolution satellite images; Panchromatic images; QuickBird satellite; Reconstruction process; Recovery process; Tsunami disaster; Image reconstruction";"Continuous assessing for the recovery and reconstruction of post-tsunami disaster of rikuzentakata city in RIA coast by using time series high resolution satellite images It is important to monitor the recovery and reconstruction process after the disaster for the long term for the following revival plans. Especially, a time series mapping of the change of condition and shape of land surface assessed in the high resolution is hoped for the reconstruction of post-disaster. Extensive damage occurred because of an earthquake on March 11, 2011 in eastern Japan, which generated a huge tsunami. Every municipal district town and village in the coastal regions of Iwate, Miyagi, and Fukushima Prefecture were damaged due to this tsunami. Especially, the town located in the ria coast fell into a catastrophic situation. The inundation was assessed so far by a lot of satellite and airborne sensing. Moreover, various struck situations immediately after disaster was investigated in detail. However, the case continuously assessed is few the recovery and reconstruction of the tsunami-devastated areas by using the high resolution satellite images in time series. In this study, the change in land cover for one and a half years after disaster were assessed by the high resolution satellite images in Rikuzentakata City located in the ria coast. The image data of multi spectral and the panchromatic images that had been observed in the time series by Worldview-2 and Quickbird satellite were used. A time series changes process of inundated area were pursued from the classification of the land cover and the operation between images. Especially, the change on the land surface after the inundation had disappeared was assessed in detail. Additionally, the recovery process of the shape of a public road and block has been quantitatively extracted by operating between images and filtering of edge detection. In addition, the changes in distribution of tsunami debris were shown by assessing the shape and roughness of the images. As a result, the recovery process was effectively made a map in Rikuzentakata City for one and a half years after tsunami disaster. Copyright © (2013) by the American Society for Photogrammetry & Remote Sensing. NULL Disasters; Edge detection; Floods; Photogrammetry; Recovery; Remote sensing; Satellites; Surface measurement; Time series; Tsunamis; Coastal regions; High resolution; High resolution satellite images; Panchromatic images; QuickBird satellite; Reconstruction process; Recovery process; Tsunami disaster; Image reconstruction";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
581;A deep reinforcement learning model for resilient road network recovery under earthquake or flooding hazards;As the backbone and the ‘blood vessel’ of modern cities, road networks provide critical support for community activities and economic growth, with their roles even more crucial due to the dramatic progress in urbanization. The service of road networks is subjected to the increasing frequency of high-consequence natural hazards such as earthquakes, floods, hurricanes, etc. Identifying resilient restoration sequences is essential to mitigate the disruption of such important infrastructure networks. This paper investigates a novel decision-support model to optimize post-disaster road network repair sequence. The model, named as GCN-DRL model, integrates the advantages of deep reinforced learning (DRL) with graph convolutional neural network (GCN), two emerging artificial intelligence (AI) techniques to achieve efficient recovery of road network service. The model is applied to analyze two cases of community road networks in the US that are subjected to different types of hazards, i.e., earthquakes and flooding. The performance of repair sequence by the GCN-DRL model is compared with two commonly used methods, i.e., repair sequence by the genetic algorithm and by prioritization based on graph importance with betweenness centrality. The results showed the decision sequence by GCN-DRL model consistently achieved superior performance in road network restoration than the conventional methods. The AI-based decision model also features high computational efficiency since the GCN-DRL model can be trained before the hazard. With a pre-trained GCN-DRL model, a close to optimal decision-making process can be made available rapidly for different types of new hazards, which is advantageous in efficiently responding to hazards when they happen. This study demonstrates the promise of a new AI-based decision support model to improve the resilience of road networks by enabling efficient post-hazards recovery. © The Author(s) 2023.;"Decision support; Deep reinforcement learning; Graph convolutional neural network; Infrastructure resilience; Recovery; Road network";"Accidents; Computational efficiency; Computer system recovery; Convolution; Convolutional neural networks; Decision making; Decision support systems; Earthquakes; Economics; Floods; Genetic algorithms; Hazards; Learning systems; Motor transportation; Restoration; Roads and streets; Convolutional neural network; Decision support modelling; Decision supports; Deep reinforcement learning; Graph convolutional neural network; Infrastructure resiliences; Learning models; Reinforced learning; Reinforcement learnings; Road network; Recovery";"A deep reinforcement learning model for resilient road network recovery under earthquake or flooding hazards As the backbone and the ‘blood vessel’ of modern cities, road networks provide critical support for community activities and economic growth, with their roles even more crucial due to the dramatic progress in urbanization. The service of road networks is subjected to the increasing frequency of high-consequence natural hazards such as earthquakes, floods, hurricanes, etc. Identifying resilient restoration sequences is essential to mitigate the disruption of such important infrastructure networks. This paper investigates a novel decision-support model to optimize post-disaster road network repair sequence. The model, named as GCN-DRL model, integrates the advantages of deep reinforced learning (DRL) with graph convolutional neural network (GCN), two emerging artificial intelligence (AI) techniques to achieve efficient recovery of road network service. The model is applied to analyze two cases of community road networks in the US that are subjected to different types of hazards, i.e., earthquakes and flooding. The performance of repair sequence by the GCN-DRL model is compared with two commonly used methods, i.e., repair sequence by the genetic algorithm and by prioritization based on graph importance with betweenness centrality. The results showed the decision sequence by GCN-DRL model consistently achieved superior performance in road network restoration than the conventional methods. The AI-based decision model also features high computational efficiency since the GCN-DRL model can be trained before the hazard. With a pre-trained GCN-DRL model, a close to optimal decision-making process can be made available rapidly for different types of new hazards, which is advantageous in efficiently responding to hazards when they happen. This study demonstrates the promise of a new AI-based decision support model to improve the resilience of road networks by enabling efficient post-hazards recovery. © The Author(s) 2023. Decision support; Deep reinforcement learning; Graph convolutional neural network; Infrastructure resilience; Recovery; Road network Accidents; Computational efficiency; Computer system recovery; Convolution; Convolutional neural networks; Decision making; Decision support systems; Earthquakes; Economics; Floods; Genetic algorithms; Hazards; Learning systems; Motor transportation; Restoration; Roads and streets; Convolutional neural network; Decision support modelling; Decision supports; Deep reinforcement learning; Graph convolutional neural network; Infrastructure resiliences; Learning models; Reinforced learning; Reinforcement learnings; Road network; Recovery";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;-1;NULL;4;Recovery
582;Building Extraction Method in Remote Sensing Image;Identifying buildings in disaster areas quickly and conveniently plays an important role in post-disaster reconstruction and disaster assessment. Aiming at the technical requirements of earthquake relief projects, this paper proposed a building extraction method in remote sensing images by combining traditional digital image processing methods and convolution neural network. First, In the pretreatment process, remote sensing images are transformed from RGB space to HSV space. Then, shadows in the images are weakened by using the Retinex model de-shadowing algorithm. Finally, buildings in the image are classified and recognized by using a convolution neural network. Experiments prove that compared with the fast R-CNN algorithm, the detection accuracy is improved. © 2020, Springer Nature Singapore Pte Ltd.;"Deep-learning; Fast R-CNN; HSV model; Remote sensing image; Target detection";"Convolution; Data handling; Deep learning; Disasters; Embedded systems; Extraction; Maintenance; Remote sensing; Space optics; Target tracking; Building extraction; Convolution neural network; Fast R-CNN; HSV model; Post-disaster reconstruction; Pretreatment process; Remote sensing images; Technical requirement; Image processing";"Building Extraction Method in Remote Sensing Image Identifying buildings in disaster areas quickly and conveniently plays an important role in post-disaster reconstruction and disaster assessment. Aiming at the technical requirements of earthquake relief projects, this paper proposed a building extraction method in remote sensing images by combining traditional digital image processing methods and convolution neural network. First, In the pretreatment process, remote sensing images are transformed from RGB space to HSV space. Then, shadows in the images are weakened by using the Retinex model de-shadowing algorithm. Finally, buildings in the image are classified and recognized by using a convolution neural network. Experiments prove that compared with the fast R-CNN algorithm, the detection accuracy is improved. © 2020, Springer Nature Singapore Pte Ltd. Deep-learning; Fast R-CNN; HSV model; Remote sensing image; Target detection Convolution; Data handling; Deep learning; Disasters; Embedded systems; Extraction; Maintenance; Remote sensing; Space optics; Target tracking; Building extraction; Convolution neural network; Fast R-CNN; HSV model; Post-disaster reconstruction; Pretreatment process; Remote sensing images; Technical requirement; Image processing";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
583;Tsunami Building Damage Assessment using Multiclass Segmentation Model;Natural Disaster is an event caused by environment, it has been concerned as it can caused casualties that makes manual damage assessment become inefficient. Automated damage assessment is one of field of study in Remote Sensing which already studied for several years, from using Traditional Machine Learning into Deep Learning. Recently, semantic segmentation with multitemporal fusion is a method used for Damage Assessment using Deep Learning. Multitemporal Fusion is a method fusing two features from Pre and Post Disaster Images as one using concatenation to get the feature of all two images. Semantic Segmentation is a method to classify each pixel in images into specified class given. This research creates Baseline Model (ResNet-50 + Panoptic FPN + Multitemporal Fusion) for comparison with our proposed method, called SCAMU-Net, which consists of U-Net (with different backbone, DenseNet 121, 169, and 201 layers) and followed by Spatial Channel Attention Module (SCAM) using xBD Dataset in Sunda and Palu Dataset. According to finding of the study, SCAMU-Net with DenseNet 121 shows biggest result in Macro F1 in Palu Dataset with 89.8% outperforms the Baseline Model about 3.6%. Sunda Dataset cannot perform for Training and Testing caused by destroyed class too few for Models to generalized. SCAMU-Net has 1,203,549 less parameters than baseline model. SCAMU-Net also good for detecting different class (No Damaged and Destroyed) that adjacent each other. Results shown that SCAMU-Net DenseNet 121 is enough for classify damage in this research, it shown that extending from DenseNet 121 provide no significant results. © 2022, Ismail Saritas. All rights reserved.;"Deep Learning; Remote Sensing; Semantic Segmentation; Spatial Channel Attention; U-Net";NULL;"Tsunami Building Damage Assessment using Multiclass Segmentation Model Natural Disaster is an event caused by environment, it has been concerned as it can caused casualties that makes manual damage assessment become inefficient. Automated damage assessment is one of field of study in Remote Sensing which already studied for several years, from using Traditional Machine Learning into Deep Learning. Recently, semantic segmentation with multitemporal fusion is a method used for Damage Assessment using Deep Learning. Multitemporal Fusion is a method fusing two features from Pre and Post Disaster Images as one using concatenation to get the feature of all two images. Semantic Segmentation is a method to classify each pixel in images into specified class given. This research creates Baseline Model (ResNet-50 + Panoptic FPN + Multitemporal Fusion) for comparison with our proposed method, called SCAMU-Net, which consists of U-Net (with different backbone, DenseNet 121, 169, and 201 layers) and followed by Spatial Channel Attention Module (SCAM) using xBD Dataset in Sunda and Palu Dataset. According to finding of the study, SCAMU-Net with DenseNet 121 shows biggest result in Macro F1 in Palu Dataset with 89.8% outperforms the Baseline Model about 3.6%. Sunda Dataset cannot perform for Training and Testing caused by destroyed class too few for Models to generalized. SCAMU-Net has 1,203,549 less parameters than baseline model. SCAMU-Net also good for detecting different class (No Damaged and Destroyed) that adjacent each other. Results shown that SCAMU-Net DenseNet 121 is enough for classify damage in this research, it shown that extending from DenseNet 121 provide no significant results. © 2022, Ismail Saritas. All rights reserved. Deep Learning; Remote Sensing; Semantic Segmentation; Spatial Channel Attention; U-Net NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
584;Empirical evaluation of dissimilarity measures for use in urban structural damage detection;Multi-temporal earth-observation imagery is now available at sub-meter accuracy and has been found very useful for performing quick damage detection for urban areas affected by large-scale disasters. The detection of structural damage using images taken before and after disaster events is usually modeled as a change detection problem. In this paper, we propose a new perspective for performing change detection, where dissimilarity measures are used to extract urban structural damage. First, image gradient magnitudes and spatial variances are used as a means to capture urban structural features. Subsequently, a family of distribution dissimilarity measures, including: Euclidean distance, Cosine, Jeffery divergence, and Bhattacharyya distance, are used to extract structural damage. We particularly focus on evaluating the performance of these dissimilarity-based change detection methods under the framework of pattern classification and crossvalidation, and with the use of a pair of bi-temporal satellite images captured before and after a major earthquake in Bam, Iran. The paper concludes that the proposed change detection methods for urban structural damage detection, which are conceptually simple and computationally efficient, outperform the traditional correlation analysis in terms of both classification accuracy and tolerance to local alignment errors. © 2007 SPIE-IS&T.;"Change detection; Dissimilarity measures; Satellite imagery; Urban structural damage";"Computational efficiency; Disasters; Image acquisition; Image analysis; Image reconstruction; Pattern recognition; Satellite imagery; Change detection; Dissimilarity measures; Urban structural damage; Damage detection";"Empirical evaluation of dissimilarity measures for use in urban structural damage detection Multi-temporal earth-observation imagery is now available at sub-meter accuracy and has been found very useful for performing quick damage detection for urban areas affected by large-scale disasters. The detection of structural damage using images taken before and after disaster events is usually modeled as a change detection problem. In this paper, we propose a new perspective for performing change detection, where dissimilarity measures are used to extract urban structural damage. First, image gradient magnitudes and spatial variances are used as a means to capture urban structural features. Subsequently, a family of distribution dissimilarity measures, including: Euclidean distance, Cosine, Jeffery divergence, and Bhattacharyya distance, are used to extract structural damage. We particularly focus on evaluating the performance of these dissimilarity-based change detection methods under the framework of pattern classification and crossvalidation, and with the use of a pair of bi-temporal satellite images captured before and after a major earthquake in Bam, Iran. The paper concludes that the proposed change detection methods for urban structural damage detection, which are conceptually simple and computationally efficient, outperform the traditional correlation analysis in terms of both classification accuracy and tolerance to local alignment errors. © 2007 SPIE-IS&T. Change detection; Dissimilarity measures; Satellite imagery; Urban structural damage Computational efficiency; Disasters; Image acquisition; Image analysis; Image reconstruction; Pattern recognition; Satellite imagery; Change detection; Dissimilarity measures; Urban structural damage; Damage detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
585;Towards automation of building damage detection using WorldView-2 satellite image: The case of Haiti earthquake;Information of disaster damage assessment is very significant to disaster mitigation, aid and post disaster redevelopment planning. Remotely sensed data, especially very high resolution image data from aircraft and satellite have been long recognized very essential and objective source for disaster mapping. However feature extraction from these data remains a very challenge task currently. In this paper, we present a method to extract building damage caused by earthquake from two pairs of Worldview-2© high resolution satellite image. Targeting at implementing a practically operational system, we develop a novel framework integrating semi-automatic building extraction with machine learning mechanism to maximize the automation level of system. We also present a rectilinear building model to deal with a wide variety of rooftops. Through the study case of Haiti earthquake, we demonstrate our method is highly effective for detecting building damage from high resolution satellite image. © 2010 SPIE.;"Building damage; Building detection; Earthquake; WorldView-2";"Aircraft accidents; Buildings; Earthquakes; Feature extraction; Remote sensing; Building damage; Building detection; Damage assessments; Disaster mitigation; Remotely sensed data; Satellite images; Very high resolution image; WorldView-2; Damage detection";"Towards automation of building damage detection using WorldView-2 satellite image: The case of Haiti earthquake Information of disaster damage assessment is very significant to disaster mitigation, aid and post disaster redevelopment planning. Remotely sensed data, especially very high resolution image data from aircraft and satellite have been long recognized very essential and objective source for disaster mapping. However feature extraction from these data remains a very challenge task currently. In this paper, we present a method to extract building damage caused by earthquake from two pairs of Worldview-2© high resolution satellite image. Targeting at implementing a practically operational system, we develop a novel framework integrating semi-automatic building extraction with machine learning mechanism to maximize the automation level of system. We also present a rectilinear building model to deal with a wide variety of rooftops. Through the study case of Haiti earthquake, we demonstrate our method is highly effective for detecting building damage from high resolution satellite image. © 2010 SPIE. Building damage; Building detection; Earthquake; WorldView-2 Aircraft accidents; Buildings; Earthquakes; Feature extraction; Remote sensing; Building damage; Building detection; Damage assessments; Disaster mitigation; Remotely sensed data; Satellite images; Very high resolution image; WorldView-2; Damage detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
586;One Versus All: Best Practices in Combining Multi-hazard Damage Imagery Training Datasets for Damage Detection for a Deep Learning Neural Network;Accurate and timely damage assessment is important after any natural disaster event. Accurate damage assessments enhance the efficient distribution of resources. Building damage levels are an important outcome of damage assessment, especially in urban areas. Although at present most building damage assessments are collected manually from post-disaster satellite images or aerial photographs, efforts are now underway to automate the process. Some of these efforts use deep learning algorithms to first identify buildings and then to classify them into damage levels. One of these efforts initiated in 2019, through the Defense Innovation Unit (DIU) and with Humanitarian Assistance and Disaster Recovery (HADR) organizations, created a multi-hazard training dataset using high-resolution satellite imagery from pre- and post-event imagery (xBD). Across 19 natural disaster events including tornados, wildfire, earthquake, hurricanes, volcanos, flood, and tsunami, buildings were identified and classified into four classes: no damage, minor damage, major damage, and destroyed. Participants in the challenge were expected to use deep learning algorithms to perform the classification. They were also provided with a base classification algorithm, in which participants were encouraged to improve. The base algorithm contained RESNET50 trained on ImageNet database and three additional convolution and max pooling layers. This project analyzes the quality of the training dataset, discusses the pros and cons of combining training dataset across multiple natural disaster events, and provides recommendations on using the provided training dataset to optimize classification accuracy. Specifically, we will provide recommendations on creating class balance in the training dataset, in which damage labels are the most identifiable. We will also provide an assessment on which natural disasters lead to damage that is most identifiable using satellite imagery in which natural disasters lead to less accurate damage assessments. We will also examine pooling training data across natural disasters to achieve more accurate classifications. © 2022, The Society for Experimental Mechanics, Inc.;"Damage assessment; Deep learning; Imbalance; Overfitting; xBD";"Aerial photography; Antennas; Classification (of information); Deep learning; Disasters; Hazards; Learning algorithms; Satellite imagery; Structural dynamics; Building damage; Damage assessments; Damage level; Deep learning; Imbalance; Multi-hazards; Natural disasters; Overfitting; Training dataset; XBD; Damage detection";"One Versus All: Best Practices in Combining Multi-hazard Damage Imagery Training Datasets for Damage Detection for a Deep Learning Neural Network Accurate and timely damage assessment is important after any natural disaster event. Accurate damage assessments enhance the efficient distribution of resources. Building damage levels are an important outcome of damage assessment, especially in urban areas. Although at present most building damage assessments are collected manually from post-disaster satellite images or aerial photographs, efforts are now underway to automate the process. Some of these efforts use deep learning algorithms to first identify buildings and then to classify them into damage levels. One of these efforts initiated in 2019, through the Defense Innovation Unit (DIU) and with Humanitarian Assistance and Disaster Recovery (HADR) organizations, created a multi-hazard training dataset using high-resolution satellite imagery from pre- and post-event imagery (xBD). Across 19 natural disaster events including tornados, wildfire, earthquake, hurricanes, volcanos, flood, and tsunami, buildings were identified and classified into four classes: no damage, minor damage, major damage, and destroyed. Participants in the challenge were expected to use deep learning algorithms to perform the classification. They were also provided with a base classification algorithm, in which participants were encouraged to improve. The base algorithm contained RESNET50 trained on ImageNet database and three additional convolution and max pooling layers. This project analyzes the quality of the training dataset, discusses the pros and cons of combining training dataset across multiple natural disaster events, and provides recommendations on using the provided training dataset to optimize classification accuracy. Specifically, we will provide recommendations on creating class balance in the training dataset, in which damage labels are the most identifiable. We will also provide an assessment on which natural disasters lead to damage that is most identifiable using satellite imagery in which natural disasters lead to less accurate damage assessments. We will also examine pooling training data across natural disasters to achieve more accurate classifications. © 2022, The Society for Experimental Mechanics, Inc. Damage assessment; Deep learning; Imbalance; Overfitting; xBD Aerial photography; Antennas; Classification (of information); Deep learning; Disasters; Hazards; Learning algorithms; Satellite imagery; Structural dynamics; Building damage; Damage assessments; Damage level; Deep learning; Imbalance; Multi-hazards; Natural disasters; Overfitting; Training dataset; XBD; Damage detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;3;Response
587;Decisions of Landlords about Their Renters and Rental Units after Disasters;Postdisaster recovery in the rental housing submarket has attracted scant attention in both policy and scholarly circles. In particular, little is known about landlords' decisions on whether to keep their previous renters, change their rental fees, and invest in rebuilding or repair of their rental units in lower-income cities. The literature presents a monolithic understanding of landlords' decisions, grounding them solely within economic choices concerned with their investment return. We explored the factors that influence these decisions, focusing on the earthquake-affected city of Sarpol Zahab, Iran. Data on 142 reconstructed/repaired rental units were collected. Employing the logistic regression and multivariate regression models, we focused on the decisions of landlords about (1) keeping their previous renters or not and (2) increasing the rent. This paper underscores the importance of social as well as economic factors in these decisions. In terms of policy implications, the paper argues that assisting landlords with reconstruction/repair of their rental units without conditions supporting predisaster renters' rights increases the commercialization and formalization of rental housing and commodification of the city.  © 2021 American Society of Civil Engineers.;"Landlords; Postdisaster housing recovery; Rental housing recovery; Unhoming";"Apartment houses; Public policy; Economic factors; Investment returns; Multivariate regression models; Policy implications; Post disasters; Submarket; decision analysis; disaster management; natural disaster; rental sector; Logistic regression";"Decisions of Landlords about Their Renters and Rental Units after Disasters Postdisaster recovery in the rental housing submarket has attracted scant attention in both policy and scholarly circles. In particular, little is known about landlords' decisions on whether to keep their previous renters, change their rental fees, and invest in rebuilding or repair of their rental units in lower-income cities. The literature presents a monolithic understanding of landlords' decisions, grounding them solely within economic choices concerned with their investment return. We explored the factors that influence these decisions, focusing on the earthquake-affected city of Sarpol Zahab, Iran. Data on 142 reconstructed/repaired rental units were collected. Employing the logistic regression and multivariate regression models, we focused on the decisions of landlords about (1) keeping their previous renters or not and (2) increasing the rent. This paper underscores the importance of social as well as economic factors in these decisions. In terms of policy implications, the paper argues that assisting landlords with reconstruction/repair of their rental units without conditions supporting predisaster renters' rights increases the commercialization and formalization of rental housing and commodification of the city.  © 2021 American Society of Civil Engineers. Landlords; Postdisaster housing recovery; Rental housing recovery; Unhoming Apartment houses; Public policy; Economic factors; Investment returns; Multivariate regression models; Policy implications; Post disasters; Submarket; decision analysis; disaster management; natural disaster; rental sector; Logistic regression";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;4;Recovery
588;Machine learning prediction models for ground motion parameters and seismic damage assessment of buildings at a regional scale;This study examines the feasibility of using a machine learning approach for rapid damage assessment of reinforced concrete (RC) buildings after the earthquake. Since the real-world damaged datasets are lacking, have limited access, or are imbalanced, a simulation dataset is prepared by conducting a nonlinear time history analysis. Different machine learning (ML) models are trained considering the structural parameters and ground motion characteristics to predict the RC building damage into five categories: null, slight, moderate, heavy, and collapse. The random forest classifier (RFC) has achieved a higher prediction accuracy on testing and real-world damaged datasets. The structural parameters can be extracted using different means such as Google Earth, Open Street Map, unmanned aerial vehicles, etc. However, recording the ground motion at a closer distance requires the installation of a dense array of sensors which requires a higher cost. For places with no earthquake recording station/device, it is difficult to have ground motion characteristics. For that different ML-based regressor models are developed utilizing past-earthquake information to predict ground motion parameters such as peak ground acceleration and peak ground velocity. The random forest regressor (RFR) achieved better results than other regression models on testing and validation datasets. Furthermore, compared with the results of similar research works, a better result is obtained using RFC and RFR on validation datasets. In the end, these models are utilized to predict the damage categories of RC buildings at Saitama University and Okubo Danchi, Saitama, Japan after an earthquake. This damage information is crucial for government agencies or decision-makers to respond systematically in post-disaster situations. © 2024 The Author(s);"Ground motion parameter; Machine learning algorithms; Nonlinear time history analysis; RC buildings; Seismic damage prediction";"Acceleration; Antennas; Classification (of information); Damage detection; Decision making; Earthquake effects; Forecasting; Forestry; Learning algorithms; Machine learning; Motion estimation; Regression analysis; Ground motion characteristics; Ground motion parameters; Machine learning algorithms; Machine-learning; Nonlinear time history analysis; Random forest classifier; Real-world; Reinforced concrete buildings; Seismic damage prediction; Structural parameter; Reinforced concrete";"Machine learning prediction models for ground motion parameters and seismic damage assessment of buildings at a regional scale This study examines the feasibility of using a machine learning approach for rapid damage assessment of reinforced concrete (RC) buildings after the earthquake. Since the real-world damaged datasets are lacking, have limited access, or are imbalanced, a simulation dataset is prepared by conducting a nonlinear time history analysis. Different machine learning (ML) models are trained considering the structural parameters and ground motion characteristics to predict the RC building damage into five categories: null, slight, moderate, heavy, and collapse. The random forest classifier (RFC) has achieved a higher prediction accuracy on testing and real-world damaged datasets. The structural parameters can be extracted using different means such as Google Earth, Open Street Map, unmanned aerial vehicles, etc. However, recording the ground motion at a closer distance requires the installation of a dense array of sensors which requires a higher cost. For places with no earthquake recording station/device, it is difficult to have ground motion characteristics. For that different ML-based regressor models are developed utilizing past-earthquake information to predict ground motion parameters such as peak ground acceleration and peak ground velocity. The random forest regressor (RFR) achieved better results than other regression models on testing and validation datasets. Furthermore, compared with the results of similar research works, a better result is obtained using RFC and RFR on validation datasets. In the end, these models are utilized to predict the damage categories of RC buildings at Saitama University and Okubo Danchi, Saitama, Japan after an earthquake. This damage information is crucial for government agencies or decision-makers to respond systematically in post-disaster situations. © 2024 The Author(s) Ground motion parameter; Machine learning algorithms; Nonlinear time history analysis; RC buildings; Seismic damage prediction Acceleration; Antennas; Classification (of information); Damage detection; Decision making; Earthquake effects; Forecasting; Forestry; Learning algorithms; Machine learning; Motion estimation; Regression analysis; Ground motion characteristics; Ground motion parameters; Machine learning algorithms; Machine-learning; Nonlinear time history analysis; Random forest classifier; Real-world; Reinforced concrete buildings; Seismic damage prediction; Structural parameter; Reinforced concrete";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
589;Integrated multispectral image and LiDAR for landslides vegetation restoration monitoring;"Landslides are one of the major hazards in Taiwan, especially after typhoon and earthquake, to maintain the slope stability is the most important task after disaster; vegetation cover is the key factor of slope stability. Vegetative cover can contribute to improving the stability of steep slopes by reducing erosion and the its growing are usually controlled by climate, soil, water, topography and geology condition, the growing conditions indicated that vegetation growing condition may also reflect the landslides spatial characteristics. The landslides and vegetation restoration evaluation was implement by high resolution air-photo, SPOT-5 satellite image and air-borne LiDAR in this study. Landslides were identified by using air-photo in 2004 which before Long -Wang typhoon, the satellite image and LiDAR data were acquired for calculated vegetation restoration rate, two different source data were both taken in the same month in 2005.The LiDAR data are used for derived surface roughness data to obtain the vegetation growing. The vegetation restoration result shows different spatial distribution in landslide area, the vegetation cover near river bank are less than where in slope area, the growing rate also shows the same appearance. The steep slope appears with low vegetation restoration rate, it may have relations with high frequency of landslides and lower disturbed by human in this area.";"Image fusion; Landslides classification; Spatial analysis; Surface roughness";"Hurricanes; Image fusion; Landslides; Optical radar; Remote sensing; Restoration; Surface roughness; Vegetation; Growing conditions; High frequency; High resolution; Key factors; LIDAR data; Major hazards; Multispectral images; River bank; Satellite images; Source data; Spatial analysis; Spatial characteristics; SPOT 5 satellite; Steep slope; Vegetation cover; Vegetation restoration; Vegetative cover; Slope protection";"Integrated multispectral image and LiDAR for landslides vegetation restoration monitoring Landslides are one of the major hazards in Taiwan, especially after typhoon and earthquake, to maintain the slope stability is the most important task after disaster; vegetation cover is the key factor of slope stability. Vegetative cover can contribute to improving the stability of steep slopes by reducing erosion and the its growing are usually controlled by climate, soil, water, topography and geology condition, the growing conditions indicated that vegetation growing condition may also reflect the landslides spatial characteristics. The landslides and vegetation restoration evaluation was implement by high resolution air-photo, SPOT-5 satellite image and air-borne LiDAR in this study. Landslides were identified by using air-photo in 2004 which before Long -Wang typhoon, the satellite image and LiDAR data were acquired for calculated vegetation restoration rate, two different source data were both taken in the same month in 2005.The LiDAR data are used for derived surface roughness data to obtain the vegetation growing. The vegetation restoration result shows different spatial distribution in landslide area, the vegetation cover near river bank are less than where in slope area, the growing rate also shows the same appearance. The steep slope appears with low vegetation restoration rate, it may have relations with high frequency of landslides and lower disturbed by human in this area. Image fusion; Landslides classification; Spatial analysis; Surface roughness Hurricanes; Image fusion; Landslides; Optical radar; Remote sensing; Restoration; Surface roughness; Vegetation; Growing conditions; High frequency; High resolution; Key factors; LIDAR data; Major hazards; Multispectral images; River bank; Satellite images; Source data; Spatial analysis; Spatial characteristics; SPOT 5 satellite; Steep slope; Vegetation cover; Vegetation restoration; Vegetative cover; Slope protection";-1;Não Classificado;NULL;1.1;Geological;4;Recovery
590;An Intelligent Packet Forwarding Approach for Disaster Recovery Networks;Disasters, such as earthquakes, typhoons, and tsunamis, usually cause extreme damages to the communication infrastructures, which results in a heavy recovery workload and seriously affects people's life. The disaster recovery networks play a critical role to reduce the loss caused by the disasters. However, the suddenly varying traffic demand and limited resources after disasters may lead to the repetitive reconfigurations for running the existing packet forwarding strategies, such as the shortest path algorithms. To handle this problem, it is necessary to adopt the deep learning technique to develop a disaster-resilient solution. In this paper, we utilize the deep reinforcement learning technique to propose a self-adaptive routing method for the Movable and Deployable Resource Unit (MDRU) based backbone network. Compared with existing deep learning based routing strategy, our proposal can adapt to the sudden network errors. Moreover, we also analyze the deployment manner and consider a centralized control structure to significantly balance the traffic. © 2019 IEEE.;NULL;"Deep learning; Disasters; Learning algorithms; Network routing; Packet networks; Recovery; Reinforcement learning; Centralized control; Communication infrastructure; Disaster recovery networks; Learning techniques; Packet forwarding; Reinforcement learning techniques; Routing strategies; Shortest path algorithms; Computer system recovery";"An Intelligent Packet Forwarding Approach for Disaster Recovery Networks Disasters, such as earthquakes, typhoons, and tsunamis, usually cause extreme damages to the communication infrastructures, which results in a heavy recovery workload and seriously affects people's life. The disaster recovery networks play a critical role to reduce the loss caused by the disasters. However, the suddenly varying traffic demand and limited resources after disasters may lead to the repetitive reconfigurations for running the existing packet forwarding strategies, such as the shortest path algorithms. To handle this problem, it is necessary to adopt the deep learning technique to develop a disaster-resilient solution. In this paper, we utilize the deep reinforcement learning technique to propose a self-adaptive routing method for the Movable and Deployable Resource Unit (MDRU) based backbone network. Compared with existing deep learning based routing strategy, our proposal can adapt to the sudden network errors. Moreover, we also analyze the deployment manner and consider a centralized control structure to significantly balance the traffic. © 2019 IEEE. NULL Deep learning; Disasters; Learning algorithms; Network routing; Packet networks; Recovery; Reinforcement learning; Centralized control; Communication infrastructure; Disaster recovery networks; Learning techniques; Packet forwarding; Reinforcement learning techniques; Routing strategies; Shortest path algorithms; Computer system recovery";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.1;Geological;3;Response
591;Maize yield forecasts for Sub-Saharan Africa using Earth Observation data and machine learning;"Food insecurity continues to grow in Sub-Saharan Africa (SSA). In 2019, chronically malnourished people numbered nearly 240 million, or 20% of the population in SSA. Globally, numerous efforts have been made to anticipate potential droughts, crop conditions, and food shortages in order to improve early warning and risk management for food insecurity. To support this goal, we develop an Earth Observation (EO) and machine-learning-based operational, subnational maize yield forecast system and evaluate its out-of-sample forecast skills during the growing seasons for Kenya, Somalia, Malawi, and Burkina Faso. In general, forecast skills improve substantially during the vegetative growth period (VP) and gradually during the reproductive development period (RP). Thus, mid-season assessment can provide effective early warning months before harvest. Skillful forecasts (Nash Sutcliffe Efficiency (NSE) > 0.6 and Mean Absolute Percentage Error (MAPE) < 20%) appear approximately two dekads after the VP; for example, skillful forecasts appear in May in Kenya and Somalia, January in Malawi, and July in Burkina Faso. During model development, effective EO features are also identified, such as precipitation and available water during VP, and dry days and extreme temperatures in early VP. Compared to monthly standard EO features, sub-monthly (dekadal), non-standard, and serial EO features significantly improve forecast skills by ＋ 0.3 NSE and -10% of MAPE, demonstrating the ability to precisely and effectively capture favorable or detrimental crop development conditions. Finally, skillful forecasts and practical utility are demonstrated in the recent normal and dry years in each region. Overall, the developed yield forecasting system can provide skillful predictions during the growing season, supporting regional and international agricultural decision-making processes, including informing food-security planning and management, thereby helping to mitigate food shortages caused by unfavorable climate conditions. © 2022 The Author(s)";"Earth observation; Food security; Machine learning; Maize yield; Operational forecast; Sub-Saharan Africa";NULL;"Maize yield forecasts for Sub-Saharan Africa using Earth Observation data and machine learning Food insecurity continues to grow in Sub-Saharan Africa (SSA). In 2019, chronically malnourished people numbered nearly 240 million, or 20% of the population in SSA. Globally, numerous efforts have been made to anticipate potential droughts, crop conditions, and food shortages in order to improve early warning and risk management for food insecurity. To support this goal, we develop an Earth Observation (EO) and machine-learning-based operational, subnational maize yield forecast system and evaluate its out-of-sample forecast skills during the growing seasons for Kenya, Somalia, Malawi, and Burkina Faso. In general, forecast skills improve substantially during the vegetative growth period (VP) and gradually during the reproductive development period (RP). Thus, mid-season assessment can provide effective early warning months before harvest. Skillful forecasts (Nash Sutcliffe Efficiency (NSE) > 0.6 and Mean Absolute Percentage Error (MAPE) < 20%) appear approximately two dekads after the VP; for example, skillful forecasts appear in May in Kenya and Somalia, January in Malawi, and July in Burkina Faso. During model development, effective EO features are also identified, such as precipitation and available water during VP, and dry days and extreme temperatures in early VP. Compared to monthly standard EO features, sub-monthly (dekadal), non-standard, and serial EO features significantly improve forecast skills by ＋ 0.3 NSE and -10% of MAPE, demonstrating the ability to precisely and effectively capture favorable or detrimental crop development conditions. Finally, skillful forecasts and practical utility are demonstrated in the recent normal and dry years in each region. Overall, the developed yield forecasting system can provide skillful predictions during the growing season, supporting regional and international agricultural decision-making processes, including informing food-security planning and management, thereby helping to mitigate food shortages caused by unfavorable climate conditions. © 2022 The Author(s) Earth observation; Food security; Machine learning; Maize yield; Operational forecast; Sub-Saharan Africa NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
592;Forecasting indoor temperatures during heatwaves using time series models;Early prediction of impending high temperatures in buildings could play a vital role in reducing heat-related morbidity and mortality. A recursive, AutoRegressive time series model using eXogenous inputs (ARX) and a rolling forecasting origin has been developed to provide reliable short-term forecasts of the internal temperatures in dwellings during hot summer conditions, especially heatwaves. The model was tested using monitored data from three case study dwellings recorded during the 2015 heatwave. The predictor variables were selected by minimising the Akaike Information Criterion (AIC), in order to automatically identify a near-optimal model. The model proved capable of performing multi-step-ahead predictions during extreme heat events with an acceptable accuracy for periods up to 72 h, with hourly results achieving a Mean Absolute Error (MAE) below 0.7 °C for every forecast. Comparison between ARX and AutoRegressive Moving Average models with eXogenous inputs (ARMAX) models showed that the ARX models provided consistently more reliable multi-step-ahead predictions. Prediction intervals, at the 95% probability level, were adopted to define a credible interval for the forecast temperatures at different prediction horizons. The results point to the potential for using time series forecasting as part of an overheating early-warning system in buildings housing vulnerable occupants or contents. © 2018 The Authors;"ARMAX model; ARX model; Black-box model; Machine learning; Overheating; Time series forecasting";"Forecasting; Hot rolling; Housing; Time series; ARMAX model; ARX model; Black box modelling; Exogenous input; Heatwaves; In-buildings; Machine-learning; Overheating; Time series forecasting; Times series models; air temperature; Akaike information criterion; forecasting method; heat wave; indoor air; machine learning; numerical model; time series analysis; Machine learning";"Forecasting indoor temperatures during heatwaves using time series models Early prediction of impending high temperatures in buildings could play a vital role in reducing heat-related morbidity and mortality. A recursive, AutoRegressive time series model using eXogenous inputs (ARX) and a rolling forecasting origin has been developed to provide reliable short-term forecasts of the internal temperatures in dwellings during hot summer conditions, especially heatwaves. The model was tested using monitored data from three case study dwellings recorded during the 2015 heatwave. The predictor variables were selected by minimising the Akaike Information Criterion (AIC), in order to automatically identify a near-optimal model. The model proved capable of performing multi-step-ahead predictions during extreme heat events with an acceptable accuracy for periods up to 72 h, with hourly results achieving a Mean Absolute Error (MAE) below 0.7 °C for every forecast. Comparison between ARX and AutoRegressive Moving Average models with eXogenous inputs (ARMAX) models showed that the ARX models provided consistently more reliable multi-step-ahead predictions. Prediction intervals, at the 95% probability level, were adopted to define a credible interval for the forecast temperatures at different prediction horizons. The results point to the potential for using time series forecasting as part of an overheating early-warning system in buildings housing vulnerable occupants or contents. © 2018 The Authors ARMAX model; ARX model; Black-box model; Machine learning; Overheating; Time series forecasting Forecasting; Hot rolling; Housing; Time series; ARMAX model; ARX model; Black box modelling; Exogenous input; Heatwaves; In-buildings; Machine-learning; Overheating; Time series forecasting; Times series models; air temperature; Akaike information criterion; forecasting method; heat wave; indoor air; machine learning; numerical model; time series analysis; Machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
593;Prep-Seqnet:An Integrated ANN and Sequential Deep Learning Framework for Rainfall Prediction;"Rainfall prediction is critical in regions prone to flooding, where changes in weather patterns can significantly impact water levels, transportation, environmental health, evacuation strategies, and urban planning initiatives. Accurate forecasts enable both governmental and private entities to make informed decisions, particularly for disaster response and early flood warnings. Artificial Intelligence (AI) has emerged as a leader in generating detailed rainfall predictions using a variety of methods, including Support Vector Machines, Random Forest, Decision Trees, XGBoost, and others. This study employs AI techniques to forecast the next day's rainfall in areas susceptible to flooding, analyzing daily weather data spanning a decade from multiple high-risk locations. The proposed integrated ANN and Sequential Deep learning framework named ""Prep-SeqNet""incorporates a range of meteorological data, including temperature extremes, precipitation, evaporation, sunlight hours, wind speed, humidity, atmospheric pressure, cloud coverage, and bi-daily temperature readings (at 9 AM and 3 PM). The utilized machine learning architecture is a sequential layered model with four levels, enhanced with dropout for improved regularization. Initially, an ANN base model achieved a 77.5% accuracy rate. However, with the integration of a ""Reduce Learning Rate on Plateau""function and a tailored accuracy metric, the refined model exhibited a consistent 18% increase in precision, achieving an accuracy of 95.9%, markedly outperforming the base model. © 2024 IEEE.";"and Meteorological indicators; Artificial intelligence; Rainfall prediction; Sequential model";NULL;"Prep-Seqnet:An Integrated ANN and Sequential Deep Learning Framework for Rainfall Prediction Rainfall prediction is critical in regions prone to flooding, where changes in weather patterns can significantly impact water levels, transportation, environmental health, evacuation strategies, and urban planning initiatives. Accurate forecasts enable both governmental and private entities to make informed decisions, particularly for disaster response and early flood warnings. Artificial Intelligence (AI) has emerged as a leader in generating detailed rainfall predictions using a variety of methods, including Support Vector Machines, Random Forest, Decision Trees, XGBoost, and others. This study employs AI techniques to forecast the next day's rainfall in areas susceptible to flooding, analyzing daily weather data spanning a decade from multiple high-risk locations. The proposed integrated ANN and Sequential Deep learning framework named ""Prep-SeqNet""incorporates a range of meteorological data, including temperature extremes, precipitation, evaporation, sunlight hours, wind speed, humidity, atmospheric pressure, cloud coverage, and bi-daily temperature readings (at 9 AM and 3 PM). The utilized machine learning architecture is a sequential layered model with four levels, enhanced with dropout for improved regularization. Initially, an ANN base model achieved a 77.5% accuracy rate. However, with the integration of a ""Reduce Learning Rate on Plateau""function and a tailored accuracy metric, the refined model exhibited a consistent 18% increase in precision, achieving an accuracy of 95.9%, markedly outperforming the base model. © 2024 IEEE. and Meteorological indicators; Artificial intelligence; Rainfall prediction; Sequential model NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
594;Conceptual framework of an intelligent decision support system for smart city disaster management;In order to protect human lives and infrastructure, as well as to minimize the risk of damage, it is important to predict and respond to natural disasters in advance. However, currently, the standardized disaster response system in South Korea still needs further advancement, and the response phase systems need to be improved to ensure that they are properly equipped to cope with natural disasters. Existing studies on intelligent disaster management systems (IDSSs) in South Korea have focused only on storms, floods, and earthquakes, and they have not used past data. This research proposes a new conceptual framework of an IDSS for disaster management, with particular attention paid to wildfires and cold/heat waves. The IDSS uses big data collected from open application programming interface (API) and artificial intelligence (AI) algorithms to help decision-makers make faster and more accurate decisions. In addition, a simple example of the use of a convolutional neural network (CNN) to detect fire in surveillance video has been developed, which can be used for automatic fire detection and provide an appropriate response. The system will also consider connecting to open source intelligence (OSINT) to identify vulnerabilities, mitigate risks, and develop more robust security policies than those currently in place to prevent cyber-attacks. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.;"Artificial intelligence; Big data; Decision support system; Disaster management; Internet of things";NULL;"Conceptual framework of an intelligent decision support system for smart city disaster management In order to protect human lives and infrastructure, as well as to minimize the risk of damage, it is important to predict and respond to natural disasters in advance. However, currently, the standardized disaster response system in South Korea still needs further advancement, and the response phase systems need to be improved to ensure that they are properly equipped to cope with natural disasters. Existing studies on intelligent disaster management systems (IDSSs) in South Korea have focused only on storms, floods, and earthquakes, and they have not used past data. This research proposes a new conceptual framework of an IDSS for disaster management, with particular attention paid to wildfires and cold/heat waves. The IDSS uses big data collected from open application programming interface (API) and artificial intelligence (AI) algorithms to help decision-makers make faster and more accurate decisions. In addition, a simple example of the use of a convolutional neural network (CNN) to detect fire in surveillance video has been developed, which can be used for automatic fire detection and provide an appropriate response. The system will also consider connecting to open source intelligence (OSINT) to identify vulnerabilities, mitigate risks, and develop more robust security policies than those currently in place to prevent cyber-attacks. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. Artificial intelligence; Big data; Decision support system; Disaster management; Internet of things NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
595;Targeting the spatial context of risk factors associated with heat-related mortality via multiscale geographically weighted regression;Extreme heat events appear to be a major cause to weather-related human morality in much of the world. The association between heat stress and public health is recognized as a complex interplay of multifaceted factors. Effective policy-making and action plans require a better knowledge of where and which of those factors should be targeted for intervention. However, little research has separated the underlying scales of effect of key components or taken into account geographical context in an analysis of those factors, which could lead to misguided policy actions in heat health risk reduction. In a case study of Hong Kong, we use the most recent multi-scale geographically weighted regression (MGWR) methodology to narrow this gap. We find that via MGWR, a combination of global and local processes could produce a better fit for the risk of heat-related mortality. Explanatory variables can be divided into three groups: global variables (such as age, educational attainment, and socioeconomic status), intermediate variables that vary on a relatively small scale (such as work environment, place of birth, and language), and local variables (i.e. thermal environment, low income). These findings suggest the need for targeting spatial context to multi-dimensional factors associated with heat-related mortality and highlight the hierarchical policy-making processes and site-specific action plans.  © 2022 IEEE.;"extreme heat; geographically weighted regression (GWR); heat health planning; heat-related mortality; multiscale";"Decision making; Regression analysis; Risk assessment; Extreme heat; Geographically weighted regression; Health planning; Heat health planning; Heat-related mortalities; Multiscale; Policy actions; Policy making; Spatial context; Health risks";"Targeting the spatial context of risk factors associated with heat-related mortality via multiscale geographically weighted regression Extreme heat events appear to be a major cause to weather-related human morality in much of the world. The association between heat stress and public health is recognized as a complex interplay of multifaceted factors. Effective policy-making and action plans require a better knowledge of where and which of those factors should be targeted for intervention. However, little research has separated the underlying scales of effect of key components or taken into account geographical context in an analysis of those factors, which could lead to misguided policy actions in heat health risk reduction. In a case study of Hong Kong, we use the most recent multi-scale geographically weighted regression (MGWR) methodology to narrow this gap. We find that via MGWR, a combination of global and local processes could produce a better fit for the risk of heat-related mortality. Explanatory variables can be divided into three groups: global variables (such as age, educational attainment, and socioeconomic status), intermediate variables that vary on a relatively small scale (such as work environment, place of birth, and language), and local variables (i.e. thermal environment, low income). These findings suggest the need for targeting spatial context to multi-dimensional factors associated with heat-related mortality and highlight the hierarchical policy-making processes and site-specific action plans.  © 2022 IEEE. extreme heat; geographically weighted regression (GWR); heat health planning; heat-related mortality; multiscale Decision making; Regression analysis; Risk assessment; Extreme heat; Geographically weighted regression; Health planning; Heat health planning; Heat-related mortalities; Multiscale; Policy actions; Policy making; Spatial context; Health risks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
596;Temperature Forecasting of Grain in Storage: An Improved Approach Based on Broad Learning Network;Temperature forecasting of grain in storage is crucial for timely granary temperature control, mitigating adverse effects of extreme temperatures on grain quality. Traditional machine learning methods struggle with stability and high error rates in grain storage temperature forecasting, while deep learning models are more accurate but time-consuming and have heavy parameters. To address these problems, an improved model with light weight and good accuracy is proposed in this paper, which broad learning network is combined with one-dimensional convolution module and multi-head self-attention mechanism (BLN-1DCNN-MHSA). Firstly, we employ a one-dimensional convolution module at the feature nodes of the model to extract local temporal correlations, compensating for temporal sequence learning limitations of the BLN. Secondly, a multi-head self-attention mechanism at the enhancement nodes to captures important features dependencies and global temporal correlations. Lastly, our model achieves better prediction through enhanced representation ability of model nodes. The results with real grain storage temperature data demonstrate that the RMSE, MAPE, and MAE of the proposed model are 0.341, 0.54%, 0.28, respectively, which represent more than 2 times improvement in accuracy compared to the BLN, and it also reduces training time by more than 90% compared with LSTM and Transformer models. Additionally, the generalization and robustness of the improved approach are demonstrated through promising results in a classification experiment on the MNIST dataset. In general, the model provides a certain feasibility for early warning of grain storage risks by predicting its temperature trends. © 2013 IEEE.;"broad learning network; convolutional neural network; grain storage security; Grain storage temperature forecasting; multi-head self-attention";"Classification (of information); Convolution; Digital storage; Long short-term memory; Network security; Accuracy; Broad learning network; Convolutional neural network; Features extraction; Grain storage; Grain storage security; Grain storage temperature forecasting; Learning network; Multi-head self-attention; Predictive models; Storage security; Storage temperatures; Temperature forecasting; Forecasting";"Temperature Forecasting of Grain in Storage: An Improved Approach Based on Broad Learning Network Temperature forecasting of grain in storage is crucial for timely granary temperature control, mitigating adverse effects of extreme temperatures on grain quality. Traditional machine learning methods struggle with stability and high error rates in grain storage temperature forecasting, while deep learning models are more accurate but time-consuming and have heavy parameters. To address these problems, an improved model with light weight and good accuracy is proposed in this paper, which broad learning network is combined with one-dimensional convolution module and multi-head self-attention mechanism (BLN-1DCNN-MHSA). Firstly, we employ a one-dimensional convolution module at the feature nodes of the model to extract local temporal correlations, compensating for temporal sequence learning limitations of the BLN. Secondly, a multi-head self-attention mechanism at the enhancement nodes to captures important features dependencies and global temporal correlations. Lastly, our model achieves better prediction through enhanced representation ability of model nodes. The results with real grain storage temperature data demonstrate that the RMSE, MAPE, and MAE of the proposed model are 0.341, 0.54%, 0.28, respectively, which represent more than 2 times improvement in accuracy compared to the BLN, and it also reduces training time by more than 90% compared with LSTM and Transformer models. Additionally, the generalization and robustness of the improved approach are demonstrated through promising results in a classification experiment on the MNIST dataset. In general, the model provides a certain feasibility for early warning of grain storage risks by predicting its temperature trends. © 2013 IEEE. broad learning network; convolutional neural network; grain storage security; Grain storage temperature forecasting; multi-head self-attention Classification (of information); Convolution; Digital storage; Long short-term memory; Network security; Accuracy; Broad learning network; Convolutional neural network; Features extraction; Grain storage; Grain storage security; Grain storage temperature forecasting; Learning network; Multi-head self-attention; Predictive models; Storage security; Storage temperatures; Temperature forecasting; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
597;A Transformer Warning Method Based on Pattern Recognition and Statistical Analysis;"Temperature is one of the most important characteristics reflecting internal faults in transformers. By monitoring the unconventional operating behavior of temperature, potential fault risks that may exist in transformers can be timely grasped. This study takes the transformer's multiple parameters as the analysis object, and applies a combination of pattern recognition and statistical analysis to achieve early warning of potential transformer hazards. In the proposed method, firstly, the OPTICS+K-Means dual clustering algorithm is used to construct a transformer temperature early warning state model from multi-parameter historical data; secondly, the state model is used to match a set of most similar data for the observed data, and the residuals between the two are used to measure the degree of equipment abnormality; then, the Mann-Kendall test method is used to test the trend characteristics of the residuals, so as to determine whether the transformer has abnormal operating behavior. The experimental results show that the proposed method has the ability to detect transformer temperature anomalies in advance.  © 2024 IEEE.";"Mann-Kendall Test; OPTICS; Transformer";"Electric fault currents; Electric transformer testing; Higher order statistics; K-means clustering; Dual clustering; Early warning; Internal faults; K-means; Mann-Kendall test; Multiple parameters; Operating behavior; Potential faults; State models; Transformer; Pattern recognition";"A Transformer Warning Method Based on Pattern Recognition and Statistical Analysis Temperature is one of the most important characteristics reflecting internal faults in transformers. By monitoring the unconventional operating behavior of temperature, potential fault risks that may exist in transformers can be timely grasped. This study takes the transformer's multiple parameters as the analysis object, and applies a combination of pattern recognition and statistical analysis to achieve early warning of potential transformer hazards. In the proposed method, firstly, the OPTICS+K-Means dual clustering algorithm is used to construct a transformer temperature early warning state model from multi-parameter historical data; secondly, the state model is used to match a set of most similar data for the observed data, and the residuals between the two are used to measure the degree of equipment abnormality; then, the Mann-Kendall test method is used to test the trend characteristics of the residuals, so as to determine whether the transformer has abnormal operating behavior. The experimental results show that the proposed method has the ability to detect transformer temperature anomalies in advance.  © 2024 IEEE. Mann-Kendall Test; OPTICS; Transformer Electric fault currents; Electric transformer testing; Higher order statistics; K-means clustering; Dual clustering; Early warning; Internal faults; K-means; Mann-Kendall test; Multiple parameters; Operating behavior; Potential faults; State models; Transformer; Pattern recognition";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;-1;NULL;2;Preparation
598;WHEAT YIELD PREDICTION USING SOIL DROUGHT INDEX-BASED MACHINE/DEEP LEARNING;In recent years, the food crisis has become more pronounced due to climate change and extreme weather events, such as extreme temperature fluctuations and severe droughts. Wheat is one of the crucial economy crops in Kinmen, and its growth and harvest are highly contingent on the soil moistures in the farmland. Both excessive and insufficient rainfall can severely impact yields, highlighting the vital importance of maintaining a balanced soil moisture environment to ensure the prosperous cultivation of wheat. In this research, three soil drought indices, namely the Perpendicular Drought Index (PDI), Modified PDI (MPDI), and Vegetation-Adjusted PDI (VAPDI), and two vegetation indices, including the Normalized Difference Vegetation Index (NDVI) and Perpendicular Vegetation Index (PVI), were calculated from the monitoring data by UAV multispectral imagery to predict wheat yields using three learning model, including Support Vector Machine (SVM) regressor, Gradient Boosting (GB) regressor, and Convolutional Neural Network (CNN) regressor. The experimental results indicate that the accuracy of the three learning models for wheat yield estimation is comparable, with the CNN learning model offering a slightly higher accuracy compared to the other two. The CNN learning model can attain a validation accuracy exceeding 90% when predicting wheat yields during years characterized by relatively consistent wheat yields. Furthermore, the model has demonstrated its capacity to provide early warnings of reductions in wheat yield. © 2023 ACRS. All Rights Reserved.;"Machine/Deep learning regressor; Soil drought; UAV multispectral imagery; Wheat yield prediction";"Climate change; Forecasting; Learning systems; Remote sensing; Risk assessment; Soil moisture; Space applications; Space optics; Support vector machines; Unmanned aerial vehicles (UAV); Vegetation mapping; Convolutional neural network; Learning models; Machine/deep learning regressor; Multispectral imagery; Neural network learning; Soil drought; UAV multispectral imagery; Wheat yield; Wheat yield prediction; Yield prediction; Drought";"WHEAT YIELD PREDICTION USING SOIL DROUGHT INDEX-BASED MACHINE/DEEP LEARNING In recent years, the food crisis has become more pronounced due to climate change and extreme weather events, such as extreme temperature fluctuations and severe droughts. Wheat is one of the crucial economy crops in Kinmen, and its growth and harvest are highly contingent on the soil moistures in the farmland. Both excessive and insufficient rainfall can severely impact yields, highlighting the vital importance of maintaining a balanced soil moisture environment to ensure the prosperous cultivation of wheat. In this research, three soil drought indices, namely the Perpendicular Drought Index (PDI), Modified PDI (MPDI), and Vegetation-Adjusted PDI (VAPDI), and two vegetation indices, including the Normalized Difference Vegetation Index (NDVI) and Perpendicular Vegetation Index (PVI), were calculated from the monitoring data by UAV multispectral imagery to predict wheat yields using three learning model, including Support Vector Machine (SVM) regressor, Gradient Boosting (GB) regressor, and Convolutional Neural Network (CNN) regressor. The experimental results indicate that the accuracy of the three learning models for wheat yield estimation is comparable, with the CNN learning model offering a slightly higher accuracy compared to the other two. The CNN learning model can attain a validation accuracy exceeding 90% when predicting wheat yields during years characterized by relatively consistent wheat yields. Furthermore, the model has demonstrated its capacity to provide early warnings of reductions in wheat yield. © 2023 ACRS. All Rights Reserved. Machine/Deep learning regressor; Soil drought; UAV multispectral imagery; Wheat yield prediction Climate change; Forecasting; Learning systems; Remote sensing; Risk assessment; Soil moisture; Space applications; Space optics; Support vector machines; Unmanned aerial vehicles (UAV); Vegetation mapping; Convolutional neural network; Learning models; Machine/deep learning regressor; Multispectral imagery; Neural network learning; Soil drought; UAV multispectral imagery; Wheat yield; Wheat yield prediction; Yield prediction; Drought";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.4;Climatological;2;Preparation
599;Predicting indoor personalized heat stress using wearable sensors and data-driven models;Rising global temperatures pose heightened heat stress challenges, especially impacting vulnerable populations such as older adults in low-income urban areas of color, who disproportionately face energy inefficiencies and environmental burdens. Traditional research often overlooks individual metabolic rates and behaviors in heat stress assessments, relying instead on isolated environmental measures in laboratory settings. This paper presents a novel, occupant-centric approach that integrates wearable sensor data, building information, and air-conditioning usage to predict extreme heat events and assess their impact on occupants through calibrated thermal building simulations for seven older adults in Houston, Texas. We developed a personalized heat stress model based on existing heat index equations and dynamic predicted mean votes, enhanced by measured metabolic rates. A data-driven machine learning algorithm was then used to predict extreme heat events with this model serving as the ground truth. Our findings reveal that elevated activity levels significantly contribute to discomfort during extreme heat events, often undetected by the heat index equation—an environmental metric—alone. Furthermore, we observe that combining metabolic rates with measured indoor temperatures consistently predicts extreme heat events with over 95 % accuracy, demonstrating almost 13 % improvement compared to using indoor temperature in isolation. This study provides a valuable proof of concept that underscores the merits of integrating multiple data sources to raise the accuracy and predictability of indoor extreme heat events. Our research not only offers potential to enhance early warning systems and guide architectural decisions but also influences building material choices, significantly mitigating the adverse effects of extreme heat. © 2024 Elsevier Ltd;"Data-driven model; Extreme heat events; Metabolic rate; Personalized predictions; Thermal comfort; Wearable sensing";"Data-driven model; Extreme heat event; Heat indices; Heat stress; Index equations; Metabolic rates; Older adults; Personalized prediction; Thermal; Wearable sensing";"Predicting indoor personalized heat stress using wearable sensors and data-driven models Rising global temperatures pose heightened heat stress challenges, especially impacting vulnerable populations such as older adults in low-income urban areas of color, who disproportionately face energy inefficiencies and environmental burdens. Traditional research often overlooks individual metabolic rates and behaviors in heat stress assessments, relying instead on isolated environmental measures in laboratory settings. This paper presents a novel, occupant-centric approach that integrates wearable sensor data, building information, and air-conditioning usage to predict extreme heat events and assess their impact on occupants through calibrated thermal building simulations for seven older adults in Houston, Texas. We developed a personalized heat stress model based on existing heat index equations and dynamic predicted mean votes, enhanced by measured metabolic rates. A data-driven machine learning algorithm was then used to predict extreme heat events with this model serving as the ground truth. Our findings reveal that elevated activity levels significantly contribute to discomfort during extreme heat events, often undetected by the heat index equation—an environmental metric—alone. Furthermore, we observe that combining metabolic rates with measured indoor temperatures consistently predicts extreme heat events with over 95 % accuracy, demonstrating almost 13 % improvement compared to using indoor temperature in isolation. This study provides a valuable proof of concept that underscores the merits of integrating multiple data sources to raise the accuracy and predictability of indoor extreme heat events. Our research not only offers potential to enhance early warning systems and guide architectural decisions but also influences building material choices, significantly mitigating the adverse effects of extreme heat. © 2024 Elsevier Ltd Data-driven model; Extreme heat events; Metabolic rate; Personalized predictions; Thermal comfort; Wearable sensing Data-driven model; Extreme heat event; Heat indices; Heat stress; Index equations; Metabolic rates; Older adults; Personalized prediction; Thermal; Wearable sensing";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
600;Modeling and simulating spatial extremes by combining extreme value theory with generative adversarial networks;Modeling dependencies between climate extremes is important for climate risk assessment, for instance when allocating emergency management funds. In statistics, multivariate extreme value theory is often used to model spatial extremes. However, most commonly used approaches require strong assumptions and are either too simplistic or over-parameterized. From a machine learning perspective, generative adversarial networks (GANs) are a powerful tool to model dependencies in high-dimensional spaces. Yet in the standard setting, GANs do not well represent dependencies in the extremes. Here we combine GANs with extreme value theory (evtGAN) to model spatial dependencies in summer maxima of temperature and winter maxima in precipitation over a large part of western Europe. We use data from a stationary 2000-year climate model simulation to validate the approach and explore its sensitivity to small sample sizes. Our results show that evtGAN outperforms classical GANs and standard statistical approaches to model spatial extremes. Already with about 50 years of data, which corresponds to commonly available climate records, we obtain reasonably good performance. In general, dependencies between temperature extremes are better captured than dependencies between precipitation extremes due to the high spatial coherence in temperature fields. Our approach can be applied to other climate variables and can be used to emulate climate models when running very long simulations to determine dependencies in the extremes is deemed infeasible. © The Author(s), 2022. Published by Cambridge University Press.;"Climate model simulations; extreme value theory; generative adversarial networks; spatial extremes";NULL;"Modeling and simulating spatial extremes by combining extreme value theory with generative adversarial networks Modeling dependencies between climate extremes is important for climate risk assessment, for instance when allocating emergency management funds. In statistics, multivariate extreme value theory is often used to model spatial extremes. However, most commonly used approaches require strong assumptions and are either too simplistic or over-parameterized. From a machine learning perspective, generative adversarial networks (GANs) are a powerful tool to model dependencies in high-dimensional spaces. Yet in the standard setting, GANs do not well represent dependencies in the extremes. Here we combine GANs with extreme value theory (evtGAN) to model spatial dependencies in summer maxima of temperature and winter maxima in precipitation over a large part of western Europe. We use data from a stationary 2000-year climate model simulation to validate the approach and explore its sensitivity to small sample sizes. Our results show that evtGAN outperforms classical GANs and standard statistical approaches to model spatial extremes. Already with about 50 years of data, which corresponds to commonly available climate records, we obtain reasonably good performance. In general, dependencies between temperature extremes are better captured than dependencies between precipitation extremes due to the high spatial coherence in temperature fields. Our approach can be applied to other climate variables and can be used to emulate climate models when running very long simulations to determine dependencies in the extremes is deemed infeasible. © The Author(s), 2022. Published by Cambridge University Press. Climate model simulations; extreme value theory; generative adversarial networks; spatial extremes NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.3;Meteorological;1;Prevention
601;Numerical analysis of extreme heat in Nagpur city using heat stress indices, all-cause mortality and local climate zone classification;Spatio-temporal investigation of heat-related impacts forms the basis for planning resources and formulating possible countermeasures. The present study aims to examine the effect of temperature extremes on all-cause mortality in Nagpur, a tropical Indian city and investigates (i) long-term (1969–2015) daily air temperature trend (ii) long-term (1985–2015) heat stress pattern using heat stress indices (iii) relationship between heat stress indices and all-cause mortality, and (iv) role of local climate zones to estimate intra-urban variation in mortality. The findings reveal a steady rise in maximum temperature, heat wave, severe heat wave, and hot nights. It was observed that during 1985–2015, heat index and humidex increased by 4 °C and 1 °C respectively. A cubic spline regression revealed a strong influence (R2 > 0.75) between heat stress indices and all-cause mortality. The results indicate that large low-rise and sparsely built areas (i.e., local climate zones 8, 9 and 93) present higher levels of heat stress indicating greater susceptibility to mortality. In contrast, compact/open low-rise developments with vegetation cover showed lower susceptibility to heat stress. The findings provide potential evidence to develop hyperlocal thresholds, and strengthen early warning systems by selecting appropriate indices for a given climate zone. © 2023 Elsevier Ltd;"All-cause mortality; Cubic spline regression; Heat stress indices; Local climate zone; Local threshold; Time-series analysis";"India; Maharashtra; Nagpur; Atmospheric temperature; Interpolation; Thermal stress; All-cause mortality; Cubic spline; Cubic spline regression; Heat stress; Heat stress index; Local climate; Local climate zone; Local thresholds; Stress indices; Time-series analysis; climate classification; extreme event; heat wave; mortality; numerical model; regression analysis; threshold; time series analysis; Time series analysis";"Numerical analysis of extreme heat in Nagpur city using heat stress indices, all-cause mortality and local climate zone classification Spatio-temporal investigation of heat-related impacts forms the basis for planning resources and formulating possible countermeasures. The present study aims to examine the effect of temperature extremes on all-cause mortality in Nagpur, a tropical Indian city and investigates (i) long-term (1969–2015) daily air temperature trend (ii) long-term (1985–2015) heat stress pattern using heat stress indices (iii) relationship between heat stress indices and all-cause mortality, and (iv) role of local climate zones to estimate intra-urban variation in mortality. The findings reveal a steady rise in maximum temperature, heat wave, severe heat wave, and hot nights. It was observed that during 1985–2015, heat index and humidex increased by 4 °C and 1 °C respectively. A cubic spline regression revealed a strong influence (R2 > 0.75) between heat stress indices and all-cause mortality. The results indicate that large low-rise and sparsely built areas (i.e., local climate zones 8, 9 and 93) present higher levels of heat stress indicating greater susceptibility to mortality. In contrast, compact/open low-rise developments with vegetation cover showed lower susceptibility to heat stress. The findings provide potential evidence to develop hyperlocal thresholds, and strengthen early warning systems by selecting appropriate indices for a given climate zone. © 2023 Elsevier Ltd All-cause mortality; Cubic spline regression; Heat stress indices; Local climate zone; Local threshold; Time-series analysis India; Maharashtra; Nagpur; Atmospheric temperature; Interpolation; Thermal stress; All-cause mortality; Cubic spline; Cubic spline regression; Heat stress; Heat stress index; Local climate; Local climate zone; Local thresholds; Stress indices; Time-series analysis; climate classification; extreme event; heat wave; mortality; numerical model; regression analysis; threshold; time series analysis; Time series analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
602;Heatwave damage prediction using random forest model in Korea;Climate change increases the frequency and intensity of heatwaves, causing significant human and material losses every year. Big data, whose volumes are rapidly increasing, are expected to be used for preemptive responses. However, human cognitive abilities are limited, which can lead to ineffective decision making during disaster responses when artificial intelligence-based analysis models are not employed. Existing prediction models have limitations with regard to their validation, and most models focus only on heat-associated deaths. In this study, a random forest model was developed for the weekly prediction of heat-related damages on the basis of four years (2015–2018) of statistical, meteorological, and floating population data from South Korea. The model was evaluated through comparisons with other traditional regression models in terms of mean absolute error, root mean squared error, root mean squared logarithmic error, and coefficient of determination (R2). In a comparative analysis with observed values, the proposed model showed an R2 value of 0.804. The results show that the proposed model outperforms existing models. They also show that the floating population variable collected from mobile global positioning systems contributes more to predictions than the aggregate population variable. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.;"Big data; Heatwaves; Machine learning; Prediction; Random forest regression model";NULL;"Heatwave damage prediction using random forest model in Korea Climate change increases the frequency and intensity of heatwaves, causing significant human and material losses every year. Big data, whose volumes are rapidly increasing, are expected to be used for preemptive responses. However, human cognitive abilities are limited, which can lead to ineffective decision making during disaster responses when artificial intelligence-based analysis models are not employed. Existing prediction models have limitations with regard to their validation, and most models focus only on heat-associated deaths. In this study, a random forest model was developed for the weekly prediction of heat-related damages on the basis of four years (2015–2018) of statistical, meteorological, and floating population data from South Korea. The model was evaluated through comparisons with other traditional regression models in terms of mean absolute error, root mean squared error, root mean squared logarithmic error, and coefficient of determination (R2). In a comparative analysis with observed values, the proposed model showed an R2 value of 0.804. The results show that the proposed model outperforms existing models. They also show that the floating population variable collected from mobile global positioning systems contributes more to predictions than the aggregate population variable. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. Big data; Heatwaves; Machine learning; Prediction; Random forest regression model NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
603;Flash flood prediction model based on multiple regression analysis for decision support system;Philippines experience frequent flash flooding, usually with insufficient lead time which causes panic and fear to the people. In spite of decades of effort by the government and the private sectors to improve observations and warning, flash floods continue to be one of nature's worst killers. On the average, there is at least one disastrous flood every four (4) years in the Philippines [1|. This paper proposed to develop a technology on Flash Flood Warning System Using SMS with advanced warning information based on prediction algorithm devised by the researcher regarding increasing water level and water speed. These two factors were considered as triggers to the flashflood, thus become components of the regression algorithm devised by the researchers. Based on the training data captured for seven days, the regression equation was developed while the actual/real time data were input to the regression model. Prediction of the current and forthcoming risk on flood is computed by the system based on the model and is sent through SMS to registered users for early warning purposes.;"Data mining; Decision support system; Flash flood; Multiple regression analysis; Prediction algorithm";"Algorithms; Artificial intelligence; Computer science; Data mining; Decision support systems; Mathematical models; Regression analysis; Water levels; Advanced warnings; Flash flood; Flash flood predictions; Flash-flood-warning; Multiple regression analysis; Prediction algorithms; Regression algorithms; Regression equation; Floods";"Flash flood prediction model based on multiple regression analysis for decision support system Philippines experience frequent flash flooding, usually with insufficient lead time which causes panic and fear to the people. In spite of decades of effort by the government and the private sectors to improve observations and warning, flash floods continue to be one of nature's worst killers. On the average, there is at least one disastrous flood every four (4) years in the Philippines [1|. This paper proposed to develop a technology on Flash Flood Warning System Using SMS with advanced warning information based on prediction algorithm devised by the researcher regarding increasing water level and water speed. These two factors were considered as triggers to the flashflood, thus become components of the regression algorithm devised by the researchers. Based on the training data captured for seven days, the regression equation was developed while the actual/real time data were input to the regression model. Prediction of the current and forthcoming risk on flood is computed by the system based on the model and is sent through SMS to registered users for early warning purposes. Data mining; Decision support system; Flash flood; Multiple regression analysis; Prediction algorithm Algorithms; Artificial intelligence; Computer science; Data mining; Decision support systems; Mathematical models; Regression analysis; Water levels; Advanced warnings; Flash flood; Flash flood predictions; Flash-flood-warning; Multiple regression analysis; Prediction algorithms; Regression algorithms; Regression equation; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
604;A sustainable early warning system using rolling forecasts based on ANN and golden ratio optimization methods to accurately predict real-time water levels and flash flood;Remote monitoring sensor systems play a significant role in the evaluation and minimiza-tion of natural disasters and risk. This article presents a sustainable and real-time early warning system of sensors employed in flash flood prediction by using a rolling forecast model based on Artificial Neural Network (ANN) and Golden Ratio Optimization (GROM) methods. This Early Flood Warning System (EFWS) aims to support decision makers by providing reliable and accurate information and warning about any possible flood events within an efficient lead-time to reduce any damages due to flash floods. In this work, to improve the performance of the EFWS, an ANN forecast model based on a new optimization method, GROM, is developed and compared to the traditional ANN model. Furthermore, due to the lack of literature regarding the optimal ANN structural model for forecasting the flash flood, this paper is one of the first extensive investigations into the impact of using different exogenous variables and parameters on the ANN structure. The effect of using a rolling forecast model compared to fixed model on the accuracy of the forecasts is investigated as well. The results indicate that the rolling ANN forecast model based on GROM successfully improved the model accuracy by 40% compared to the traditional ANN model and by 93.5% compared to the fixed forecast model. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.;"ANN; Flash floods; Jordan; Real-time early warning; Remote sensing; Rolling forecast; Sustainable monitoring system";"Disasters; Floods; Forecasting; Neural Networks, Computer; Water; Decision making; Disasters; Floods; Neural networks; Structural optimization; Water levels; water; Early Warning System; Exogenous variables; Flash flood predictions; Flood warning system; Natural disasters; Optimization method; Remote monitoring; Structural modeling; disaster; flooding; forecasting; Forecasting";"A sustainable early warning system using rolling forecasts based on ANN and golden ratio optimization methods to accurately predict real-time water levels and flash flood Remote monitoring sensor systems play a significant role in the evaluation and minimiza-tion of natural disasters and risk. This article presents a sustainable and real-time early warning system of sensors employed in flash flood prediction by using a rolling forecast model based on Artificial Neural Network (ANN) and Golden Ratio Optimization (GROM) methods. This Early Flood Warning System (EFWS) aims to support decision makers by providing reliable and accurate information and warning about any possible flood events within an efficient lead-time to reduce any damages due to flash floods. In this work, to improve the performance of the EFWS, an ANN forecast model based on a new optimization method, GROM, is developed and compared to the traditional ANN model. Furthermore, due to the lack of literature regarding the optimal ANN structural model for forecasting the flash flood, this paper is one of the first extensive investigations into the impact of using different exogenous variables and parameters on the ANN structure. The effect of using a rolling forecast model compared to fixed model on the accuracy of the forecasts is investigated as well. The results indicate that the rolling ANN forecast model based on GROM successfully improved the model accuracy by 40% compared to the traditional ANN model and by 93.5% compared to the fixed forecast model. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. ANN; Flash floods; Jordan; Real-time early warning; Remote sensing; Rolling forecast; Sustainable monitoring system Disasters; Floods; Forecasting; Neural Networks, Computer; Water; Decision making; Disasters; Floods; Neural networks; Structural optimization; Water levels; water; Early Warning System; Exogenous variables; Flash flood predictions; Flood warning system; Natural disasters; Optimization method; Remote monitoring; Structural modeling; disaster; flooding; forecasting; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
605;Emergency management system of urban waterlogging based on cloud computing platform and 3D visualization;In recent years, the problem of urban waterlogging has been highly valued. The application of information technology and image simulation to emergency management of urban waterlogging can improve urban flood prevention and disaster reduction capabilities and reduce disaster losses. In this paper, the author analyze the emergency management system of urban waterlogging based on cloud computing platform and 3D visualization. Collect data through street monitoring and drones, re-analyze the collected images, and screen cities for easy waterlogging. Researchers can rely on the high-performance computing power of the system and the visualized integrated environment to achieve online monitoring and early warning of waterlogging and 3D visual display. The system can provide early warning services in the form of alarms for monitoring results that exceed the threshold, and use mobile agents to send messages to relevant personnel in a variety of ways, providing fast auxiliary decision-making services. The simulation results show that the system has high simulation accuracy and can provide fast and efficient emergency services. © 2020-IOS Press and the authors.;"cloud computing; emergency management; Neural Network; Urban Waterlogging";"Civil defense; Cloud computing; Decision making; Disaster prevention; Flood control; Image enhancement; Mobile agents; Risk management; Three dimensional computer graphics; Visualization; Application of information technologies; Cloud computing platforms; Emergency management; Emergency management systems; High performance computing; Integrated environment; Monitoring results; Simulation accuracy; Emergency services";"Emergency management system of urban waterlogging based on cloud computing platform and 3D visualization In recent years, the problem of urban waterlogging has been highly valued. The application of information technology and image simulation to emergency management of urban waterlogging can improve urban flood prevention and disaster reduction capabilities and reduce disaster losses. In this paper, the author analyze the emergency management system of urban waterlogging based on cloud computing platform and 3D visualization. Collect data through street monitoring and drones, re-analyze the collected images, and screen cities for easy waterlogging. Researchers can rely on the high-performance computing power of the system and the visualized integrated environment to achieve online monitoring and early warning of waterlogging and 3D visual display. The system can provide early warning services in the form of alarms for monitoring results that exceed the threshold, and use mobile agents to send messages to relevant personnel in a variety of ways, providing fast auxiliary decision-making services. The simulation results show that the system has high simulation accuracy and can provide fast and efficient emergency services. © 2020-IOS Press and the authors. cloud computing; emergency management; Neural Network; Urban Waterlogging Civil defense; Cloud computing; Decision making; Disaster prevention; Flood control; Image enhancement; Mobile agents; Risk management; Three dimensional computer graphics; Visualization; Application of information technologies; Cloud computing platforms; Emergency management; Emergency management systems; High performance computing; Integrated environment; Monitoring results; Simulation accuracy; Emergency services";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
606;A method for urban flood risk assessment and zoning considering road environments and terrain;Floods have been severely threatening social development worldwide. The occurrence of floods has multiple factors, and the flood risk considering road environments needs comprehensive analysis from meteorology, underlying surface, and urban road network. Thus, this study proposes an integrated method and constructs a road risk zoning model (RRZM). In the RRZM, submerged depth was obtained by the Soil Conservation Service (SCS) model, and the degree of road importance was obtained by the analytical hierarchy process (AHP) method. These two parts were used to characterize road vulnerability. Then the flood risk grade was evaluated based on the optimized artificial neural network (ANN). Finally, the results of flood risk assessment were obtained by road vulnerability and flood risk grade. The RRZM was applied to the Chang-Zhu-Tan Urban Agglomeration (CZTUA), China. The results showed that the spatial distributions of flood risk and the extent of road damage varied remarkably in different cities. Changsha was the most sensitive city to floods in the CZTUA. The flood risk zones were classified into six levels, and the vulnerable road sections identified from the risk zones at level 6 in the maps carried more traffic volume than others. By comparing with existing methods, it was found that the RRZM effectively reflected the spatial characteristics of flood risk considering road environments. It provides a new perspective for urban flood risk assessment and disaster response decision-making. © 2019 by the authors.;"Chang-Zhu-Tan urban agglomeration; Flood risk assessment; Road environments; Road risk zoning model; Urban flood";"Changsha; China; Hunan; agglomeration; analytical hierarchy process; artificial neural network; decision making; disaster management; flood; flood damage; model; risk assessment; road; social development; terrain; vulnerability";"A method for urban flood risk assessment and zoning considering road environments and terrain Floods have been severely threatening social development worldwide. The occurrence of floods has multiple factors, and the flood risk considering road environments needs comprehensive analysis from meteorology, underlying surface, and urban road network. Thus, this study proposes an integrated method and constructs a road risk zoning model (RRZM). In the RRZM, submerged depth was obtained by the Soil Conservation Service (SCS) model, and the degree of road importance was obtained by the analytical hierarchy process (AHP) method. These two parts were used to characterize road vulnerability. Then the flood risk grade was evaluated based on the optimized artificial neural network (ANN). Finally, the results of flood risk assessment were obtained by road vulnerability and flood risk grade. The RRZM was applied to the Chang-Zhu-Tan Urban Agglomeration (CZTUA), China. The results showed that the spatial distributions of flood risk and the extent of road damage varied remarkably in different cities. Changsha was the most sensitive city to floods in the CZTUA. The flood risk zones were classified into six levels, and the vulnerable road sections identified from the risk zones at level 6 in the maps carried more traffic volume than others. By comparing with existing methods, it was found that the RRZM effectively reflected the spatial characteristics of flood risk considering road environments. It provides a new perspective for urban flood risk assessment and disaster response decision-making. © 2019 by the authors. Chang-Zhu-Tan urban agglomeration; Flood risk assessment; Road environments; Road risk zoning model; Urban flood Changsha; China; Hunan; agglomeration; analytical hierarchy process; artificial neural network; decision making; disaster management; flood; flood damage; model; risk assessment; road; social development; terrain; vulnerability";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
607;An efficient urban flood mapping framework towards disaster response driven by weakly supervised semantic segmentation with decoupled training samples;Despite the proven effectiveness of data-driven deep learning techniques in urban flood mapping, the availability of annotation data remains a critical factor impeding their timeliness in real applications. Recent progress in weakly supervised semantic segmentation (WSSS) presents promising solutions for addressing this limitation. To accomplish prompt and accurate flood mapping in complex urban areas from high-resolution remote sensing images to support disaster management and response, this study makes contributions in three key aspects: weak training data generation, the improvement of WSSS algorithm, and the construction of benchmark datasets. Firstly, we present a novel yet efficient weak training data generation strategy by decoupling the acquisition of positive and negative samples. This strategy enables the rapid generation of block-level weak annotations assisted by pre-flood river data or the segment anything model (SAM) for zero-shot segmentation, thereby alleviating the burden of weak data labeling. Secondly, to enhance the flood detection results in complex urban landscapes based on low-cost weak labels, we design an end-to-end WSSS framework incorporating tree filtering-based structure constraints and a perturbed dual-branch cross self-distillation mechanism. Lastly, to evaluate the performance of the proposed approach, we constructed two large aerial imagery datasets, namely Calgary-Flood and Huston-Flood. These datasets encompass diverse urban land covers and include challenging scenarios with extensive shadows, providing a robust benchmark for evaluating our method against various urban environments. Experimental results demonstrate that our weak data annotation strategy substantially enhances efficiency. Additionally, the proposed WSSS framework exhibits superior performance in comparison to existing state-of-the-art methods, particularly in terms of edge delineation and algorithmic stability. The advancements in weak data annotation strategy, end-to-end model architecture, and benchmark dataset development in this study collectively exploit the potential of the weakly supervised paradigm for rapid flood mapping in urgent situations. The datasets and code will be publicly available at (https://github.com/YJ-He/Flood_Mapping_WSSS). © 2023 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS);"Rapid flood mapping; Segment anything model; Self-distillation; Sparse label; Tree filtering; Weakly supervised semantic segmentation";"Alberta; Calgary; Canada; Aerial photography; Deep learning; Disaster prevention; Distillation; Emergency services; Image enhancement; Large dataset; Learning systems; Mapping; Remote sensing; Semantic Segmentation; Semantics; Disaster-response; Flood mapping; Rapid flood mapping; Segment anything model; Self-distillation; Semantic segmentation; Sparse label; Tree filtering; Urban floods; Weakly supervised semantic segmentation; algorithm; disaster management; flood; flood control; mapping method; natural disaster; remote sensing; segmentation; spatiotemporal analysis; Antennas";"An efficient urban flood mapping framework towards disaster response driven by weakly supervised semantic segmentation with decoupled training samples Despite the proven effectiveness of data-driven deep learning techniques in urban flood mapping, the availability of annotation data remains a critical factor impeding their timeliness in real applications. Recent progress in weakly supervised semantic segmentation (WSSS) presents promising solutions for addressing this limitation. To accomplish prompt and accurate flood mapping in complex urban areas from high-resolution remote sensing images to support disaster management and response, this study makes contributions in three key aspects: weak training data generation, the improvement of WSSS algorithm, and the construction of benchmark datasets. Firstly, we present a novel yet efficient weak training data generation strategy by decoupling the acquisition of positive and negative samples. This strategy enables the rapid generation of block-level weak annotations assisted by pre-flood river data or the segment anything model (SAM) for zero-shot segmentation, thereby alleviating the burden of weak data labeling. Secondly, to enhance the flood detection results in complex urban landscapes based on low-cost weak labels, we design an end-to-end WSSS framework incorporating tree filtering-based structure constraints and a perturbed dual-branch cross self-distillation mechanism. Lastly, to evaluate the performance of the proposed approach, we constructed two large aerial imagery datasets, namely Calgary-Flood and Huston-Flood. These datasets encompass diverse urban land covers and include challenging scenarios with extensive shadows, providing a robust benchmark for evaluating our method against various urban environments. Experimental results demonstrate that our weak data annotation strategy substantially enhances efficiency. Additionally, the proposed WSSS framework exhibits superior performance in comparison to existing state-of-the-art methods, particularly in terms of edge delineation and algorithmic stability. The advancements in weak data annotation strategy, end-to-end model architecture, and benchmark dataset development in this study collectively exploit the potential of the weakly supervised paradigm for rapid flood mapping in urgent situations. The datasets and code will be publicly available at (https://github.com/YJ-He/Flood_Mapping_WSSS). © 2023 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS) Rapid flood mapping; Segment anything model; Self-distillation; Sparse label; Tree filtering; Weakly supervised semantic segmentation Alberta; Calgary; Canada; Aerial photography; Deep learning; Disaster prevention; Distillation; Emergency services; Image enhancement; Large dataset; Learning systems; Mapping; Remote sensing; Semantic Segmentation; Semantics; Disaster-response; Flood mapping; Rapid flood mapping; Segment anything model; Self-distillation; Semantic segmentation; Sparse label; Tree filtering; Urban floods; Weakly supervised semantic segmentation; algorithm; disaster management; flood; flood control; mapping method; natural disaster; remote sensing; segmentation; spatiotemporal analysis; Antennas";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
608;Three-Dimensional Convolutional Neural Network on Multi-Temporal Synthetic Aperture Radar Images for Urban Flood Potential Mapping in Jakarta;Flooding in urban areas is counted as a significant disaster that must be correctly mitigated due to the huge amount of affected people, material losses, hampered economic activity, and flood-related diseases. One of the technologies available for disaster mitigation and prevention is satellites providing image data on previously flooded areas. In most cases, floods occur in conjunction with heavy rain. Thus, from a satellite’s optical sensor, the flood area is mostly covered with clouds which indicates ineffective observation. One solution to this problem is to use Synthetic Aperture Radar (SAR) sensors by observing backscatter differences before and after flood events. This research proposes mapping the flood-prone areas using machine learning to classify the areas using the 3D CNN method. The method was applied on a combination of co-polarized and cross-polarized SAR multi-temporal image datasets covering Jakarta City and the coastal area of Bekasi Regency. Testing with multiple combinations of training/testing data proportion split and a different number of epochs gave the optimum performance at an 80/20 split with 150 epochs achieving an overall accuracy of 0.71 after training in 283 min. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.;"3D Convolutional Neural Network; Multi-temporal data; Sentinel-1a; Synthetic Aperture Radar (SAR); Urban flood";NULL;"Three-Dimensional Convolutional Neural Network on Multi-Temporal Synthetic Aperture Radar Images for Urban Flood Potential Mapping in Jakarta Flooding in urban areas is counted as a significant disaster that must be correctly mitigated due to the huge amount of affected people, material losses, hampered economic activity, and flood-related diseases. One of the technologies available for disaster mitigation and prevention is satellites providing image data on previously flooded areas. In most cases, floods occur in conjunction with heavy rain. Thus, from a satellite’s optical sensor, the flood area is mostly covered with clouds which indicates ineffective observation. One solution to this problem is to use Synthetic Aperture Radar (SAR) sensors by observing backscatter differences before and after flood events. This research proposes mapping the flood-prone areas using machine learning to classify the areas using the 3D CNN method. The method was applied on a combination of co-polarized and cross-polarized SAR multi-temporal image datasets covering Jakarta City and the coastal area of Bekasi Regency. Testing with multiple combinations of training/testing data proportion split and a different number of epochs gave the optimum performance at an 80/20 split with 150 epochs achieving an overall accuracy of 0.71 after training in 283 min. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. 3D Convolutional Neural Network; Multi-temporal data; Sentinel-1a; Synthetic Aperture Radar (SAR); Urban flood NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
609;Analysis and dynamic simulation of urban rainstorm waterlogging;As global climates continuous to deteriorate, urban typhoons, rainstorms and other natural disasters increase in frequency. Urban flood control and disaster mitigation issues, which are related to the people's safety and property and the implement of the sustainable development, have become increasingly noteworthy. And with the rapid expansion of urbanization, these problems will be more obvious in the future. So it is crucial to study the problem of urban rainstorm waterlogging. In this paper, the study area is located in Shenzhen City in China, and the storm sewer model (Storm Water Management Model (SWMM)) was used to calculate relative data of the urban rainstorm. The methods of subcatchment partitions were studied. GIS technology was not only used to manage massive amounts of data, such as meteorological data, GIS data and pipeline data, but also used to obtain parameters for the model. Remote Sensing (RS) technology was applied in the land utilization classification and identifying water feature. The water depth in pipeline was calculated, the depth of the waterlogged area and the time that waterlogging lasted under different rainfall frequencies were calculated. The characteristics and the process of rainstorm water on the ground and in the pipeline were also analyzed. Lastly, the methods of integrating GIS and SWMM were studied, and a 3D dynamic simulation for the process of rainstorm waterlogging realized. The results show that the application of GIS and RS technology in the process of calculating can extremely improve the efficiency of calculation and the precision of its results. The 3D dynamic simulation realized by integrating GIS technology and SWMM offers a kind of prediction method with more direct-viewing and effective characteristics, which can be applied to establish flood-mitigation measures.© 2010 SPIE.;"3D dynamic simulation; GIS; RS; SWMM; Urban rainstorm waterlogging";"Disasters; Dynamic analysis; Flood control; Meteorology; Pipelines; Remote sensing; Technology; Three dimensional; Three dimensional computer graphics; Thunderstorms; Water management; Dynamic simulation; GIS; RS; SWMM; Urban rainstorm waterlogging; Data handling";"Analysis and dynamic simulation of urban rainstorm waterlogging As global climates continuous to deteriorate, urban typhoons, rainstorms and other natural disasters increase in frequency. Urban flood control and disaster mitigation issues, which are related to the people's safety and property and the implement of the sustainable development, have become increasingly noteworthy. And with the rapid expansion of urbanization, these problems will be more obvious in the future. So it is crucial to study the problem of urban rainstorm waterlogging. In this paper, the study area is located in Shenzhen City in China, and the storm sewer model (Storm Water Management Model (SWMM)) was used to calculate relative data of the urban rainstorm. The methods of subcatchment partitions were studied. GIS technology was not only used to manage massive amounts of data, such as meteorological data, GIS data and pipeline data, but also used to obtain parameters for the model. Remote Sensing (RS) technology was applied in the land utilization classification and identifying water feature. The water depth in pipeline was calculated, the depth of the waterlogged area and the time that waterlogging lasted under different rainfall frequencies were calculated. The characteristics and the process of rainstorm water on the ground and in the pipeline were also analyzed. Lastly, the methods of integrating GIS and SWMM were studied, and a 3D dynamic simulation for the process of rainstorm waterlogging realized. The results show that the application of GIS and RS technology in the process of calculating can extremely improve the efficiency of calculation and the precision of its results. The 3D dynamic simulation realized by integrating GIS technology and SWMM offers a kind of prediction method with more direct-viewing and effective characteristics, which can be applied to establish flood-mitigation measures.© 2010 SPIE. 3D dynamic simulation; GIS; RS; SWMM; Urban rainstorm waterlogging Disasters; Dynamic analysis; Flood control; Meteorology; Pipelines; Remote sensing; Technology; Three dimensional; Three dimensional computer graphics; Thunderstorms; Water management; Dynamic simulation; GIS; RS; SWMM; Urban rainstorm waterlogging; Data handling";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
610;Multi-class segmentation of urban floods from multispectral imagery using deep learning;Natural disasters such as floods, earthquakes, hurricanes, etc. have a huge impact on a society—causing destruction of life and property in their wake. During disasters such as flood, it is crucial to understand the dynamics of the situation as it occurs for effective response. In this paper, we address the problem of satellite image classification for urban floods using deep learning. We propose an encoder-decoder neural network based on the Efficient Residual Factorized Convnet(ERFNet), for multi-class segmentation of urban floods from multi-spectral satellite imagery. The ERFNet architecture capitalizes on skip connections and one dimensional convolutions to achieve the best possible trade-off between accuracy and efficiency. Since time is of essence during a disaster, the choice of the ERFNet architecture on a high performance computing (HPC) platform is apt. Satellite imagery from WorldView-2 of floods in Srinagar, India during September 2014 have been used for this study. The tool ‘markGT’ has been developed to assist end-to-end annotation of satellite imagery. The urban flood dataset used for this study has been generated using markGT. The proposed deep learning model over urban flood satellite imagery gives promising results on Nvidia Tesla K80 GPU. We envisage that the proposed model could be extended and improved for real-time classification of urban floods, thereby aiding disaster response personnel in making informed decisions. © 2019 IEEE;"Classification; Flood; Multi-class; Neural networks; Segmentation";"Convolutional neural networks; Disasters; Economic and social effects; Floods; Network architecture; Remote sensing; Satellite imagery; Disaster response; High performance computing (HPC); Informed decision; Learning models; Multi-class segmentations; Multi-spectral imagery; Natural disasters; Satellite image classification; Deep learning";"Multi-class segmentation of urban floods from multispectral imagery using deep learning Natural disasters such as floods, earthquakes, hurricanes, etc. have a huge impact on a society—causing destruction of life and property in their wake. During disasters such as flood, it is crucial to understand the dynamics of the situation as it occurs for effective response. In this paper, we address the problem of satellite image classification for urban floods using deep learning. We propose an encoder-decoder neural network based on the Efficient Residual Factorized Convnet(ERFNet), for multi-class segmentation of urban floods from multi-spectral satellite imagery. The ERFNet architecture capitalizes on skip connections and one dimensional convolutions to achieve the best possible trade-off between accuracy and efficiency. Since time is of essence during a disaster, the choice of the ERFNet architecture on a high performance computing (HPC) platform is apt. Satellite imagery from WorldView-2 of floods in Srinagar, India during September 2014 have been used for this study. The tool ‘markGT’ has been developed to assist end-to-end annotation of satellite imagery. The urban flood dataset used for this study has been generated using markGT. The proposed deep learning model over urban flood satellite imagery gives promising results on Nvidia Tesla K80 GPU. We envisage that the proposed model could be extended and improved for real-time classification of urban floods, thereby aiding disaster response personnel in making informed decisions. © 2019 IEEE Classification; Flood; Multi-class; Neural networks; Segmentation Convolutional neural networks; Disasters; Economic and social effects; Floods; Network architecture; Remote sensing; Satellite imagery; Disaster response; High performance computing (HPC); Informed decision; Learning models; Multi-class segmentations; Multi-spectral imagery; Natural disasters; Satellite image classification; Deep learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
611;A rapid and efficient method for flash flood simulation based on deep learning;"Among the various natural disasters, the death caused by flash flood is the highest. Recently, the combination of deep learning methods and hydrodynamic models has shown superior performance in the simulation of urban and plain areas. However, when dealing with flash flood simulation, the research still faces numerous challenges due to limitations such as data scarcity, small sample sizes, complex terrain, and high levels of uncertainty. Therefore, in this study, we innovatively combined deep learning methods with flash flood simulation and proposed a TCN model to predict the spatiotemporal dynamics of flash floods. First, we extracted the typical rainfall patterns in the study area and used design storm methods to generate a hydrograph dataset, which includes various rainfall patterns and return periods. Then, we developed a Temporal Convolutional Network (TCN) model to predict flash floods. Finally, the benchmark test was carried out by Convolutional Neural Network (CNN), which further proved the performance of TCN. The study found that: (1) The TCN model effectively predicts flash floods, with average MAE, RMSE and NSE reaching 0.04, 0.17 and 0.834 on the validation set. However, the CNN model performed better in small flood scenarios; (2) Error boxplots show that simulation errors for both models increase with the flood volume, and reach the maximum around the flood peak, but the TCN model demonstrated better stability and fewer outliers; (3) For the change of water depth at key points, both TCN and CNN effectively capture the fluctuation of water depth with time in the early stage of flood, but TCN showed higher consistency in the recession period. The results show that the rapid simulation method of flash flood based on TCN can better capture the dynamic characteristics of flash flood, and has been well applied in mountainous areas, which provides a new method for the prediction and early warning of flash flood disasters. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.";"deep learning; Flash flood simulation; rapid flood modelling; surrogate model";NULL;"A rapid and efficient method for flash flood simulation based on deep learning Among the various natural disasters, the death caused by flash flood is the highest. Recently, the combination of deep learning methods and hydrodynamic models has shown superior performance in the simulation of urban and plain areas. However, when dealing with flash flood simulation, the research still faces numerous challenges due to limitations such as data scarcity, small sample sizes, complex terrain, and high levels of uncertainty. Therefore, in this study, we innovatively combined deep learning methods with flash flood simulation and proposed a TCN model to predict the spatiotemporal dynamics of flash floods. First, we extracted the typical rainfall patterns in the study area and used design storm methods to generate a hydrograph dataset, which includes various rainfall patterns and return periods. Then, we developed a Temporal Convolutional Network (TCN) model to predict flash floods. Finally, the benchmark test was carried out by Convolutional Neural Network (CNN), which further proved the performance of TCN. The study found that: (1) The TCN model effectively predicts flash floods, with average MAE, RMSE and NSE reaching 0.04, 0.17 and 0.834 on the validation set. However, the CNN model performed better in small flood scenarios; (2) Error boxplots show that simulation errors for both models increase with the flood volume, and reach the maximum around the flood peak, but the TCN model demonstrated better stability and fewer outliers; (3) For the change of water depth at key points, both TCN and CNN effectively capture the fluctuation of water depth with time in the early stage of flood, but TCN showed higher consistency in the recession period. The results show that the rapid simulation method of flash flood based on TCN can better capture the dynamic characteristics of flash flood, and has been well applied in mountainous areas, which provides a new method for the prediction and early warning of flash flood disasters. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. deep learning; Flash flood simulation; rapid flood modelling; surrogate model NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
612;Road Traffic Waterlogging Detection Based on YOLOv5;"In view of the frequent occurrence of waterlogging in urban areas and the problems that traditional waterlogging monitoring methods consume a lot of human and material resources with high cost and low timeliness, an improved YOLOv5 waterlogging detection method for road traffic is proposed, which enhances the feature extraction of road traffic waterlogging information by feature extraction of waterlogging in urban waterlogging scenarios, and adds the CBAM attention mechanism in the backbone network; and adds a CIoU loss function to optimize the model in the prediction layer to improve the identification accuracy of road traffic waterlogging so as to construct a road traffic waterlogging detection model. In the prediction layer, a CIoU loss function is added to optimize the model and improve the detection accuracy of road water, thus constructing a road water detection model. By screening 5000 road traffic waterlogging images on the public dataset RSCD for training, the experimental results show that the average accuracy of the method is 84.4%, which is 3.7% higher than the original YOLOv5 algorithm, and it can more accurately extract and identify the waterlogged area of the image automatically, which can pave the way for further development of related research, and provide technical support for urban waterlogging monitoring and emergency management. The method can pave the way for further related research and provide technical support for urban flood monitoring and emergency management. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.";"attention mechanism; deep learning; target detection; urban flooding; YOLOv5";"Deep learning; Disasters; Extraction; Floods; Image enhancement; Risk management; Roads and streets; Water; Attention mechanisms; Deep learning; Detection models; Features extraction; Loss functions; Road traffic; Targets detection; Technical support; Urban flooding; YOLOv5; Feature extraction";"Road Traffic Waterlogging Detection Based on YOLOv5 In view of the frequent occurrence of waterlogging in urban areas and the problems that traditional waterlogging monitoring methods consume a lot of human and material resources with high cost and low timeliness, an improved YOLOv5 waterlogging detection method for road traffic is proposed, which enhances the feature extraction of road traffic waterlogging information by feature extraction of waterlogging in urban waterlogging scenarios, and adds the CBAM attention mechanism in the backbone network; and adds a CIoU loss function to optimize the model in the prediction layer to improve the identification accuracy of road traffic waterlogging so as to construct a road traffic waterlogging detection model. In the prediction layer, a CIoU loss function is added to optimize the model and improve the detection accuracy of road water, thus constructing a road water detection model. By screening 5000 road traffic waterlogging images on the public dataset RSCD for training, the experimental results show that the average accuracy of the method is 84.4%, which is 3.7% higher than the original YOLOv5 algorithm, and it can more accurately extract and identify the waterlogged area of the image automatically, which can pave the way for further development of related research, and provide technical support for urban waterlogging monitoring and emergency management. The method can pave the way for further related research and provide technical support for urban flood monitoring and emergency management. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024. attention mechanism; deep learning; target detection; urban flooding; YOLOv5 Deep learning; Disasters; Extraction; Floods; Image enhancement; Risk management; Roads and streets; Water; Attention mechanisms; Deep learning; Detection models; Features extraction; Loss functions; Road traffic; Targets detection; Technical support; Urban flooding; YOLOv5; Feature extraction";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
613;LOGISTIC MODELING TO PREDICT THE INTEREST OF THE INDONESIAN PEOPLE FOR BUYING FLOOD IMPACTED INSURANCE PRODUCTS;Indonesia is a country located on the equator and in the form of an archipelago. It has a high potential for various types of hydrometeorological-related disasters, such as floods, flash floods, droughts, extreme weather, etc. Almost all cities in Indonesia experience flooding every year, including DKI Jakarta, the capital city of Indonesia. Based on data from the National Disaster Management Agency (BNPB) in 2020, East Jakarta is a city that is prone to flooding. According to BNPB (2013), flooding is a disaster that relatively causes the most losses. Losses caused by floods, especially indirect losses, may rank first or second after an earthquake or tsunami. Floods cause so many losses, and it is necessary to have disaster mitigation efforts to minimize the possibility of flood risks. One risk mitigation due to natural disasters is buying insurance products. However, not everyone buys flood-impact insurance products due to economic and social factors. This study aims to create a model with the Logistics Regression Model to determine the factors influencing Indonesian people's interest in purchasing flood-impact insurance products. The research data is from 140 households in East Jakarta, Indonesia, using a non-probability purposive sampling technique. Furthermore, with a significance level of 10%, the logistic regression model obtained 14 significant regression coefficients. In the end, the obtained model is evaluated based on its level of accuracy. The results showed that the accuracy rate was almost excellent, namely 89.3%. © 2023 Author(s).;"Flood; Insurance Product; Logistic Regression Model; Risk Mitigation";NULL;"LOGISTIC MODELING TO PREDICT THE INTEREST OF THE INDONESIAN PEOPLE FOR BUYING FLOOD IMPACTED INSURANCE PRODUCTS Indonesia is a country located on the equator and in the form of an archipelago. It has a high potential for various types of hydrometeorological-related disasters, such as floods, flash floods, droughts, extreme weather, etc. Almost all cities in Indonesia experience flooding every year, including DKI Jakarta, the capital city of Indonesia. Based on data from the National Disaster Management Agency (BNPB) in 2020, East Jakarta is a city that is prone to flooding. According to BNPB (2013), flooding is a disaster that relatively causes the most losses. Losses caused by floods, especially indirect losses, may rank first or second after an earthquake or tsunami. Floods cause so many losses, and it is necessary to have disaster mitigation efforts to minimize the possibility of flood risks. One risk mitigation due to natural disasters is buying insurance products. However, not everyone buys flood-impact insurance products due to economic and social factors. This study aims to create a model with the Logistics Regression Model to determine the factors influencing Indonesian people's interest in purchasing flood-impact insurance products. The research data is from 140 households in East Jakarta, Indonesia, using a non-probability purposive sampling technique. Furthermore, with a significance level of 10%, the logistic regression model obtained 14 significant regression coefficients. In the end, the obtained model is evaluated based on its level of accuracy. The results showed that the accuracy rate was almost excellent, namely 89.3%. © 2023 Author(s). Flood; Insurance Product; Logistic Regression Model; Risk Mitigation NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
614;Towards a better consideration of rainfall and hydrological spatial features by a deep neural network model to improve flash floods forecasting: case study on the Gardon basin, France;Flash floods frequently hit the Mediterranean regions and cause numerous fatalities and heavy damage. Their forecast is still a challenge because of the poor knowledge of the processes involved and because of the difficulty to forecast heavy convective rainfall. In any case, early warning remains a strong need. In this study, the authors propose to build a deep artificial neural network for flash flood forecasting, allowing, by its specific architecture, to take better account of the spatial variability and the scales of the rainfall as well as the hydrological responses. The outcomes of the deep model are then compared to a classical global multilayer perceptron previously published. For this purpose, a database of 58 heavy rainfall events extracted from 16 years of hydrometeorological observations on a well-studied basin in Southern France is applied to train a deep recurrent neural network. The results are of twofold: first, the deep model improves the lead time from two hours to three hours providing then suitable forecast for an operational use. Second, the model selection process converged towards an architecture that explicitly considers spatial scales of the basin. More generally, this study shows that the implementation of a rigorous selection process mobilizing several well-known regularization methods has enabled the deep model to converge towards a parsimonious model highlighting some of the known physical processes of the basin: the roles of elevation and distance to the outlet. This work provides, thus, a very interesting piece of evidence to fuel the controversy on the interpretability of modern AI. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.;"Deep learning; Flash flood; Flood forecasting; Interpretability of AI; Nested basins";"France; Gardon Basin; Occitanie; artificial intelligence; artificial neural network; flash flood; flood forecasting; hydrological response; hydrology; rainfall";"Towards a better consideration of rainfall and hydrological spatial features by a deep neural network model to improve flash floods forecasting: case study on the Gardon basin, France Flash floods frequently hit the Mediterranean regions and cause numerous fatalities and heavy damage. Their forecast is still a challenge because of the poor knowledge of the processes involved and because of the difficulty to forecast heavy convective rainfall. In any case, early warning remains a strong need. In this study, the authors propose to build a deep artificial neural network for flash flood forecasting, allowing, by its specific architecture, to take better account of the spatial variability and the scales of the rainfall as well as the hydrological responses. The outcomes of the deep model are then compared to a classical global multilayer perceptron previously published. For this purpose, a database of 58 heavy rainfall events extracted from 16 years of hydrometeorological observations on a well-studied basin in Southern France is applied to train a deep recurrent neural network. The results are of twofold: first, the deep model improves the lead time from two hours to three hours providing then suitable forecast for an operational use. Second, the model selection process converged towards an architecture that explicitly considers spatial scales of the basin. More generally, this study shows that the implementation of a rigorous selection process mobilizing several well-known regularization methods has enabled the deep model to converge towards a parsimonious model highlighting some of the known physical processes of the basin: the roles of elevation and distance to the outlet. This work provides, thus, a very interesting piece of evidence to fuel the controversy on the interpretability of modern AI. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG. Deep learning; Flash flood; Flood forecasting; Interpretability of AI; Nested basins France; Gardon Basin; Occitanie; artificial intelligence; artificial neural network; flash flood; flood forecasting; hydrological response; hydrology; rainfall";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
615;A novel and efficient method for real-time simulating spatial and temporal evolution of coastal urban pluvial flood without drainage network;With increasing urban pluvial flood risks, proposing a real-time simulation method is essential. However, accurate simulation of spatiotemporal flood evolution is often impeded by incomplete or missing drainage data. This study proposes a hybrid method where a machine learning module is applied to generate point waterlogging depth for immediate calibration of equivalent infiltration and flood maps in the equivalent drainage module to address this issue. The accuracy and efficiency of hybrid method in flood real-time simulation under missing drainage data are highlighted by comparing with two hydrodynamic models. The outcomes evince that the waterlogging simulation deviation of the hybrid method is less than 0.1 m during design storms, while the computational efficiency can ideally reach up to 5 times of the traditional 1D/2D coupled hydrodynamic model. Overall, the hybrid method offers a promising solution for early warning and mitigation of urban pluvial floods, especially for cities lacking drainage data. © 2023 Elsevier Ltd;"Drainage data missing; Equivalent drainage hydrodynamic model; Flood real-time simulation; Hybrid method; Immediate calibration; Machine learning";"Calibration; Computational efficiency; Hydrodynamics; Machine learning; Data missing; Drainage data missing; Equivalent drainage hydrodynamic model; Flood real-time simulation; Hybrid method; Hydrodynamic modeling; Immediate calibration; Machine-learning; Pluvials; Realtime simulation (RTS); calibration; coastal zone; drainage network; flooding; hydrodynamics; machine learning; real time; simulation; spatiotemporal analysis; urban area; Floods";"A novel and efficient method for real-time simulating spatial and temporal evolution of coastal urban pluvial flood without drainage network With increasing urban pluvial flood risks, proposing a real-time simulation method is essential. However, accurate simulation of spatiotemporal flood evolution is often impeded by incomplete or missing drainage data. This study proposes a hybrid method where a machine learning module is applied to generate point waterlogging depth for immediate calibration of equivalent infiltration and flood maps in the equivalent drainage module to address this issue. The accuracy and efficiency of hybrid method in flood real-time simulation under missing drainage data are highlighted by comparing with two hydrodynamic models. The outcomes evince that the waterlogging simulation deviation of the hybrid method is less than 0.1 m during design storms, while the computational efficiency can ideally reach up to 5 times of the traditional 1D/2D coupled hydrodynamic model. Overall, the hybrid method offers a promising solution for early warning and mitigation of urban pluvial floods, especially for cities lacking drainage data. © 2023 Elsevier Ltd Drainage data missing; Equivalent drainage hydrodynamic model; Flood real-time simulation; Hybrid method; Immediate calibration; Machine learning Calibration; Computational efficiency; Hydrodynamics; Machine learning; Data missing; Drainage data missing; Equivalent drainage hydrodynamic model; Flood real-time simulation; Hybrid method; Hydrodynamic modeling; Immediate calibration; Machine-learning; Pluvials; Realtime simulation (RTS); calibration; coastal zone; drainage network; flooding; hydrodynamics; machine learning; real time; simulation; spatiotemporal analysis; urban area; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
616;Machine Learning Approximation for Rapid Prediction of High-Dimensional Storm Surge and Wave Responses;Storm surge and waves are responsible for a substantial portion of the tropical and extratropical cyclones-induced damage in coastal areas of the USA and Canada. High-fidelity, numerical models can provide accurate simulation results of the water elevation, where a hydrodynamic model (e.g., ADCIRC) is coupled with a wave model (e.g., SWAN). However, they are computationally expensive, hence cannot be employed as part of an early warning system for urban flooding hazards or implemented in probabilistic tropical and extratropical cyclones’ risk assessment. In this study, an alternative and efficient approach is proposed based on hybrid machine learning approaches. First, a dimensionality reduction technique based on deep autoencoder is developed to encode the spatial information in a reduced state space. Then, a machine learning-based model is developed in the latent space to predict the maximum surge and significant wave height. The latent space is then decompressed back to the original high-dimensional space using the decoder. The high-fidelity data are retrieved from the North Atlantic Comprehensive Coastal Study (NACCS), released by the US Army Corps of Engineers. Due to its high efficiency and accuracy, the proposed methodology can be employed to analyze the impact of input uncertainties on the simulation results. Four machine learning algorithms are used to predict the maximum surge and significant wave height including artificial neural network (ANN), support vector regression (SVR), gradient boosting regression (GBR), and random forest regression (RFR). The coupled autoencoder-ANN model for the prediction of the storm surge (significant wave height) outperformed all other algorithms with a coefficient of determination R2 of 0.953 (0.921) for the testing set. In addition, the comparison between deep autoencoder and the widely used principal component analysis (PCA) technique indicated the superior performance of the former since it is able to accurately capture the inherent nonlinearities within the data. © 2023, Canadian Society for Civil Engineering.;"Deep autoencoder; Machine learning; Principal component analysis; Significant wave height; Storm surge";"Adaptive boosting; Deep learning; Floods; Forecasting; Forestry; Hurricanes; Learning systems; Neural networks; Random forests; Risk assessment; Storms; Tropics; Uncertainty analysis; Water waves; Auto encoders; Deep autoencoder; Extratropical cyclones; High-fidelity; Machine-learning; Principal-component analysis; Significant wave height; Storm surges; Storm waves; Tropical cyclone; Principal component analysis";"Machine Learning Approximation for Rapid Prediction of High-Dimensional Storm Surge and Wave Responses Storm surge and waves are responsible for a substantial portion of the tropical and extratropical cyclones-induced damage in coastal areas of the USA and Canada. High-fidelity, numerical models can provide accurate simulation results of the water elevation, where a hydrodynamic model (e.g., ADCIRC) is coupled with a wave model (e.g., SWAN). However, they are computationally expensive, hence cannot be employed as part of an early warning system for urban flooding hazards or implemented in probabilistic tropical and extratropical cyclones’ risk assessment. In this study, an alternative and efficient approach is proposed based on hybrid machine learning approaches. First, a dimensionality reduction technique based on deep autoencoder is developed to encode the spatial information in a reduced state space. Then, a machine learning-based model is developed in the latent space to predict the maximum surge and significant wave height. The latent space is then decompressed back to the original high-dimensional space using the decoder. The high-fidelity data are retrieved from the North Atlantic Comprehensive Coastal Study (NACCS), released by the US Army Corps of Engineers. Due to its high efficiency and accuracy, the proposed methodology can be employed to analyze the impact of input uncertainties on the simulation results. Four machine learning algorithms are used to predict the maximum surge and significant wave height including artificial neural network (ANN), support vector regression (SVR), gradient boosting regression (GBR), and random forest regression (RFR). The coupled autoencoder-ANN model for the prediction of the storm surge (significant wave height) outperformed all other algorithms with a coefficient of determination R2 of 0.953 (0.921) for the testing set. In addition, the comparison between deep autoencoder and the widely used principal component analysis (PCA) technique indicated the superior performance of the former since it is able to accurately capture the inherent nonlinearities within the data. © 2023, Canadian Society for Civil Engineering. Deep autoencoder; Machine learning; Principal component analysis; Significant wave height; Storm surge Adaptive boosting; Deep learning; Floods; Forecasting; Forestry; Hurricanes; Learning systems; Neural networks; Random forests; Risk assessment; Storms; Tropics; Uncertainty analysis; Water waves; Auto encoders; Deep autoencoder; Extratropical cyclones; High-fidelity; Machine-learning; Principal-component analysis; Significant wave height; Storm surges; Storm waves; Tropical cyclone; Principal component analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
617;Relative Importance of Radar Variables for Nowcasting Heavy Rainfall: A Machine Learning Approach;Highly short-term forecasting, or nowcasting, of heavy rainfall due to rapidly evolving mesoscale convective systems (MCSs) is particularly challenging for traditional numerical weather prediction (NWP) models. To overcome such a challenge, a growing number of studies have shown significant advantages of using machine learning (ML) modeling techniques with remote sensing data, especially weather radar data, for high-resolution rainfall nowcasting. To improve ML model performance, it is essential first and foremost to quantify the importance of radar variables and identify pertinent predictors of rainfall that can also be associated with domain knowledge. In this study, a set of MCS types consisting of convective cell (CC), mesoscale CC, diagonal squall line (SLD), and parallel squall line (SLP), was adopted to categorize MCS storm cells, following the fuzzy logic algorithm for storm tracking (FAST), over the Korean Peninsula. The relationships between rain rates and over 15 variables derived from data products of dual-polarimetric weather radar were investigated and quantified via five ML regression methods and a permutation importance algorithm. As an applicational example, ML classification models were also developed to predict locations of storm cells. Recalibrated ML regression models with identified pertinent predictors were coupled with the ML classification models to provide early warnings of heavy rainfall. Results imply that future work needs to consider MCS type information to improve ML modeling for nowcasting and early warning of heavy rainfall.  © 1980-2012 IEEE.;"Artificial neural network (ANN); convolutional neural network (CNN); deep learning; dual-polarimetric weather radar; early warning; flash flood; hydrometeorological hazard; Lasso; mesoscale convective system (MCS); permutation importance; random forest; remote sensing; storm; support vector regression (SVR)";"Korea; Cells; Cytology; Deep neural networks; Forestry; Fuzzy logic; Meteorological radar; Multilayer neural networks; Polarimeters; Rain gages; Regression analysis; Remote sensing; Storms; Weather forecasting; Convolutional neural network; Deep learning; Dual-polarimetric weather radar; Early warning; Flash-floods; Hydrometeorological hazard; Input variables; Lasso; Mesoscale Convective System; Permutation importance; Predictive models; Random forests; Remote-sensing; Support vector regressions; algorithm; artificial neural network; early warning system; hydrometeorology; machine learning; nowcasting; radar; rainfall; remote sensing; satellite data; storm; support vector machine; weather forecasting; Rain";"Relative Importance of Radar Variables for Nowcasting Heavy Rainfall: A Machine Learning Approach Highly short-term forecasting, or nowcasting, of heavy rainfall due to rapidly evolving mesoscale convective systems (MCSs) is particularly challenging for traditional numerical weather prediction (NWP) models. To overcome such a challenge, a growing number of studies have shown significant advantages of using machine learning (ML) modeling techniques with remote sensing data, especially weather radar data, for high-resolution rainfall nowcasting. To improve ML model performance, it is essential first and foremost to quantify the importance of radar variables and identify pertinent predictors of rainfall that can also be associated with domain knowledge. In this study, a set of MCS types consisting of convective cell (CC), mesoscale CC, diagonal squall line (SLD), and parallel squall line (SLP), was adopted to categorize MCS storm cells, following the fuzzy logic algorithm for storm tracking (FAST), over the Korean Peninsula. The relationships between rain rates and over 15 variables derived from data products of dual-polarimetric weather radar were investigated and quantified via five ML regression methods and a permutation importance algorithm. As an applicational example, ML classification models were also developed to predict locations of storm cells. Recalibrated ML regression models with identified pertinent predictors were coupled with the ML classification models to provide early warnings of heavy rainfall. Results imply that future work needs to consider MCS type information to improve ML modeling for nowcasting and early warning of heavy rainfall.  © 1980-2012 IEEE. Artificial neural network (ANN); convolutional neural network (CNN); deep learning; dual-polarimetric weather radar; early warning; flash flood; hydrometeorological hazard; Lasso; mesoscale convective system (MCS); permutation importance; random forest; remote sensing; storm; support vector regression (SVR) Korea; Cells; Cytology; Deep neural networks; Forestry; Fuzzy logic; Meteorological radar; Multilayer neural networks; Polarimeters; Rain gages; Regression analysis; Remote sensing; Storms; Weather forecasting; Convolutional neural network; Deep learning; Dual-polarimetric weather radar; Early warning; Flash-floods; Hydrometeorological hazard; Input variables; Lasso; Mesoscale Convective System; Permutation importance; Predictive models; Random forests; Remote-sensing; Support vector regressions; algorithm; artificial neural network; early warning system; hydrometeorology; machine learning; nowcasting; radar; rainfall; remote sensing; satellite data; storm; support vector machine; weather forecasting; Rain";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
618;Machine Learning Models Applied to Weather Series Analysis;In recent years the explosion in high-performance computing systems and high-capacity storage has led to an exponential increase in the amount of information, generating the phenomenon of big data and the development of automatic processing models like machine learning analysis. In this paper a machine learning time series analysis was experimentally developed in relation to the paroxysmal meteorological event “cloudburst” characterized by a very intense storm, concentrated in a few hours and highly localized. These extreme phenomena such as hail, overflows and sudden floods are found in both urban and rural areas. The predictability over time of these phenomena is very short and depends on the event considered, therefore it is useful to add data driven methods to the deterministic modeling tools to get the anticipated predictability of the event, also known as nowcasting. The detailed knowledge of these phenomena, together with the development of simulation models for the propagation of cloudbursts, can be a useful tool for monitoring and mitigating risk in civil protection contingency plans. © 2021, Springer Nature Switzerland AG.;"Cloudburst; Machine learning; Nowcasting";"Digital storage; Machine learning; Metadata; Precipitation (meteorology); Semantics; Turing machines; Amount of information; Automatic processing; Deterministic modeling; Exponential increase; High capacity storage; High performance computing systems; Machine learning models; Urban and rural areas; Time series analysis";"Machine Learning Models Applied to Weather Series Analysis In recent years the explosion in high-performance computing systems and high-capacity storage has led to an exponential increase in the amount of information, generating the phenomenon of big data and the development of automatic processing models like machine learning analysis. In this paper a machine learning time series analysis was experimentally developed in relation to the paroxysmal meteorological event “cloudburst” characterized by a very intense storm, concentrated in a few hours and highly localized. These extreme phenomena such as hail, overflows and sudden floods are found in both urban and rural areas. The predictability over time of these phenomena is very short and depends on the event considered, therefore it is useful to add data driven methods to the deterministic modeling tools to get the anticipated predictability of the event, also known as nowcasting. The detailed knowledge of these phenomena, together with the development of simulation models for the propagation of cloudbursts, can be a useful tool for monitoring and mitigating risk in civil protection contingency plans. © 2021, Springer Nature Switzerland AG. Cloudburst; Machine learning; Nowcasting Digital storage; Machine learning; Metadata; Precipitation (meteorology); Semantics; Turing machines; Amount of information; Automatic processing; Deterministic modeling; Exponential increase; High capacity storage; High performance computing systems; Machine learning models; Urban and rural areas; Time series analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
619;River Stage Forecasting Using Multiple Additive Regression Trees;Accurate real-time forecasts of river stages can serve as a reference for flood evacuation to minimize losses and casualties. Machine learning has been widely used for river stage forecasting because of its simple modeling and quick computation. However, many machine learning models have drawbacks such as excessive learning time, difficult evaluation of input variables, and lack of explanatory capacity, which limit their performance as practical tools. To overcome these drawbacks, this study employs multiple additive regression trees (MART) for river stage forecasting. Three MART models are proposed, namely the original MART model, the real-time MART model, and the naïve MART model, with different considerations of model training and error correction. Model training and testing were conducted based on the rainfall and river stage data for 16 typhoon events between 2005 and 2009 in the Bazhang River Basin in Taiwan. In the training process, variables are automatically selected by the MART models which reasonably describes the mechanism of flood transportation. The testing results show that all three models can reasonably forecast the river stages with a three-hour lead-time. Compared with the original MART, the real-time MART performs better in describing overall river stage variations, whereas the naïve MART is more accurate in the prediction of peak river stages. The proposed MART models are efficient and accurate and can thus serve as practical tools for flash flood early warning. © 2019, Springer Nature B.V.;"Early warning; Flash flood; Machine learning; Real-time; River stage forecast";"Bazhang Creek; Taiwan; Additives; Error correction; Floods; Forecasting; Forestry; Learning systems; Machine learning; Additive regression; Early warning; Flash flood; Machine learning models; Real time; Real-time forecasts; River stages; Training process; early warning system; flood forecasting; hydrological modeling; machine learning; real time; regression analysis; river water; water level; Rivers";"River Stage Forecasting Using Multiple Additive Regression Trees Accurate real-time forecasts of river stages can serve as a reference for flood evacuation to minimize losses and casualties. Machine learning has been widely used for river stage forecasting because of its simple modeling and quick computation. However, many machine learning models have drawbacks such as excessive learning time, difficult evaluation of input variables, and lack of explanatory capacity, which limit their performance as practical tools. To overcome these drawbacks, this study employs multiple additive regression trees (MART) for river stage forecasting. Three MART models are proposed, namely the original MART model, the real-time MART model, and the naïve MART model, with different considerations of model training and error correction. Model training and testing were conducted based on the rainfall and river stage data for 16 typhoon events between 2005 and 2009 in the Bazhang River Basin in Taiwan. In the training process, variables are automatically selected by the MART models which reasonably describes the mechanism of flood transportation. The testing results show that all three models can reasonably forecast the river stages with a three-hour lead-time. Compared with the original MART, the real-time MART performs better in describing overall river stage variations, whereas the naïve MART is more accurate in the prediction of peak river stages. The proposed MART models are efficient and accurate and can thus serve as practical tools for flash flood early warning. © 2019, Springer Nature B.V. Early warning; Flash flood; Machine learning; Real-time; River stage forecast Bazhang Creek; Taiwan; Additives; Error correction; Floods; Forecasting; Forestry; Learning systems; Machine learning; Additive regression; Early warning; Flash flood; Machine learning models; Real time; Real-time forecasts; River stages; Training process; early warning system; flood forecasting; hydrological modeling; machine learning; real time; regression analysis; river water; water level; Rivers";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
620;Predicting road flooding risk with crowdsourced reports and fine-grained traffic data;The objective of this study is to predict road flooding risks based on topographic, hydrologic, and temporal precipitation features using machine learning models. Existing road inundation studies either lack empirical data for model validations or focus mainly on road inundation exposure assessment based on flood maps. This study addresses this limitation by using crowdsourced and fine-grained traffic data as an indicator of road inundation, and topographic, hydrologic, and temporal precipitation features as predictor variables. Two tree-based machine learning models (random forest and AdaBoost) were then tested and trained for predicting road inundations in the contexts of 2017 Hurricane Harvey and 2019 Tropical Storm Imelda in Harris County, Texas. The findings from Hurricane Harvey indicate that precipitation is the most important feature for predicting road inundation susceptibility, and that topographic features are more critical than hydrologic features for predicting road inundations in both storm cases. The random forest and AdaBoost models had relatively high AUC scores (0.860 and 0.810 for Harvey respectively and 0.790 and 0.720 for Imelda respectively) with the random forest model performing better in both cases. The random forest model showed stable performance for Harvey, while varying significantly for Imelda. This study advances the emerging field of smart flood resilience in terms of predictive flood risk mapping at the road level. In particular, such models could help impacted communities and emergency management agencies develop better preparedness and response strategies with improved situational awareness of road inundation likelihood as an extreme weather event unfolds. © 2023, The Author(s).;"Big data; Machine learning; Road flood risk; Smart resilience; Urban flood";NULL;"Predicting road flooding risk with crowdsourced reports and fine-grained traffic data The objective of this study is to predict road flooding risks based on topographic, hydrologic, and temporal precipitation features using machine learning models. Existing road inundation studies either lack empirical data for model validations or focus mainly on road inundation exposure assessment based on flood maps. This study addresses this limitation by using crowdsourced and fine-grained traffic data as an indicator of road inundation, and topographic, hydrologic, and temporal precipitation features as predictor variables. Two tree-based machine learning models (random forest and AdaBoost) were then tested and trained for predicting road inundations in the contexts of 2017 Hurricane Harvey and 2019 Tropical Storm Imelda in Harris County, Texas. The findings from Hurricane Harvey indicate that precipitation is the most important feature for predicting road inundation susceptibility, and that topographic features are more critical than hydrologic features for predicting road inundations in both storm cases. The random forest and AdaBoost models had relatively high AUC scores (0.860 and 0.810 for Harvey respectively and 0.790 and 0.720 for Imelda respectively) with the random forest model performing better in both cases. The random forest model showed stable performance for Harvey, while varying significantly for Imelda. This study advances the emerging field of smart flood resilience in terms of predictive flood risk mapping at the road level. In particular, such models could help impacted communities and emergency management agencies develop better preparedness and response strategies with improved situational awareness of road inundation likelihood as an extreme weather event unfolds. © 2023, The Author(s). Big data; Machine learning; Road flood risk; Smart resilience; Urban flood NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
621;A novel hybrid swarm optimized multilayer neural network for spatial prediction of flash floods in tropical areas using sentinel-1 SAR imagery and geospatial data;Flash floods are widely recognized as one of the most devastating natural hazards in the world, therefore prediction of flash flood-prone areas is crucial for public safety and emergency management. This research proposes a new methodology for spatial prediction of flash floods based on Sentinel-1 SAR imagery and a new hybrid machine learning technique. The SAR imagery is used to detect flash flood inundation areas, whereas the new machine learning technique, which is a hybrid of the firefly algorithm (FA), Levenberg–Marquardt (LM) backpropagation, and an artificial neural network (named as FA-LM-ANN), was used to construct the prediction model. The Bac Ha Bao Yen (BHBY) area in the northwestern region of Vietnam was used as a case study. Accordingly, a Geographical Information System (GIS) database was constructed using 12 input variables (elevation, slope, aspect, curvature, topographic wetness index, stream power index, toposhade, stream density, rainfall, normalized difference vegetation index, soil type, and lithology) and subsequently the output of flood inundation areas was mapped. Using the database and FA-LM-ANN, the flash flood model was trained and verified. The model performance was validated via various performance metrics including the classification accuracy rate, the area under the curve, precision, and recall. Then, the flash flood model that produced the highest performance was compared with benchmarks, indicating that the combination of FA and LM backpropagation is proven to be very effective and the proposed FA-LM-ANN is a new and useful tool for predicting flash flood susceptibility. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.;"Artificial neural network; Firefly algorithm; Flash floods; GIS; Levenberg-Marquardt backpropagation; Sentinel-1";"Backpropagation algorithms; Benchmarking; Bioluminescence; Flood control; Floods; Forecasting; Geographic information systems; Learning algorithms; Lithology; Machine learning; Multilayer neural networks; Neural networks; Optimization; Risk management; Tropics; Firefly algorithms; Flash flood; Geographical information system databases (GIS); Levenberg Marquardt backpropagation; Machine learning techniques; Normalized difference vegetation index; Sentinel-1; Topographic wetness index; Radar imaging";"A novel hybrid swarm optimized multilayer neural network for spatial prediction of flash floods in tropical areas using sentinel-1 SAR imagery and geospatial data Flash floods are widely recognized as one of the most devastating natural hazards in the world, therefore prediction of flash flood-prone areas is crucial for public safety and emergency management. This research proposes a new methodology for spatial prediction of flash floods based on Sentinel-1 SAR imagery and a new hybrid machine learning technique. The SAR imagery is used to detect flash flood inundation areas, whereas the new machine learning technique, which is a hybrid of the firefly algorithm (FA), Levenberg–Marquardt (LM) backpropagation, and an artificial neural network (named as FA-LM-ANN), was used to construct the prediction model. The Bac Ha Bao Yen (BHBY) area in the northwestern region of Vietnam was used as a case study. Accordingly, a Geographical Information System (GIS) database was constructed using 12 input variables (elevation, slope, aspect, curvature, topographic wetness index, stream power index, toposhade, stream density, rainfall, normalized difference vegetation index, soil type, and lithology) and subsequently the output of flood inundation areas was mapped. Using the database and FA-LM-ANN, the flash flood model was trained and verified. The model performance was validated via various performance metrics including the classification accuracy rate, the area under the curve, precision, and recall. Then, the flash flood model that produced the highest performance was compared with benchmarks, indicating that the combination of FA and LM backpropagation is proven to be very effective and the proposed FA-LM-ANN is a new and useful tool for predicting flash flood susceptibility. © 2018 by the authors. Licensee MDPI, Basel, Switzerland. Artificial neural network; Firefly algorithm; Flash floods; GIS; Levenberg-Marquardt backpropagation; Sentinel-1 Backpropagation algorithms; Benchmarking; Bioluminescence; Flood control; Floods; Forecasting; Geographic information systems; Learning algorithms; Lithology; Machine learning; Multilayer neural networks; Neural networks; Optimization; Risk management; Tropics; Firefly algorithms; Flash flood; Geographical information system databases (GIS); Levenberg Marquardt backpropagation; Machine learning techniques; Normalized difference vegetation index; Sentinel-1; Topographic wetness index; Radar imaging";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
622;Convolutional Neural Network - Optimized Moth Flame Algorithm for Shallow Landslide Susceptible Analysis;Convolutional neural network (CNN) is a widely used method in solving classification and regression applications in industries, engineering, and science. This study investigates the optimizing capability of a swarm intelligence algorithm named moth flame optimizer (MFO) for the optimal search of a CNN hyper-parameters (values of filters) and weights of fully connected layers. The proposed model was run with a 3-dimensional dataset (7 width $	imes 7$ height $	imes12$ depth), which was constructed through including seven neighbor pixels (vertically and horizontally) from landslide location and 12 predictor variables. Muong Te district, Lai Chau province, Vietnam was selected as the case study, as it had recently undergone severe impacts of landslides and flash floods. The performance of this proposed model was compared with conventional classifiers, i.e., Random forest, Random subspace, and CNN-optimized Adaptive gradient descend, by using standard metrics. The results showed that the CNN-optimized MFO (Root mean square error = 0.3685, Mean absolute error = 0.2888, Area under Receiver characteristic curve = 0.889 and Overall accuracy = 80.1056%) outperformed the benchmarked methods in all comparing indicators. Besides, the statistical test of difference was also carried out by using the Wilcoxon signed ranked test for non-parametric variables. With these statistical measurements, the proposed model could be used as an alternative solution for landslide susceptibility mapping to support local disaster preparedness plans. © 2013 IEEE.;"Convolutional neural network; landslide susceptibility; meta-heuristic algorithm; moth flame optimization algorithm";"Convolution; Curve fitting; Decision trees; Disaster prevention; Heuristic algorithms; Landslides; Mean square error; Optimization; Random forests; Conventional classifier; Disaster preparedness plans; Landslide susceptibility; Landslide susceptibility mapping; Meta heuristic algorithm; Optimization algorithms; Root mean square errors; Swarm intelligence algorithms; Convolutional neural networks";"Convolutional Neural Network - Optimized Moth Flame Algorithm for Shallow Landslide Susceptible Analysis Convolutional neural network (CNN) is a widely used method in solving classification and regression applications in industries, engineering, and science. This study investigates the optimizing capability of a swarm intelligence algorithm named moth flame optimizer (MFO) for the optimal search of a CNN hyper-parameters (values of filters) and weights of fully connected layers. The proposed model was run with a 3-dimensional dataset (7 width $	imes 7$ height $	imes12$ depth), which was constructed through including seven neighbor pixels (vertically and horizontally) from landslide location and 12 predictor variables. Muong Te district, Lai Chau province, Vietnam was selected as the case study, as it had recently undergone severe impacts of landslides and flash floods. The performance of this proposed model was compared with conventional classifiers, i.e., Random forest, Random subspace, and CNN-optimized Adaptive gradient descend, by using standard metrics. The results showed that the CNN-optimized MFO (Root mean square error = 0.3685, Mean absolute error = 0.2888, Area under Receiver characteristic curve = 0.889 and Overall accuracy = 80.1056%) outperformed the benchmarked methods in all comparing indicators. Besides, the statistical test of difference was also carried out by using the Wilcoxon signed ranked test for non-parametric variables. With these statistical measurements, the proposed model could be used as an alternative solution for landslide susceptibility mapping to support local disaster preparedness plans. © 2013 IEEE. Convolutional neural network; landslide susceptibility; meta-heuristic algorithm; moth flame optimization algorithm Convolution; Curve fitting; Decision trees; Disaster prevention; Heuristic algorithms; Landslides; Mean square error; Optimization; Random forests; Conventional classifier; Disaster preparedness plans; Landslide susceptibility; Landslide susceptibility mapping; Meta heuristic algorithm; Optimization algorithms; Root mean square errors; Swarm intelligence algorithms; Convolutional neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
623;Evaluation of Comprehensive Emergency Capacity to Urban Flood Disaster: An Example from Zhengzhou City in Henan Province, China;In the context of climate change and urbanization, increasing flood disasters leads to severe losses and impacts on urban inhabitants. In order to enhance urban capacity to cope with floods and reduce losses, the comprehensive emergency-response capacity to flood disaster (CERCF) was studied in Zhengzhou City, which is seriously affected by floods. Firstly, the evaluation index system of flood emergency capacity was constructed from three aspects, including pre-disaster prevention capacity, during-disaster disposal capacity and post-disaster recovery capacity. Secondly, the weight of each index was calculated by the combination of the entropy weight method and the coefficient of variation method, and the evaluation model was established by the comprehensive index method. Thirdly, the CERCF of Zhengzhou City was classified into three grades by the Jenks natural-breakpoint classification method. Finally, the contribution model was used to reveal the contribution factors of flood emergency capacity in Zhengzhou city. The following beneficial conclusions were drawn: (1) The overall CERCF of Zhengzhou City was on a low level. The proportions of the study area at low, medium and high levels were 58.33%, 33.33% and 8.34%, respectively. Spatially, the CERCF was high in central regions and low in in the west and east parts of Zhengzhou City. (2) It was found that PDPC and PDRC made the greatest contribution, while DDDC has a relatively low contribution degree. © 2022 by the authors.;"coefficient of variation; contribution analysis; disaster prevention and mitigation; flood disasters";"China; Henan; Zhengzhou; climate change; disaster management; flood control; mitigation";"Evaluation of Comprehensive Emergency Capacity to Urban Flood Disaster: An Example from Zhengzhou City in Henan Province, China In the context of climate change and urbanization, increasing flood disasters leads to severe losses and impacts on urban inhabitants. In order to enhance urban capacity to cope with floods and reduce losses, the comprehensive emergency-response capacity to flood disaster (CERCF) was studied in Zhengzhou City, which is seriously affected by floods. Firstly, the evaluation index system of flood emergency capacity was constructed from three aspects, including pre-disaster prevention capacity, during-disaster disposal capacity and post-disaster recovery capacity. Secondly, the weight of each index was calculated by the combination of the entropy weight method and the coefficient of variation method, and the evaluation model was established by the comprehensive index method. Thirdly, the CERCF of Zhengzhou City was classified into three grades by the Jenks natural-breakpoint classification method. Finally, the contribution model was used to reveal the contribution factors of flood emergency capacity in Zhengzhou city. The following beneficial conclusions were drawn: (1) The overall CERCF of Zhengzhou City was on a low level. The proportions of the study area at low, medium and high levels were 58.33%, 33.33% and 8.34%, respectively. Spatially, the CERCF was high in central regions and low in in the west and east parts of Zhengzhou City. (2) It was found that PDPC and PDRC made the greatest contribution, while DDDC has a relatively low contribution degree. © 2022 by the authors. coefficient of variation; contribution analysis; disaster prevention and mitigation; flood disasters China; Henan; Zhengzhou; climate change; disaster management; flood control; mitigation";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
624;Flash Flood Disaster Mitigation Through Environmental Education;Flash flood disasters often hit many areas in Indonesia and can generate various losses. These conditions are exacerbated by people’s low knowledge and interest in their environment. Therefore, if people’s environmental education increases, especially their flood disaster mitigation knowledge, they can be prepared and better protect themselves from such disasters. People’s environmental education must start from determining the reason of the flood disaster in their environment to discovering how to avoid the disaster. This research design is a one-group post-test design. The collected data in this research is a written test result about knowledge in flash flood disaster mitigation materials as an implementation in environmental education in the community. The research data results were analyzed by means of simple regression, logistics regression, and correlation. Based on this research, environmental education with disaster mitigation materials has a high understanding level. In the disaster step, the highest understanding is before it happens. The correlation between educational strata and gender on environmental education and disaster mitigation materials is low. Finally, the role of women in disaster mitigation needs to improve because they have a higher understanding than males of disaster mitigation materials. © 2022 Author.;"disaster; environmental education; flash flood; mitigation";NULL;"Flash Flood Disaster Mitigation Through Environmental Education Flash flood disasters often hit many areas in Indonesia and can generate various losses. These conditions are exacerbated by people’s low knowledge and interest in their environment. Therefore, if people’s environmental education increases, especially their flood disaster mitigation knowledge, they can be prepared and better protect themselves from such disasters. People’s environmental education must start from determining the reason of the flood disaster in their environment to discovering how to avoid the disaster. This research design is a one-group post-test design. The collected data in this research is a written test result about knowledge in flash flood disaster mitigation materials as an implementation in environmental education in the community. The research data results were analyzed by means of simple regression, logistics regression, and correlation. Based on this research, environmental education with disaster mitigation materials has a high understanding level. In the disaster step, the highest understanding is before it happens. The correlation between educational strata and gender on environmental education and disaster mitigation materials is low. Finally, the role of women in disaster mitigation needs to improve because they have a higher understanding than males of disaster mitigation materials. © 2022 Author. disaster; environmental education; flash flood; mitigation NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
625;Research on waterlogging early warning system based on adaptive urban flooding model;In recent years, with the global climate change, intra-urban rainstorms are frequent. The backward means of urban waterlogging prevention and control do not match with the high-speed urbanization process, bringing serious waterlogging disasters to major cities in China. The current mainstream urban flooding early warning system is an integrated system integrating various information technologies with urban rainfall and flood model as the theoretical basis. However, the urban rainfall and flooding model is affected by the lack of basic data, the complicated modeling process which is not easy to implement, and the poor flexibility in analyzing the actual urban waterlogging time series characteristics. Therefore, this paper designs and implements a waterlogging early warning system based on an adaptive urban flooding model by combining the respective advantages of data-driven technology and urban rainfall model for impervious areas in cities that are prone to flooding during short-duration rainstorms.  © 2023 IEEE.;"convolutional neural network; deep learning; gated recurrent unit; Urban flooding; urban stormwater model";"Climate change; Convolutional neural networks; Disaster prevention; Disasters; Rain; Recurrent neural networks; Storms; Convolutional neural network; Deep learning; Early Warning System; Flooding models; Gated recurrent unit; Rainfall modelling; Stormwater models; Urban flooding; Urban stormwater; Urban stormwater model; Floods";"Research on waterlogging early warning system based on adaptive urban flooding model In recent years, with the global climate change, intra-urban rainstorms are frequent. The backward means of urban waterlogging prevention and control do not match with the high-speed urbanization process, bringing serious waterlogging disasters to major cities in China. The current mainstream urban flooding early warning system is an integrated system integrating various information technologies with urban rainfall and flood model as the theoretical basis. However, the urban rainfall and flooding model is affected by the lack of basic data, the complicated modeling process which is not easy to implement, and the poor flexibility in analyzing the actual urban waterlogging time series characteristics. Therefore, this paper designs and implements a waterlogging early warning system based on an adaptive urban flooding model by combining the respective advantages of data-driven technology and urban rainfall model for impervious areas in cities that are prone to flooding during short-duration rainstorms.  © 2023 IEEE. convolutional neural network; deep learning; gated recurrent unit; Urban flooding; urban stormwater model Climate change; Convolutional neural networks; Disaster prevention; Disasters; Rain; Recurrent neural networks; Storms; Convolutional neural network; Deep learning; Early Warning System; Flooding models; Gated recurrent unit; Rainfall modelling; Stormwater models; Urban flooding; Urban stormwater; Urban stormwater model; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
626;Identifying flash flood hotspots with explainable machine learning using urban features;Pluvial flash floods are fast-moving hazards and causes significant disruptions in urban areas. This study presents interpretable machine learning models for predicting urban flash flood hotspots based on intertwined land and built environment features. Various features related to land and built environment characteristics are constructed using diverse datasets, and the occurrences of flash floods are captured using crowdsource data from the events. Using these features and datasets, the flash flood hotspots of cities are predicted with two ensemble models based on decision trees. The results demonstrate that the models can achieve good accuracy in identifying flooded/non-flooded locations. The model interpretation results indicate that land features related to hydrological and topological features have greater impacts on flash flood risk, than built environment features. The data-driven machine learning models presented in this study provide a useful tool for predicting flash flood hotspots based on the intertwined features of land and the built environment in cities to enable nowcasting and proactive monitoring of flash flood hotspots for emergency response and also inform integrated urban design and development towards flash flood risk reduction.  © 2023 Owner/Author(s).;"built environment; crowdsourced data; flash flood; machine learning; urban AI";"Crowdsourcing; Floods; Machine learning; Built environment; Crowdsourced data; Flash-floods; Hotspots; Land environments; Machine learning models; Machine-learning; Pluvials; Urban AI; Urban features; Decision trees";"Identifying flash flood hotspots with explainable machine learning using urban features Pluvial flash floods are fast-moving hazards and causes significant disruptions in urban areas. This study presents interpretable machine learning models for predicting urban flash flood hotspots based on intertwined land and built environment features. Various features related to land and built environment characteristics are constructed using diverse datasets, and the occurrences of flash floods are captured using crowdsource data from the events. Using these features and datasets, the flash flood hotspots of cities are predicted with two ensemble models based on decision trees. The results demonstrate that the models can achieve good accuracy in identifying flooded/non-flooded locations. The model interpretation results indicate that land features related to hydrological and topological features have greater impacts on flash flood risk, than built environment features. The data-driven machine learning models presented in this study provide a useful tool for predicting flash flood hotspots based on the intertwined features of land and the built environment in cities to enable nowcasting and proactive monitoring of flash flood hotspots for emergency response and also inform integrated urban design and development towards flash flood risk reduction.  © 2023 Owner/Author(s). built environment; crowdsourced data; flash flood; machine learning; urban AI Crowdsourcing; Floods; Machine learning; Built environment; Crowdsourced data; Flash-floods; Hotspots; Land environments; Machine learning models; Machine-learning; Pluvials; Urban AI; Urban features; Decision trees";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
627;A flash flood susceptibility prediction and partitioning method based on GeoDetector and random forest;Flash floods cause substantial economic losses and casualties worldwide. Modeling susceptibility to flash floods using hybrid models that combine statistical and machine learning methods is integral to flood mitigation strategies and disaster preparedness. Although the classification of flash flood conditioning factors is a critical step before applying such hybrid models, most previous studies have defaulted to using the natural breaks (NB) method for classification without exploring the potential impact of alternative discretization methods on model accuracy. Moreover, the classification system used to generate susceptibility maps determines the final appearance of the maps, which may influence decision-making tasks. In this context, this study introduces GeoDetector-based optimal discretization (OPGD) into factor classification and susceptibility partitioning. This approach is integrated with the Random Forest (RF) algorithm, resulting in the OPGD-RF model. Since the flexible number of classifications in OPGD-RF complicates objective comparisons, the GD-RF model was also constructed with a fixed number of classifications and compared with the NB-RF model built using the commonly used NB classification algorithm. The results show that GeoDetector-based models exhibit superior predictive performance in terms of area under the receiver operating characteristic curve (AUC = 0.946 and 0.934), followed by the NB-based model (AUC = 0.931). Additionally, OPGD-based partitioning results align well with historical flash flood events, explaining 64% of their spatial distribution. The application of GeoDetector provides a new perspective for improving the accuracy of integrated modeling and generating reliable susceptibility maps, in addition to providing effective support for rational resource allocation and targeted defense measures. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.;"Flash flood; GeoDetector; Qinling Mountains; Random forest; Susceptibility mapping";"China; Qinling Mountains; Banks (bodies of water); Floods; Geodesy; Higher order statistics; Machine learning; Random forests; Stability criteria; Flash-floods; Geodetector; Hybrid model; Partitioning methods; Prediction methods; Qinling mountains; Random forest modeling; Random forests; Susceptibility mapping; Susceptibility maps; algorithm; classification; comparative study; flash flood; mapping; numerical model; prediction; spatial distribution; Resource allocation";"A flash flood susceptibility prediction and partitioning method based on GeoDetector and random forest Flash floods cause substantial economic losses and casualties worldwide. Modeling susceptibility to flash floods using hybrid models that combine statistical and machine learning methods is integral to flood mitigation strategies and disaster preparedness. Although the classification of flash flood conditioning factors is a critical step before applying such hybrid models, most previous studies have defaulted to using the natural breaks (NB) method for classification without exploring the potential impact of alternative discretization methods on model accuracy. Moreover, the classification system used to generate susceptibility maps determines the final appearance of the maps, which may influence decision-making tasks. In this context, this study introduces GeoDetector-based optimal discretization (OPGD) into factor classification and susceptibility partitioning. This approach is integrated with the Random Forest (RF) algorithm, resulting in the OPGD-RF model. Since the flexible number of classifications in OPGD-RF complicates objective comparisons, the GD-RF model was also constructed with a fixed number of classifications and compared with the NB-RF model built using the commonly used NB classification algorithm. The results show that GeoDetector-based models exhibit superior predictive performance in terms of area under the receiver operating characteristic curve (AUC = 0.946 and 0.934), followed by the NB-based model (AUC = 0.931). Additionally, OPGD-based partitioning results align well with historical flash flood events, explaining 64% of their spatial distribution. The application of GeoDetector provides a new perspective for improving the accuracy of integrated modeling and generating reliable susceptibility maps, in addition to providing effective support for rational resource allocation and targeted defense measures. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025. Flash flood; GeoDetector; Qinling Mountains; Random forest; Susceptibility mapping China; Qinling Mountains; Banks (bodies of water); Floods; Geodesy; Higher order statistics; Machine learning; Random forests; Stability criteria; Flash-floods; Geodetector; Hybrid model; Partitioning methods; Prediction methods; Qinling mountains; Random forest modeling; Random forests; Susceptibility mapping; Susceptibility maps; algorithm; classification; comparative study; flash flood; mapping; numerical model; prediction; spatial distribution; Resource allocation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
628;A new avenue to improve the performance of integrated modeling for flash flood susceptibility assessment: Applying cluster algorithms;Flash flood is one of the most severe natural disasters around the world, and has caused sizeable economic losses and countless death. Assessing flash flood susceptibility by hybrid models of statistical and machine learning methods is essential for flood mitigation strategies and disaster preparedness. Although classifying the flash flood conditioning factors becomes a crucial step before applying these hybrid models, their impact on the accuracy of integrated modeling is still unclear. Most previous studies used natural break classification (NBC) and quantile classification methods by default to conduct the classification, but more classification methods have not been tried. In this context, this study introduced three clustering algorithms of K-Means, Expectation Maximization, and ISOMaximum likelihood algorithm (ISOMax) into the classification of factors, and compared them to NBC and quantile classification. To test the impact of classification methods on integrated modeling, these classification results were applied into the construction of three hybrid models (i.e., the integrating of frequency ratio with support vector machines, random forest, and bayesian-regularization neural networks). Then, the accuracy of these hybrid models was evaluated by using ROC curves and statistical indicators. The classification results show that the clustering intervals in the same factor varied with classification algorithms. It can be found from the model performance evaluation results that different classification algorithms will lead to discrepancies in accuracy of integrated modeling. Compared to NBC, the ISOMax allows a better fitting and prediction ability of hybrid models in this study. The application of clustering algorithm provides a new perspective for improving the accuracy of integrated modeling. © 2022 The Authors;"Clustering algorithms; Flash flood susceptibility; Integrated modeling; Machine learning models; Statistical models";"Bayesian networks; Disaster prevention; Disasters; Floods; Forestry; K-means clustering; Learning systems; Losses; Maximum principle; Statistical tests; Classification algorithm; Classification methods; Classification results; Flash flood susceptibility; Flash-floods; Hybrid model; Integrated modeling; Machine learning models; Quantile classifications; Statistic modeling; algorithm; assessment method; flash flood; flood; flood control; integrated approach; machine learning; strategic approach; Support vector machines";"A new avenue to improve the performance of integrated modeling for flash flood susceptibility assessment: Applying cluster algorithms Flash flood is one of the most severe natural disasters around the world, and has caused sizeable economic losses and countless death. Assessing flash flood susceptibility by hybrid models of statistical and machine learning methods is essential for flood mitigation strategies and disaster preparedness. Although classifying the flash flood conditioning factors becomes a crucial step before applying these hybrid models, their impact on the accuracy of integrated modeling is still unclear. Most previous studies used natural break classification (NBC) and quantile classification methods by default to conduct the classification, but more classification methods have not been tried. In this context, this study introduced three clustering algorithms of K-Means, Expectation Maximization, and ISOMaximum likelihood algorithm (ISOMax) into the classification of factors, and compared them to NBC and quantile classification. To test the impact of classification methods on integrated modeling, these classification results were applied into the construction of three hybrid models (i.e., the integrating of frequency ratio with support vector machines, random forest, and bayesian-regularization neural networks). Then, the accuracy of these hybrid models was evaluated by using ROC curves and statistical indicators. The classification results show that the clustering intervals in the same factor varied with classification algorithms. It can be found from the model performance evaluation results that different classification algorithms will lead to discrepancies in accuracy of integrated modeling. Compared to NBC, the ISOMax allows a better fitting and prediction ability of hybrid models in this study. The application of clustering algorithm provides a new perspective for improving the accuracy of integrated modeling. © 2022 The Authors Clustering algorithms; Flash flood susceptibility; Integrated modeling; Machine learning models; Statistical models Bayesian networks; Disaster prevention; Disasters; Floods; Forestry; K-means clustering; Learning systems; Losses; Maximum principle; Statistical tests; Classification algorithm; Classification methods; Classification results; Flash flood susceptibility; Flash-floods; Hybrid model; Integrated modeling; Machine learning models; Quantile classifications; Statistic modeling; algorithm; assessment method; flash flood; flood; flood control; integrated approach; machine learning; strategic approach; Support vector machines";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.2;Hydrological;1;Prevention
629;Application of artificial intelligence method in urban flooding warning and forecast;Heavy rain is one of the main reasons for urban flooding. In the face of the floods, the application the artificial intelligence (AI) can provide timely flood forecasting, and combined with space display, providing spatial information on possible flooding disasters, allowing flooding disaster management units to have more early warning time to respond and provide. This paper uses the spatial rainfall uncertainty analysis and the hydrological, hydraulic and urban inundation model simulation results in urban areas to build big data databases, such as 1000 sets of different spatial rainfall and urban flooding simulation results. Through the AI data classification, the rainfall threshold value setting and feature parameter train. After the completion, the real-time and forecast rainfall data can use to forecast the rainfall and real-time display of the urban flooding. Finally, the real-time CCTV image data and the flooding analysis result can use to provide the urban flooded image and the flooding depth. And strive for effective disaster prevention response time. © 2019, National Technical University of Athens. All rights reserved.;"Artificial intelligence; Flood forecasting; Real-time display; Urban flooding";NULL;"Application of artificial intelligence method in urban flooding warning and forecast Heavy rain is one of the main reasons for urban flooding. In the face of the floods, the application the artificial intelligence (AI) can provide timely flood forecasting, and combined with space display, providing spatial information on possible flooding disasters, allowing flooding disaster management units to have more early warning time to respond and provide. This paper uses the spatial rainfall uncertainty analysis and the hydrological, hydraulic and urban inundation model simulation results in urban areas to build big data databases, such as 1000 sets of different spatial rainfall and urban flooding simulation results. Through the AI data classification, the rainfall threshold value setting and feature parameter train. After the completion, the real-time and forecast rainfall data can use to forecast the rainfall and real-time display of the urban flooding. Finally, the real-time CCTV image data and the flooding analysis result can use to provide the urban flooded image and the flooding depth. And strive for effective disaster prevention response time. © 2019, National Technical University of Athens. All rights reserved. Artificial intelligence; Flood forecasting; Real-time display; Urban flooding NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
630;SPATIAL PREDICTION OF FLOOD IN KUALA LUMPUR CITY OF MALAYSIA USING LOGISTIC REGRESSION;Flooding is one of the most prevalent natural disasters affecting people worldwide. Flooding is a devastating natural disaster in Malaysia regarding the number of people affected, socioeconomic damage, severity, and scale of the impact. Urban flooding is currently a major concern due to the possible consequences and frequency with which it occurs in urban areas as urbanization and population increase. Due to the paved surfaces, paved roads, high population, and buildings that prevent water infiltration and movement to the nearby river, urban floods pose a significant threat to the sustainability of lives and properties in the city. The recent floods in Kuala Lumpur in December 2021 and January 2022 affected many buildings, infrastructure, and lives. As a result, this city needs to model the susceptibility of flood-prone areas for an early warning system against future flood hazards in Kuala Lumpur. This is because flooding can never be eradicated but can be minimized and managed. Therefore, this study integrates geospatial technology and a statistical model (logistic regression) to assess flood hazards in Kuala Lumpur. Ten flood conditioning factors such as altitude, slope, TWI, drainage density, distance to river, LULC, NDVI, NDWI, rainfall and MNDWI were used to predict the areas susceptible to flood. The prediction shows an overall accuracy of 0.84, precision of 0.91, recall of 0.72, and F1-score of 0.80. Distance to river, MNDWI, TWI, and LULC are the critical variables that showed high significance in the model prediction. Thus, stakeholders should prioritize urban planning and increase the drainage system to avoid flood effects.  Copyright © 2023 A. Tella et al.;"Flood Hazards; Flood Susceptibility; Logistic Regression; Malaysia; Urban Flooding";"Disasters; Forecasting; Hazards; Rivers; Urban planning; Flood hazards; Flood susceptibility; Floodings; Logistics regressions; Malaysia; Natural disasters; Number of peoples; Socio-economics; Spatial prediction; Urban flooding; Floods";"SPATIAL PREDICTION OF FLOOD IN KUALA LUMPUR CITY OF MALAYSIA USING LOGISTIC REGRESSION Flooding is one of the most prevalent natural disasters affecting people worldwide. Flooding is a devastating natural disaster in Malaysia regarding the number of people affected, socioeconomic damage, severity, and scale of the impact. Urban flooding is currently a major concern due to the possible consequences and frequency with which it occurs in urban areas as urbanization and population increase. Due to the paved surfaces, paved roads, high population, and buildings that prevent water infiltration and movement to the nearby river, urban floods pose a significant threat to the sustainability of lives and properties in the city. The recent floods in Kuala Lumpur in December 2021 and January 2022 affected many buildings, infrastructure, and lives. As a result, this city needs to model the susceptibility of flood-prone areas for an early warning system against future flood hazards in Kuala Lumpur. This is because flooding can never be eradicated but can be minimized and managed. Therefore, this study integrates geospatial technology and a statistical model (logistic regression) to assess flood hazards in Kuala Lumpur. Ten flood conditioning factors such as altitude, slope, TWI, drainage density, distance to river, LULC, NDVI, NDWI, rainfall and MNDWI were used to predict the areas susceptible to flood. The prediction shows an overall accuracy of 0.84, precision of 0.91, recall of 0.72, and F1-score of 0.80. Distance to river, MNDWI, TWI, and LULC are the critical variables that showed high significance in the model prediction. Thus, stakeholders should prioritize urban planning and increase the drainage system to avoid flood effects.  Copyright © 2023 A. Tella et al. Flood Hazards; Flood Susceptibility; Logistic Regression; Malaysia; Urban Flooding Disasters; Forecasting; Hazards; Rivers; Urban planning; Flood hazards; Flood susceptibility; Floodings; Logistics regressions; Malaysia; Natural disasters; Number of peoples; Socio-economics; Spatial prediction; Urban flooding; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
631;RAPIDS: Early warning system for urban flooding and water quality hazards;"This paper describes the application of Artificial Neural Networks (ANNs) as Data Driven Models (DDMs) to predict urban flooding in real-time based on weather radar and/or raingauge rainfall data. A time-lagged ANN is configured for prediction of flooding at sewerage nodes and outfalls based on input parameters including rainfall. In the absence of observed flood data, a hydrodynamic simulator may be used to predict flooding surcharge levels at nodes of interest in sewer networks and thus provide the target data for training and testing the ANN. The model, once trained, acts as a rapid surrogate for the hydrodynamic simulator and can thus be used as part of an urban flooding Early Warning System (EWS). Predicted rainfall over the catchment is required as input, to extend prediction times to operationally useful levels. Both flood-level analogue and flood-severity classification schemes are implemented. An initial case study using Keighley, W Yorks, UK demonstrated proof-of-concept. Three further case studies for UK cities of different sizes explore issues of soil-moisture, early operation of pumps as flood-mitigation/prevention strategy and spatially variable rainfall. We investigate the use of ANNs for nowcasting of rainfall based on the relationship between radar data and recorded rainfall history; a feature extraction scheme is described. This would allow the two ANNs to be cascaded to predict flooding in real-time based on current weather radar Quantitative Precipitation Estimates (QPE). We also briefly describe the extension of this methodology to Bathing Water Quality (BWQ) prediction.";"ANN; Early warning system; Flood risk; Machine learning; Neural network; Nowcasting; Prediction; Rainfall; Urban flood";NULL;"RAPIDS: Early warning system for urban flooding and water quality hazards This paper describes the application of Artificial Neural Networks (ANNs) as Data Driven Models (DDMs) to predict urban flooding in real-time based on weather radar and/or raingauge rainfall data. A time-lagged ANN is configured for prediction of flooding at sewerage nodes and outfalls based on input parameters including rainfall. In the absence of observed flood data, a hydrodynamic simulator may be used to predict flooding surcharge levels at nodes of interest in sewer networks and thus provide the target data for training and testing the ANN. The model, once trained, acts as a rapid surrogate for the hydrodynamic simulator and can thus be used as part of an urban flooding Early Warning System (EWS). Predicted rainfall over the catchment is required as input, to extend prediction times to operationally useful levels. Both flood-level analogue and flood-severity classification schemes are implemented. An initial case study using Keighley, W Yorks, UK demonstrated proof-of-concept. Three further case studies for UK cities of different sizes explore issues of soil-moisture, early operation of pumps as flood-mitigation/prevention strategy and spatially variable rainfall. We investigate the use of ANNs for nowcasting of rainfall based on the relationship between radar data and recorded rainfall history; a feature extraction scheme is described. This would allow the two ANNs to be cascaded to predict flooding in real-time based on current weather radar Quantitative Precipitation Estimates (QPE). We also briefly describe the extension of this methodology to Bathing Water Quality (BWQ) prediction. ANN; Early warning system; Flood risk; Machine learning; Neural network; Nowcasting; Prediction; Rainfall; Urban flood NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
632;Deep learning for flood forecasting and monitoring in urban environments;This paper describes the core computational mechanisms used by an urban flood forecasting and monitoring platform developed as part of a UK Newton Fund project in Malaysia. FLUD-FLood monitoring and forecasting platform for Urban Deployment - is a novel system aiming to deliver an effective and low cost urban flood forecasting solution, which is able to accurately forecast flood risk at street level, and deliver optimized recommendations to the relevant authorities as well as an early warning alerts to members of the public. This platform is based on a hybrid Deep Learning and Fuzzy Logic based architecture. As demonstrated by the experimental results and the analysis presented in this paper, this architecture enables the proposed system to account for factors that are not included in other modern flood forecasting systems, and simultaneously process high volumes of data originating from diverse data sources, in order to deliver accurate predictions concerning urban flood events. © 2019 IEEE.;"Deep learning; Flood monitoring and forecasting; Fuzzy logic; Urban flooding";"Computer circuits; Deep learning; Flood control; Fuzzy logic; Learning systems; Weather forecasting; Accurate prediction; Data-sources; Early warning; Flood forecasting; Flood monitoring; Monitoring platform; Urban environments; Urban flooding; Floods";"Deep learning for flood forecasting and monitoring in urban environments This paper describes the core computational mechanisms used by an urban flood forecasting and monitoring platform developed as part of a UK Newton Fund project in Malaysia. FLUD-FLood monitoring and forecasting platform for Urban Deployment - is a novel system aiming to deliver an effective and low cost urban flood forecasting solution, which is able to accurately forecast flood risk at street level, and deliver optimized recommendations to the relevant authorities as well as an early warning alerts to members of the public. This platform is based on a hybrid Deep Learning and Fuzzy Logic based architecture. As demonstrated by the experimental results and the analysis presented in this paper, this architecture enables the proposed system to account for factors that are not included in other modern flood forecasting systems, and simultaneously process high volumes of data originating from diverse data sources, in order to deliver accurate predictions concerning urban flood events. © 2019 IEEE. Deep learning; Flood monitoring and forecasting; Fuzzy logic; Urban flooding Computer circuits; Deep learning; Flood control; Fuzzy logic; Learning systems; Weather forecasting; Accurate prediction; Data-sources; Early warning; Flood forecasting; Flood monitoring; Monitoring platform; Urban environments; Urban flooding; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
633;Real-Time Flash-Flood Monitoring, Alerting and Forecasting System using Data Mining and wireless sensor Network;In this paper, we present Real-Time Flash-Flood Monitoring, Alerting and Forecasting System using Data mining and wireless sensor Network. Our System not only Measures River Water level and Different weather conditions such as temperature, humidity and vibration through wireless sensor nodes but also we can forecast possibility of future disasters by using Data mining algorithm on our database. Hazardous condition information and forecasted information is employed for early-warning with the use of server to different types of mobile phones. Android mobile phones or smart phone connected via a web service. That's allow the users to interact with the system via android application b Non-smart phones are connected via SMS, especially for rural area users. We use micro-controller (AVR ATmega32) for connecting the server to the different sensors like Water Level sensor, Temperature sensor (LM35), Humidity sensor (SY-HS230) and Vibration sensor with the help of MAX 232 Level Shifter and ADC. We apply Naive Bayes Data mining on our database for forecasting, Naive Bayes algorithm is simple probabilistic classifier which finds output for YES and NO Probability and we also use WSN for collecting input values from different environmental conditions. © 2015 IEEE.;"ADC; AVR ATmega32; Data mining; LM35; MAX 232; Naïve Bayes; Short Message Service(SMS); SY-HS230; Wireless Sensor Network(WSN)";"Algorithms; Android (operating system); Cellular telephone systems; Cellular telephones; Classification (of information); Classifiers; Floods; Forecasting; Humidity control; Mobile devices; Mobile phones; Sensor nodes; Signal processing; Smartphones; Telephone sets; Text messaging; Water levels; Web services; Wireless sensor networks; Avr atmega32; LM35; MAX 232; Short message services; SY-HS230; Data mining";"Real-Time Flash-Flood Monitoring, Alerting and Forecasting System using Data Mining and wireless sensor Network In this paper, we present Real-Time Flash-Flood Monitoring, Alerting and Forecasting System using Data mining and wireless sensor Network. Our System not only Measures River Water level and Different weather conditions such as temperature, humidity and vibration through wireless sensor nodes but also we can forecast possibility of future disasters by using Data mining algorithm on our database. Hazardous condition information and forecasted information is employed for early-warning with the use of server to different types of mobile phones. Android mobile phones or smart phone connected via a web service. That's allow the users to interact with the system via android application b Non-smart phones are connected via SMS, especially for rural area users. We use micro-controller (AVR ATmega32) for connecting the server to the different sensors like Water Level sensor, Temperature sensor (LM35), Humidity sensor (SY-HS230) and Vibration sensor with the help of MAX 232 Level Shifter and ADC. We apply Naive Bayes Data mining on our database for forecasting, Naive Bayes algorithm is simple probabilistic classifier which finds output for YES and NO Probability and we also use WSN for collecting input values from different environmental conditions. © 2015 IEEE. ADC; AVR ATmega32; Data mining; LM35; MAX 232; Naïve Bayes; Short Message Service(SMS); SY-HS230; Wireless Sensor Network(WSN) Algorithms; Android (operating system); Cellular telephone systems; Cellular telephones; Classification (of information); Classifiers; Floods; Forecasting; Humidity control; Mobile devices; Mobile phones; Sensor nodes; Signal processing; Smartphones; Telephone sets; Text messaging; Water levels; Web services; Wireless sensor networks; Avr atmega32; LM35; MAX 232; Short message services; SY-HS230; Data mining";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
634;Evaluating the Utility of Selected Machine Learning Models for Predicting Stormwater Levels in Small Streams;The consequences of climate change include extreme weather events, such as heavy rainfall. As a result, many places around the world are experiencing an increase in flood risk. The aim of this research was to assess the usefulness of selected machine learning models, including artificial neural networks (ANNs) and eXtreme Gradient Boosting (XGBoost) v2.0.3., for predicting peak stormwater levels in a small stream. The innovation of the research results from the combination of the specificity of small watersheds with machine learning techniques and the use of SHapley Additive exPlanations (SHAP) analysis, which enabled the identification of key factors, such as rainfall depth and meteorological data, significantly affect the accuracy of forecasts. The analysis showed the superiority of ANN models (R2 = 0.803–0.980, RMSE = 1.547–4.596) over XGBoost v2.0.3. (R2 = 0.796–0.951, RMSE = 2.304–4.872) in terms of forecasting effectiveness for the analyzed small stream. In addition, conducting the SHAP analysis allowed for the identification of the most crucial factors influencing forecast accuracy. The key parameters affecting the predictions included rainfall depth, stormwater level, and meteorological data such as air temperature and dew point temperature for the last day. Although the study focused on a specific stream, the methodology can be adapted for other watersheds. The results could significantly contribute to improving real-time flood warning systems, enabling local authorities and emergency management agencies to plan responses to flood threats more accurately and in a timelier manner. Additionally, the use of these models can help protect infrastructure such as roads and bridges by better predicting potential threats and enabling the implementation of appropriate preventive measures. Finally, these results can be used to inform local communities about flood risk and recommended precautions, thereby increasing awareness and preparedness for flash floods. © 2024 by the authors.;"flood hazard assessment; regional implementation; urban flash floods";"air temperature; climate change; dew point; flash flood; hazard assessment; machine learning; rainfall; stormwater";"Evaluating the Utility of Selected Machine Learning Models for Predicting Stormwater Levels in Small Streams The consequences of climate change include extreme weather events, such as heavy rainfall. As a result, many places around the world are experiencing an increase in flood risk. The aim of this research was to assess the usefulness of selected machine learning models, including artificial neural networks (ANNs) and eXtreme Gradient Boosting (XGBoost) v2.0.3., for predicting peak stormwater levels in a small stream. The innovation of the research results from the combination of the specificity of small watersheds with machine learning techniques and the use of SHapley Additive exPlanations (SHAP) analysis, which enabled the identification of key factors, such as rainfall depth and meteorological data, significantly affect the accuracy of forecasts. The analysis showed the superiority of ANN models (R2 = 0.803–0.980, RMSE = 1.547–4.596) over XGBoost v2.0.3. (R2 = 0.796–0.951, RMSE = 2.304–4.872) in terms of forecasting effectiveness for the analyzed small stream. In addition, conducting the SHAP analysis allowed for the identification of the most crucial factors influencing forecast accuracy. The key parameters affecting the predictions included rainfall depth, stormwater level, and meteorological data such as air temperature and dew point temperature for the last day. Although the study focused on a specific stream, the methodology can be adapted for other watersheds. The results could significantly contribute to improving real-time flood warning systems, enabling local authorities and emergency management agencies to plan responses to flood threats more accurately and in a timelier manner. Additionally, the use of these models can help protect infrastructure such as roads and bridges by better predicting potential threats and enabling the implementation of appropriate preventive measures. Finally, these results can be used to inform local communities about flood risk and recommended precautions, thereby increasing awareness and preparedness for flash floods. © 2024 by the authors. flood hazard assessment; regional implementation; urban flash floods air temperature; climate change; dew point; flash flood; hazard assessment; machine learning; rainfall; stormwater";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
635;Flash flood risk evaluation based on variable fuzzy method and fuzzy clustering analysis;Flash flood is one of the most significant natural disasters in China, particularly in mountainous area, causing heavy economic damage and casualties of life. For numerous small hilly basins that need flash flood prevention and control with limited funds, it is necessary to give a priority order or to determine which basin needs to be harnessed firstly. Flash flood risk assessment is critical to an efficient flash flood management. Among many flash flood risk evaluation methods in literatures, variable fuzzy method (VFM) was chosen in this paper. To verify the results of VFM, fuzzy clustering analysis (FCA) is also used. First, taking Licheng county with 119 small basins in China as an example, 9 indexes were identified among index system, based on disaster-breeding environment (or underlying surface conditions) of small basin in hilly region. Risk levels are divided into three grading levels such as high, medium and low. Second, VFM was introduced, and the flash flood risk grade eigenvalue (H) of each small basin was calculated. The results show that no small basin belongs to high risk level, 14 basins belong to low risk level, and the remaining 105 small basins belong to medium risk level. Third, FCA was used to verify the result of VFM. The results of two methods show that they are nearly in consistence. This paper shows that VFM is feasible for flash flood risk evaluation. Finally, the priorities for flash flood mitigation of 119 small watersheds in Licheng county are mapped out, which will provide effective help for flood disaster mitigation of small basin. © 2019 - IOS Press and the authors. All rights reserved.;"disaster-breeding environment; flash flood risk; fuzzy clustering analysis; small basin; Variable fuzzy method";"Disasters; Eigenvalues and eigenfunctions; Flood control; Fuzzy clustering; Grading; Risk assessment; Breeding environments; Flash flood; Fuzzy clustering analysis; Fuzzy methods; small basin; Floods";"Flash flood risk evaluation based on variable fuzzy method and fuzzy clustering analysis Flash flood is one of the most significant natural disasters in China, particularly in mountainous area, causing heavy economic damage and casualties of life. For numerous small hilly basins that need flash flood prevention and control with limited funds, it is necessary to give a priority order or to determine which basin needs to be harnessed firstly. Flash flood risk assessment is critical to an efficient flash flood management. Among many flash flood risk evaluation methods in literatures, variable fuzzy method (VFM) was chosen in this paper. To verify the results of VFM, fuzzy clustering analysis (FCA) is also used. First, taking Licheng county with 119 small basins in China as an example, 9 indexes were identified among index system, based on disaster-breeding environment (or underlying surface conditions) of small basin in hilly region. Risk levels are divided into three grading levels such as high, medium and low. Second, VFM was introduced, and the flash flood risk grade eigenvalue (H) of each small basin was calculated. The results show that no small basin belongs to high risk level, 14 basins belong to low risk level, and the remaining 105 small basins belong to medium risk level. Third, FCA was used to verify the result of VFM. The results of two methods show that they are nearly in consistence. This paper shows that VFM is feasible for flash flood risk evaluation. Finally, the priorities for flash flood mitigation of 119 small watersheds in Licheng county are mapped out, which will provide effective help for flood disaster mitigation of small basin. © 2019 - IOS Press and the authors. All rights reserved. disaster-breeding environment; flash flood risk; fuzzy clustering analysis; small basin; Variable fuzzy method Disasters; Eigenvalues and eigenfunctions; Flood control; Fuzzy clustering; Grading; Risk assessment; Breeding environments; Flash flood; Fuzzy clustering analysis; Fuzzy methods; small basin; Floods";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.2;Hydrological;1;Prevention
636;Trash Detection on Water Channels;Rivers and canals flowing through cities are often used illegally for dumping trash that contaminates freshwater channels, causes blockage in sewerage leading to urban flooding. The dumped trash is often found floating on the water surface. We propose to automatically identify this trash through visual inspection with the eventual goal of quantification, an early warning system to avoid blockages and urban flooding. The trash could be disfigured, partially submerged, or clumped together with other objects which obscure its shape and appearance. Thus, we consider surface trash as a blob detection problem that could either be solved as object detection or image segmentation or both. To this extent, we evaluate and compare several deep-learning-based object detection and segmentation algorithms. Unlike ocean trash, to the best of our knowledge, there is no large dataset on urban trash on water channels. Thus, using IoT-based camera nodes at multiple water channels, we collected a large dataset containing 48, 450 trash objects annotated for both bounding box and segmentation (the dataset will be made publicly available (Dataset is available at https://cvlab.lums.edu.pk/watertrash/ )). In addition, we also propose modifications in state-of-the-art detection and segmentation algorithms to cater to an issue such as partially submerged, varying object sizes, and edge-based computing. © 2021, Springer Nature Switzerland AG.;"Object detection; Segmentation; Trash";"Deep learning; Floods; Image segmentation; Large dataset; Object recognition; Early Warning System; Fresh Water; Large datasets; Segmentation; Segmentation algorithms; Trash; Urban flooding; Visual inspection; Water channels; Water surface; Object detection";"Trash Detection on Water Channels Rivers and canals flowing through cities are often used illegally for dumping trash that contaminates freshwater channels, causes blockage in sewerage leading to urban flooding. The dumped trash is often found floating on the water surface. We propose to automatically identify this trash through visual inspection with the eventual goal of quantification, an early warning system to avoid blockages and urban flooding. The trash could be disfigured, partially submerged, or clumped together with other objects which obscure its shape and appearance. Thus, we consider surface trash as a blob detection problem that could either be solved as object detection or image segmentation or both. To this extent, we evaluate and compare several deep-learning-based object detection and segmentation algorithms. Unlike ocean trash, to the best of our knowledge, there is no large dataset on urban trash on water channels. Thus, using IoT-based camera nodes at multiple water channels, we collected a large dataset containing 48, 450 trash objects annotated for both bounding box and segmentation (the dataset will be made publicly available (Dataset is available at https://cvlab.lums.edu.pk/watertrash/ )). In addition, we also propose modifications in state-of-the-art detection and segmentation algorithms to cater to an issue such as partially submerged, varying object sizes, and edge-based computing. © 2021, Springer Nature Switzerland AG. Object detection; Segmentation; Trash Deep learning; Floods; Image segmentation; Large dataset; Object recognition; Early Warning System; Fresh Water; Large datasets; Segmentation; Segmentation algorithms; Trash; Urban flooding; Visual inspection; Water channels; Water surface; Object detection";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
637;Towards generic real-time mapping algorithms for environmental monitoring and emergency detection;Real-time analysis of data reported by environmental monitoring networks poses a number of challenges, one of which is the conversion of point measurements of phenomena that display some spatial dependence into maps. This is the case for the many variables that cannot be monitored efficiently over large regions by satellites. Environmental pollutants, radiation levels, rainfall fields and seismic activity are but a few of these variables that are usually interpolated for the production of maps. These maps will then further serve as an essential support for decision-making. Ideally, in order to allow real-time assessments and minimize human intervention in case of hazards and emergencies that are frequently linked to the above mentioned variables (e.g. air pollution peaks, nuclear accidents, flash-floods, earthquakes), these maps should be established in near real time and thus automatically. The ability of real-time mapping systems running in the routine mode to be able to cope with extreme events is not straightforward, and few systems are today used automatically for both monitoring the environment and triggering early warnings in case of necessity. Alternatively, adopting a decision-centered view of environmental monitoring and mapping systems allows us to re-formulate their final objective as a classification problem that consists of discriminating routine against emergency conditions, or background information against outliers. It is the purpose of this paper to give an overview of the main challenges for developing and evaluating automatic mapping systems for critical environmental variables, as well as to discuss steps toward the development of generic real-time mapping algorithms. © Springer-Verlag 2007.;"Automatic real-time mapping; Environmental monitoring; Hot-spot detection; Spatial interpolation";"Accidents; Air quality; Algorithms; Animal cell culture; Computer networks; Conformal mapping; Decision making; Environmental engineering; Environmental management; Environmental protection; Evolutionary algorithms; Hazardous materials; Information systems; Interpolation; Maps; Metropolitan area networks; Monitoring; Network protocols; Numerical analysis; Optical projectors; Pollution; Problem solving; Reactor cores; Rhenium; Seismology; Variational techniques; Well stimulation; Automatic mapping systems; Background information; classificatio n problems; emergency conditions; Environmental monitoring; Environmental pollutants; Environmental variables; Essential support; extreme events; Final objective; Flash-floods; Human intervention; In order; Large regions; mapping systems; Near-real time (NRT); Nuclear accidents; Point measurements; radiation levels; rainfall fields; Real time analysis; Real time assessments; Real-time mapping; Running in; Seismic activities; Spatial dependence; Springer (CO); algorithm; decision making; environmental monitoring; hot spot; interpolation; map; mapping; spatial analysis; Real time systems";"Towards generic real-time mapping algorithms for environmental monitoring and emergency detection Real-time analysis of data reported by environmental monitoring networks poses a number of challenges, one of which is the conversion of point measurements of phenomena that display some spatial dependence into maps. This is the case for the many variables that cannot be monitored efficiently over large regions by satellites. Environmental pollutants, radiation levels, rainfall fields and seismic activity are but a few of these variables that are usually interpolated for the production of maps. These maps will then further serve as an essential support for decision-making. Ideally, in order to allow real-time assessments and minimize human intervention in case of hazards and emergencies that are frequently linked to the above mentioned variables (e.g. air pollution peaks, nuclear accidents, flash-floods, earthquakes), these maps should be established in near real time and thus automatically. The ability of real-time mapping systems running in the routine mode to be able to cope with extreme events is not straightforward, and few systems are today used automatically for both monitoring the environment and triggering early warnings in case of necessity. Alternatively, adopting a decision-centered view of environmental monitoring and mapping systems allows us to re-formulate their final objective as a classification problem that consists of discriminating routine against emergency conditions, or background information against outliers. It is the purpose of this paper to give an overview of the main challenges for developing and evaluating automatic mapping systems for critical environmental variables, as well as to discuss steps toward the development of generic real-time mapping algorithms. © Springer-Verlag 2007. Automatic real-time mapping; Environmental monitoring; Hot-spot detection; Spatial interpolation Accidents; Air quality; Algorithms; Animal cell culture; Computer networks; Conformal mapping; Decision making; Environmental engineering; Environmental management; Environmental protection; Evolutionary algorithms; Hazardous materials; Information systems; Interpolation; Maps; Metropolitan area networks; Monitoring; Network protocols; Numerical analysis; Optical projectors; Pollution; Problem solving; Reactor cores; Rhenium; Seismology; Variational techniques; Well stimulation; Automatic mapping systems; Background information; classificatio n problems; emergency conditions; Environmental monitoring; Environmental pollutants; Environmental variables; Essential support; extreme events; Final objective; Flash-floods; Human intervention; In order; Large regions; mapping systems; Near-real time (NRT); Nuclear accidents; Point measurements; radiation levels; rainfall fields; Real time analysis; Real time assessments; Real-time mapping; Running in; Seismic activities; Spatial dependence; Springer (CO); algorithm; decision making; environmental monitoring; hot spot; interpolation; map; mapping; spatial analysis; Real time systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
638;Does deep learning advance hourly runoff predictions?;Timely and accurate flash flood predictions are crucial for early warning of this costly and deadly natural hazard. There are many modeling approaches tackling hourly runoff formation mechanisms and utilizing different levels of computational complexity and requirements to an input data. However, to keep an efficient trade-off between speed, data availability, robustness and interpretability, the focus of operational runoff forecasting systems development often shifts to conceptual hydrological models. However, the developing field of deep learning provides us an opportunity to advance flash flood predictions using new data-driven technologies. In the present study, we extensively search for optimal structure and parameters and examine the predictive performance of the Long Short-Term Memory (LSTM) model for continuous runoff predictions at an hourly time step and compare results with the process-based hydrological model. Results highlight that the LSTM model provides reliable hourly runoff predictions. Yet, it demonstrates only comparable efficiency with a conceptual hydrological model, but with a significant computational overhead. © 2019 CEUR-WS. All rights reserved.;NULL;"Climate models; Economic and social effects; Floods; Forecasting; Long short-term memory; Runoff; Structural optimization; Computational overheads; Flash flood predictions; Formation mechanism; Hydrological modeling; Hydrological models; Optimal structures; Predictive performance; Runoff forecasting; Deep learning";"Does deep learning advance hourly runoff predictions? Timely and accurate flash flood predictions are crucial for early warning of this costly and deadly natural hazard. There are many modeling approaches tackling hourly runoff formation mechanisms and utilizing different levels of computational complexity and requirements to an input data. However, to keep an efficient trade-off between speed, data availability, robustness and interpretability, the focus of operational runoff forecasting systems development often shifts to conceptual hydrological models. However, the developing field of deep learning provides us an opportunity to advance flash flood predictions using new data-driven technologies. In the present study, we extensively search for optimal structure and parameters and examine the predictive performance of the Long Short-Term Memory (LSTM) model for continuous runoff predictions at an hourly time step and compare results with the process-based hydrological model. Results highlight that the LSTM model provides reliable hourly runoff predictions. Yet, it demonstrates only comparable efficiency with a conceptual hydrological model, but with a significant computational overhead. © 2019 CEUR-WS. All rights reserved. NULL Climate models; Economic and social effects; Floods; Forecasting; Long short-term memory; Runoff; Structural optimization; Computational overheads; Flash flood predictions; Formation mechanism; Hydrological modeling; Hydrological models; Optimal structures; Predictive performance; Runoff forecasting; Deep learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
639;Hybrid Surrogate Model for Timely Prediction of Flash Flood Inundation Maps Caused by Rapid River Overflow;Timely generation of accurate and reliable forecasts of flash flood events is of paramount importance for flood early warning systems in urban areas. Although physically based models are able to provide realistic reproductions of fast-developing inundation maps in high resolutions, the high computational demand of such hydraulic models makes them difficult to be implemented as part of real-time forecasting systems. This paper evaluates the use of a hybrid machine learning approach as a surrogate of a quasi-2D urban flood inundation model developed in PCSWMM for an urban catchment located in Toronto (Ontario, Canada). The capability to replicate the behavior of the hydraulic model was evaluated through multiple performance metrics considering error, bias, correlation, and contingency table analysis. Results indicate that the surrogate system can provide useful forecasts for decision makers by rapidly generating future flood inundation maps comparable to the simulations of physically based models. The experimental tool developed can issue reliable alerts of upcoming inundation depths on traffic locations within one to two hours of lead time, which is sufficient for the adoption of important preventive actions. These promising outcomes were achieved in a deterministic setup and use only past records of precipitation and discharge as input during runtime. © 2022 by the authors.;"flash flood; flood inundation; machine learning; rapid flood forecasting; surrogate model";NULL;"Hybrid Surrogate Model for Timely Prediction of Flash Flood Inundation Maps Caused by Rapid River Overflow Timely generation of accurate and reliable forecasts of flash flood events is of paramount importance for flood early warning systems in urban areas. Although physically based models are able to provide realistic reproductions of fast-developing inundation maps in high resolutions, the high computational demand of such hydraulic models makes them difficult to be implemented as part of real-time forecasting systems. This paper evaluates the use of a hybrid machine learning approach as a surrogate of a quasi-2D urban flood inundation model developed in PCSWMM for an urban catchment located in Toronto (Ontario, Canada). The capability to replicate the behavior of the hydraulic model was evaluated through multiple performance metrics considering error, bias, correlation, and contingency table analysis. Results indicate that the surrogate system can provide useful forecasts for decision makers by rapidly generating future flood inundation maps comparable to the simulations of physically based models. The experimental tool developed can issue reliable alerts of upcoming inundation depths on traffic locations within one to two hours of lead time, which is sufficient for the adoption of important preventive actions. These promising outcomes were achieved in a deterministic setup and use only past records of precipitation and discharge as input during runtime. © 2022 by the authors. flash flood; flood inundation; machine learning; rapid flood forecasting; surrogate model NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
640;Flood Forecasting with Machine Learning Technique on Hydrological Modeling;"Urban flooding is a major problem in Thailand. An essential countermeasure towards better flooding management is to forecast flood water levels in the real-time manner. Most existing early warning systems (EWS) in Thailand contain a lot of miscalculations when they face with real situations. Towards prediction improvement, this paper presents hydrological modeling augmented with alternative five machine learning techniques; linear regression, neural network regression, Bayesian linear regression and boosted decision tree regression. As the testbed system, the so-called MIKE-11 hydrologic forecasting model, developed by Danish Hydraulic Institute (DHI), Denmark, is used. To test error reduction in runoff forecasting, the water-level records during 2012-2016 data are used for training and the derived model is tested on the record of 2017, in the experiments. © 2018 Elsevier B.V. All rights reserved.";"Flood forecasting; Machine learning; Yom river";"Decision trees; Flood control; Floods; Knowledge based systems; Learning algorithms; Learning systems; Machine learning; Regression analysis; Water levels; Boosted decision trees; Danish hydraulic institutes; Early warning systems; Flood forecasting; Hydrologic forecasting models; Hydrological modeling; Machine learning techniques; Runoff forecasting; Weather forecasting";"Flood Forecasting with Machine Learning Technique on Hydrological Modeling Urban flooding is a major problem in Thailand. An essential countermeasure towards better flooding management is to forecast flood water levels in the real-time manner. Most existing early warning systems (EWS) in Thailand contain a lot of miscalculations when they face with real situations. Towards prediction improvement, this paper presents hydrological modeling augmented with alternative five machine learning techniques; linear regression, neural network regression, Bayesian linear regression and boosted decision tree regression. As the testbed system, the so-called MIKE-11 hydrologic forecasting model, developed by Danish Hydraulic Institute (DHI), Denmark, is used. To test error reduction in runoff forecasting, the water-level records during 2012-2016 data are used for training and the derived model is tested on the record of 2017, in the experiments. © 2018 Elsevier B.V. All rights reserved. Flood forecasting; Machine learning; Yom river Decision trees; Flood control; Floods; Knowledge based systems; Learning algorithms; Learning systems; Machine learning; Regression analysis; Water levels; Boosted decision trees; Danish hydraulic institutes; Early warning systems; Flood forecasting; Hydrologic forecasting models; Hydrological modeling; Machine learning techniques; Runoff forecasting; Weather forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
641;A novel approach combining particle swarm optimization and deep learning for flash flood detection from satellite images;"Flood is one of the deadliest natural hazards worldwide, with the population affected being more than 2 billion between 1998–2017 with a lack of warning systems according to WHO. Especially, flash floods have the potential to generate fatal damages due to their rapid evolution and the limited warning and response time. An effective Early Warning Systems (EWS) could support detection and recognition of flash floods. Information about a flash flood can be mainly provided from observations of hydrology and from satellite images taken before the flash flood happens. Then, predictions from satellite images can be integrated with predictions based on sensors’ information to improve the accuracy of a forecasting system and subsequently trigger warning systems. The existing Deep Learning models such as UNET has been effectively used to segment the flash flood with high performance, but there are no ways to determine the most suitable model architecture with the proper number of layers showing the best performance in the task. In this paper, we propose a novel Deep Learning architecture, namely PSO-UNET, which combines Particle Swarm Optimization (PSO) with UNET to seek the best number of layers and the parameters of layers in the UNET based architecture; thereby improving the performance of flash flood segmentation from satellite images. Since the original UNET has a symmetrical architecture, the evolutionary computation is performed by paying attention to the contracting path and the expanding path is synchronized with the following layers in the contracting path. The UNET convolutional process is performed four times. Indeed, we consider each process as a block of the convolution having two convolutional layers in the original architecture. Training of inputs and hyper-parameters is performed by executing the PSO algorithm. In practice, the value of Dice Coefficient of our proposed model exceeds 79.75% (8.59% higher than that of the original UNET model). Experimental results on various satellite images prove the advantages and superiority of the PSO-UNET approach. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.";"Deep learning; Flash flood detection; Particle Swarm Optimization (PSO); Satellite images; Semantic segmentation; UNET";NULL;"A novel approach combining particle swarm optimization and deep learning for flash flood detection from satellite images Flood is one of the deadliest natural hazards worldwide, with the population affected being more than 2 billion between 1998–2017 with a lack of warning systems according to WHO. Especially, flash floods have the potential to generate fatal damages due to their rapid evolution and the limited warning and response time. An effective Early Warning Systems (EWS) could support detection and recognition of flash floods. Information about a flash flood can be mainly provided from observations of hydrology and from satellite images taken before the flash flood happens. Then, predictions from satellite images can be integrated with predictions based on sensors’ information to improve the accuracy of a forecasting system and subsequently trigger warning systems. The existing Deep Learning models such as UNET has been effectively used to segment the flash flood with high performance, but there are no ways to determine the most suitable model architecture with the proper number of layers showing the best performance in the task. In this paper, we propose a novel Deep Learning architecture, namely PSO-UNET, which combines Particle Swarm Optimization (PSO) with UNET to seek the best number of layers and the parameters of layers in the UNET based architecture; thereby improving the performance of flash flood segmentation from satellite images. Since the original UNET has a symmetrical architecture, the evolutionary computation is performed by paying attention to the contracting path and the expanding path is synchronized with the following layers in the contracting path. The UNET convolutional process is performed four times. Indeed, we consider each process as a block of the convolution having two convolutional layers in the original architecture. Training of inputs and hyper-parameters is performed by executing the PSO algorithm. In practice, the value of Dice Coefficient of our proposed model exceeds 79.75% (8.59% higher than that of the original UNET model). Experimental results on various satellite images prove the advantages and superiority of the PSO-UNET approach. © 2021 by the authors. Licensee MDPI, Basel, Switzerland. Deep learning; Flash flood detection; Particle Swarm Optimization (PSO); Satellite images; Semantic segmentation; UNET NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
642;Application of CNN-LSTM and Internet of Things in rainfall accumulation prediction at urban flooding sites;In order to accurately and quickly predict the trend of water accumulation at urban flooding sites, a combined neural network-based time series prediction model (CNN-LSTM) is proposed to model and predict multivariate water accumulation time series data. This model utilizes convolutional neural network (CNN) to extract spatial features between multivariate data to obtain spatially correlated feature quantities, and long short-term memory (LSTM) to extract temporal correlation between feature quantities to predict the future water level of waterlogging. Taking one month's waterlogging data of Sino-Singapore Tianjin Eco-city as an example, the results show that the CNN-LSTM prediction model can capture the nonlinear relationship between the water level at the waterlogging point and the inputs very well, and it has a better fit to the actual water level, higher accuracy and generalization ability than the CNN, LSTM and the back propagation (BP) neural network. The effectiveness and applicability of this model in the prediction of urban waterlogging is verified, and it can provide a reliable reference for the early warning and preparation of waterlogging points, as well as the development of pre-flood, flood and post-flood management programs. © 2023 IEEE.;"CNN; LSTM; Prediction of waterlogging";"Backpropagation; Convolutional neural networks; Flood control; Floods; Internet of things; Long short-term memory; Smart city; Time series; Urban growth; Water levels; Accumulation time; Combined neural networks; Convolutional neural network; Network-based; Prediction modelling; Prediction of waterlogging; Time series prediction; Time-series data; Urban flooding; Water accumulation; Forecasting";"Application of CNN-LSTM and Internet of Things in rainfall accumulation prediction at urban flooding sites In order to accurately and quickly predict the trend of water accumulation at urban flooding sites, a combined neural network-based time series prediction model (CNN-LSTM) is proposed to model and predict multivariate water accumulation time series data. This model utilizes convolutional neural network (CNN) to extract spatial features between multivariate data to obtain spatially correlated feature quantities, and long short-term memory (LSTM) to extract temporal correlation between feature quantities to predict the future water level of waterlogging. Taking one month's waterlogging data of Sino-Singapore Tianjin Eco-city as an example, the results show that the CNN-LSTM prediction model can capture the nonlinear relationship between the water level at the waterlogging point and the inputs very well, and it has a better fit to the actual water level, higher accuracy and generalization ability than the CNN, LSTM and the back propagation (BP) neural network. The effectiveness and applicability of this model in the prediction of urban waterlogging is verified, and it can provide a reliable reference for the early warning and preparation of waterlogging points, as well as the development of pre-flood, flood and post-flood management programs. © 2023 IEEE. CNN; LSTM; Prediction of waterlogging Backpropagation; Convolutional neural networks; Flood control; Floods; Internet of things; Long short-term memory; Smart city; Time series; Urban growth; Water levels; Accumulation time; Combined neural networks; Convolutional neural network; Network-based; Prediction modelling; Prediction of waterlogging; Time series prediction; Time-series data; Urban flooding; Water accumulation; Forecasting";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
643;Integrating Machine Learning with Data Assimilation for High Resolution Soil Moisture Estimation;Spatiotemporal seamless high-resolution soil moisture data are of great value in disaster prevention and mitigation and in geoscientific research, e.g. in studies related to landslide monitoring, flash flood early warning and digital twins. Current spatiotemporally continuous soil moisture data come mainly from reanalysis data and land surface data assimilation systems, but their spatial resolution is coarse on the scale of tens of kilometers, which cannot meet the requirements. To overcome this problem, we integrated a machine learning algorithm with a dual-cycle land data assimilation system for high-resolution soil moisture estimation. A random forest was deployed to downscale coarse soil moisture data provided by passive microwave sensors such as SMAP, SMOS and AMSR2 to high resolution data. A dual-cycle land data assimilation system, previously developed by the authors and capable of simultaneously correcting errors in the soil moisture products and optimizing data assimilation system parameters, was then used to assimilate the high-resolution soil moisture data generated by the random forest. Finally, high-resolution land surface state data sets, including soil moisture in the surface layer and root zone, were estimated. The results were validated against soil moisture observations from three networks on the Tibetan Plateau. It is shown that the integrated system is capable of producing reliable soil moisture products at high resolutions, such as 5 km, 10 km, etc., with a ubRMSE less than 0.04 m3/m3 © 2024 IEEE.;"high resolution soil moisture; Land data assimilation; parameter optimization; random forest; remote sensing";"Microwave sensors; %moisture; High resolution; High resolution soil moisture; Land data assimilation; Land data assimilation systems; Moisture data; Parameter optimization; Random forests; Remote-sensing; Soil moisture estimation; Random forests";"Integrating Machine Learning with Data Assimilation for High Resolution Soil Moisture Estimation Spatiotemporal seamless high-resolution soil moisture data are of great value in disaster prevention and mitigation and in geoscientific research, e.g. in studies related to landslide monitoring, flash flood early warning and digital twins. Current spatiotemporally continuous soil moisture data come mainly from reanalysis data and land surface data assimilation systems, but their spatial resolution is coarse on the scale of tens of kilometers, which cannot meet the requirements. To overcome this problem, we integrated a machine learning algorithm with a dual-cycle land data assimilation system for high-resolution soil moisture estimation. A random forest was deployed to downscale coarse soil moisture data provided by passive microwave sensors such as SMAP, SMOS and AMSR2 to high resolution data. A dual-cycle land data assimilation system, previously developed by the authors and capable of simultaneously correcting errors in the soil moisture products and optimizing data assimilation system parameters, was then used to assimilate the high-resolution soil moisture data generated by the random forest. Finally, high-resolution land surface state data sets, including soil moisture in the surface layer and root zone, were estimated. The results were validated against soil moisture observations from three networks on the Tibetan Plateau. It is shown that the integrated system is capable of producing reliable soil moisture products at high resolutions, such as 5 km, 10 km, etc., with a ubRMSE less than 0.04 m3/m3 © 2024 IEEE. high resolution soil moisture; Land data assimilation; parameter optimization; random forest; remote sensing Microwave sensors; %moisture; High resolution; High resolution soil moisture; Land data assimilation; Land data assimilation systems; Moisture data; Parameter optimization; Random forests; Remote-sensing; Soil moisture estimation; Random forests";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
644;Deep learning enables super-resolution hydrodynamic flooding process modeling under spatiotemporally varying rainstorms;Real-time information on flooding extent, severity, and duration is necessary for effective metropolitan flood emergency management. Existing pluvial flood analysis methods are unable to simulate real-time regional flooding processes under spatiotemporally varying rainstorms. This paper presents a deep learning-enabled super-resolution hydrodynamic flood analysis method to simulate the real-time pluvial flooding process over a large area under spatiotemporally varying rainstorms. Compared with existing flood downscaling techniques, which are limited to flow depth, the proposed method produces high-resolution flow depth and velocity predictions, providing more comprehensive information for flood emergency management. The proposed method adopts a coarse-grid hydrodynamic model to generate a low-resolution flood map time series, which is subsequently converted to high-resolution flood maps by a deep learning model. The deep learning model can be trained using a limited number of assumed rainfall scenarios, which greatly reduces data preparation effort. The proposed method is applied to a complex terrain of 352 km2 in Hong Kong that covers both mountainous and urban areas. Results show that the proposed method simulates the spatiotemporal variations of flood depth and velocity with root mean square errors as low as 0.082 m and 0.088 m/s, respectively, and correlation coefficients of 0.962 and 0.921, respectively. The computation time for a 48-h rainfall event in the study area is less than 30 s, which is 2690 times faster than the direct fine-grid hydrodynamic analysis. The deep learning-enabled super-resolution hydrodynamic flood analysis method provides a promising computational tool for emergency flood risk management. © 2023 Elsevier Ltd;"Deep learning; Downscaling; Emergency management; Flooding; Hydrodynamic analysis; Super-resolution";"Deep Learning; Floods; Hong Kong; Hydrodynamics; Models, Theoretical; China; Hong Kong; Deep learning; Disaster prevention; Disasters; Flood control; Hydrodynamics; Learning systems; Mean square error; Optical resolving power; Rain; Risk assessment; Risk management; Storms; rain; Analysis method; Deep learning; Down-scaling; Emergency management; Floodings; Flow depth; High resolution; Hydrodynamic analysis; Real- time; Superresolution; complex terrain; correlation; downscaling; flood control; flooding; hydrodynamics; machine learning; numerical model; rainfall; spatial resolution; spatiotemporal analysis; Article; computer simulation; controlled study; deep learning; emergency management; flooding; flow rate; Hong Kong; hydrodynamics; process model; risk management; urban area; hydrodynamics; theoretical model; Floods";"Deep learning enables super-resolution hydrodynamic flooding process modeling under spatiotemporally varying rainstorms Real-time information on flooding extent, severity, and duration is necessary for effective metropolitan flood emergency management. Existing pluvial flood analysis methods are unable to simulate real-time regional flooding processes under spatiotemporally varying rainstorms. This paper presents a deep learning-enabled super-resolution hydrodynamic flood analysis method to simulate the real-time pluvial flooding process over a large area under spatiotemporally varying rainstorms. Compared with existing flood downscaling techniques, which are limited to flow depth, the proposed method produces high-resolution flow depth and velocity predictions, providing more comprehensive information for flood emergency management. The proposed method adopts a coarse-grid hydrodynamic model to generate a low-resolution flood map time series, which is subsequently converted to high-resolution flood maps by a deep learning model. The deep learning model can be trained using a limited number of assumed rainfall scenarios, which greatly reduces data preparation effort. The proposed method is applied to a complex terrain of 352 km2 in Hong Kong that covers both mountainous and urban areas. Results show that the proposed method simulates the spatiotemporal variations of flood depth and velocity with root mean square errors as low as 0.082 m and 0.088 m/s, respectively, and correlation coefficients of 0.962 and 0.921, respectively. The computation time for a 48-h rainfall event in the study area is less than 30 s, which is 2690 times faster than the direct fine-grid hydrodynamic analysis. The deep learning-enabled super-resolution hydrodynamic flood analysis method provides a promising computational tool for emergency flood risk management. © 2023 Elsevier Ltd Deep learning; Downscaling; Emergency management; Flooding; Hydrodynamic analysis; Super-resolution Deep Learning; Floods; Hong Kong; Hydrodynamics; Models, Theoretical; China; Hong Kong; Deep learning; Disaster prevention; Disasters; Flood control; Hydrodynamics; Learning systems; Mean square error; Optical resolving power; Rain; Risk assessment; Risk management; Storms; rain; Analysis method; Deep learning; Down-scaling; Emergency management; Floodings; Flow depth; High resolution; Hydrodynamic analysis; Real- time; Superresolution; complex terrain; correlation; downscaling; flood control; flooding; hydrodynamics; machine learning; numerical model; rainfall; spatial resolution; spatiotemporal analysis; Article; computer simulation; controlled study; deep learning; emergency management; flooding; flow rate; Hong Kong; hydrodynamics; process model; risk management; urban area; hydrodynamics; theoretical model; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
645;Identifying Factors for Supporting Early Warning Flood using Clustering Approach And Geo-Spatial Analysis;Floods in Jakarta are very frequent and caused by many factors, from very high rainfall to flash floods. This natural disaster has greatlyimpact the society, from economy to health problems. In this research, we used ArcGIS Pro with unsupervised clustering method HDBSCAN to create a prototype that would make it easier for people to find out the area that is prone to floods. We received a lot of positive input from many respondents who had seen our prototype design and how it can assist people as the early warning to floods. We analyzed the factors lead the flood early warning system, those factors are high density population distance from river to the high-density population area, drainage system, rainfall intensity, and flood history. We will group flood point with the unsupervised clustering method. © 2023 The Authors. Published by Elsevier B.V.;"clustering; early warning system; flood; HDBSCAN; Jakarta; unsupervised learning";"Cluster analysis; Disaster prevention; Disasters; Rain; Clustering approach; Clusterings; Early warning; Early Warning System; Flash-floods; Geo-spatial analysis; HDBSCAN; Jakarta; Natural disasters; Unsupervised clustering methods; Floods";"Identifying Factors for Supporting Early Warning Flood using Clustering Approach And Geo-Spatial Analysis Floods in Jakarta are very frequent and caused by many factors, from very high rainfall to flash floods. This natural disaster has greatlyimpact the society, from economy to health problems. In this research, we used ArcGIS Pro with unsupervised clustering method HDBSCAN to create a prototype that would make it easier for people to find out the area that is prone to floods. We received a lot of positive input from many respondents who had seen our prototype design and how it can assist people as the early warning to floods. We analyzed the factors lead the flood early warning system, those factors are high density population distance from river to the high-density population area, drainage system, rainfall intensity, and flood history. We will group flood point with the unsupervised clustering method. © 2023 The Authors. Published by Elsevier B.V. clustering; early warning system; flood; HDBSCAN; Jakarta; unsupervised learning Cluster analysis; Disaster prevention; Disasters; Rain; Clustering approach; Clusterings; Early warning; Early Warning System; Flash-floods; Geo-spatial analysis; HDBSCAN; Jakarta; Natural disasters; Unsupervised clustering methods; Floods";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.2;Hydrological;2;Preparation
646;Urban flood susceptibility mapping based on social media data in Chengdu city, China;Increase in urban flood hazards has become a major threat to cities, causing considerable losses of life and in the economy. To improve pre-disaster strategies and to mitigate potential losses, it is important to make urban flood susceptibility assessments and to carry out spatiotemporal analyses. In this study, we used standard deviation ellipse (SDE) to analyze the spatial pattern of urban floods and find the area of interest (AOI) based upon related social media data that were collected in Chengdu city, China. We used the social media data as the response variable and selected 10 urban flood-influencing factors as independent variables. We estimated the susceptibility model using the Naïve Bayes (NB) method. The results show that the urban flood events are concentrated in the northeast-central part of Chengdu city, especially around the city center. Results of the susceptibility model were checked by the Receiver Operating Characteristic (ROC) curve, showing that the area under the curve (AUC) was equal to 0.8299. This validation result confirmed that the susceptibility model can predict urban flood with a satisfactory accuracy. The urban flood susceptibility map in the city center area provides a realistic reference for flood monitoring and early warning. © 2022 The Author(s);"Chengdu city; Naïve Bayes; Social media data; Standard deviation ellipse; Urban flood susceptibility mapping";"Chengdu; China; Sichuan; Barium compounds; Floods; Social networking (online); Statistics; Chengdu; Chengdu city; City centers; Naive bayes; Social media datum; Standard deviation; Standard deviation ellipse; Susceptibility mapping; Urban flood susceptibility mapping; Urban floods; early warning system; flood; machine learning; mapping; monitoring; social media; urban area; Mapping";"Urban flood susceptibility mapping based on social media data in Chengdu city, China Increase in urban flood hazards has become a major threat to cities, causing considerable losses of life and in the economy. To improve pre-disaster strategies and to mitigate potential losses, it is important to make urban flood susceptibility assessments and to carry out spatiotemporal analyses. In this study, we used standard deviation ellipse (SDE) to analyze the spatial pattern of urban floods and find the area of interest (AOI) based upon related social media data that were collected in Chengdu city, China. We used the social media data as the response variable and selected 10 urban flood-influencing factors as independent variables. We estimated the susceptibility model using the Naïve Bayes (NB) method. The results show that the urban flood events are concentrated in the northeast-central part of Chengdu city, especially around the city center. Results of the susceptibility model were checked by the Receiver Operating Characteristic (ROC) curve, showing that the area under the curve (AUC) was equal to 0.8299. This validation result confirmed that the susceptibility model can predict urban flood with a satisfactory accuracy. The urban flood susceptibility map in the city center area provides a realistic reference for flood monitoring and early warning. © 2022 The Author(s) Chengdu city; Naïve Bayes; Social media data; Standard deviation ellipse; Urban flood susceptibility mapping Chengdu; China; Sichuan; Barium compounds; Floods; Social networking (online); Statistics; Chengdu; Chengdu city; City centers; Naive bayes; Social media datum; Standard deviation; Standard deviation ellipse; Susceptibility mapping; Urban flood susceptibility mapping; Urban floods; early warning system; flood; machine learning; mapping; monitoring; social media; urban area; Mapping";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
647;Deep Learning-based Critical Infrastructure Simulation Model for Disaster Monitoring;In this paper, we describe a Deep learning-based approach for Real-time Critical Infrastructure Protection in the scenario of a Flood event. This would help us understand the dependencies among various Critical Infrastructures and the severity of the current situation. We propose a Multiagent Deep Reinforcement Learning technique to design the policy and reward functions, which the agent must follow. Further, integrating Reinforcement Learning with GIS aids in putting the model in a spatial context and multiple-layer visualization leading to enhanced awareness of the situation. It also helps in the understanding of the spatiotemporal relationships among them. Each of the Geospatial agents will have its state and a set of actions that it needs to take. The agents will act with respect to their dependence on other related Infrastructures and take the best possible action as the disaster unfolds so that immediate response can reduce the severity of the damage. Real-time information simulation would help disaster response personnel to develop various scenarios in the simulation environment and see how the set of critical infrastructures are responding over time as the severity of the flood increases. © 2020 IEEE.;"Critical Infrastructure; Deep Reinforcement learning; Disaster; Geographic Information system; Urban Floods";"Critical infrastructures; Data mining; Emergency services; Floods; Learning systems; Public works; Reinforcement learning; Current situation; Disaster monitoring; Geospatial agents; Learning-based approach; Real-time information; Reinforcement learning techniques; Simulation environment; Spatio-temporal relationships; Deep learning";"Deep Learning-based Critical Infrastructure Simulation Model for Disaster Monitoring In this paper, we describe a Deep learning-based approach for Real-time Critical Infrastructure Protection in the scenario of a Flood event. This would help us understand the dependencies among various Critical Infrastructures and the severity of the current situation. We propose a Multiagent Deep Reinforcement Learning technique to design the policy and reward functions, which the agent must follow. Further, integrating Reinforcement Learning with GIS aids in putting the model in a spatial context and multiple-layer visualization leading to enhanced awareness of the situation. It also helps in the understanding of the spatiotemporal relationships among them. Each of the Geospatial agents will have its state and a set of actions that it needs to take. The agents will act with respect to their dependence on other related Infrastructures and take the best possible action as the disaster unfolds so that immediate response can reduce the severity of the damage. Real-time information simulation would help disaster response personnel to develop various scenarios in the simulation environment and see how the set of critical infrastructures are responding over time as the severity of the flood increases. © 2020 IEEE. Critical Infrastructure; Deep Reinforcement learning; Disaster; Geographic Information system; Urban Floods Critical infrastructures; Data mining; Emergency services; Floods; Learning systems; Public works; Reinforcement learning; Current situation; Disaster monitoring; Geospatial agents; Learning-based approach; Real-time information; Reinforcement learning techniques; Simulation environment; Spatio-temporal relationships; Deep learning";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.2;Hydrological;3;Response
648;A big-data-based Urban flood defense decision support system;As cities in developing countries are expanding rapidly in recent years, flood has an increasing impact on urban management. In this paper, we present the design and implementation of an urban flood defense decision support system based on big data. The system connects real-time sensor to collect streaming data, and uses a data-driven method that considers temporal and spatial factors to forecast water level in the next 6 hours. Thus, it can provide enough time for the authorities to take pertinent flood protection measures such as evacuation. Our predictive model is a hybrid of linear regression and artificial neural network, and can give early warning of potential flood using the forecast results. The system is implemented on Java EE platform, and integrated with Baidu Maps API to provide a user-friendly interface. © 2015 SERSC.;"Digital Urban management; Flood defense; Java EE; Neural network";"Application programming interfaces (API); Artificial intelligence; Decision support systems; Developing countries; Flood control; Floods; Neural networks; Water levels; Data-driven methods; Design and implementations; Flood defense; Java EE; Predictive modeling; Temporal and spatial; Urban management; User friendly interface; Big data";"A big-data-based Urban flood defense decision support system As cities in developing countries are expanding rapidly in recent years, flood has an increasing impact on urban management. In this paper, we present the design and implementation of an urban flood defense decision support system based on big data. The system connects real-time sensor to collect streaming data, and uses a data-driven method that considers temporal and spatial factors to forecast water level in the next 6 hours. Thus, it can provide enough time for the authorities to take pertinent flood protection measures such as evacuation. Our predictive model is a hybrid of linear regression and artificial neural network, and can give early warning of potential flood using the forecast results. The system is implemented on Java EE platform, and integrated with Baidu Maps API to provide a user-friendly interface. © 2015 SERSC. Digital Urban management; Flood defense; Java EE; Neural network Application programming interfaces (API); Artificial intelligence; Decision support systems; Developing countries; Flood control; Floods; Neural networks; Water levels; Data-driven methods; Design and implementations; Flood defense; Java EE; Predictive modeling; Temporal and spatial; Urban management; User friendly interface; Big data";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
649;Application of machine learning-based surrogate models for urban flood depth modeling in Ho Chi Minh City, Vietnam [Formula presented];Rapid flood prediction in coastal urban areas is an important but challenging task. However, multi-driver floods in coastal areas and their non-linearity in physical processes are hard to represent in physics-based numerical models (PBNMs). In this study, we investigated the performance of surrogate machine learning (ML) models and their flood prediction capability. Initially, we utilize the MIKE+ coupled 1D–2D model to simulate coastal urban flooding in one of the severely flood-affected areas of Ho Chi Minh City (HCMC), Vietnam. Then, nine ML models, including AdaBoost (AB), Decision Tree (DT), Gaussian Process (GP), k-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Naive Bayes (NB), Neural Network (NN), Random Forest (RF), and Support Vector Machine (SVM) are employed to surrogate the PBNM flood prediction performance and engaged to predict flood depths of the study area domain. 806 simulation scenarios of MIKE+ modeling having a spatial grid of 1107 ×1513, grid size = 2 m, extracting 270,000 inundation points to generate input data for nine ML models are used to simulate surface flood depths for the study area. Results show three ML models, GP, RF, and NN, outperform the remaining models, with R2 value of 0.997, 0.996, and 0.995, respectively. Thus, applying ML models can significantly reduce the simulation time by a PBNM, improve accuracy, and potentially be adopted for real-time forecasting and emergency management. © 2023 Elsevier B.V.;"Ho Chi Minh City; Machine learning; MIKE + modelling; Surrogate model; Tidal inundation; Urban flooding";"Adaptive boosting; Decision trees; Discriminant analysis; Flood control; Forecasting; Learning systems; Nearest neighbor search; Risk management; Support vector machines; Flood prediction; Ho chi minh city; Machine learning models; Machine-learning; MIKE + modeling; Physics-based; Surrogate modeling; Tidal inundation; Urban flooding; Viet Nam; Floods";"Application of machine learning-based surrogate models for urban flood depth modeling in Ho Chi Minh City, Vietnam [Formula presented] Rapid flood prediction in coastal urban areas is an important but challenging task. However, multi-driver floods in coastal areas and their non-linearity in physical processes are hard to represent in physics-based numerical models (PBNMs). In this study, we investigated the performance of surrogate machine learning (ML) models and their flood prediction capability. Initially, we utilize the MIKE+ coupled 1D–2D model to simulate coastal urban flooding in one of the severely flood-affected areas of Ho Chi Minh City (HCMC), Vietnam. Then, nine ML models, including AdaBoost (AB), Decision Tree (DT), Gaussian Process (GP), k-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Naive Bayes (NB), Neural Network (NN), Random Forest (RF), and Support Vector Machine (SVM) are employed to surrogate the PBNM flood prediction performance and engaged to predict flood depths of the study area domain. 806 simulation scenarios of MIKE+ modeling having a spatial grid of 1107 ×1513, grid size = 2 m, extracting 270,000 inundation points to generate input data for nine ML models are used to simulate surface flood depths for the study area. Results show three ML models, GP, RF, and NN, outperform the remaining models, with R2 value of 0.997, 0.996, and 0.995, respectively. Thus, applying ML models can significantly reduce the simulation time by a PBNM, improve accuracy, and potentially be adopted for real-time forecasting and emergency management. © 2023 Elsevier B.V. Ho Chi Minh City; Machine learning; MIKE + modelling; Surrogate model; Tidal inundation; Urban flooding Adaptive boosting; Decision trees; Discriminant analysis; Flood control; Forecasting; Learning systems; Nearest neighbor search; Risk management; Support vector machines; Flood prediction; Ho chi minh city; Machine learning models; Machine-learning; MIKE + modeling; Physics-based; Surrogate modeling; Tidal inundation; Urban flooding; Viet Nam; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
650;A Comparison of Global and Local Statistical and Machine Learning Techniques in Estimating Flash Flood Susceptibility;Flash floods, as a type of devastating natural disasters, can cause significant damage to infrastructure, agriculture, and people’s livelihoods. Mapping flash flood susceptibility has long been an effective measure to help with the development of flash flood risk reduction and management strategies. Recent studies have shown that machine learning (ML) techniques perform better than traditional statistical and process-based models in estimating flash flood susceptibility. However, a major limitation of standard ML models is that they ignore the local geographic context where flash floods occur. To address this limitation, we developed a local Geographically Weighted Random Forest (GWRF) model and compared its performance against other global and local statistical and ML alternatives using an empirical flash floods model of Jiangxi Province, China. © Jing Yao, Ziqi Li, Xiaoxiang Zhang, Changjun Liu, and Liliang Ren.;"Flash floods; Machine Learning; Spatial Statistics; Susceptibility";"Disasters; Floods; Forestry; Risk assessment; Effective measures; Flash-floods; Flood risk management; Flood risk reduction; Machine learning techniques; Machine-learning; Natural disasters; Spatial statistics; Statistical learning techniques; Susceptibility; Machine learning";"A Comparison of Global and Local Statistical and Machine Learning Techniques in Estimating Flash Flood Susceptibility Flash floods, as a type of devastating natural disasters, can cause significant damage to infrastructure, agriculture, and people’s livelihoods. Mapping flash flood susceptibility has long been an effective measure to help with the development of flash flood risk reduction and management strategies. Recent studies have shown that machine learning (ML) techniques perform better than traditional statistical and process-based models in estimating flash flood susceptibility. However, a major limitation of standard ML models is that they ignore the local geographic context where flash floods occur. To address this limitation, we developed a local Geographically Weighted Random Forest (GWRF) model and compared its performance against other global and local statistical and ML alternatives using an empirical flash floods model of Jiangxi Province, China. © Jing Yao, Ziqi Li, Xiaoxiang Zhang, Changjun Liu, and Liliang Ren. Flash floods; Machine Learning; Spatial Statistics; Susceptibility Disasters; Floods; Forestry; Risk assessment; Effective measures; Flash-floods; Flood risk management; Flood risk reduction; Machine learning techniques; Machine-learning; Natural disasters; Spatial statistics; Statistical learning techniques; Susceptibility; Machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
651;Feasibility Study of Urban Flood Mapping Using Traffic Signs for Route Optimization;Water events are the most frequent and costliest climate disasters around the world. In the U.S., an estimated 127 million people who live in coastal areas are at risk of substantial home damage from hurricanes or flooding. In flood emergency management, timely and effective spatial decision-making and intelligent routing depend on flood depth information at a fine spatiotemporal scale. In this paper, crowdsourcing is utilized to collect photos of submerged stop signs, and pair each photo with a pre-flood photo taken at the same location. Each photo pair is then analyzed using deep neural network and image processing to estimate the depth of floodwater in the location of the photo. Generated point-by-point depth data is converted to a flood inundation map and used by an A* search algorithm to determine an optimal flood-free path connecting points of interest. Results provide crucial information to rescue teams and evacuees by enabling effective wayfinding during flooding events. © 2021 Universitätsverlag der Technischen Universität Berlin. All Rights Reserved.;NULL;"Behavioral research; Deep neural networks; Disasters; Flood control; Image processing; Risk management; Risk perception; Traffic signs; Coastal area; Decisions makings; Depth information; Emergency management; Feasibility studies; Flood mapping; Floodings; Intelligent routing; Route optimization; Urban floods; Floods";"Feasibility Study of Urban Flood Mapping Using Traffic Signs for Route Optimization Water events are the most frequent and costliest climate disasters around the world. In the U.S., an estimated 127 million people who live in coastal areas are at risk of substantial home damage from hurricanes or flooding. In flood emergency management, timely and effective spatial decision-making and intelligent routing depend on flood depth information at a fine spatiotemporal scale. In this paper, crowdsourcing is utilized to collect photos of submerged stop signs, and pair each photo with a pre-flood photo taken at the same location. Each photo pair is then analyzed using deep neural network and image processing to estimate the depth of floodwater in the location of the photo. Generated point-by-point depth data is converted to a flood inundation map and used by an A* search algorithm to determine an optimal flood-free path connecting points of interest. Results provide crucial information to rescue teams and evacuees by enabling effective wayfinding during flooding events. © 2021 Universitätsverlag der Technischen Universität Berlin. All Rights Reserved. NULL Behavioral research; Deep neural networks; Disasters; Flood control; Image processing; Risk management; Risk perception; Traffic signs; Coastal area; Decisions makings; Depth information; Emergency management; Feasibility studies; Flood mapping; Floodings; Intelligent routing; Route optimization; Urban floods; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
652;Water level prediction using artificial neural network with particle swarm optimization model;Flash flood is a natural disaster that causes great losses. It happens mostly in rural areas when heavy rainfall is gathered into the main river in watershed areas. Lots of water comes into the river. This causes a great volume of water flows down to the downstream river area. The water level at the downstream river should be predicted to issue the warning messages to the villagers in the floodplains before the flood arrival. Thus, a flash flood early warning system is a solution to reduce damage from flash floods. Although the artificial neural network (ANN) can be applied as the prediction model, the accuracy of the prediction results depends on the parameter values (e.g., the number of previous data, the period of previous data). This paper proposes to apply the particle swarm optimization technique to tune up the parameter values in the ANN. The proposed model, called W-POpt model, consists of two components, which are 1) PSO is applied as optimizer to search for the optimal parameter values for the ANN training process, and 2) ANN is applied to find the predicted water level. The evaluation results show that PSO yields the optimal parameter values. Applying PSO can reduce the training process time in ANN. The predicted water level from the W-POpt model is acceptable for applying in flash flood early warning systems. © 2017 IEEE.;"artificial neural networks; disaster; early warning systems; flash flood; particle swarm optimization; prediction";"Alarm systems; Disasters; Floods; Forecasting; Neural networks; Optimization; Rain; Rivers; Rural areas; Water levels; Water resources; Early Warning System; Evaluation results; Flash flood; Natural disasters; Optimal parameter; Particle swarm optimization models; Particle swarm optimization technique; Water level prediction; Particle swarm optimization (PSO)";"Water level prediction using artificial neural network with particle swarm optimization model Flash flood is a natural disaster that causes great losses. It happens mostly in rural areas when heavy rainfall is gathered into the main river in watershed areas. Lots of water comes into the river. This causes a great volume of water flows down to the downstream river area. The water level at the downstream river should be predicted to issue the warning messages to the villagers in the floodplains before the flood arrival. Thus, a flash flood early warning system is a solution to reduce damage from flash floods. Although the artificial neural network (ANN) can be applied as the prediction model, the accuracy of the prediction results depends on the parameter values (e.g., the number of previous data, the period of previous data). This paper proposes to apply the particle swarm optimization technique to tune up the parameter values in the ANN. The proposed model, called W-POpt model, consists of two components, which are 1) PSO is applied as optimizer to search for the optimal parameter values for the ANN training process, and 2) ANN is applied to find the predicted water level. The evaluation results show that PSO yields the optimal parameter values. Applying PSO can reduce the training process time in ANN. The predicted water level from the W-POpt model is acceptable for applying in flash flood early warning systems. © 2017 IEEE. artificial neural networks; disaster; early warning systems; flash flood; particle swarm optimization; prediction Alarm systems; Disasters; Floods; Forecasting; Neural networks; Optimization; Rain; Rivers; Rural areas; Water levels; Water resources; Early Warning System; Evaluation results; Flash flood; Natural disasters; Optimal parameter; Particle swarm optimization models; Particle swarm optimization technique; Water level prediction; Particle swarm optimization (PSO)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
653;Open-Access Precipitation Networks and Machine Learning Algorithms as Tools for Flood Severity Prediction;During the past decades, convective rainfall in the Echaz catchment, which is characterized by steep topography and a high degree of urbanization, led to recurring flash floods. The high spatial and temporal variability of precipitation, in combination with the small drainage basin, contribute to low predictability and to the considerable damage potential of such events. The aim of this study is the development of a simple model to predict flood severity in the Echaz catchment. This model is based on open-access precipitation data from a personal weather station (PWS) network and water level measurements from a low-cost ultrasonic sensor. Machine learning classification methods (logistic regression and decision tree) are trained with observational data to determine maximum rainfall thresholds for different accumulation periods, ranging from 5 to 60 min. Hence, the proposed model uses multiple triggers to predict the exceedance of critical water levels. As a result, severe floods can be recognized earlier and with higher reliability, providing more response time for local authorities. Although the limited data availability increases the risk of overfitting and lower performance for the first upcoming events, the model quality will increase with the incorporation of new measurement data in the future. The reduced complexity and high interpretability of the model allow for a fast decision-making process. Additionally, the model has high potential and can easily be adapted to similar small catchments. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.;"Early warning; Flash floods; Flood severity prediction; Heavy rainfall; Machine learning; Open-access data; Real-time information system";"Catchments; Decision trees; Forecasting; Learning algorithms; Machine learning; Open Data; Open systems; Rain; Risk assessment; Runoff; Topography; Water levels; Early warning; Flash-floods; Flood severity prediction; Heavy rainfall; Machine learning algorithms; Network learning algorithms; Open-access data; OpenAccess; Real-time information systems; Steep topography; Floods";"Open-Access Precipitation Networks and Machine Learning Algorithms as Tools for Flood Severity Prediction During the past decades, convective rainfall in the Echaz catchment, which is characterized by steep topography and a high degree of urbanization, led to recurring flash floods. The high spatial and temporal variability of precipitation, in combination with the small drainage basin, contribute to low predictability and to the considerable damage potential of such events. The aim of this study is the development of a simple model to predict flood severity in the Echaz catchment. This model is based on open-access precipitation data from a personal weather station (PWS) network and water level measurements from a low-cost ultrasonic sensor. Machine learning classification methods (logistic regression and decision tree) are trained with observational data to determine maximum rainfall thresholds for different accumulation periods, ranging from 5 to 60 min. Hence, the proposed model uses multiple triggers to predict the exceedance of critical water levels. As a result, severe floods can be recognized earlier and with higher reliability, providing more response time for local authorities. Although the limited data availability increases the risk of overfitting and lower performance for the first upcoming events, the model quality will increase with the incorporation of new measurement data in the future. The reduced complexity and high interpretability of the model allow for a fast decision-making process. Additionally, the model has high potential and can easily be adapted to similar small catchments. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. Early warning; Flash floods; Flood severity prediction; Heavy rainfall; Machine learning; Open-access data; Real-time information system Catchments; Decision trees; Forecasting; Learning algorithms; Machine learning; Open Data; Open systems; Rain; Risk assessment; Runoff; Topography; Water levels; Early warning; Flash-floods; Flood severity prediction; Heavy rainfall; Machine learning algorithms; Network learning algorithms; Open-access data; OpenAccess; Real-time information systems; Steep topography; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
654;A smart urban flood control and warning system based on big data;With the great-leap-forward development of social economy in recent years, urban-scale has expanded rapidly and the problem of urban flood control has become more prominent. The normal flood control system has been unable to meet the requirement of rapid urban development. As the urban drainage facilities improve and the internet of things monitoring equipment increase, the big data era has come. Therefore, a smart urban flood control and warning system based on big data will be crucial. In this paper, a system named smart urban flood control and warning system (SUFCWS) based on big data is proposed. The system is composed of user login, flood control basic data entry, water level and rainfall data search, real-time display, statistical analysis and flood warning, which integrates J2EE platform, SSH2 (Spring+Struts2+Hibernate) framework, the bootstrap front-end development kit, highcharts graphics library and Baidu Maps API. Using GM(1, 1) algorithm of grey forecasting model and back propagation neural network algorithm, SUFCWS can give available early warning of potential urban flood. Copyright © 2018 Inderscience Enterprises Ltd.;"Big data; Flood control and warning system; J2EE; JavaScript; Neural network";"Backpropagation; Big data; Flood control; Floods; Neural networks; System theory; Urban growth; Water levels; Back propagation neural networks; Grey forecasting model; J2EE; Javascript; Monitoring equipment; Rapid urban development; Real time display; Urban flood control; Search engines";"A smart urban flood control and warning system based on big data With the great-leap-forward development of social economy in recent years, urban-scale has expanded rapidly and the problem of urban flood control has become more prominent. The normal flood control system has been unable to meet the requirement of rapid urban development. As the urban drainage facilities improve and the internet of things monitoring equipment increase, the big data era has come. Therefore, a smart urban flood control and warning system based on big data will be crucial. In this paper, a system named smart urban flood control and warning system (SUFCWS) based on big data is proposed. The system is composed of user login, flood control basic data entry, water level and rainfall data search, real-time display, statistical analysis and flood warning, which integrates J2EE platform, SSH2 (Spring+Struts2+Hibernate) framework, the bootstrap front-end development kit, highcharts graphics library and Baidu Maps API. Using GM(1, 1) algorithm of grey forecasting model and back propagation neural network algorithm, SUFCWS can give available early warning of potential urban flood. Copyright © 2018 Inderscience Enterprises Ltd. Big data; Flood control and warning system; J2EE; JavaScript; Neural network Backpropagation; Big data; Flood control; Floods; Neural networks; System theory; Urban growth; Water levels; Back propagation neural networks; Grey forecasting model; J2EE; Javascript; Monitoring equipment; Rapid urban development; Real time display; Urban flood control; Search engines";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
655;PREDICTING HURRICANE-INDUCED INTENSE RAINFALL TRAJECTORIES USING GNSS;The tropospheric products of Global Navigation Satellite Systems (GNSS) can be used to determine the density and distribution of water vapor in the atmosphere, and therefore has been used to monitor precipitation events. This study analyzes GNSS-based precipitable water vapor (PWV) measurements calculated from the NOAA Continuously Operating Reference Stations (CORS) GNSS observations, to 1) track spatiotemporal variability of PWV, 2) identify the abnormal fluctuations in PWV level before the arrival of hurricane at a ground station, and 3) predict the path and relative intensity of hurricane-induced rainfalls. Firstly, we examined PWV perturbations with the local atmospheric elements including temperature and pressure, and relative humidity and revealed the relationship between the atmospheric parameters and the formation process of a severe precipitation. The CORSs are then classified into a training set and a test dataset. Numerically analyzed meteorological constituents for the training dataset were used to derive a PWV prediction model by applying a multivariate regression approach. The PWV prediction model quantifies the relationship among a spatiotemporal hurricane intensification, the PWV rate of change, and meteorological variables. To avoid the correlation effect between these variables, a principal component regression (PCR) was applied. From the PWV prediction model, the PWV at each test station was predicted with 12 and 24 hours of time scale. The PCR is then applied to test the dataset, and the model's residuals, which are the discrepancy between the model and measurements, are calculated to verify the model. The residual of the predicted model is a key factor to determine the trajectory of hurricane-induced rainfall and its intensity. By analyzing the distribution pattern of the predicted PWV residuals, their magnitude, and the observed PWV at the test site, the probable locations of intense rainfalls due to the storm front passage can be identified. For a robust analysis considering the uncertainty from the measurement noise and other error sources in the GNSS-derived PWV, we defined a grid in the test site that allows evaluating multiple stations' PWV prediction/measurements. The grid size was determined with the consideration of the test site and the geometric distribution of available CORSs, which their GNSS observations were used as the data feeds into the prediction model. Because various hurricanes have their own spatial and temporal characteristics, the approach is assessed for two different hurricanes occurred in the same location and showed different types of rainfall events that are Hurricane Mathew in 2016, and Hurricane Irma in 2017. Because both hurricanes landed in Florida and proceeded to Georgia, and South Carolina, the model's performance can be evaluated under similar geographic and climate characteristics of the study area. The results were validated by the radar reflectivity map and reported NHC hurricane landfall centers. The results showed that for both hurricanes, the highly probable locations of heavy precipitation by the grid-based prediction coincide to the grids with the minimum residuals of the prediction model. In addition, the negative correlation between the residuals of PWV measurements with the prediction model and the magnitude of precipitation was revealed. The magnitude of the predicted model residuals was used for hurricane tracking and applied to evaluation of the storm relative intensity. The study showed that predicted locations (grids) were contained at maximums of less than 25% and 32% of total residuals in the area for 12h and 24h prediction time lags, respectively. This study demonstrates the effectiveness of the statistical model for forecasting the intense precipitation path at least several hours before the arrival of a storm that can be applied to a hazard early warning system. © 2022 Proceedings of the International Technical Meeting of The Institute of Navigation, ITM. All rights reserved.;NULL;"Atmospheric humidity; Global positioning system; Probability distributions; Rain; Regression analysis; Statistical tests; Uncertainty analysis; Water vapor; Weather forecasting; Density of water; Global Navigation Satellite Systems; Intense rainfalls; Precipitable water vapour; Predicted models; Prediction modelling; Principal Components; Relative intensity; Test site; Water vapor measurement; Hurricanes";"PREDICTING HURRICANE-INDUCED INTENSE RAINFALL TRAJECTORIES USING GNSS The tropospheric products of Global Navigation Satellite Systems (GNSS) can be used to determine the density and distribution of water vapor in the atmosphere, and therefore has been used to monitor precipitation events. This study analyzes GNSS-based precipitable water vapor (PWV) measurements calculated from the NOAA Continuously Operating Reference Stations (CORS) GNSS observations, to 1) track spatiotemporal variability of PWV, 2) identify the abnormal fluctuations in PWV level before the arrival of hurricane at a ground station, and 3) predict the path and relative intensity of hurricane-induced rainfalls. Firstly, we examined PWV perturbations with the local atmospheric elements including temperature and pressure, and relative humidity and revealed the relationship between the atmospheric parameters and the formation process of a severe precipitation. The CORSs are then classified into a training set and a test dataset. Numerically analyzed meteorological constituents for the training dataset were used to derive a PWV prediction model by applying a multivariate regression approach. The PWV prediction model quantifies the relationship among a spatiotemporal hurricane intensification, the PWV rate of change, and meteorological variables. To avoid the correlation effect between these variables, a principal component regression (PCR) was applied. From the PWV prediction model, the PWV at each test station was predicted with 12 and 24 hours of time scale. The PCR is then applied to test the dataset, and the model's residuals, which are the discrepancy between the model and measurements, are calculated to verify the model. The residual of the predicted model is a key factor to determine the trajectory of hurricane-induced rainfall and its intensity. By analyzing the distribution pattern of the predicted PWV residuals, their magnitude, and the observed PWV at the test site, the probable locations of intense rainfalls due to the storm front passage can be identified. For a robust analysis considering the uncertainty from the measurement noise and other error sources in the GNSS-derived PWV, we defined a grid in the test site that allows evaluating multiple stations' PWV prediction/measurements. The grid size was determined with the consideration of the test site and the geometric distribution of available CORSs, which their GNSS observations were used as the data feeds into the prediction model. Because various hurricanes have their own spatial and temporal characteristics, the approach is assessed for two different hurricanes occurred in the same location and showed different types of rainfall events that are Hurricane Mathew in 2016, and Hurricane Irma in 2017. Because both hurricanes landed in Florida and proceeded to Georgia, and South Carolina, the model's performance can be evaluated under similar geographic and climate characteristics of the study area. The results were validated by the radar reflectivity map and reported NHC hurricane landfall centers. The results showed that for both hurricanes, the highly probable locations of heavy precipitation by the grid-based prediction coincide to the grids with the minimum residuals of the prediction model. In addition, the negative correlation between the residuals of PWV measurements with the prediction model and the magnitude of precipitation was revealed. The magnitude of the predicted model residuals was used for hurricane tracking and applied to evaluation of the storm relative intensity. The study showed that predicted locations (grids) were contained at maximums of less than 25% and 32% of total residuals in the area for 12h and 24h prediction time lags, respectively. This study demonstrates the effectiveness of the statistical model for forecasting the intense precipitation path at least several hours before the arrival of a storm that can be applied to a hazard early warning system. © 2022 Proceedings of the International Technical Meeting of The Institute of Navigation, ITM. All rights reserved. NULL Atmospheric humidity; Global positioning system; Probability distributions; Rain; Regression analysis; Statistical tests; Uncertainty analysis; Water vapor; Weather forecasting; Density of water; Global Navigation Satellite Systems; Intense rainfalls; Precipitable water vapour; Predicted models; Prediction modelling; Principal Components; Relative intensity; Test site; Water vapor measurement; Hurricanes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
656;Research on Intelligent Disaster Prevention and Mitigation Method in High Flood Risk Area of River Basin Based on Artificial Neural Network;"The continuous and rapid development of industrialization and urbanization in the past 40 years has caused the urban and rural landuse based on natural ecosystem in the western region of the Taihu Lake Basin to face the problems of landuse scale reduction and spatial structure fragmentation, which has led to a serious reduction in stormwater regulation and storage capacity. Starting from the effects of flood disaster mitigation in the river basin, this paper introduces the theory of complex adaptive systems, natural-based solutions, combined with the theoretical connotation of sponge city planning and construction, constructs a theoretical model of flood disaster complex system, and further builds a disaster mitigation effect evaluation model for sponge city planning and construction; the 3S method is used to evaluate and visualize the indicators, and the artificial intelligence BP neural network method is used to calculate the disaster reduction effect level of 357 plots in Yixing City. The results show that the disaster mitigation level in the mountain forest area and central urban area in the south of Yixing City is high, and the level in the north is low. This shows that the natural ecological large sponge has an obvious effect on urban flood disaster mitigation; appropriate LID measures can effectively compensate for the fewer defects of the large sponge; complementary, the through river network can reduce the flood disaster to a certain extent.  © 2021 IEEE.";"Artificial neural network; Complex adaptive system; Intelligent disaster mitigation method; Nature-based solutions; Sponge city; stormwater regulation and storage capacity; Taihu Lake Basin; Yixing";"Adaptive systems; Complex networks; Disaster prevention; Disasters; Floods; Lakes; Planning; Storms; Urban planning; Watersheds; Complex adaptive systems; Disaster mitigation; Intelligent disaster mitigation method; Mitigation methods; Nature-based solution; Regulation capacity; Sponge city; Storage capacity; Stormwater regulation; Stormwater regulation and storage capacity; Stormwater storage; Taihu Lake basin; Yixing; Neural networks";"Research on Intelligent Disaster Prevention and Mitigation Method in High Flood Risk Area of River Basin Based on Artificial Neural Network The continuous and rapid development of industrialization and urbanization in the past 40 years has caused the urban and rural landuse based on natural ecosystem in the western region of the Taihu Lake Basin to face the problems of landuse scale reduction and spatial structure fragmentation, which has led to a serious reduction in stormwater regulation and storage capacity. Starting from the effects of flood disaster mitigation in the river basin, this paper introduces the theory of complex adaptive systems, natural-based solutions, combined with the theoretical connotation of sponge city planning and construction, constructs a theoretical model of flood disaster complex system, and further builds a disaster mitigation effect evaluation model for sponge city planning and construction; the 3S method is used to evaluate and visualize the indicators, and the artificial intelligence BP neural network method is used to calculate the disaster reduction effect level of 357 plots in Yixing City. The results show that the disaster mitigation level in the mountain forest area and central urban area in the south of Yixing City is high, and the level in the north is low. This shows that the natural ecological large sponge has an obvious effect on urban flood disaster mitigation; appropriate LID measures can effectively compensate for the fewer defects of the large sponge; complementary, the through river network can reduce the flood disaster to a certain extent.  © 2021 IEEE. Artificial neural network; Complex adaptive system; Intelligent disaster mitigation method; Nature-based solutions; Sponge city; stormwater regulation and storage capacity; Taihu Lake Basin; Yixing Adaptive systems; Complex networks; Disaster prevention; Disasters; Floods; Lakes; Planning; Storms; Urban planning; Watersheds; Complex adaptive systems; Disaster mitigation; Intelligent disaster mitigation method; Mitigation methods; Nature-based solution; Regulation capacity; Sponge city; Storage capacity; Stormwater regulation; Stormwater regulation and storage capacity; Stormwater storage; Taihu Lake basin; Yixing; Neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
657;Urban Flood Disaster Mitigation through Image Classification Using Transfer Learning Method with MobileNet Fine-tuning;Floods severely threaten numerous countries worldwide, with Indonesia being particularly vulnerable. Floods can profoundly affect various aspects of life, underscoring the critical importance of flood mitigation measures. Leveraging advancements in artificial intelligence and computer vision, particularly in deep learning, offers novel avenues for flood monitoring, particularly in urban areas. This research encompasses several vital stages, encompassing data collection, data pre-processing, the application of fine-tuned transfer learning techniques, and evaluation using a confusion matrix. The dataset comprises 2000 images categorized into flood and non-flood classes. Following the training, validation, and testing phases, impressive outcomes emerged. Training accuracy soared to 99% with a minimal loss of 0.01, while validation accuracy reached 96% with a loss of 0.19. Test results were equally stellar, with the confusion matrix displaying accuracy of 95%, recall of 97%, and an F-1 score of 96%. With a computing time of just 48.9 seconds for testing 200 images and a 99% confidence level in classifying each image further underscored the model's exceptional performance. These findings attest to the model's optimal operation and ability to effectively detect and classify flood images. The information produced by this model is beneficial. © 2023 IEEE.;"Computer Vision; Deep Learning; Fine-tuning; Flood Detection; MobileNet; Transfer Learning";"Computer vision; Data handling; Deep learning; Image classification; Image resolution; Learning systems; Matrix algebra; Confusion matrix; Deep learning; Disaster mitigation; Fine tuning; Flood detections; Images classification; Mobilenet; Transfer learning; Transfer learning methods; Urban flood disasters; Floods";"Urban Flood Disaster Mitigation through Image Classification Using Transfer Learning Method with MobileNet Fine-tuning Floods severely threaten numerous countries worldwide, with Indonesia being particularly vulnerable. Floods can profoundly affect various aspects of life, underscoring the critical importance of flood mitigation measures. Leveraging advancements in artificial intelligence and computer vision, particularly in deep learning, offers novel avenues for flood monitoring, particularly in urban areas. This research encompasses several vital stages, encompassing data collection, data pre-processing, the application of fine-tuned transfer learning techniques, and evaluation using a confusion matrix. The dataset comprises 2000 images categorized into flood and non-flood classes. Following the training, validation, and testing phases, impressive outcomes emerged. Training accuracy soared to 99% with a minimal loss of 0.01, while validation accuracy reached 96% with a loss of 0.19. Test results were equally stellar, with the confusion matrix displaying accuracy of 95%, recall of 97%, and an F-1 score of 96%. With a computing time of just 48.9 seconds for testing 200 images and a 99% confidence level in classifying each image further underscored the model's exceptional performance. These findings attest to the model's optimal operation and ability to effectively detect and classify flood images. The information produced by this model is beneficial. © 2023 IEEE. Computer Vision; Deep Learning; Fine-tuning; Flood Detection; MobileNet; Transfer Learning Computer vision; Data handling; Deep learning; Image classification; Image resolution; Learning systems; Matrix algebra; Confusion matrix; Deep learning; Disaster mitigation; Fine tuning; Flood detections; Images classification; Mobilenet; Transfer learning; Transfer learning methods; Urban flood disasters; Floods";4;Multi-task Learning;Transfer Learning,Lifelong Machine Learning,Learning under Covariate Shift;1.2;Hydrological;2;Preparation
658;Framework for IOT Based Real-Time Monitoring System of Rainfall Water Level for Flood Prediction Using LSTM Network;Recent floods have consistently resulted in human casualties in addition to harm to the environment and the economy. People are less likely to be aware of oncoming floods since there is not a reliable early warning system. A deep learning algorithm is suggested in this study to forecast groundwater levels. Even with inadequate data, the model can nevertheless successfully fulfil the prediction objective. In order to determine the degree of temporal dependence that exists between groundwater level and meteorological indicators, a hybrid model called CNN-LSTM-ML is developed. This model makes use of a network structure that is a mix of CNN and LSTM networks. Long Short-Term Memory (LSTM) networks are used by the rainfall forecasting model in order to provide predictions about future rainfall and water level values, which may lead to floods. In order to conduct experiments on the principal finding, historical data from two sites with rainfall and one location with streamflow were used. Additional information was acquired from two water level stations and one rainfall station in order to confirm the basic findings. With assessment errors for historical data MAE, RMSE, and MSE of 0.93, 1.7, and 3.025 correspondingly, the forecasting technique that used LSTM demonstrated great 'accuracy' of the outcome reaching more than 92% using IoT. According to these results, the system has the potential to be used as a cure that does not involve the construction of new structures in order to minimize the damage caused by urban floods.  © 2023 IEEE.;"CNN; Internet of Things (IoT); Long Short-Term Memory (LSTM); Water level";"Brain; Floods; Groundwater; Long short-term memory; Rain; Water levels; Weather forecasting; Early Warning System; Flood prediction; Ground water level; Historical data; Inadequate data; Internet of thing; Long short-term memory; Memory network; Real time monitoring system; Temporal dependence; Internet of things";"Framework for IOT Based Real-Time Monitoring System of Rainfall Water Level for Flood Prediction Using LSTM Network Recent floods have consistently resulted in human casualties in addition to harm to the environment and the economy. People are less likely to be aware of oncoming floods since there is not a reliable early warning system. A deep learning algorithm is suggested in this study to forecast groundwater levels. Even with inadequate data, the model can nevertheless successfully fulfil the prediction objective. In order to determine the degree of temporal dependence that exists between groundwater level and meteorological indicators, a hybrid model called CNN-LSTM-ML is developed. This model makes use of a network structure that is a mix of CNN and LSTM networks. Long Short-Term Memory (LSTM) networks are used by the rainfall forecasting model in order to provide predictions about future rainfall and water level values, which may lead to floods. In order to conduct experiments on the principal finding, historical data from two sites with rainfall and one location with streamflow were used. Additional information was acquired from two water level stations and one rainfall station in order to confirm the basic findings. With assessment errors for historical data MAE, RMSE, and MSE of 0.93, 1.7, and 3.025 correspondingly, the forecasting technique that used LSTM demonstrated great 'accuracy' of the outcome reaching more than 92% using IoT. According to these results, the system has the potential to be used as a cure that does not involve the construction of new structures in order to minimize the damage caused by urban floods.  © 2023 IEEE. CNN; Internet of Things (IoT); Long Short-Term Memory (LSTM); Water level Brain; Floods; Groundwater; Long short-term memory; Rain; Water levels; Weather forecasting; Early Warning System; Flood prediction; Ground water level; Historical data; Inadequate data; Internet of thing; Long short-term memory; Memory network; Real time monitoring system; Temporal dependence; Internet of things";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
659;The opera project: EO-based flood risk management in Italy;"This paper illustrates some applications of COSMO-SkyMed (CSK) observations for rapid mapping of flooded areas and damages in small to medium size catchments. The results presented here have been obtained within the framework of the project ""OPERA - Civil protection from floods"" funded by the Italian Space Agency and run by a team of scientific research centres and private companies. The project aims to the systematic evaluation of the added value of the use of Earth Observation techniques into operational flood prediction chains. Due to the specific geomorphology of Italy, the focus is mainly on flash floods on small sized river catchments. Monitoring and modelling processes at proper space-time scales in this environment raise several issues to be solved, compared to applications in larger river basins. Here we address some related to the suitable use of CSK imagery. ©2009 IEEE.";"Satellite applications; Synthetic aperture radar; Water";"Catchments; Flood control; Geology; Image classification; Imaging systems; Privatization; Radar; Remote sensing; Risk management; Runoff; Space optics; Synthetic aperture radar; Synthetic apertures; Added values; Civil protection; Earth observation techniques; Flash flood; Flood prediction; Flood risk management; Flooded areas; Italian Space Agency; Medium size; Modelling process; Private companies; Rapid mapping; River basins; River catchment; Satellite applications; Scientific researches; Systematic evaluation; Time-scales; Floods";"The opera project: EO-based flood risk management in Italy This paper illustrates some applications of COSMO-SkyMed (CSK) observations for rapid mapping of flooded areas and damages in small to medium size catchments. The results presented here have been obtained within the framework of the project ""OPERA - Civil protection from floods"" funded by the Italian Space Agency and run by a team of scientific research centres and private companies. The project aims to the systematic evaluation of the added value of the use of Earth Observation techniques into operational flood prediction chains. Due to the specific geomorphology of Italy, the focus is mainly on flash floods on small sized river catchments. Monitoring and modelling processes at proper space-time scales in this environment raise several issues to be solved, compared to applications in larger river basins. Here we address some related to the suitable use of CSK imagery. ©2009 IEEE. Satellite applications; Synthetic aperture radar; Water Catchments; Flood control; Geology; Image classification; Imaging systems; Privatization; Radar; Remote sensing; Risk management; Runoff; Space optics; Synthetic aperture radar; Synthetic apertures; Added values; Civil protection; Earth observation techniques; Flash flood; Flood prediction; Flood risk management; Flooded areas; Italian Space Agency; Medium size; Modelling process; Private companies; Rapid mapping; River basins; River catchment; Satellite applications; Scientific researches; Systematic evaluation; Time-scales; Floods";-1;Não Classificado;NULL;1.2;Hydrological;1;Prevention
660;Deep reinforcement learning for optimal rescue path planning in uncertain and complex urban pluvial flood scenarios;An urban pluvial flood is a devastating, costly natural disaster requiring effective rescue path planning to mitigate the loss of lives and property. The inherent uncertainty and complexity of the risks associated with urban flooding limit the ability to plan optimal rescue paths that prioritize both timeliness and safety. This study addresses the challenge by proposing an innovative assessment methodology to output risk values and probability representing safety and timeliness in each passable area while simulating real-world flood scenarios. Furthermore, the paper develops a pioneering path-planning algorithm based on deep reinforcement learning, incorporating improved stochastic reward exploitation and heterogeneous reward exploration mechanisms to function in a simulation rescue path-planning scenario with uncertainty and complexity. According to the findings, the proposed algorithm outperforms current state-of-the-art algorithms in converging to the optimal path, fully sampling, and running efficiency. The study contributes to theoretical progress on urban pluvial flood rescue, deep reinforcement learning, risk assessment, and decision intelligence while offering practical implications for smart cities, emergency management, and optimizing real-world problems by employing artificial intelligence. © 2023 Elsevier B.V.;"Deep reinforcement learning; Intelligent computing; Rescue path planning; Smart city; Urban pluvial flood";"Deep learning; Disasters; Learning systems; Motion planning; Reinforcement learning; Risk assessment; Risk management; Risk perception; Smart city; Stochastic systems; Deep reinforcement learning; Loss of life; Natural disasters; Pluvials; Property; Reinforcement learnings; Rescue path planning; Uncertainty and complexity; Urban flooding; Urban pluvial flood; Floods";"Deep reinforcement learning for optimal rescue path planning in uncertain and complex urban pluvial flood scenarios An urban pluvial flood is a devastating, costly natural disaster requiring effective rescue path planning to mitigate the loss of lives and property. The inherent uncertainty and complexity of the risks associated with urban flooding limit the ability to plan optimal rescue paths that prioritize both timeliness and safety. This study addresses the challenge by proposing an innovative assessment methodology to output risk values and probability representing safety and timeliness in each passable area while simulating real-world flood scenarios. Furthermore, the paper develops a pioneering path-planning algorithm based on deep reinforcement learning, incorporating improved stochastic reward exploitation and heterogeneous reward exploration mechanisms to function in a simulation rescue path-planning scenario with uncertainty and complexity. According to the findings, the proposed algorithm outperforms current state-of-the-art algorithms in converging to the optimal path, fully sampling, and running efficiency. The study contributes to theoretical progress on urban pluvial flood rescue, deep reinforcement learning, risk assessment, and decision intelligence while offering practical implications for smart cities, emergency management, and optimizing real-world problems by employing artificial intelligence. © 2023 Elsevier B.V. Deep reinforcement learning; Intelligent computing; Rescue path planning; Smart city; Urban pluvial flood Deep learning; Disasters; Learning systems; Motion planning; Reinforcement learning; Risk assessment; Risk management; Risk perception; Smart city; Stochastic systems; Deep reinforcement learning; Loss of life; Natural disasters; Pluvials; Property; Reinforcement learnings; Rescue path planning; Uncertainty and complexity; Urban flooding; Urban pluvial flood; Floods";3;Reinforcement Learning;Sequential Decision Making,Inverse Reinforcement Learning,Apprenticeship Learning,Multi-agent Reinforcement Learning,Adversarial Learning;1.2;Hydrological;3;Response
661;Flash flood forecasting by statistical learning in the absence of rainfall forecast: A case study;"The feasibility of flash flood forecasting without making use of rainfall predictions is investigated. After a presentation of the ""cevenol flash floods"", which caused 1.2 billion Euros of economical damages and 22 fatalities in 2002, the difficulties incurred in the forecasting of such events are analyzed, with emphasis on the nature of the database and the origins of measurement noise. The high level of noise in water level measurements raises a real challenge. For this reason, two regularization methods have been investigated and compared: early stopping and weight decay. It appears that regularization by early stopping provides networks with lower complexity and more accurate predicted hydrographs than regularization by weight decay. Satisfactory results can thus be obtained up to a forecasting horizon of three hours, thereby allowing an early warning of the populations. © 2009 Springer-Verlag.";"early stopping; Forecasting; generalization; identification; machine learning; neural network; weight decay";"Flood control; Floods; Learning systems; Rain; Water levels; Weather forecasting; Early stopping; generalization; identification; Machine-learning; Weight decay; Neural networks";"Flash flood forecasting by statistical learning in the absence of rainfall forecast: A case study The feasibility of flash flood forecasting without making use of rainfall predictions is investigated. After a presentation of the ""cevenol flash floods"", which caused 1.2 billion Euros of economical damages and 22 fatalities in 2002, the difficulties incurred in the forecasting of such events are analyzed, with emphasis on the nature of the database and the origins of measurement noise. The high level of noise in water level measurements raises a real challenge. For this reason, two regularization methods have been investigated and compared: early stopping and weight decay. It appears that regularization by early stopping provides networks with lower complexity and more accurate predicted hydrographs than regularization by weight decay. Satisfactory results can thus be obtained up to a forecasting horizon of three hours, thereby allowing an early warning of the populations. © 2009 Springer-Verlag. early stopping; Forecasting; generalization; identification; machine learning; neural network; weight decay Flood control; Floods; Learning systems; Rain; Water levels; Weather forecasting; Early stopping; generalization; identification; Machine-learning; Weight decay; Neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
662;A novel flood defense decision support system for smart urban management based on classification and regression tree;With the development of the internet of things technology and awareness technology, all kinds of big data in the city have started to emerge. Under the background in internet plus era, using big data to effectively forecast urban flood disaster, formulating the flood control and disaster mitigation countermeasures in time, is an important subject of urban flood control and research. In this paper, a novel flood defense decision support system (NFDDSS) is proposed. Using historical hydrology data in Hangzhou, this paper proposed a comprehensive consideration of time correlation and spatial correlation of water level prediction model based on classification and regression tree. This model can predict the water level in one to six hours effectively. With this system, supervisors can get timely and effective guidance of flood control and disaster mitigation when the flood season comes. Copyright © 2018 Inderscience Enterprises Ltd.;"Big data; CART; Classification and regression tree; Decision support system; Flood defense; Water level prediction";"Big data; Disasters; Flood control; Floods; Forecasting; Forestry; Network security; Predictive analytics; Trees (mathematics); Water levels; CART; Classification and regression tree; Flood defense; Historical hydrologies; Internet of things technologies; Spatial correlations; Urban flood disasters; Water level prediction; Decision support systems";"A novel flood defense decision support system for smart urban management based on classification and regression tree With the development of the internet of things technology and awareness technology, all kinds of big data in the city have started to emerge. Under the background in internet plus era, using big data to effectively forecast urban flood disaster, formulating the flood control and disaster mitigation countermeasures in time, is an important subject of urban flood control and research. In this paper, a novel flood defense decision support system (NFDDSS) is proposed. Using historical hydrology data in Hangzhou, this paper proposed a comprehensive consideration of time correlation and spatial correlation of water level prediction model based on classification and regression tree. This model can predict the water level in one to six hours effectively. With this system, supervisors can get timely and effective guidance of flood control and disaster mitigation when the flood season comes. Copyright © 2018 Inderscience Enterprises Ltd. Big data; CART; Classification and regression tree; Decision support system; Flood defense; Water level prediction Big data; Disasters; Flood control; Floods; Forecasting; Forestry; Network security; Predictive analytics; Trees (mathematics); Water levels; CART; Classification and regression tree; Flood defense; Historical hydrologies; Internet of things technologies; Spatial correlations; Urban flood disasters; Water level prediction; Decision support systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
663;Smart Transportation for Terrestrial Flooding and Lunar Search and Rescue;Traffic congestion, less-than-optimal public transit infrastructure, and the inconveniences of transportation systems are common challenges in disaster response in urban areas. Smart Transportation systems, founded upon the communication of data and revolutionized by global navigation satellite technologies, the Internet of Things, and mobile technologies, play a crucial role in mitigating disaster impacts. The participants of the Space Studies Program of the International Space University (ISU) - held at Rice University, Houston, Texas, USA during the summer of 2024 - have worked on a Team Project that explored Smart Transportation systems in disaster response, focusing on flood management in urban areas and lunar Search and Rescue (SAR) operations. These applications could integrate the use of additional technologies such as artificial intelligence, machine learning, cloud computing, and remote sensing data sources from drones and satellites to enhance real-time data dissemination and communication during emergencies. New innovations such as Al-equipped spacesuits, pressurized rovers, and robust communication networks are critical for lunar Search and Rescue and emergency response. This paper presents the Team Project findings on Smart Transportation systems and identifies emerging space-related technologies (satellite data, location-based services, artificial intelligence, Internet of Things, etc.) for Earth and lunar applications. Opportunities for the use of space data in improving the disaster response, mobility, and logistics sectors are addressed, including case studies on urban flood management and lunar emergency logistics. Copyright © 2024 by the International Astronautical Federation (IAF). All rights reserved.;"Disaster Management; Lunar Mobility; Search & Rescue; Smart Transportation; Urban flooding";"Air navigation; Global positioning system; Mass transportation; Satellite communication systems; Space flight; Urban transportation; Disaster management; Disaster-response; Lunar mobility; Search & rescue; Search and rescue; Smart transportation; Team projects; Transportation system; Urban areas; Urban flooding; Traffic congestion";"Smart Transportation for Terrestrial Flooding and Lunar Search and Rescue Traffic congestion, less-than-optimal public transit infrastructure, and the inconveniences of transportation systems are common challenges in disaster response in urban areas. Smart Transportation systems, founded upon the communication of data and revolutionized by global navigation satellite technologies, the Internet of Things, and mobile technologies, play a crucial role in mitigating disaster impacts. The participants of the Space Studies Program of the International Space University (ISU) - held at Rice University, Houston, Texas, USA during the summer of 2024 - have worked on a Team Project that explored Smart Transportation systems in disaster response, focusing on flood management in urban areas and lunar Search and Rescue (SAR) operations. These applications could integrate the use of additional technologies such as artificial intelligence, machine learning, cloud computing, and remote sensing data sources from drones and satellites to enhance real-time data dissemination and communication during emergencies. New innovations such as Al-equipped spacesuits, pressurized rovers, and robust communication networks are critical for lunar Search and Rescue and emergency response. This paper presents the Team Project findings on Smart Transportation systems and identifies emerging space-related technologies (satellite data, location-based services, artificial intelligence, Internet of Things, etc.) for Earth and lunar applications. Opportunities for the use of space data in improving the disaster response, mobility, and logistics sectors are addressed, including case studies on urban flood management and lunar emergency logistics. Copyright © 2024 by the International Astronautical Federation (IAF). All rights reserved. Disaster Management; Lunar Mobility; Search & Rescue; Smart Transportation; Urban flooding Air navigation; Global positioning system; Mass transportation; Satellite communication systems; Space flight; Urban transportation; Disaster management; Disaster-response; Lunar mobility; Search & rescue; Search and rescue; Smart transportation; Team projects; Transportation system; Urban areas; Urban flooding; Traffic congestion";-1;Não Classificado;NULL;1.2;Hydrological;3;Response
664;Mapping Severe Tropical Cyclone Tauktae Across the Arabian Sea and Western Coast of India Using Remote Sensing and Machine Learning During May 2021;In May 2021, the Arabian Sea experienced the formation of a tropical supercyclone, 'Tauktae,' which resulted in extensive destruction throughout the coastline regions of Mumbai, Goa, and Gujarat. The cyclone initiated from a low-pressure system situated over the Bay of Bengal, progressively strengthening into the cyclonic storm named 'Tauktae' and subsequently moving along the western coast of India. The objective of this study is to analyze the characteristics of cyclones by utilizing higher rainfall estimations obtained from the Meteosat-8 satellite. The analysis of Meteosat-8 observations was conducted utilizing machine learning (ML) techniques. During the cyclone, there were notable quantities of precipitation that surpassed 600mm per day. The diurnal changes exhibited pronounced levels of precipitation, with rainfall rates reaching 50-60mm/hr in regions badly impacted by the cyclone, namely Gujarat, Mumbai, and Goa. The study area encountered a substantial precipitation event on May 20, 2021, with an estimated rainfall accumulation of around 650mm. The cyclone 'Tauktae' exhibited a northwestward trajectory with a velocity of 16 kilometers per hour across the Arabian Sea. Its point of origin was located at a latitude of 16.2°N and a longitude of 72.6°E, precisely around 1430 Indian Standard Time (IST).The region of Goa saw significant consequences of Cyclone Tauktae, which manifested in the form of powerful winds, elevated sea levels, and heavy precipitation. Moreover, the occurrence of heavy rainstorms on May 20th resulted in significant cumulative precipitation, which subsequently caused landslides and catastrophic flash flooding in multiple regions along India's western coastline. The current investigation highlights the significance of employing high-resolution satellite data and machine learning techniques for real-time mapping and monitoring of atmospheric parameters. This approach is crucial for the successful implementation of catastrophe mitigation methods. Through the utilization of cutting-edge satellite technology and the meticulous analysis of up-to-the-minute data, policymakers and authorities responsible for disaster management can acquire invaluable knowledge pertaining to the characteristics and severity of cyclonic occurrences such as Tauktae. This understanding is crucial for developing effective ways to lessen the impact of such disasters and protect vulnerable coastal populations. This study highlights the importance of obtaining timely and precise information through the use of high-resolution satellite observations in the context of disaster management and preparedness. © 2023 IEEE.;"Cumulative rainfall; Disaster mitigation; flash flooding; Satellite remote sensing";"Disaster prevention; Disasters; Hurricanes; Landslides; Machine learning; Mapping; Rain; Remote sensing; Satellites; Sea level; Tropical cyclone; Tropics; Arabian sea; Cumulative rainfall; Disaster management; Disaster mitigation; Flash flooding; Gujarat; Machine learning techniques; Meteosat-8; Satellite remote sensing; Tropical cyclone; Floods";"Mapping Severe Tropical Cyclone Tauktae Across the Arabian Sea and Western Coast of India Using Remote Sensing and Machine Learning During May 2021 In May 2021, the Arabian Sea experienced the formation of a tropical supercyclone, 'Tauktae,' which resulted in extensive destruction throughout the coastline regions of Mumbai, Goa, and Gujarat. The cyclone initiated from a low-pressure system situated over the Bay of Bengal, progressively strengthening into the cyclonic storm named 'Tauktae' and subsequently moving along the western coast of India. The objective of this study is to analyze the characteristics of cyclones by utilizing higher rainfall estimations obtained from the Meteosat-8 satellite. The analysis of Meteosat-8 observations was conducted utilizing machine learning (ML) techniques. During the cyclone, there were notable quantities of precipitation that surpassed 600mm per day. The diurnal changes exhibited pronounced levels of precipitation, with rainfall rates reaching 50-60mm/hr in regions badly impacted by the cyclone, namely Gujarat, Mumbai, and Goa. The study area encountered a substantial precipitation event on May 20, 2021, with an estimated rainfall accumulation of around 650mm. The cyclone 'Tauktae' exhibited a northwestward trajectory with a velocity of 16 kilometers per hour across the Arabian Sea. Its point of origin was located at a latitude of 16.2°N and a longitude of 72.6°E, precisely around 1430 Indian Standard Time (IST).The region of Goa saw significant consequences of Cyclone Tauktae, which manifested in the form of powerful winds, elevated sea levels, and heavy precipitation. Moreover, the occurrence of heavy rainstorms on May 20th resulted in significant cumulative precipitation, which subsequently caused landslides and catastrophic flash flooding in multiple regions along India's western coastline. The current investigation highlights the significance of employing high-resolution satellite data and machine learning techniques for real-time mapping and monitoring of atmospheric parameters. This approach is crucial for the successful implementation of catastrophe mitigation methods. Through the utilization of cutting-edge satellite technology and the meticulous analysis of up-to-the-minute data, policymakers and authorities responsible for disaster management can acquire invaluable knowledge pertaining to the characteristics and severity of cyclonic occurrences such as Tauktae. This understanding is crucial for developing effective ways to lessen the impact of such disasters and protect vulnerable coastal populations. This study highlights the importance of obtaining timely and precise information through the use of high-resolution satellite observations in the context of disaster management and preparedness. © 2023 IEEE. Cumulative rainfall; Disaster mitigation; flash flooding; Satellite remote sensing Disaster prevention; Disasters; Hurricanes; Landslides; Machine learning; Mapping; Rain; Remote sensing; Satellites; Sea level; Tropical cyclone; Tropics; Arabian sea; Cumulative rainfall; Disaster management; Disaster mitigation; Flash flooding; Gujarat; Machine learning techniques; Meteosat-8; Satellite remote sensing; Tropical cyclone; Floods";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;1;Prevention
665;A novel ensemble learning based on Bayesian Belief Network coupled with an extreme learning machine for flash flood susceptibility mapping;Reliable flash flood susceptibility maps are a vital tool for land planners and emergency management officials for early flood warning and mitigation. We have developed a new ensemble learning model that predicts flash flood susceptibility at Haraz, Iran. The new model couples a Bayesian Belief Network (BBN) model with an extreme learning machine (ELM) and backpropagation (BP) structure optimized by a genetic algorithm (GA) named GA-BN-NN model. We applied the support vector machine (SVM) technique to a database of 194 flood locations with ten conditioning factors. An artificial neural network (ANN) algorithm with a multi-layer perceptron function, MLP-BP, optimized by a genetic algorithm, GA-MLP, and a shuffled frog-leaping algorithm, SFLA-MLP, were used as benchmark models for assessing the power prediction of the proposed model. Statistical measures, including sensitivity, specificity, accuracy, F1-measure and Jaccard coefficient, and root mean square error, were used to evaluate the goodness-of-fit and prediction accuracy, respectively, of the training and testing datasets. We found that all ten factors are positively correlated with flood occurrence, but slope angle has the highest average merit (AM=9.7) and thus contributes most to the occurrence of flooding. Results indicate that the GA-BN-NN model has the highest goodness-of-fit and prediction accuracy (AUC=0.966) and hence outperforms other ensemble learning models that we tested — the SFLA-MLP, MLP-BP, and GA-MLP models. We thus conclude that the proposed model is a promising technique for managing risk in flood-prone areas around the world. © 2020 Elsevier Ltd;"Ensemble learning; Environmental modeling; Flash flood; Optimization; Predicting system";"Backpropagation; Bayesian networks; Floods; Forecasting; Genetic algorithms; Knowledge acquisition; Mean square error; Multilayer neural networks; Risk management; Support vector machines; Extreme learning machine; Jaccard coefficients; Multi layer perceptron; Root mean square errors; Shuffled frog leaping algorithm (SFLA); Statistical measures; Support vector machine techniques; Susceptibility mapping; Learning systems";"A novel ensemble learning based on Bayesian Belief Network coupled with an extreme learning machine for flash flood susceptibility mapping Reliable flash flood susceptibility maps are a vital tool for land planners and emergency management officials for early flood warning and mitigation. We have developed a new ensemble learning model that predicts flash flood susceptibility at Haraz, Iran. The new model couples a Bayesian Belief Network (BBN) model with an extreme learning machine (ELM) and backpropagation (BP) structure optimized by a genetic algorithm (GA) named GA-BN-NN model. We applied the support vector machine (SVM) technique to a database of 194 flood locations with ten conditioning factors. An artificial neural network (ANN) algorithm with a multi-layer perceptron function, MLP-BP, optimized by a genetic algorithm, GA-MLP, and a shuffled frog-leaping algorithm, SFLA-MLP, were used as benchmark models for assessing the power prediction of the proposed model. Statistical measures, including sensitivity, specificity, accuracy, F1-measure and Jaccard coefficient, and root mean square error, were used to evaluate the goodness-of-fit and prediction accuracy, respectively, of the training and testing datasets. We found that all ten factors are positively correlated with flood occurrence, but slope angle has the highest average merit (AM=9.7) and thus contributes most to the occurrence of flooding. Results indicate that the GA-BN-NN model has the highest goodness-of-fit and prediction accuracy (AUC=0.966) and hence outperforms other ensemble learning models that we tested — the SFLA-MLP, MLP-BP, and GA-MLP models. We thus conclude that the proposed model is a promising technique for managing risk in flood-prone areas around the world. © 2020 Elsevier Ltd Ensemble learning; Environmental modeling; Flash flood; Optimization; Predicting system Backpropagation; Bayesian networks; Floods; Forecasting; Genetic algorithms; Knowledge acquisition; Mean square error; Multilayer neural networks; Risk management; Support vector machines; Extreme learning machine; Jaccard coefficients; Multi layer perceptron; Root mean square errors; Shuffled frog leaping algorithm (SFLA); Statistical measures; Support vector machine techniques; Susceptibility mapping; Learning systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;1;Prevention
666;Enhancing a Real-Time Flash Flood Predictive Accuracy Approach for the Development of Early Warning Systems: Hydrological Ensemble Hindcasts and Parameterizations;This study marks a significant step toward the future development of river discharges forecasted in real time for flash flood early warning system (EWS) disaster prevention frameworks in the Chugoku region of Japan, and presumably worldwide. To reduce the disaster impacts with EWSs, accurate integrated hydrometeorological real-time models for predicting extreme river water levels and discharges are needed, but they are not satisfactorily accurate due to large uncertainties. This study evaluates two calibration methods with 7 and 5 parameters using the hydrological Cell Distributed Runoff Model version 3.1.1 (CDRM), calibrated by the University of Arizona’s Shuffled Complex Evolution optimization method (SCE-UA). We hypothesize that the proposed ensemble hydrological parameter calibration approach can forecast similar future events in real time. This approach was applied to seven major rivers in the region to obtain hindcasts of the river discharges during the Heavy Rainfall Event of July 2018 (HRE18). This study introduces a new historical extreme rainfall event classification selection methodology that enables ensemble-averaged validation results of all river discharges. The reproducibility metrics obtained for all rivers cumulatively are extremely high, with Nash–Sutcliffe efficiency values of 0.98. This shows that the proposed approach enables accurate predictions of the river discharges for the HRE18 and, similarly, real-time forecasts for future extreme rainfall-induced events in the Japanese region. Although our methodology can be directly reapplied only in regions where observed rainfall data are readily available, we suggest that our approach can analogously be applied worldwide, which indicates a broad scientific contribution and multidisciplinary applications. © 2023 by the authors.;"2018 western Japan floods; extreme river discharge; hydrograph reproducibility; physically distributed hydrological model; real-time forecasting; SCE-UA optimization";"Chugoku; Honshu; Japan; early warning system; ensemble forecasting; flash flood; hydrograph; hydrological modeling; optimization; real time; river discharge";"Enhancing a Real-Time Flash Flood Predictive Accuracy Approach for the Development of Early Warning Systems: Hydrological Ensemble Hindcasts and Parameterizations This study marks a significant step toward the future development of river discharges forecasted in real time for flash flood early warning system (EWS) disaster prevention frameworks in the Chugoku region of Japan, and presumably worldwide. To reduce the disaster impacts with EWSs, accurate integrated hydrometeorological real-time models for predicting extreme river water levels and discharges are needed, but they are not satisfactorily accurate due to large uncertainties. This study evaluates two calibration methods with 7 and 5 parameters using the hydrological Cell Distributed Runoff Model version 3.1.1 (CDRM), calibrated by the University of Arizona’s Shuffled Complex Evolution optimization method (SCE-UA). We hypothesize that the proposed ensemble hydrological parameter calibration approach can forecast similar future events in real time. This approach was applied to seven major rivers in the region to obtain hindcasts of the river discharges during the Heavy Rainfall Event of July 2018 (HRE18). This study introduces a new historical extreme rainfall event classification selection methodology that enables ensemble-averaged validation results of all river discharges. The reproducibility metrics obtained for all rivers cumulatively are extremely high, with Nash–Sutcliffe efficiency values of 0.98. This shows that the proposed approach enables accurate predictions of the river discharges for the HRE18 and, similarly, real-time forecasts for future extreme rainfall-induced events in the Japanese region. Although our methodology can be directly reapplied only in regions where observed rainfall data are readily available, we suggest that our approach can analogously be applied worldwide, which indicates a broad scientific contribution and multidisciplinary applications. © 2023 by the authors. 2018 western Japan floods; extreme river discharge; hydrograph reproducibility; physically distributed hydrological model; real-time forecasting; SCE-UA optimization Chugoku; Honshu; Japan; early warning system; ensemble forecasting; flash flood; hydrograph; hydrological modeling; optimization; real time; river discharge";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
667;WF-UNet: Weather Data Fusion using 3D-UNet for Precipitation Nowcasting;Designing early warning systems for harsh weather and its effects, such as urban flooding or landslides, requires accurate short-term forecasts (nowcasts) of precipitation. Nowcasting is a significant task with several environmental applications, such as agricultural management or increasing flight safety. In this study, we investigate the use of a UNet core model and its extension for precipitation nowcasting in western Europe for up to 3 hours ahead. In particular, we propose the Weather Fusion UNet (WF-UNet) model, which utilizes the Core 3D-UNet model and integrates precipitation and wind speed variables as input in the learning process and analyzes its influences on the precipitation target task. We have collected six years of precipitation and wind radar images from Jan 2016 to Dec 2021 of 14 European countries, with 1-hour temporal resolution and 31 square km spatial resolution based on the ERA5 dataset, provided by Copernicus, the European Union's Earth observation programme. We compare the proposed WF-UNet model to the persistence model as well as other UNet-based architectures that are trained only using precipitation radar input data. The obtained results show that WF-UNet outperforms the other examined best-performing architectures by 22%, 8% and 3% lower MSE at a horizon of 1, 2 and 3 hours respectively. © 2023 The Authors. Published by Elsevier B.V.;"Autoencoders; Data Fusion; Deep Learning; Precipitation Nowcasting; Satellite imagery; UNet";"3D modeling; Deep learning; Radar imaging; Satellite imagery; Space-based radar; Wind; Auto encoders; Deep learning; Early Warning System; Nowcast; Nowcasting; Precipitation nowcasting; Short-term forecasts; Unet; Urban flooding; Weather data; Data fusion";"WF-UNet: Weather Data Fusion using 3D-UNet for Precipitation Nowcasting Designing early warning systems for harsh weather and its effects, such as urban flooding or landslides, requires accurate short-term forecasts (nowcasts) of precipitation. Nowcasting is a significant task with several environmental applications, such as agricultural management or increasing flight safety. In this study, we investigate the use of a UNet core model and its extension for precipitation nowcasting in western Europe for up to 3 hours ahead. In particular, we propose the Weather Fusion UNet (WF-UNet) model, which utilizes the Core 3D-UNet model and integrates precipitation and wind speed variables as input in the learning process and analyzes its influences on the precipitation target task. We have collected six years of precipitation and wind radar images from Jan 2016 to Dec 2021 of 14 European countries, with 1-hour temporal resolution and 31 square km spatial resolution based on the ERA5 dataset, provided by Copernicus, the European Union's Earth observation programme. We compare the proposed WF-UNet model to the persistence model as well as other UNet-based architectures that are trained only using precipitation radar input data. The obtained results show that WF-UNet outperforms the other examined best-performing architectures by 22%, 8% and 3% lower MSE at a horizon of 1, 2 and 3 hours respectively. © 2023 The Authors. Published by Elsevier B.V. Autoencoders; Data Fusion; Deep Learning; Precipitation Nowcasting; Satellite imagery; UNet 3D modeling; Deep learning; Radar imaging; Satellite imagery; Space-based radar; Wind; Auto encoders; Deep learning; Early Warning System; Nowcast; Nowcasting; Precipitation nowcasting; Short-term forecasts; Unet; Urban flooding; Weather data; Data fusion";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
668;Support Vector Machine for volcano hazard monitoring from space at Mount Etna;Lava fountains produced from explosive eruptions at Etna volcano are characterized by the development of several km high ash plumes and lava overflows from the crater rim. The formation of lava flows and tephra deposits and the dispersion of volcanic clouds have the potential to impact on population, aviation and infrastructure. For this reason, mapping and quantifying the areal extension of the products erupted is of great importance for forecasting volcanic hazards. The huge amount of multispectral satellite data provides new perspectives for the near real-time monitoring of volcanic hazards, which needs new machine learning (ML) techniques for the automatic process of data. Support Vector Machine (SVM) and satellite data have been shown can be used to track lava flows and volcanic clouds. Here, we show the potentiality of using SVM and satellite data with different spatial and temporal resolutions to fully characterize the volcanic products erupted during the lava fountain event occurred at Etna volcano on 10 February 2022. Two SVM models have been adopted using Google Earth Engine platform to map and quantify (a) the extension of the lava flows and tephra deposits, and (b) the volcanic cloud dispersed into the atmosphere. Exploiting the potentiality of machine learning and cloud computing to process satellite remote sensing data, we obtained a global quantitative analysis of products erupted during lava fountaining with a high accuracy level.  © 2022 IEEE.;"lava flow mapping; machine learning; satellite data; volcanic plume detection";"Clouds; Hazards; Mapping; Remote sensing; Satellites; Vector spaces; Volcanoes; Flow mapping; Lava flow mapping; Lava flows; Machine-learning; Plume detection; Satellite data; Support vectors machine; Volcanic clouds; Volcanic plume; Volcanic plume detection; Support vector machines";"Support Vector Machine for volcano hazard monitoring from space at Mount Etna Lava fountains produced from explosive eruptions at Etna volcano are characterized by the development of several km high ash plumes and lava overflows from the crater rim. The formation of lava flows and tephra deposits and the dispersion of volcanic clouds have the potential to impact on population, aviation and infrastructure. For this reason, mapping and quantifying the areal extension of the products erupted is of great importance for forecasting volcanic hazards. The huge amount of multispectral satellite data provides new perspectives for the near real-time monitoring of volcanic hazards, which needs new machine learning (ML) techniques for the automatic process of data. Support Vector Machine (SVM) and satellite data have been shown can be used to track lava flows and volcanic clouds. Here, we show the potentiality of using SVM and satellite data with different spatial and temporal resolutions to fully characterize the volcanic products erupted during the lava fountain event occurred at Etna volcano on 10 February 2022. Two SVM models have been adopted using Google Earth Engine platform to map and quantify (a) the extension of the lava flows and tephra deposits, and (b) the volcanic cloud dispersed into the atmosphere. Exploiting the potentiality of machine learning and cloud computing to process satellite remote sensing data, we obtained a global quantitative analysis of products erupted during lava fountaining with a high accuracy level.  © 2022 IEEE. lava flow mapping; machine learning; satellite data; volcanic plume detection Clouds; Hazards; Mapping; Remote sensing; Satellites; Vector spaces; Volcanoes; Flow mapping; Lava flow mapping; Lava flows; Machine-learning; Plume detection; Satellite data; Support vectors machine; Volcanic clouds; Volcanic plume; Volcanic plume detection; Support vector machines";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;1;Prevention
669;Machine learning methods for environmental monitoring and flood protection;More and more natural disasters are happening every year: floods, earthquakes, volcanic eruptions, etc. In order to reduce the risk of possible damages, governments all around the world are investing into development of Early Warning Systems (EWS) for environmental applications. The most important task of the EWS is identification of the onset of critical situations affecting environment and population, early enough to inform the authorities and general public. This paper describes an approach for monitoring of flood protections systems based on machine learning methods. An Artificial Intelligence (AI) component has been developed for detection of abnormal dike behaviour. The AI module has been integrated into an EWS platform of the UrbanFlood project (EU Seventh Framework Programme) and validated on real-time measurements from the sensors installed in a dike.;"Early warning system; Flood protection; Intelligent environmental monitoring; Machine learning";"Alarm systems; Artificial intelligence; Disasters; Environmental engineering; Flood control; Levees; Monitoring; Volcanoes; Early Warning System; Early warning systems; Environmental applications; Environmental Monitoring; Flood protection; General publics; Intelligent environmental monitoring; Machine learning methods; Machine-learning; Natural disasters; On-machines; Real time measurements; Volcanic eruptions; Learning systems";"Machine learning methods for environmental monitoring and flood protection More and more natural disasters are happening every year: floods, earthquakes, volcanic eruptions, etc. In order to reduce the risk of possible damages, governments all around the world are investing into development of Early Warning Systems (EWS) for environmental applications. The most important task of the EWS is identification of the onset of critical situations affecting environment and population, early enough to inform the authorities and general public. This paper describes an approach for monitoring of flood protections systems based on machine learning methods. An Artificial Intelligence (AI) component has been developed for detection of abnormal dike behaviour. The AI module has been integrated into an EWS platform of the UrbanFlood project (EU Seventh Framework Programme) and validated on real-time measurements from the sensors installed in a dike. Early warning system; Flood protection; Intelligent environmental monitoring; Machine learning Alarm systems; Artificial intelligence; Disasters; Environmental engineering; Flood control; Levees; Monitoring; Volcanoes; Early Warning System; Early warning systems; Environmental applications; Environmental Monitoring; Flood protection; General publics; Intelligent environmental monitoring; Machine learning methods; Machine-learning; Natural disasters; On-machines; Real time measurements; Volcanic eruptions; Learning systems";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;2;Preparation
670;Semantic segmentation of explosive volcanic plumes through deep learning;"Tracking explosive volcanic phenomena can provide important information for hazard monitoring and volcano research. Perhaps the simplest forms of monitoring instruments are visible-wavelength cameras, which are routinely deployed on volcanoes around the globe. Here, we present the development of deep learning models, based on convolutional neural networks (CNNs), to perform semantic segmentation of explosive volcanic plumes on visible imagery, therefore classifying each pixel of an image as either explosive plume or not explosive plume. We have developed 3 models, each with average validation accuracies of >97% under 10-fold cross-validation; although we do highlight that, due to the limited training and validation dataset, this value is likely an overestimate of real-world performance. We then present model deployment for automated retrieval of plume height, rise speed and propagation direction, all parameters which can have great utility particularly in ash dispersion modelling and associated aviation hazard identification. The 3 trained models are freely available for download at https://doi.org/10.15131/shef.data.17061509. © 2022";"Convolutional neural network; Deep learning; Semantic segmentation; Volcanic ash; Volcanic explosion";"Backpropagation; Convolution; Convolutional neural networks; Deep learning; Explosives; Hazards; Semantic Web; Semantics; Volcanoes; Convolutional neural network; Deep learning; Hazard monitoring; Monitoring instruments; Semantic segmentation; Simple++; Volcanic ash; Volcanic explosions; Volcanic plume; Volcanics; artificial neural network; data set; plume; volcanic ash; volcano; wavelength; Semantic Segmentation";"Semantic segmentation of explosive volcanic plumes through deep learning Tracking explosive volcanic phenomena can provide important information for hazard monitoring and volcano research. Perhaps the simplest forms of monitoring instruments are visible-wavelength cameras, which are routinely deployed on volcanoes around the globe. Here, we present the development of deep learning models, based on convolutional neural networks (CNNs), to perform semantic segmentation of explosive volcanic plumes on visible imagery, therefore classifying each pixel of an image as either explosive plume or not explosive plume. We have developed 3 models, each with average validation accuracies of >97% under 10-fold cross-validation; although we do highlight that, due to the limited training and validation dataset, this value is likely an overestimate of real-world performance. We then present model deployment for automated retrieval of plume height, rise speed and propagation direction, all parameters which can have great utility particularly in ash dispersion modelling and associated aviation hazard identification. The 3 trained models are freely available for download at https://doi.org/10.15131/shef.data.17061509. © 2022 Convolutional neural network; Deep learning; Semantic segmentation; Volcanic ash; Volcanic explosion Backpropagation; Convolution; Convolutional neural networks; Deep learning; Explosives; Hazards; Semantic Web; Semantics; Volcanoes; Convolutional neural network; Deep learning; Hazard monitoring; Monitoring instruments; Semantic segmentation; Simple++; Volcanic ash; Volcanic explosions; Volcanic plume; Volcanics; artificial neural network; data set; plume; volcanic ash; volcano; wavelength; Semantic Segmentation";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
671;DeepSDC: Deep Ensemble Learner for the Classification of Social-Media Flooding Events;Disasters such as earthquakes, droughts, floods, and volcanoes adversely affect human lives and valuable resources. Therefore, various response systems have been designed, which assist in mitigating the impact of disasters and facilitating relief activities in the aftermath of a disaster. These response systems require timely and accurate information about affected areas. In recent years, social media has provided access to high-volume real-time data, which can be used for advanced solutions to numerous problems, including disasters. Social-media data combines two modalities (text and associated images), and this information can be used to detect disasters, such as floods. This paper proposes an ensemble learning-based Deep Social Media Data Classification (DeepSDC) approach for social-media flood-event classification. The proposed algorithm uses datasets from Twitter to detect the flooding event. The Deep Social Media Data Classification (DeepSDC) uses a two-staged ensemble-learning approach which combines separate models for textual and visual data. These models obtain diverse information from the text and images and combine the information using an ensemble-learning approach. Additionally, DeepSDC utilizes different augmentation, upsampling and downsampling techniques to tackle the class-imbalance challenge. The performance of the proposed algorithm is assessed on three publically available flood-detection datasets. The experimental results show that the proposed DeepSDC is able to produce superior performance when compared with several state-of-the-art algorithms. For the three datasets, FRMT, FCSM and DIRSM, the proposed approach produced F1 scores of 46.52, 92.87, and 92.65, respectively. The mean average precision (MAP@480) of 91.29 and 98.94 were obtained on textual and a combination of textual and visual data, respectively. © 2023 by the authors.;"deep learning; disaster response; flood classification; social media; two-staged ensemble learning";"algorithm; data set; flooding; machine learning; precision; social media";"DeepSDC: Deep Ensemble Learner for the Classification of Social-Media Flooding Events Disasters such as earthquakes, droughts, floods, and volcanoes adversely affect human lives and valuable resources. Therefore, various response systems have been designed, which assist in mitigating the impact of disasters and facilitating relief activities in the aftermath of a disaster. These response systems require timely and accurate information about affected areas. In recent years, social media has provided access to high-volume real-time data, which can be used for advanced solutions to numerous problems, including disasters. Social-media data combines two modalities (text and associated images), and this information can be used to detect disasters, such as floods. This paper proposes an ensemble learning-based Deep Social Media Data Classification (DeepSDC) approach for social-media flood-event classification. The proposed algorithm uses datasets from Twitter to detect the flooding event. The Deep Social Media Data Classification (DeepSDC) uses a two-staged ensemble-learning approach which combines separate models for textual and visual data. These models obtain diverse information from the text and images and combine the information using an ensemble-learning approach. Additionally, DeepSDC utilizes different augmentation, upsampling and downsampling techniques to tackle the class-imbalance challenge. The performance of the proposed algorithm is assessed on three publically available flood-detection datasets. The experimental results show that the proposed DeepSDC is able to produce superior performance when compared with several state-of-the-art algorithms. For the three datasets, FRMT, FCSM and DIRSM, the proposed approach produced F1 scores of 46.52, 92.87, and 92.65, respectively. The mean average precision (MAP@480) of 91.29 and 98.94 were obtained on textual and a combination of textual and visual data, respectively. © 2023 by the authors. deep learning; disaster response; flood classification; social media; two-staged ensemble learning algorithm; data set; flooding; machine learning; precision; social media";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.2;Hydrological;3;Response
672;Artificial Intelligence and Machine Learning Tools for Improving Early Warning Systems of Volcanic Eruptions: The Case of Stromboli;Explosive volcanic blasts can occur suddenly and without any clear precursors. Many volcanoes have erupted in the last years with no evident change in the eruptive parameters and with dramatic consequences for the population living nearby the volcano and the tourists visiting the active areas. In recent years, a big effort has been made to develop Early Warning systems to issue timely alerts to the population. At Stromboli volcano, the development of sensitive instruments to measure the deformation (tilt) of the ground has revealed that the volcano edifice is inflating tens of minutes before the explosion following a recurrent exponential ramp-like pattern. This scale-invariant of ground deformation has allowed the development of a quasi-deterministic Early Warning system which is operative since 2019. In this article we show how Artificial Intelligence and Machine Learning can be successfully applied to improve the efficiency and the sensitivity of Early Warning systems, provided the availability of a comprehensive experimental data set on past explosive events. The approach presented here for the Stromboli case demonstrates promising results also in forecasting the intensity of explosive events, offering valuable insights and new perspectives into the potential risks associated with volcanic activities.  © 1979-2012 IEEE.;"Early warning; gate recurrent units; logistic regression; pattern recognition; random forest classifier; Stromboli volcano; time series classification";"Alarm systems; Explosives detection; Forestry; Machine learning; Pattern recognition; Signal processing; Volcanoes; Artificial intelligence learning; Early warning; Early Warning System; Gate recurrent unit; Logistics regressions; Machine-learning; Random forest classifier; Signal processing algorithms; Stromboli volcano; Time series classifications; article; artificial intelligence; forecasting; human; machine learning; tourism; volcano; Explosives";"Artificial Intelligence and Machine Learning Tools for Improving Early Warning Systems of Volcanic Eruptions: The Case of Stromboli Explosive volcanic blasts can occur suddenly and without any clear precursors. Many volcanoes have erupted in the last years with no evident change in the eruptive parameters and with dramatic consequences for the population living nearby the volcano and the tourists visiting the active areas. In recent years, a big effort has been made to develop Early Warning systems to issue timely alerts to the population. At Stromboli volcano, the development of sensitive instruments to measure the deformation (tilt) of the ground has revealed that the volcano edifice is inflating tens of minutes before the explosion following a recurrent exponential ramp-like pattern. This scale-invariant of ground deformation has allowed the development of a quasi-deterministic Early Warning system which is operative since 2019. In this article we show how Artificial Intelligence and Machine Learning can be successfully applied to improve the efficiency and the sensitivity of Early Warning systems, provided the availability of a comprehensive experimental data set on past explosive events. The approach presented here for the Stromboli case demonstrates promising results also in forecasting the intensity of explosive events, offering valuable insights and new perspectives into the potential risks associated with volcanic activities.  © 1979-2012 IEEE. Early warning; gate recurrent units; logistic regression; pattern recognition; random forest classifier; Stromboli volcano; time series classification Alarm systems; Explosives detection; Forestry; Machine learning; Pattern recognition; Signal processing; Volcanoes; Artificial intelligence learning; Early warning; Early Warning System; Gate recurrent unit; Logistics regressions; Machine-learning; Random forest classifier; Signal processing algorithms; Stromboli volcano; Time series classifications; article; artificial intelligence; forecasting; human; machine learning; tourism; volcano; Explosives";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
673;Evaluation Model of Urban Regional Knowledge Management Systems in Indonesia for Natural Disaster Mitigation;Nearly the whole Indonesian Archipelago is threatened by earthquake hazards, both on a small and large scale. This indicates Indonesia's susceptibility to earthquakes, tsunamis, volcanic eruptions, and other geological calamities. With the assistance of other pertinent ministries and institutions, the National Disaster Management Agency (BNPB) in Indonesia is in charge of carrying out disaster management at the national level. In order for mitigation to function effectively and efficiently, this research is vital since it is based on the establishment of an evaluation model for an urban knowledge management system in Indonesia. The goal of the project is to create an assessment model for Indonesian urban knowledge management systems that can be applied to the deployment of autonomous socialization programs for disaster mitigation. Factor analysis and regression analysis are the primary techniques used in this work. Regression analysis is used to create a model that can represent the current state of mitigation socialization and be used to develop implementation strategies in the future. Factor analysis is used to identify dominant factors in the implementation of knowledge management systems that support independent socialization. © 2024 IEEE.;"knowledge management systems; mitigation; national disaster; SECI";"Disasters; Earthquakes; Information management; Project management; Urban planning; Volcanoes; Disaster management; Earthquake hazard; Evaluation models; Factors analysis; Indonesia; Knowledge management system; Mitigation; National disaster; Natural disaster mitigation; SECI; Disaster prevention";"Evaluation Model of Urban Regional Knowledge Management Systems in Indonesia for Natural Disaster Mitigation Nearly the whole Indonesian Archipelago is threatened by earthquake hazards, both on a small and large scale. This indicates Indonesia's susceptibility to earthquakes, tsunamis, volcanic eruptions, and other geological calamities. With the assistance of other pertinent ministries and institutions, the National Disaster Management Agency (BNPB) in Indonesia is in charge of carrying out disaster management at the national level. In order for mitigation to function effectively and efficiently, this research is vital since it is based on the establishment of an evaluation model for an urban knowledge management system in Indonesia. The goal of the project is to create an assessment model for Indonesian urban knowledge management systems that can be applied to the deployment of autonomous socialization programs for disaster mitigation. Factor analysis and regression analysis are the primary techniques used in this work. Regression analysis is used to create a model that can represent the current state of mitigation socialization and be used to develop implementation strategies in the future. Factor analysis is used to identify dominant factors in the implementation of knowledge management systems that support independent socialization. © 2024 IEEE. knowledge management systems; mitigation; national disaster; SECI Disasters; Earthquakes; Information management; Project management; Urban planning; Volcanoes; Disaster management; Earthquake hazard; Evaluation models; Factors analysis; Indonesia; Knowledge management system; Mitigation; National disaster; Natural disaster mitigation; SECI; Disaster prevention";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;1;Prevention
674;Automatic Recognition of Long Period Events from Volcano Tectonic Earthquakes at Cotopaxi Volcano;Geophysics experts are interested in understanding the behavior of volcanoes and forecasting possible eruptions by monitoring and detecting the increment on volcano-seismic activity, with the aim of safeguarding human lives and material losses. This paper presents an automatic volcanic event detection and classification system, which considers feature extraction and feature selection stages, to reduce the processing time toward a reliable real-time volcano early warning system (RT-VEWS). We built the proposed approach in terms of the seismicity presented in 2009 and 2010 at the Cotopaxi Volcano located in Ecuador. In the detection stage, the recordings were time segmented by using a nonoverlapping 15-s window, and in the classification stage, the detected seismic signals were 1-min long. For each detected signal conveying seismic events, a comprehensive set of statistical, temporal, spectral, and scale-domain features were compiled and extracted, aiming to separate long-period (LP) events from volcano-tectonic (VT) earthquakes. We benchmarked two commonly used types of feature selection techniques, namely, wrapper (recursive feature extraction) and embedded (cross-validation and pruning). Each technique was used within a suitable and appropriate classification algorithm, either the support vector machine (SVM) or the decision trees. The best result was obtained by using the SVM classifier, yielding up to 99% accuracy in the detection stage and 97% accuracy and sensitivity in the event classification stage. Selected features and their interpretation were consistent among different input spaces in simple terms of the spectral content of the frequency bands at 3.1 and 6.8 Hz. A comparative analysis showed that the most relevant features for automatic discrimination between LP and VT events were one in the time domain, five in the frequency domain, and nine in the scale domain. Our study provides the framework for an event classification system with high accuracy and reduced computational requirements, according to the orientation toward a future RT-VEWS. © 2016 IEEE.;"Decision trees (DT); feature extraction; feature selection; seismic event classification; seismic event detection; support vector machines (SVM); volcanic events";"Cotopaxi Volcano; Ecuador; Classification (of information); Decision trees; Earthquakes; Extraction; Frequency bands; Frequency domain analysis; Geophysics; Image retrieval; Seismology; Signal detection; Support vector machines; Tectonics; Time domain analysis; Trees (mathematics); Volcanoes; Automatic recognition; Classification algorithm; Classification system; Comparative analysis; Computational requirements; Early Warning System; Event classification; Volcano tectonic earthquakes; accuracy assessment; automation; benchmarking; decision support system; detection method; early warning system; earthquake event; forecasting method; hazard assessment; seismicity; support vector machine; volcanic earthquake; Feature extraction";"Automatic Recognition of Long Period Events from Volcano Tectonic Earthquakes at Cotopaxi Volcano Geophysics experts are interested in understanding the behavior of volcanoes and forecasting possible eruptions by monitoring and detecting the increment on volcano-seismic activity, with the aim of safeguarding human lives and material losses. This paper presents an automatic volcanic event detection and classification system, which considers feature extraction and feature selection stages, to reduce the processing time toward a reliable real-time volcano early warning system (RT-VEWS). We built the proposed approach in terms of the seismicity presented in 2009 and 2010 at the Cotopaxi Volcano located in Ecuador. In the detection stage, the recordings were time segmented by using a nonoverlapping 15-s window, and in the classification stage, the detected seismic signals were 1-min long. For each detected signal conveying seismic events, a comprehensive set of statistical, temporal, spectral, and scale-domain features were compiled and extracted, aiming to separate long-period (LP) events from volcano-tectonic (VT) earthquakes. We benchmarked two commonly used types of feature selection techniques, namely, wrapper (recursive feature extraction) and embedded (cross-validation and pruning). Each technique was used within a suitable and appropriate classification algorithm, either the support vector machine (SVM) or the decision trees. The best result was obtained by using the SVM classifier, yielding up to 99% accuracy in the detection stage and 97% accuracy and sensitivity in the event classification stage. Selected features and their interpretation were consistent among different input spaces in simple terms of the spectral content of the frequency bands at 3.1 and 6.8 Hz. A comparative analysis showed that the most relevant features for automatic discrimination between LP and VT events were one in the time domain, five in the frequency domain, and nine in the scale domain. Our study provides the framework for an event classification system with high accuracy and reduced computational requirements, according to the orientation toward a future RT-VEWS. © 2016 IEEE. Decision trees (DT); feature extraction; feature selection; seismic event classification; seismic event detection; support vector machines (SVM); volcanic events Cotopaxi Volcano; Ecuador; Classification (of information); Decision trees; Earthquakes; Extraction; Frequency bands; Frequency domain analysis; Geophysics; Image retrieval; Seismology; Signal detection; Support vector machines; Tectonics; Time domain analysis; Trees (mathematics); Volcanoes; Automatic recognition; Classification algorithm; Classification system; Comparative analysis; Computational requirements; Early Warning System; Event classification; Volcano tectonic earthquakes; accuracy assessment; automation; benchmarking; decision support system; detection method; early warning system; earthquake event; forecasting method; hazard assessment; seismicity; support vector machine; volcanic earthquake; Feature extraction";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
675;The SECESTA project: An overview;SECESTA is a project funded under the POR FESR Sicily 2007-2013 program, recently concluded. The project aimed at the development of an early-warning wireless network of multisensor nodes for the monitoring of the volcanic ash fall-out phenomenon along the area spreading from main craters of the Etna volcano to the Fontanarossa international airport. The network will provide a spatially distributed information useful to predict the time-space evolution of the phenomenon. Such information is useful to implement an optimal planning of actions required to both restore the airport functionalities and manage air traffic during the ongoing phenomenon. This paper provide an overview of the activities of the researchers of the DIEEI of the University of Catania, in charge of developing low cost methodologies for the measurement of the typical parameters characterizing the volcanic ash fall-out (flow-rate and granulometry). © 2015 IEEE.;"ash fall-out; ash granulometry; granulometry classification; multisensors architecture; ROC curves; volcanic ash";"Flow rate; Distributed information; Granulometries; International airport; Multi sensor; Optimal planning; Parameters characterizing; ROC curves; Volcanic ash; Volcanoes";"The SECESTA project: An overview SECESTA is a project funded under the POR FESR Sicily 2007-2013 program, recently concluded. The project aimed at the development of an early-warning wireless network of multisensor nodes for the monitoring of the volcanic ash fall-out phenomenon along the area spreading from main craters of the Etna volcano to the Fontanarossa international airport. The network will provide a spatially distributed information useful to predict the time-space evolution of the phenomenon. Such information is useful to implement an optimal planning of actions required to both restore the airport functionalities and manage air traffic during the ongoing phenomenon. This paper provide an overview of the activities of the researchers of the DIEEI of the University of Catania, in charge of developing low cost methodologies for the measurement of the typical parameters characterizing the volcanic ash fall-out (flow-rate and granulometry). © 2015 IEEE. ash fall-out; ash granulometry; granulometry classification; multisensors architecture; ROC curves; volcanic ash Flow rate; Distributed information; Granulometries; International airport; Multi sensor; Optimal planning; Parameters characterizing; ROC curves; Volcanic ash; Volcanoes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
676;Classification of volcano-seismic signals with Bayesian neural networks;Whilst recent advances in the field of artificial neural networks could be applied to monitor volcanoes, its direct application remains a challenge given the complex geodynamics involved and the size of available datasets. However, Bayesian Neural Networks (BNNs) are probabilistic models that could classify and provide uncertainty estimation for transient seismic sources, even under data scarcity conditions. This research focuses on practical applications of BNNs to classify volcano-seismic signals using two variational learning approaches: Bayes by back-prop and Monte-Carlo dropout. We evaluate classification performance on seven classes of isolated events registered at “Volcán de Fuego”, Colima. Experimental results show an overall improvement for Monte-Carlo dropout approximation when compared to Bayes by backprop. Being at the intersection of Bayesian learning and geophysics, we demonstrate that BNNs provide uncertainty estimations when internal volcano-seismic sources change, which undoubtedly helps to enhance current early warning systems at volcanic observatories. © EURASIP 2018.;NULL;"Geodynamics; Monte Carlo methods; Neural networks; Seismic waves; Seismology; Signal processing; Volcanoes; Bayesian learning; Bayesian neural networks; Classification performance; Early Warning System; Learning approach; Probabilistic models; Seismic signals; Uncertainty estimation; Bayesian networks";"Classification of volcano-seismic signals with Bayesian neural networks Whilst recent advances in the field of artificial neural networks could be applied to monitor volcanoes, its direct application remains a challenge given the complex geodynamics involved and the size of available datasets. However, Bayesian Neural Networks (BNNs) are probabilistic models that could classify and provide uncertainty estimation for transient seismic sources, even under data scarcity conditions. This research focuses on practical applications of BNNs to classify volcano-seismic signals using two variational learning approaches: Bayes by back-prop and Monte-Carlo dropout. We evaluate classification performance on seven classes of isolated events registered at “Volcán de Fuego”, Colima. Experimental results show an overall improvement for Monte-Carlo dropout approximation when compared to Bayes by backprop. Being at the intersection of Bayesian learning and geophysics, we demonstrate that BNNs provide uncertainty estimations when internal volcano-seismic sources change, which undoubtedly helps to enhance current early warning systems at volcanic observatories. © EURASIP 2018. NULL Geodynamics; Monte Carlo methods; Neural networks; Seismic waves; Seismology; Signal processing; Volcanoes; Bayesian learning; Bayesian neural networks; Classification performance; Early Warning System; Learning approach; Probabilistic models; Seismic signals; Uncertainty estimation; Bayesian networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
677;NASA-ISRO Synthetic Aperture Radar (NISAR): The Last Steps to Launch;As NASA & ISRO teams prepare to operate the NASA-ISRO Synthetic Aperture Radar (NISAR) to make unparalleled Earth observations, the spacecraft is undergoing integration & test of the spacecraft bus and radar payload to prepare for launch.NISAR is a multi-disciplinary Earth-observing dual-band radar mission being jointly developed by NASA and the Indian Space Research Organization (ISRO). As a pathfinder for NASA's Earth System Observatory (ESO), NISAR will make global measurements of land surface changes from its near-polar 12-day repeating orbit, for integration into Earth system models. NISAR provides a means of understanding spatially and temporally complex phenomena, ranging from ecosystem disturbances to ice sheet collapse and natural hazards including earthquakes, tsunamis, volcanoes, and landslides. In addition to enabling scientific advances, NISAR will provide societally relevant data that will enable investments to protect human life and property.After several years of buildup and test at JPL, the NASA-developed L-band SAR, an ISRO-developed S-band SAR, a deployable 12m radar antenna, and an Engineering Payload have met the ISRO built spacecraft bus at the ISITE facility in Bangalore. While the System Integration & Test (SIT) activities at JPL integrated the L- and S- band SARs with the Engineering Payload, the integration of the bus in India has challenged both teams to understand how the other operates and work together towards a successful mission.In this year leading to launch, the team is:• Mechanically & electrically integrating the full spacecraft, including bus, payloads, and reflector antenna assembly. This includes integration of components inside the spacecraft bus in addition to the reflector antenna assembly and Radar Instrument Structure (RIS).• Testing the observatory in increasing scope & complexity of activities from checkouts to full scenarios, including faulted tests and environmental testing. Testing involved multiple organizations delivering command products and coordinated through an automated tool.• Preparing for operations through Operational Readiness Tests (ORTs) and compatibility testing. This includes development of a novel test venue to support these tests as well demonstrating data flow and coordination around the globe.• Combining an ISRO and JPL testbed at the U.R. Rao Satellite Center (URSC). The venue allows for risk reduction testing and fault scenario testing not possible in the flight venue. In addition, it can provide a valuable resource for launch and early operations testing. In parallelAlong the way, the team has dealt with some technical challenges requiring late integrations and rework, including regression testing, and replanning critical activities as the resources and plans shifted.This paper discusses the test results and design challenges of the final SIT campaign, the trials that are still ahead, and the verification and validation activities that were conducted through-out. © 2024 IEEE.;NULL;"Earth (planet); Environmental testing; Microwave antennas; Model checking; Observatories; Orbits; Radar antennas; Reflection; Risk perception; Software testing; Space-based radar; Synthetic aperture radar; Antenna assembly; Dual Band; Earth observations; Earth observing; Earth systems; Indian space research organizations; Radar mission; Reflector antennas; Spacecraft bus; Systems integration tests; NASA";"NASA-ISRO Synthetic Aperture Radar (NISAR): The Last Steps to Launch As NASA & ISRO teams prepare to operate the NASA-ISRO Synthetic Aperture Radar (NISAR) to make unparalleled Earth observations, the spacecraft is undergoing integration & test of the spacecraft bus and radar payload to prepare for launch.NISAR is a multi-disciplinary Earth-observing dual-band radar mission being jointly developed by NASA and the Indian Space Research Organization (ISRO). As a pathfinder for NASA's Earth System Observatory (ESO), NISAR will make global measurements of land surface changes from its near-polar 12-day repeating orbit, for integration into Earth system models. NISAR provides a means of understanding spatially and temporally complex phenomena, ranging from ecosystem disturbances to ice sheet collapse and natural hazards including earthquakes, tsunamis, volcanoes, and landslides. In addition to enabling scientific advances, NISAR will provide societally relevant data that will enable investments to protect human life and property.After several years of buildup and test at JPL, the NASA-developed L-band SAR, an ISRO-developed S-band SAR, a deployable 12m radar antenna, and an Engineering Payload have met the ISRO built spacecraft bus at the ISITE facility in Bangalore. While the System Integration & Test (SIT) activities at JPL integrated the L- and S- band SARs with the Engineering Payload, the integration of the bus in India has challenged both teams to understand how the other operates and work together towards a successful mission.In this year leading to launch, the team is:• Mechanically & electrically integrating the full spacecraft, including bus, payloads, and reflector antenna assembly. This includes integration of components inside the spacecraft bus in addition to the reflector antenna assembly and Radar Instrument Structure (RIS).• Testing the observatory in increasing scope & complexity of activities from checkouts to full scenarios, including faulted tests and environmental testing. Testing involved multiple organizations delivering command products and coordinated through an automated tool.• Preparing for operations through Operational Readiness Tests (ORTs) and compatibility testing. This includes development of a novel test venue to support these tests as well demonstrating data flow and coordination around the globe.• Combining an ISRO and JPL testbed at the U.R. Rao Satellite Center (URSC). The venue allows for risk reduction testing and fault scenario testing not possible in the flight venue. In addition, it can provide a valuable resource for launch and early operations testing. In parallelAlong the way, the team has dealt with some technical challenges requiring late integrations and rework, including regression testing, and replanning critical activities as the resources and plans shifted.This paper discusses the test results and design challenges of the final SIT campaign, the trials that are still ahead, and the verification and validation activities that were conducted through-out. © 2024 IEEE. NULL Earth (planet); Environmental testing; Microwave antennas; Model checking; Observatories; Orbits; Radar antennas; Reflection; Risk perception; Software testing; Space-based radar; Synthetic aperture radar; Antenna assembly; Dual Band; Earth observations; Earth observing; Earth systems; Indian space research organizations; Radar mission; Reflector antennas; Spacecraft bus; Systems integration tests; NASA";-1;Não Classificado;NULL;-1;NULL;1;Prevention
678;IMPLEMENTATION OF THE DBSCAN METHOD FOR CLUSTER MAPPING OF EARTHQUAKE SPREAD LOCATION;West Java area is located on the Pacific Circum and Mediterranean Circum routes, and this causes West Java area to be an unstable area that is characterized by many active working volcanoes and frequent earthquakes. An analysis of the grouping of earthquake data in West Java Province area is urgently needed. The purpose of this study was to classify areas based on the density of earthquake occurrence areas in West Java using Density-Based Spatial Clustering of Application with Noise (DBSCAN). The population in this study is all earthquake events that occurred in 2021. While the sample used in this study is data on the location of the distribution of earthquakes in West Java Province in 2021 taken from the BMKG online data website at dataonline.bmkg.go.id. This research began with nearest-neighbor analysis to see patterns of data distribution. If the data distribution pattern is grouped, then DBSCAN analysis can be continued. The DBSCAN algorithm uses a combination of parameters, namely minimum points (MinPts) and epsilon (Eps). Cluster results are evaluated using the silhouette coefficient. Then, in this study, deeper data exploration was carried out in three ways, namely: (1) Clustering based on the highest silhouette value, (2) clustering by lowering the MinPts value, and (3) clustering based on the smallest upper limit (supremum) value of the silhouette coefficient. The data exploration here aimed to form more clusters while still considering the silhouette coefficient value limits so that there are more areas prone to earthquakes but also maintaining the validity of the results obtained. Next, determine the best cluster results by comparing the cluster results obtained. The best cluster results were obtained at Eps=10000 and MinPts=3, which formed 12 clusters with a silhouette coefficient value of 0.713, which means that the clusters have a strong structure. It is hoped that the information regarding the grouping of areas where earthquakes frequently occur can be used as a form of earthquake disaster mitigation and minimize the impact of losses due to the earthquake. © 2023 Author(s).;"DBSCAN; Earthquake; Silhouette Coefficient; West Java";NULL;"IMPLEMENTATION OF THE DBSCAN METHOD FOR CLUSTER MAPPING OF EARTHQUAKE SPREAD LOCATION West Java area is located on the Pacific Circum and Mediterranean Circum routes, and this causes West Java area to be an unstable area that is characterized by many active working volcanoes and frequent earthquakes. An analysis of the grouping of earthquake data in West Java Province area is urgently needed. The purpose of this study was to classify areas based on the density of earthquake occurrence areas in West Java using Density-Based Spatial Clustering of Application with Noise (DBSCAN). The population in this study is all earthquake events that occurred in 2021. While the sample used in this study is data on the location of the distribution of earthquakes in West Java Province in 2021 taken from the BMKG online data website at dataonline.bmkg.go.id. This research began with nearest-neighbor analysis to see patterns of data distribution. If the data distribution pattern is grouped, then DBSCAN analysis can be continued. The DBSCAN algorithm uses a combination of parameters, namely minimum points (MinPts) and epsilon (Eps). Cluster results are evaluated using the silhouette coefficient. Then, in this study, deeper data exploration was carried out in three ways, namely: (1) Clustering based on the highest silhouette value, (2) clustering by lowering the MinPts value, and (3) clustering based on the smallest upper limit (supremum) value of the silhouette coefficient. The data exploration here aimed to form more clusters while still considering the silhouette coefficient value limits so that there are more areas prone to earthquakes but also maintaining the validity of the results obtained. Next, determine the best cluster results by comparing the cluster results obtained. The best cluster results were obtained at Eps=10000 and MinPts=3, which formed 12 clusters with a silhouette coefficient value of 0.713, which means that the clusters have a strong structure. It is hoped that the information regarding the grouping of areas where earthquakes frequently occur can be used as a form of earthquake disaster mitigation and minimize the impact of losses due to the earthquake. © 2023 Author(s). DBSCAN; Earthquake; Silhouette Coefficient; West Java NULL";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;1;Prevention
679;Tourists’ behavior for volcanic disaster risk reduction: A case study of Mount Aso in Japan;Despite preceding research on the general public's knowledge and perceptions of natural hazards, there remains a lack of information specifically on volcanoes in Japan. This study seeks to determine whether people who had visited Mount Aso—one of the active volcanoes in Japan—had taken any self-protective measures. We asked people who visited the area around the crater of Mount Aso about their thoughts and actions related to volcanic disaster risk reduction. Internet-based questionnaires were disseminated nationally via the Rakuten Research, Inc. platform. The total sample size was 400. The logistic regression analysis shows that on-site information provided near the craters—such as knowledge of past damage, bulletin boards, and broadcasts—had a relatively greater impact on safety-seeking actions than prior checking of websites and the Volcanic Alert Level by tourists. Based on our findings, future evidence-based policies regarding volcanic disaster risk reduction should emphasize on-site efforts together with wide dissemination of publicly available information, including the Volcanic Alert Level. © 2022 Elsevier Ltd;"Information access; Mount Aso; Safety seeking; Tourists' behavior; Volcanic disaster risk reduction";NULL;"Tourists’ behavior for volcanic disaster risk reduction: A case study of Mount Aso in Japan Despite preceding research on the general public's knowledge and perceptions of natural hazards, there remains a lack of information specifically on volcanoes in Japan. This study seeks to determine whether people who had visited Mount Aso—one of the active volcanoes in Japan—had taken any self-protective measures. We asked people who visited the area around the crater of Mount Aso about their thoughts and actions related to volcanic disaster risk reduction. Internet-based questionnaires were disseminated nationally via the Rakuten Research, Inc. platform. The total sample size was 400. The logistic regression analysis shows that on-site information provided near the craters—such as knowledge of past damage, bulletin boards, and broadcasts—had a relatively greater impact on safety-seeking actions than prior checking of websites and the Volcanic Alert Level by tourists. Based on our findings, future evidence-based policies regarding volcanic disaster risk reduction should emphasize on-site efforts together with wide dissemination of publicly available information, including the Volcanic Alert Level. © 2022 Elsevier Ltd Information access; Mount Aso; Safety seeking; Tourists' behavior; Volcanic disaster risk reduction NULL";-1;Não Classificado;NULL;1.1;Geological;1;Prevention
680;Microwave radar remote sensing of Plinian volcanic ash clouds for aviation hazard and civil protection applications;"The potential of ground-based microwave weather radar systems for volcanic ash cloud detection and quantitative retrieval is evaluated. A prototype algorithm for volcanic ash radar retrieval (VARR) is discussed. Starting from measured single-polarization reflectivity, the statistical inversion technique to retrieve ash concentration and fall rate is based on two cascade steps: i) classification of eruption regime and volcanic ash category; ii) estimation of ash concentration and fall rate. An application of the VARR technique is finally shown taking into consideration the eruption of the Grímsvötn volcano in Iceland on Nov. 2004. Volume scan data from a Doppler C-band radar, located at 260 km from the volcano vent, are processed by means of the VARR algorithm. Examples of the achievable VARR products are presented and discussed. © 2007 IEEE.";"Inversion model; Microwave modeling; Natural hazards; Volcanic eruption; Weather radar";"Civil aviation; Hazards; Meteorological radar; Polarization; Remote sensing; Statistical methods; Volcanoes; Inversion models; Microwave modeling; Natural hazards; Volcanic eruption; Clouds";"Microwave radar remote sensing of Plinian volcanic ash clouds for aviation hazard and civil protection applications The potential of ground-based microwave weather radar systems for volcanic ash cloud detection and quantitative retrieval is evaluated. A prototype algorithm for volcanic ash radar retrieval (VARR) is discussed. Starting from measured single-polarization reflectivity, the statistical inversion technique to retrieve ash concentration and fall rate is based on two cascade steps: i) classification of eruption regime and volcanic ash category; ii) estimation of ash concentration and fall rate. An application of the VARR technique is finally shown taking into consideration the eruption of the Grímsvötn volcano in Iceland on Nov. 2004. Volume scan data from a Doppler C-band radar, located at 260 km from the volcano vent, are processed by means of the VARR algorithm. Examples of the achievable VARR products are presented and discussed. © 2007 IEEE. Inversion model; Microwave modeling; Natural hazards; Volcanic eruption; Weather radar Civil aviation; Hazards; Meteorological radar; Polarization; Remote sensing; Statistical methods; Volcanoes; Inversion models; Microwave modeling; Natural hazards; Volcanic eruption; Clouds";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
681;On Using a Microearthquake Recognition System for an Early Warning System at Cotopaxi Volcano;Volcanic activity has been increasing throughout the world, posing a significant threat to populations in the event of eruptions. Ecuador, which hosts several active volcanoes, requires robust methods for identifying potential eruptions and issuing reliable alerts to protect lives and minimize damages. This paper presents the development of an automatic microseism recognition system integrated with the Early Warning Broadcast System (EWBS). Using models such as k-Nearest Neighbors, Support Vector Machine, and Decision Trees, along with frequency-based features extracted from seismic data provided by the Instituto Geofísico de la Escuela Politécnica Nacional, the recognition system aims to accurately detect and classify microeartquakes associated with the Cotopaxi volcano during 2012. During the detection stage, the system achieves an impressive Balanced Error Rate (BER) of 0.01, indicating its effectiveness in identifying events. In the subsequent classification stage, the system achieves a BER of 0.11, demonstrating its ability to classify events accurately. The classifiers were further evaluated using 82 microearthquakes, comprising 41 LP events and 41 VT events, resulting in an accuracy of 85% and a BER of 0.15. In addition, a larger data set of 563 earthquakes, consisting of 522 LP events and 41 VT events, was used to assess the performance of the classifiers. The results showed a 7% increase in accuracy compared to the previous test, demonstrating improved performance. However, 11 earthquakes were still misclassified. Integration of these classifiers with a voting system improves their performance. The selected set of 50 features plays a crucial role in achieving accurate results. The recognition system seamlessly interfaces with the EWBS, ensuring a 30 s delay before launching an early warning. This delay provides valuable time for preparation and response measures. In conclusion, the developed microearthquake recognition system, combined with the EWBS, demonstrates its effectiveness in detecting and classifying events, thereby enhancing the ability to issue timely and reliable alerts for volcanic activity. The findings contribute to improved volcano monitoring and risk mitigation strategies. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.;"EWBS; ISDB-T; Recognition system; TDT";"Classification (of information); Decision trees; Nearest neighbor search; Support vector machines; Volcanoes; Voting machines; Broadcast systems; Early warning; Early warning broadcast system; Error rate; ISDB-T; Microearthquakes; Performance; Recognition systems; TDT; Warning broadcasts; Earthquakes";"On Using a Microearthquake Recognition System for an Early Warning System at Cotopaxi Volcano Volcanic activity has been increasing throughout the world, posing a significant threat to populations in the event of eruptions. Ecuador, which hosts several active volcanoes, requires robust methods for identifying potential eruptions and issuing reliable alerts to protect lives and minimize damages. This paper presents the development of an automatic microseism recognition system integrated with the Early Warning Broadcast System (EWBS). Using models such as k-Nearest Neighbors, Support Vector Machine, and Decision Trees, along with frequency-based features extracted from seismic data provided by the Instituto Geofísico de la Escuela Politécnica Nacional, the recognition system aims to accurately detect and classify microeartquakes associated with the Cotopaxi volcano during 2012. During the detection stage, the system achieves an impressive Balanced Error Rate (BER) of 0.01, indicating its effectiveness in identifying events. In the subsequent classification stage, the system achieves a BER of 0.11, demonstrating its ability to classify events accurately. The classifiers were further evaluated using 82 microearthquakes, comprising 41 LP events and 41 VT events, resulting in an accuracy of 85% and a BER of 0.15. In addition, a larger data set of 563 earthquakes, consisting of 522 LP events and 41 VT events, was used to assess the performance of the classifiers. The results showed a 7% increase in accuracy compared to the previous test, demonstrating improved performance. However, 11 earthquakes were still misclassified. Integration of these classifiers with a voting system improves their performance. The selected set of 50 features plays a crucial role in achieving accurate results. The recognition system seamlessly interfaces with the EWBS, ensuring a 30 s delay before launching an early warning. This delay provides valuable time for preparation and response measures. In conclusion, the developed microearthquake recognition system, combined with the EWBS, demonstrates its effectiveness in detecting and classifying events, thereby enhancing the ability to issue timely and reliable alerts for volcanic activity. The findings contribute to improved volcano monitoring and risk mitigation strategies. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG. EWBS; ISDB-T; Recognition system; TDT Classification (of information); Decision trees; Nearest neighbor search; Support vector machines; Volcanoes; Voting machines; Broadcast systems; Early warning; Early warning broadcast system; Error rate; ISDB-T; Microearthquakes; Performance; Recognition systems; TDT; Warning broadcasts; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
682;Evaluating robustness of a HMM-based classification system of volcano-seismic events at COLIMA and Popocatepetl volcanoes;This work presents a continuous volcano-seismic classification system based in the Hidden Markov Models as solution to recently strong needs for automatic event detection and recognition methods in early warning and monitoring scenarios. Furthermore, our system includes a reliable method to assign confidence measures to the recognized signals in order to evaluate the robustness of the results. Data from the two most active volcanoes have been used to probe the system reliability on a complex joint corpus achieving a recognition accuracy higher than 78% in blind recognition tests. ©2009 IEEE.;"Classification; Hidden Markov models; Reliability; Volcano monitoring; Volcano-seismic events";"Geology; Reliability; Remote sensing; Seismology; Volcanoes; Active volcanoes; Classification; Classification system; Complex joints; Confidence Measure; Early warning; Event detection; Recognition accuracy; Recognition methods; Seismic event; System reliability; Volcano monitoring; Hidden Markov models";"Evaluating robustness of a HMM-based classification system of volcano-seismic events at COLIMA and Popocatepetl volcanoes This work presents a continuous volcano-seismic classification system based in the Hidden Markov Models as solution to recently strong needs for automatic event detection and recognition methods in early warning and monitoring scenarios. Furthermore, our system includes a reliable method to assign confidence measures to the recognized signals in order to evaluate the robustness of the results. Data from the two most active volcanoes have been used to probe the system reliability on a complex joint corpus achieving a recognition accuracy higher than 78% in blind recognition tests. ©2009 IEEE. Classification; Hidden Markov models; Reliability; Volcano monitoring; Volcano-seismic events Geology; Reliability; Remote sensing; Seismology; Volcanoes; Active volcanoes; Classification; Classification system; Complex joints; Confidence Measure; Early warning; Event detection; Recognition accuracy; Recognition methods; Seismic event; System reliability; Volcano monitoring; Hidden Markov models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
683;A novel vision-based classification system for explosion phenomena;"                             The need for a proper design and implementation of adequate surveillance system for detecting and categorizing explosion phenomena is nowadays rising as a part of the development planning for risk reduction processes including mitigation and preparedness. In this context, we introduce state-of-the-art explosions classification using pattern recognition techniques. Consequently, we define seven patterns for some of explosion and non-explosion phenomena including: pyroclastic density currents, lava fountains, lava and tephra fallout, nuclear explosions, wildfires, fireworks, and sky clouds. Towards the classification goal, we collected a new dataset of 5327 2D RGB images that are used for training the classifier. Furthermore, in order to achieve high reliability in the proposed explosion classification system and to provide multiple analysis for the monitored phenomena, we propose employing multiple approaches for feature extraction on images including texture features, features in the spatial domain, and features in the transform domain. Texture features are measured on intensity levels using the Principal Component Analysis (PCA) algorithm to obtain the highest 100 eigenvectors and eigenvalues. Moreover, features in the spatial domain are calculated using amplitude features such as the YC                             b                             C                             r                              color model; then, PCA is used to reduce vectors' dimensionality to 100 features. Lastly, features in the transform domain are calculated using Radix-2 Fast Fourier Transform (Radix-2 FFT), and PCA is then employed to extract the highest 100 eigenvectors. In addition, these textures, amplitude and frequency features are combined in an input vector of length 300 which provides a valuable insight into the images under consideration. Accordingly, these features are fed into a combiner to map the input frames to the desired outputs and divide the space into regions or categories. Thus, we propose to employ one-against-one multi-class degree-3 polynomial kernel Support Vector Machine (SVM). The efficiency of the proposed research methodology was evaluated on a totality of 980 frames that were retrieved from multiple YouTube videos. These videos were taken in real outdoor environments for the seven scenarios of the respective defined classes. As a result, we obtained an accuracy of 94.08%, and the total time for categorizing one frame was approximately 0.12 s.                          © 2016 by the authors.";"Nuclear explosions; PCA; Radix-2 FFT; SVM; Volcanic eruptions; YCbCr";NULL;"A novel vision-based classification system for explosion phenomena                              The need for a proper design and implementation of adequate surveillance system for detecting and categorizing explosion phenomena is nowadays rising as a part of the development planning for risk reduction processes including mitigation and preparedness. In this context, we introduce state-of-the-art explosions classification using pattern recognition techniques. Consequently, we define seven patterns for some of explosion and non-explosion phenomena including: pyroclastic density currents, lava fountains, lava and tephra fallout, nuclear explosions, wildfires, fireworks, and sky clouds. Towards the classification goal, we collected a new dataset of 5327 2D RGB images that are used for training the classifier. Furthermore, in order to achieve high reliability in the proposed explosion classification system and to provide multiple analysis for the monitored phenomena, we propose employing multiple approaches for feature extraction on images including texture features, features in the spatial domain, and features in the transform domain. Texture features are measured on intensity levels using the Principal Component Analysis (PCA) algorithm to obtain the highest 100 eigenvectors and eigenvalues. Moreover, features in the spatial domain are calculated using amplitude features such as the YC                             b                             C                             r                              color model; then, PCA is used to reduce vectors' dimensionality to 100 features. Lastly, features in the transform domain are calculated using Radix-2 Fast Fourier Transform (Radix-2 FFT), and PCA is then employed to extract the highest 100 eigenvectors. In addition, these textures, amplitude and frequency features are combined in an input vector of length 300 which provides a valuable insight into the images under consideration. Accordingly, these features are fed into a combiner to map the input frames to the desired outputs and divide the space into regions or categories. Thus, we propose to employ one-against-one multi-class degree-3 polynomial kernel Support Vector Machine (SVM). The efficiency of the proposed research methodology was evaluated on a totality of 980 frames that were retrieved from multiple YouTube videos. These videos were taken in real outdoor environments for the seven scenarios of the respective defined classes. As a result, we obtained an accuracy of 94.08%, and the total time for categorizing one frame was approximately 0.12 s.                          © 2016 by the authors. Nuclear explosions; PCA; Radix-2 FFT; SVM; Volcanic eruptions; YCbCr NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
684;EFFICIENT EMPIRICAL ENVIRONMENTAL FORECASTING USING INTERNET OF THINGS AND MACHINE LEARNING ASPECTS;The Internet of Things (IoT) gives a virtual view, via the Internet Protocol, to a huge variety of real life objects, ranging from a car, to a teacup, weather atmosphere etc. The Internet of Things (IoT) is the network of physical objects, devices, vehicles, buildings and other items which are embedded with electronics, software, sensors, and network connectivity, which enables these articles to gather and trade information. WSNs are integrated into the Internet of Things (IoT), where sensor nodes join the Internet dynamically, and use it to collaborate and accomplish their tasks. Wireless sensor networks (WSN) are well suited for long term environmental data acquisition for IoT representation. And we also discuss More and more natural disasters are happening every year: floods, earthquakes, volcanic eruptions, etc. So as to decrease the risk of possible damages, governments all around the world are investing into development of Early Warning Systems (EWS) for environmental applications. The most essential assignment of the EWS is identification of the onset of critical situations affecting environment and population, early enough to inform the authorities and general public. This paper portrays a methodology for observing of surge securities systems based on machine learning methods. © 2022 Little Lion Scientific. All rights reserved.;"Effecient; Environment; Forecasting; IoT; Machine Learning Aspects";NULL;"EFFICIENT EMPIRICAL ENVIRONMENTAL FORECASTING USING INTERNET OF THINGS AND MACHINE LEARNING ASPECTS The Internet of Things (IoT) gives a virtual view, via the Internet Protocol, to a huge variety of real life objects, ranging from a car, to a teacup, weather atmosphere etc. The Internet of Things (IoT) is the network of physical objects, devices, vehicles, buildings and other items which are embedded with electronics, software, sensors, and network connectivity, which enables these articles to gather and trade information. WSNs are integrated into the Internet of Things (IoT), where sensor nodes join the Internet dynamically, and use it to collaborate and accomplish their tasks. Wireless sensor networks (WSN) are well suited for long term environmental data acquisition for IoT representation. And we also discuss More and more natural disasters are happening every year: floods, earthquakes, volcanic eruptions, etc. So as to decrease the risk of possible damages, governments all around the world are investing into development of Early Warning Systems (EWS) for environmental applications. The most essential assignment of the EWS is identification of the onset of critical situations affecting environment and population, early enough to inform the authorities and general public. This paper portrays a methodology for observing of surge securities systems based on machine learning methods. © 2022 Little Lion Scientific. All rights reserved. Effecient; Environment; Forecasting; IoT; Machine Learning Aspects NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
685;Rapid Structure Detection in Support of Disaster Response: A Case Study of the 2018 Kilauea Volcano Eruption;Disaster response requires timely damage assessment to prioritize rescue and restoration resources. However, providing critical and actionable knowledge after a natural disaster can be challenging due to the scale and the type of damages. This paper describes how remote sensing and machine learning techniques can be used to support rapid structure detection in the wake of a disaster. We use high resolution satellite imagery to identify structures on Hawaii's Big Island to support the Federal Emergency Management Agency's response efforts during the 2018 Kİlauea lava flow incident. This framework specifically showcases the generalizability of CNN models with no need to collect additional training samples to quickly map structures in pre- and post-event imagery and provide timely information to assist government agencies evaluating the extent and potential loss of disaster. With this case study, we further point out future directions to benefit similar larger scale efforts based on the lessons learned. © 2020 IEEE.;"convolutional neural network; disaster response; structure mapping";"Damage detection; Geology; Learning systems; Remote sensing; Risk management; Satellite imagery; Damage assessments; Disaster response; Federal Emergency Management Agency; Government agencies; High resolution satellite imagery; Machine learning techniques; Natural disasters; Structure detection; Emergency services";"Rapid Structure Detection in Support of Disaster Response: A Case Study of the 2018 Kilauea Volcano Eruption Disaster response requires timely damage assessment to prioritize rescue and restoration resources. However, providing critical and actionable knowledge after a natural disaster can be challenging due to the scale and the type of damages. This paper describes how remote sensing and machine learning techniques can be used to support rapid structure detection in the wake of a disaster. We use high resolution satellite imagery to identify structures on Hawaii's Big Island to support the Federal Emergency Management Agency's response efforts during the 2018 Kİlauea lava flow incident. This framework specifically showcases the generalizability of CNN models with no need to collect additional training samples to quickly map structures in pre- and post-event imagery and provide timely information to assist government agencies evaluating the extent and potential loss of disaster. With this case study, we further point out future directions to benefit similar larger scale efforts based on the lessons learned. © 2020 IEEE. convolutional neural network; disaster response; structure mapping Damage detection; Geology; Learning systems; Remote sensing; Risk management; Satellite imagery; Damage assessments; Disaster response; Federal Emergency Management Agency; Government agencies; High resolution satellite imagery; Machine learning techniques; Natural disasters; Structure detection; Emergency services";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
686;Tsunami Signal Classification Based on Sea Level Data using Extreme Gradient Boosting Method for Tsunami Early Warning System Modeling;Eruptive activity that started in June 2018 caused a landslide on Anak Krakatau volcano in December 2018. Volcanic material was released into the sea by the landslide and caused a tsunami that killed 437 people, making it one of the deadliest volcano-generated tsunamis. The tsunami early warning system deployed in Indonesia was ineffective since it could only detect tsunamis based on earthquake data. Therefore, a tsunami early warning system based on sea level data is needed. In this paper, we designed a machine-learning-based tsunami early warning system using the Extreme Gradient Boosting (XGBoost) method to classify tsunami and non-Tsunami signals based on sea level data. As a study case, we use sea level data obtained from the Inexpensive Device for Sea Level Measurement (IDSL) in Marina Jambu that have been artificially added with tsunami signals from the 2018 tsunami caused by the landslide of Mount Anak Krakatau to increase the number of tsunami signals. After performing feature engineering and data imbalanced handling techniques such as adding lag and time features and applying class weight, we obtained good results from the XGBoost with a macro average F1-score of 0.76. The XGBoost outperformed other machine-learning methods, such as Support Vector Machine (SVM) and Random Forests.  © 2023 IEEE.;"classification; machine learning; tsunami early warning system; xgboost";"Adaptive boosting; Data handling; Landslides; Learning systems; Sea level; Support vector machines; Tsunamis; Volcanoes; Boosting method; Eruptive activity; Gradient boosting; Indonesia; Machine-learning; Signal classification; System models; Tsunami early-warning systems; Volcanic materials; Xgboost; Classification (of information)";"Tsunami Signal Classification Based on Sea Level Data using Extreme Gradient Boosting Method for Tsunami Early Warning System Modeling Eruptive activity that started in June 2018 caused a landslide on Anak Krakatau volcano in December 2018. Volcanic material was released into the sea by the landslide and caused a tsunami that killed 437 people, making it one of the deadliest volcano-generated tsunamis. The tsunami early warning system deployed in Indonesia was ineffective since it could only detect tsunamis based on earthquake data. Therefore, a tsunami early warning system based on sea level data is needed. In this paper, we designed a machine-learning-based tsunami early warning system using the Extreme Gradient Boosting (XGBoost) method to classify tsunami and non-Tsunami signals based on sea level data. As a study case, we use sea level data obtained from the Inexpensive Device for Sea Level Measurement (IDSL) in Marina Jambu that have been artificially added with tsunami signals from the 2018 tsunami caused by the landslide of Mount Anak Krakatau to increase the number of tsunami signals. After performing feature engineering and data imbalanced handling techniques such as adding lag and time features and applying class weight, we obtained good results from the XGBoost with a macro average F1-score of 0.76. The XGBoost outperformed other machine-learning methods, such as Support Vector Machine (SVM) and Random Forests.  © 2023 IEEE. classification; machine learning; tsunami early warning system; xgboost Adaptive boosting; Data handling; Landslides; Learning systems; Sea level; Support vector machines; Tsunamis; Volcanoes; Boosting method; Eruptive activity; Gradient boosting; Indonesia; Machine-learning; Signal classification; System models; Tsunami early-warning systems; Volcanic materials; Xgboost; Classification (of information)";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
687;Predictive analysis of the seismicity level at Campi Flegrei volcano using a data-driven approach;This work aims to provide a short-term tool to estimate the possible trend of the seismicity level in the area of Campi Flegrei (southern Italy) for Civil Protection purposes. During the last relevant period of seismic activity, between 1982 and 1984, an uplift of the ground (bradyseism) of more than 1.5 m occurred. It was accompanied by more than 16,000 earthquakes up to magnitude 4.2 which forced the civil authorities to order the evacuation of about 40,000 people from Pozzuoli town for several months. Scientific studies evidenced a temporal correlation between these geophysical phenomena. This has led us to consider a data-driven approach to obtain a forecast of the seismicity level for this area. In particular, a technique based on a Multilayer Perceptron (MLP) network has been used for this intent. Neural networks are data processing mechanisms capable of relating input data with output ones without any prior correlation model but only using empirical evidences obtained from the analysis of available data. The proposed method has been tested on a set of seismic and deformation data acquired between 1983 and 1985 and then including the data of the aforementioned crisis which affected the Campi Flegrei. Once defined the seismicity levels on the basis of the maximum magnitude recorded within a week, three MLP networks were implemented with respectively 2, 3 and 4 output classes. The first network (2 classes) provides only an indication about the possible occurrence of earthquakes felt by people (with magnitude higher than 1.7), while the remaining nets (3 and 4 classes) give also a rough suggestion of their intensity. Furthermore, for these last two networks one of the output classes allows to obtain a forecast about the possible occurrence of strong potentially damaging earthquakes with magnitude higher than 3.5. Each network has been trained on a fixed interval and then tested for the forecast on the subsequent period. The results show that the performance decreases as a function of the complexity of the examined task that is the number of covered classes. However, the obtained results are very promising, for which the proposed system deserves further studies since it could be of support to the Civil Protection operations in the case of possible future crises. © Springer International Publishing Switzerland 2014.;"Campi Flegrei volcano; MLP neural networks; Seismicity forecast";"Complex networks; Data processing; Forecasting; Neural networks; Signaling; Volcanoes; Campi Flegrei; Correlation modeling; Data-driven approach; Geophysical phenomena; MLP neural networks; Multi layer perceptron; Predictive analysis; Temporal correlations; Earthquakes";"Predictive analysis of the seismicity level at Campi Flegrei volcano using a data-driven approach This work aims to provide a short-term tool to estimate the possible trend of the seismicity level in the area of Campi Flegrei (southern Italy) for Civil Protection purposes. During the last relevant period of seismic activity, between 1982 and 1984, an uplift of the ground (bradyseism) of more than 1.5 m occurred. It was accompanied by more than 16,000 earthquakes up to magnitude 4.2 which forced the civil authorities to order the evacuation of about 40,000 people from Pozzuoli town for several months. Scientific studies evidenced a temporal correlation between these geophysical phenomena. This has led us to consider a data-driven approach to obtain a forecast of the seismicity level for this area. In particular, a technique based on a Multilayer Perceptron (MLP) network has been used for this intent. Neural networks are data processing mechanisms capable of relating input data with output ones without any prior correlation model but only using empirical evidences obtained from the analysis of available data. The proposed method has been tested on a set of seismic and deformation data acquired between 1983 and 1985 and then including the data of the aforementioned crisis which affected the Campi Flegrei. Once defined the seismicity levels on the basis of the maximum magnitude recorded within a week, three MLP networks were implemented with respectively 2, 3 and 4 output classes. The first network (2 classes) provides only an indication about the possible occurrence of earthquakes felt by people (with magnitude higher than 1.7), while the remaining nets (3 and 4 classes) give also a rough suggestion of their intensity. Furthermore, for these last two networks one of the output classes allows to obtain a forecast about the possible occurrence of strong potentially damaging earthquakes with magnitude higher than 3.5. Each network has been trained on a fixed interval and then tested for the forecast on the subsequent period. The results show that the performance decreases as a function of the complexity of the examined task that is the number of covered classes. However, the obtained results are very promising, for which the proposed system deserves further studies since it could be of support to the Civil Protection operations in the case of possible future crises. © Springer International Publishing Switzerland 2014. Campi Flegrei volcano; MLP neural networks; Seismicity forecast Complex networks; Data processing; Forecasting; Neural networks; Signaling; Volcanoes; Campi Flegrei; Correlation modeling; Data-driven approach; Geophysical phenomena; MLP neural networks; Multi layer perceptron; Predictive analysis; Temporal correlations; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
688;Pattern recognition of volcanic tremor data on Mt. Etna (Italy) with KKAnalysis-A software program for unsupervised classification;Continuous seismic monitoring plays a key role in the surveillance of the Mt. Etna volcano. Besides earthquakes, which often herald eruptive episodes, the persistent background signal, known as volcanic tremor, provides important information on the volcano status. Changes in the regimes of activity are usually concurrent with variations in tremor characteristics. As continuous recording leads rapidly to the accumulation of large amounts of data, parameter extraction and automated processing become crucial. We propose techniques of unsupervised classification and present a software, named KKAnalysis, developed for this purpose. Essentials of KKAnalysis are demonstrated on tremor data recorded on Mt. Etna during various states of volcanic activity encountered in 2007 and 2008. KKAnalysis is based on MATLAB and combines various unsupervised pattern recognition techniques, in particular self-organizing maps (SOM) and cluster analysis. An early software version was successfully applied to seismic signals recorded on Mt. Etna during the eruption in 2001. Since each situation may require different configurations, we designed KKAnalysis with a specific GUI allowing users to easily modify parameters. All results are given graphically, in screen plots and metafiles (MATLAB and TIF format), as well as in alphanumeric form. The synoptic visualization of results from SOM and cluster analysis facilitates an immediate inspection. The potential of this representation is demonstrated by focusing on data recorded during a flank eruption on May 13, 2008. Changes of tremor characteristics can be clearly identified at a very early stage, well before enhanced volcanic activity becomes visible in the time series. At the same time, data reduction to less than 1% of the original amount is achieved, which facilitates interpretation and storage of the essential information. Running the program in a typical configuration requires computing time less than 1. min, allowing an on-line application for early warning purposes at INGV-Sezione di Catania. © 2011 Elsevier Ltd.;"Cluster analysis; Fuzzy C-means; K-means; Self-organizing map; Volcano monitoring; Volcano seismology";"Catania [Sicily]; Etna; Italy; Sicily; Cluster analysis; Conformal mapping; MATLAB; Parameter extraction; Pattern recognition; Seismology; Self organizing maps; Time series; Visualization; Fuzzy C mean; K-means; Self organizing; Volcano monitoring; Volcano seismology; classification; cluster analysis; computer simulation; data set; early warning system; numerical model; pattern recognition; seismicity; software; visualization; volcanic earthquake; volcanic eruption; Volcanoes";"Pattern recognition of volcanic tremor data on Mt. Etna (Italy) with KKAnalysis-A software program for unsupervised classification Continuous seismic monitoring plays a key role in the surveillance of the Mt. Etna volcano. Besides earthquakes, which often herald eruptive episodes, the persistent background signal, known as volcanic tremor, provides important information on the volcano status. Changes in the regimes of activity are usually concurrent with variations in tremor characteristics. As continuous recording leads rapidly to the accumulation of large amounts of data, parameter extraction and automated processing become crucial. We propose techniques of unsupervised classification and present a software, named KKAnalysis, developed for this purpose. Essentials of KKAnalysis are demonstrated on tremor data recorded on Mt. Etna during various states of volcanic activity encountered in 2007 and 2008. KKAnalysis is based on MATLAB and combines various unsupervised pattern recognition techniques, in particular self-organizing maps (SOM) and cluster analysis. An early software version was successfully applied to seismic signals recorded on Mt. Etna during the eruption in 2001. Since each situation may require different configurations, we designed KKAnalysis with a specific GUI allowing users to easily modify parameters. All results are given graphically, in screen plots and metafiles (MATLAB and TIF format), as well as in alphanumeric form. The synoptic visualization of results from SOM and cluster analysis facilitates an immediate inspection. The potential of this representation is demonstrated by focusing on data recorded during a flank eruption on May 13, 2008. Changes of tremor characteristics can be clearly identified at a very early stage, well before enhanced volcanic activity becomes visible in the time series. At the same time, data reduction to less than 1% of the original amount is achieved, which facilitates interpretation and storage of the essential information. Running the program in a typical configuration requires computing time less than 1. min, allowing an on-line application for early warning purposes at INGV-Sezione di Catania. © 2011 Elsevier Ltd. Cluster analysis; Fuzzy C-means; K-means; Self-organizing map; Volcano monitoring; Volcano seismology Catania [Sicily]; Etna; Italy; Sicily; Cluster analysis; Conformal mapping; MATLAB; Parameter extraction; Pattern recognition; Seismology; Self organizing maps; Time series; Visualization; Fuzzy C mean; K-means; Self organizing; Volcano monitoring; Volcano seismology; classification; cluster analysis; computer simulation; data set; early warning system; numerical model; pattern recognition; seismicity; software; visualization; volcanic earthquake; volcanic eruption; Volcanoes";2;Unsupervised Learning;Cluster Analysis,Anomaly Detection,Mixture Modeling,Topic Modeling,Source Separation,Motif Discovery,Dimensionality Reduction and Manifold Learning;1.1;Geological;2;Preparation
689;A brief frequency analysis of various types of volcanic microearthquakes;"The seismic behavior of volcanoes in the Andean mountain range plays an important role in improving detection, classification and possible early warning systems; in this article, a study of the frequency bands of the main types of volcanic microearthquakes of the Cotopaxi (Ecuador) and Llaima (Chile) volcanoes is made, and the determined frequency bands are compared with the bands presented in the literature. Two databases were used to determine the frequency bands, and the Welch periodogram and a study with histograms of the principal components in frequency were used. Additionally, the analysis with decision trees (machine learning) of the frequency bands is proposed to determine the starting and ending points in frequency of the different types of microearthquakes events.  © 2021 IEEE.";"Cotopaxi volcano; Frequency band; Llaima volcano; Microearthquakes; Spectral estimation";"Decision trees; Frequency estimation; Volcanoes; Analysis of various; Andean mountain range; Cotopaxi volcano; Early Warning System; Frequency Analysis; Llaima volcano; Microearthquakes; Seismic behaviour; Spectral Estimation; Volcanics; Spectrum analysis";"A brief frequency analysis of various types of volcanic microearthquakes The seismic behavior of volcanoes in the Andean mountain range plays an important role in improving detection, classification and possible early warning systems; in this article, a study of the frequency bands of the main types of volcanic microearthquakes of the Cotopaxi (Ecuador) and Llaima (Chile) volcanoes is made, and the determined frequency bands are compared with the bands presented in the literature. Two databases were used to determine the frequency bands, and the Welch periodogram and a study with histograms of the principal components in frequency were used. Additionally, the analysis with decision trees (machine learning) of the frequency bands is proposed to determine the starting and ending points in frequency of the different types of microearthquakes events.  © 2021 IEEE. Cotopaxi volcano; Frequency band; Llaima volcano; Microearthquakes; Spectral estimation Decision trees; Frequency estimation; Volcanoes; Analysis of various; Andean mountain range; Cotopaxi volcano; Early Warning System; Frequency Analysis; Llaima volcano; Microearthquakes; Seismic behaviour; Spectral Estimation; Volcanics; Spectrum analysis";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
690;Influence of structural-geometric features on seismic vulnerability of masonry buildings based on post-earthquake damage data;In this study, seismic vulnerability of Italian masonry buildings is analyzed taking advantage of the post-earthquake damage data recently released by the Italian Department of Civil Protection through the online platform Da.D.O. (Dolce et al. 2019). The main purpose of the study is to describe the damage attitude of several building's typologies, univocally defined by means of structural and geometrical features. To this aim, a time-consuming data processing has been performed to obtain a more effective taxonomy, based on masonry quality layout, type of horizontal structure, presence/absence of retrofit interventions, construction age and number of storeys. Moreover, to avoid bias in vulnerability analysis, the most reliable datasets has been detected through a comprehensive data analysis, also integrating the original database with census data. Then, damage analysis has been done, converting the damage observed on vertical structures in 5+1 damage levels of the whole building according to the European Macroseismic Scale classification. ShakeMaps in terms of peak parameters released by National Institute of Geophysics and Volcanology are also considered to characterize ground motion shaking at each building's location. Lastly, a lognormal statistical model has been used to derive vulnerability curves for considered typologies, fully investigating the influence of each building's feature on damage attitude. © 2023 The Authors. Published by Elsevier B.V.;"AeDES form; construction age; number of storeys; post-earthquake damage data; residential masonry buildings; Vulnerability curves";NULL;"Influence of structural-geometric features on seismic vulnerability of masonry buildings based on post-earthquake damage data In this study, seismic vulnerability of Italian masonry buildings is analyzed taking advantage of the post-earthquake damage data recently released by the Italian Department of Civil Protection through the online platform Da.D.O. (Dolce et al. 2019). The main purpose of the study is to describe the damage attitude of several building's typologies, univocally defined by means of structural and geometrical features. To this aim, a time-consuming data processing has been performed to obtain a more effective taxonomy, based on masonry quality layout, type of horizontal structure, presence/absence of retrofit interventions, construction age and number of storeys. Moreover, to avoid bias in vulnerability analysis, the most reliable datasets has been detected through a comprehensive data analysis, also integrating the original database with census data. Then, damage analysis has been done, converting the damage observed on vertical structures in 5+1 damage levels of the whole building according to the European Macroseismic Scale classification. ShakeMaps in terms of peak parameters released by National Institute of Geophysics and Volcanology are also considered to characterize ground motion shaking at each building's location. Lastly, a lognormal statistical model has been used to derive vulnerability curves for considered typologies, fully investigating the influence of each building's feature on damage attitude. © 2023 The Authors. Published by Elsevier B.V. AeDES form; construction age; number of storeys; post-earthquake damage data; residential masonry buildings; Vulnerability curves NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;3;Response
691;A Comparative Study of Deep Learning Methods for the Detection and Classification of Natural Disasters from Social Media;Disaster Management, defined as a coordinated social effort to successfully prepare for and respond to disasters, can benefit greatly as an industrial process from modern Deep Learning methods. Disaster prevention organizations can benefit greatly from the processing of disaster response data. In an attempt to detect and subsequently categorise disaster-related information from tweets via tweet text analysis, a Feedforward Neural Network (FNN), a Convolutional Neural Network, a Bi-directional Long Short-Term Memory (BLSTM), as well as several Transformer-based network architectures, namely BERT, DistilBERT, Albert, RoBERTa and DeBERTa, are employed. The two defined main tasks of the work presented in this paper are: (1) distinguishing tweets into disaster related and non relevant ones, and (2) categorising already labeled disaster tweets into eight predefined natural disaster categories. These supported types of natural disasters are earthquakes, floods, hurricanes, wildfires, tornadoes, explosions, volcano eruptions and general disasters. To achieve this goal, several accessible related datasets are collected and combined to suit the two tasks. In addition, the combination of preprocessing tasks that is most beneficial for inference is investigated. Finally, experiments have been conducted using bias mitigation techniques. © 2023 by SCITEPRESS-Science and Technology Publications, Lda.;"Bias Mitigation; Deep Learning; Disaster Management; Preprocessing; Twitter";NULL;"A Comparative Study of Deep Learning Methods for the Detection and Classification of Natural Disasters from Social Media Disaster Management, defined as a coordinated social effort to successfully prepare for and respond to disasters, can benefit greatly as an industrial process from modern Deep Learning methods. Disaster prevention organizations can benefit greatly from the processing of disaster response data. In an attempt to detect and subsequently categorise disaster-related information from tweets via tweet text analysis, a Feedforward Neural Network (FNN), a Convolutional Neural Network, a Bi-directional Long Short-Term Memory (BLSTM), as well as several Transformer-based network architectures, namely BERT, DistilBERT, Albert, RoBERTa and DeBERTa, are employed. The two defined main tasks of the work presented in this paper are: (1) distinguishing tweets into disaster related and non relevant ones, and (2) categorising already labeled disaster tweets into eight predefined natural disaster categories. These supported types of natural disasters are earthquakes, floods, hurricanes, wildfires, tornadoes, explosions, volcano eruptions and general disasters. To achieve this goal, several accessible related datasets are collected and combined to suit the two tasks. In addition, the combination of preprocessing tasks that is most beneficial for inference is investigated. Finally, experiments have been conducted using bias mitigation techniques. © 2023 by SCITEPRESS-Science and Technology Publications, Lda. Bias Mitigation; Deep Learning; Disaster Management; Preprocessing; Twitter NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;-1;NULL;2;Preparation
692;Early Detection of Volcanic Eruption through Artificial Intelligence on board;In this manuscript we propose a warning system based on Machine Learning (ML) techniques to allow early detection of volcanic eruptions. The use of Artificial Intelligence (AI)-based algorithms and the early detection purpose can be exploited together to carry out the specific detection and inform the Civil Protection and the authorities in charge to guarantee immediate intervention for the protection of people. Indeed, the main goal presented in this work is to conceptualize and realize a system to be mounted on-board of satellites with the intent of producing swift alerts useful for decision makers. In the initial phase the proposed ML model is trained with SO2 data acquired by Sentine1-5P. The final scope is to define an on-board system similar to Phi-Sat 1, with its own sensor and its own systems. Our feasibility study has shown that it is possible to classify and identify volcanic eruptions in advance with an accuracy of 80% and 70% respectively. Work is in progress to improve these results.  © 2022 IEEE.;"Artificial Intelligence on board; Early Detection; Machine Learning; Phi-Sat 1; Sentine1-5P";"Decision making; Intelligent systems; Volcanoes; Artificial intelligence on board; Civil protection; Early detection; Machine learning techniques; Machine-learning; On-machines; Phi-sit 1; Sentine1-5p; Specific detection; Volcanic eruptions; Machine learning";"Early Detection of Volcanic Eruption through Artificial Intelligence on board In this manuscript we propose a warning system based on Machine Learning (ML) techniques to allow early detection of volcanic eruptions. The use of Artificial Intelligence (AI)-based algorithms and the early detection purpose can be exploited together to carry out the specific detection and inform the Civil Protection and the authorities in charge to guarantee immediate intervention for the protection of people. Indeed, the main goal presented in this work is to conceptualize and realize a system to be mounted on-board of satellites with the intent of producing swift alerts useful for decision makers. In the initial phase the proposed ML model is trained with SO2 data acquired by Sentine1-5P. The final scope is to define an on-board system similar to Phi-Sat 1, with its own sensor and its own systems. Our feasibility study has shown that it is possible to classify and identify volcanic eruptions in advance with an accuracy of 80% and 70% respectively. Work is in progress to improve these results.  © 2022 IEEE. Artificial Intelligence on board; Early Detection; Machine Learning; Phi-Sat 1; Sentine1-5P Decision making; Intelligent systems; Volcanoes; Artificial intelligence on board; Civil protection; Early detection; Machine learning techniques; Machine-learning; On-machines; Phi-sit 1; Sentine1-5p; Specific detection; Volcanic eruptions; Machine learning";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
693;Design of a smart multimodal earthquake response mobile application;Indonesia is located in the Pacific Ring of fire, putting it under constant risk of natural disasters such as volcanic eruptions, earthquakes, and tsunamis. Earthquakes are one of the biggest threat of natural disasters in Indonesia and can strike anytime in any area. A key example is the 2004 Aceh earthquake, which caused a large tsunami, killing more than 160,000 people and destroyed more than 200 shops and homes. While Indonesia has significantly improved its disaster mitigation systems in the past decade, problems remain. Seismological stations are still relatively few and in between, community readiness and resilience for earthquakes remains low, and response activities are often hampered by lack of equipment, such as for finding potential survivors trapped in rubble. In order to help alleviate these issues, this paper describes the design of a smart multimodal earthquake response mobile application. The proposed system has four main functionalities, namely (1) broadcast of earthquake alert to mobile phones from the local earthquake measurement centre, (2) smart voice activated interactive guide to guide community members on how to react to an earthquake event and arrive to a safe place based on their current situation, (3) A system to search for trapped survivors based on Bluetooth and wifi hotspot emitted by survivors, and (4) recording of earthquake waves based on mobile phone accelerators to be used to build a more granular geospatial database on earthquake features. The system implements machine learning algorithm, utilizes voice, picture and text activated interface to match any situation’s need, and basic augmented reality to help guide users to a safe place. © Springer Nature Singapore Pte Ltd. 2019.;"First keyword; Second keyword; Third keyword";"Augmented reality; Biomedical engineering; Cellular telephone systems; Cellular telephones; Disasters; Learning algorithms; Learning systems; mHealth; Mobile computing; Mobile phones; Telephone sets; Tsunamis; Volcanoes; Disaster mitigation; Earthquake measurements; Earthquake response; First keyword; Geo-spatial database; Mobile applications; Second keyword; Third keyword; Earthquakes";"Design of a smart multimodal earthquake response mobile application Indonesia is located in the Pacific Ring of fire, putting it under constant risk of natural disasters such as volcanic eruptions, earthquakes, and tsunamis. Earthquakes are one of the biggest threat of natural disasters in Indonesia and can strike anytime in any area. A key example is the 2004 Aceh earthquake, which caused a large tsunami, killing more than 160,000 people and destroyed more than 200 shops and homes. While Indonesia has significantly improved its disaster mitigation systems in the past decade, problems remain. Seismological stations are still relatively few and in between, community readiness and resilience for earthquakes remains low, and response activities are often hampered by lack of equipment, such as for finding potential survivors trapped in rubble. In order to help alleviate these issues, this paper describes the design of a smart multimodal earthquake response mobile application. The proposed system has four main functionalities, namely (1) broadcast of earthquake alert to mobile phones from the local earthquake measurement centre, (2) smart voice activated interactive guide to guide community members on how to react to an earthquake event and arrive to a safe place based on their current situation, (3) A system to search for trapped survivors based on Bluetooth and wifi hotspot emitted by survivors, and (4) recording of earthquake waves based on mobile phone accelerators to be used to build a more granular geospatial database on earthquake features. The system implements machine learning algorithm, utilizes voice, picture and text activated interface to match any situation’s need, and basic augmented reality to help guide users to a safe place. © Springer Nature Singapore Pte Ltd. 2019. First keyword; Second keyword; Third keyword Augmented reality; Biomedical engineering; Cellular telephone systems; Cellular telephones; Disasters; Learning algorithms; Learning systems; mHealth; Mobile computing; Mobile phones; Telephone sets; Tsunamis; Volcanoes; Disaster mitigation; Earthquake measurements; Earthquake response; First keyword; Geo-spatial database; Mobile applications; Second keyword; Third keyword; Earthquakes";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
694;Infrasound threat classification: A statistical comparison of deep learning architectures;Infrasound propagation through various atmospheric conditions and interaction with environmental factors in- duce highly non-linear and non-stationary effects that make it difficult to extract reliable attributes for classi- fication. We present featureless classification results on the Library of Typical Infrasonic Signals using several deep learning techniques, including long short-term memory, self-normalizing, and fully convolutional neural net- works with statistical analysis to establish significantly superior models. In general, the deep classifiers achieve near-perfect classification accuracies on the four classes of infrasonic events including mountain associated waves, microbaroms, auroral infrasonic waves, and volcanic eruptions. Our results provide evidence that deep neural network architectures be considered the leading candidate for classifying infrasound waveforms which can directly benefit applications that seek to identify infrasonic events such as severe weather forecasting, natural disaster early warning systems, and nuclear weapons monitoring. © 2018 SPIE.;"Deep learning; infrasound; signal classification; threat detection";"Deep learning; Disasters; Explosives; Network architecture; Neural networks; Nuclear weapons; Volcanoes; Weather forecasting; Atmospheric conditions; Classification accuracy; Infrasound; Non-stationary effects; Signal classification; Statistical comparisons; Threat classifications; Threat detection; Deep neural networks";"Infrasound threat classification: A statistical comparison of deep learning architectures Infrasound propagation through various atmospheric conditions and interaction with environmental factors in- duce highly non-linear and non-stationary effects that make it difficult to extract reliable attributes for classi- fication. We present featureless classification results on the Library of Typical Infrasonic Signals using several deep learning techniques, including long short-term memory, self-normalizing, and fully convolutional neural net- works with statistical analysis to establish significantly superior models. In general, the deep classifiers achieve near-perfect classification accuracies on the four classes of infrasonic events including mountain associated waves, microbaroms, auroral infrasonic waves, and volcanic eruptions. Our results provide evidence that deep neural network architectures be considered the leading candidate for classifying infrasound waveforms which can directly benefit applications that seek to identify infrasonic events such as severe weather forecasting, natural disaster early warning systems, and nuclear weapons monitoring. © 2018 SPIE. Deep learning; infrasound; signal classification; threat detection Deep learning; Disasters; Explosives; Network architecture; Neural networks; Nuclear weapons; Volcanoes; Weather forecasting; Atmospheric conditions; Classification accuracy; Infrasound; Non-stationary effects; Signal classification; Statistical comparisons; Threat classifications; Threat detection; Deep neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.1;Geological;2;Preparation
695;Excess Mortality Risk Due to Heat Stress in Different Climatic Zones of India;India is at a high risk of heat stress-induced health impacts and economic losses owing to its tropical climate, high population density, and inadequate adaptive planning. The health impacts of heat stress across climate zones in India have not been adequately explored. Here, we examine and report the vulnerability to heat stress in India using 42 years (1979-2020) of meteorological data from ERA-5 and developed climate-zone-specific percentile-based human comfort class thresholds. We found that the heat stress is usually 1-4 °C higher on heatwave (HW) days than on nonheatwave (NHW) days. However, the stress on NHW days remains considerable and cannot be neglected. We then showed the association of a newly formulated India heat index (IHI) with daily all-cause mortality in three cities - Delhi (semiarid), Varanasi (humid subtropical), and Chennai (tropical wet and dry), using a semiparametric quasi-Poisson regression model, adjusted for nonlinear confounding effects of time and PM2.5. The all-cause mortality risk was enhanced by 8.1% (95% confidence interval, CI: 6.0-10.3), 5.9% (4.6-7.2), and 8.0% (1.7-14.2) during “sweltering” days in Varanasi, Delhi, and Chennai, respectively, relative to “comfortable” days. Across four age groups, the impact was more severe in Varanasi (ranging from a 3.2 to 7.5% increase in mortality risk for a unit rise in IHI) than in Delhi (2.6-4.2% higher risk) and Chennai (0.9-5.7% higher risk). We observed a 3-6 days lag effect of heat stress on mortality in these cities. Our results reveal heterogeneity in heat stress impact across diverse climate zones in India and call for developing an early warning system keeping in mind these regional variations. © 2023 American Chemical Society;"heat stress; heatwave; human comfort; India Heat Index; mortality; relative risk";"Cities; Hot Temperature; Humans; India; Mortality; Tropical Climate; Chennai; Delhi; India; Tamil Nadu; Uttar Pradesh; Varanasi; Health risks; Losses; Meteorology; Population statistics; Regression analysis; Thermal stress; Chennai; Health impact; Heat indices; Heat stress; Heatwaves; Human comforts; India heat index; Mortality; Mortality risk; Relative risks; early warning system; heat wave; heterogeneity; index method; mortality risk; numerical model; particulate matter; adult; all cause mortality; Article; case fatality rate; child; climate; comfort; epidemiology; excess mortality; female; groups by age; heat stress; heat wave; human; India; infant; male; mortality; mortality risk; particulate matter 2.5; physiological stress; population density; risk factor; tropic climate; vulnerability; city; high temperature; mortality; tropic climate; Tropics";"Excess Mortality Risk Due to Heat Stress in Different Climatic Zones of India India is at a high risk of heat stress-induced health impacts and economic losses owing to its tropical climate, high population density, and inadequate adaptive planning. The health impacts of heat stress across climate zones in India have not been adequately explored. Here, we examine and report the vulnerability to heat stress in India using 42 years (1979-2020) of meteorological data from ERA-5 and developed climate-zone-specific percentile-based human comfort class thresholds. We found that the heat stress is usually 1-4 °C higher on heatwave (HW) days than on nonheatwave (NHW) days. However, the stress on NHW days remains considerable and cannot be neglected. We then showed the association of a newly formulated India heat index (IHI) with daily all-cause mortality in three cities - Delhi (semiarid), Varanasi (humid subtropical), and Chennai (tropical wet and dry), using a semiparametric quasi-Poisson regression model, adjusted for nonlinear confounding effects of time and PM2.5. The all-cause mortality risk was enhanced by 8.1% (95% confidence interval, CI: 6.0-10.3), 5.9% (4.6-7.2), and 8.0% (1.7-14.2) during “sweltering” days in Varanasi, Delhi, and Chennai, respectively, relative to “comfortable” days. Across four age groups, the impact was more severe in Varanasi (ranging from a 3.2 to 7.5% increase in mortality risk for a unit rise in IHI) than in Delhi (2.6-4.2% higher risk) and Chennai (0.9-5.7% higher risk). We observed a 3-6 days lag effect of heat stress on mortality in these cities. Our results reveal heterogeneity in heat stress impact across diverse climate zones in India and call for developing an early warning system keeping in mind these regional variations. © 2023 American Chemical Society heat stress; heatwave; human comfort; India Heat Index; mortality; relative risk Cities; Hot Temperature; Humans; India; Mortality; Tropical Climate; Chennai; Delhi; India; Tamil Nadu; Uttar Pradesh; Varanasi; Health risks; Losses; Meteorology; Population statistics; Regression analysis; Thermal stress; Chennai; Health impact; Heat indices; Heat stress; Heatwaves; Human comforts; India heat index; Mortality; Mortality risk; Relative risks; early warning system; heat wave; heterogeneity; index method; mortality risk; numerical model; particulate matter; adult; all cause mortality; Article; case fatality rate; child; climate; comfort; epidemiology; excess mortality; female; groups by age; heat stress; heat wave; human; India; infant; male; mortality; mortality risk; particulate matter 2.5; physiological stress; population density; risk factor; tropic climate; vulnerability; city; high temperature; mortality; tropic climate; Tropics";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
696;Forecasting summer marine heatwaves in the South China Sea using explainable machine learning models;Marine Heatwaves (MHWs), classified as extreme oceanographic meteorological phenomena, have profound effects on ecological systems and socio-economic activities within the South China Sea (SCS). Multiple machine learning (ML) algorithms, specifically Random Forest (RF) and Ridge models, are employed to predict summer Sea Surface Temperature Anomalies (SSTA) and occurrence of MHWs in the SCS. The ML forecast results are also compared with climatology forecasts, persistence forecasts, and the European Centre for Medium-Range Weather Forecasts Sub-seasonal to Seasonal (ECMWF S2S) hindcasts. The results reveal that for regression forecasting with a one-week lead time, the Ridge model performs the best among all models. However, for longer lead times, all models tend towards climatology forecasts. When it comes to the classification forecasting, the RF model demonstrates superior performance to predict extreme MHWs compared to typical MHWs. Moreover, the feature importance is measured via the mean decrease in impurity (MDI) incorporated in the RF model to identify the predictors that contribute to predicting the MHWs. The SHapley Additive exPlanation (SHAP) algorithm is additionally employed to evaluate both the positive and negative impacts of these predictors on the model. SSTA and the Indian Ocean Basin-Wide (IOBW) index are the most two critical forecasting factors influencing the occurrence of MHWs, exhibiting a pronounced positive contribution to the model. The insights derived from this study are expected to provide strong support for the MHWs early warning system in the SCS and provide essential information for the conservation of marine ecosystems and the management of fisheries resources. © 2025 Elsevier Ltd;"Machine learning; Marine heatwaves; Random forests; SHAP importance; South China sea";"Pacific Ocean; South China Sea; Prediction models; Surface waters; Weather forecasting; Heatwaves; Machine learning models; Machine-learning; Marine heatwave; Random forest modeling; Random forests; Sea surface temperature anomalies; Shapley; Shapley additive explanation importance; South China sea; algorithm; climatology; early warning system; heat wave; machine learning; marine ecosystem; sea surface temperature; weather forecasting; Random forests";"Forecasting summer marine heatwaves in the South China Sea using explainable machine learning models Marine Heatwaves (MHWs), classified as extreme oceanographic meteorological phenomena, have profound effects on ecological systems and socio-economic activities within the South China Sea (SCS). Multiple machine learning (ML) algorithms, specifically Random Forest (RF) and Ridge models, are employed to predict summer Sea Surface Temperature Anomalies (SSTA) and occurrence of MHWs in the SCS. The ML forecast results are also compared with climatology forecasts, persistence forecasts, and the European Centre for Medium-Range Weather Forecasts Sub-seasonal to Seasonal (ECMWF S2S) hindcasts. The results reveal that for regression forecasting with a one-week lead time, the Ridge model performs the best among all models. However, for longer lead times, all models tend towards climatology forecasts. When it comes to the classification forecasting, the RF model demonstrates superior performance to predict extreme MHWs compared to typical MHWs. Moreover, the feature importance is measured via the mean decrease in impurity (MDI) incorporated in the RF model to identify the predictors that contribute to predicting the MHWs. The SHapley Additive exPlanation (SHAP) algorithm is additionally employed to evaluate both the positive and negative impacts of these predictors on the model. SSTA and the Indian Ocean Basin-Wide (IOBW) index are the most two critical forecasting factors influencing the occurrence of MHWs, exhibiting a pronounced positive contribution to the model. The insights derived from this study are expected to provide strong support for the MHWs early warning system in the SCS and provide essential information for the conservation of marine ecosystems and the management of fisheries resources. © 2025 Elsevier Ltd Machine learning; Marine heatwaves; Random forests; SHAP importance; South China sea Pacific Ocean; South China Sea; Prediction models; Surface waters; Weather forecasting; Heatwaves; Machine learning models; Machine-learning; Marine heatwave; Random forest modeling; Random forests; Sea surface temperature anomalies; Shapley; Shapley additive explanation importance; South China sea; algorithm; climatology; early warning system; heat wave; machine learning; marine ecosystem; sea surface temperature; weather forecasting; Random forests";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
697;Historical and projected changes in Extreme High Temperature events over East Africa and associated with meteorological conditions using CMIP6 models;Extreme high temperature (EHT) events are the major indicator of global warming and their effect on natural ecosystems and agricultural production. The present study investigates the historical changes and projected trends of EHT events using Coupled Model Intercomparison Project phase 6 (CMIP6) Global Climate Models (GCMs) and observation data in East Africa. The distribution mapping (DM) approach effectively reduces the biases and improves the CMIP6 GCMs to match the observation data. This study considers the intensity of hot days (TXx) and hot nights (TNx) and two frequency indices estimated from the 90th percentile of the hot days (TX90p) and hot nights (TN90p) to evaluate the EHT events. The Mann-Kendall (MK) test and Sens's slope were used to estimate the trend of the EHT indices and significance level. Moreover, a regression approach was utilized to examine the relationship between EHT indices and meteorological conditions during the historical period (1990–2011). The results demonstrate that the recent period (2001−2010) experienced more frequent hot days (∼10–15%) and nights (∼12–20%) compared to the first decade (∼0–7%) (1990–2000) in most of the East Africa region. In addition, most stations across East Africa show a significant positive trend for the frequency of hot nights. The ensemble of the CMIP6 model simulation showed that the EHT events were projected to continue rising during the end of the 21st century (2071–2095). The intensity of hot days and nights is projected to escalate between 1.5 and 3 °C and 1–4 °C under SSP2–4.5 and SSP5–8.5 scenarios. Although, a high upsurge was detected in the frequency of hot nights (∼23–30%) during far future (2071–2095) compared to mid-future (2041–2070) under SSP5–8.5 and SSP2–4.5 scenarios. In addition, a strong correlation (R2 = ±0.8) was found between meteorological conditions and EHT indices, indicating the influence of atmospheric circulation on extreme temperature over East Africa. Our study provides valuable information regarding developing new policies and early warning systems for water resource managers and policymakers. © 2023 Elsevier B.V.;"CMIP6; East Africa; Extreme Temperature; KNN imputation; Observation";"East Africa; Agriculture; Atmospheric temperature; Global warming; Coupled Model Intercomparison Project; Coupled model intercomparison project phase 6; East Africa; Extreme temperatures; High temperature events; Highest temperature; KNN imputation; Meteorological condition; Observation; Project phasis; atmospheric circulation; CMIP; extreme event; global warming; high temperature; observational method; Climate models";"Historical and projected changes in Extreme High Temperature events over East Africa and associated with meteorological conditions using CMIP6 models Extreme high temperature (EHT) events are the major indicator of global warming and their effect on natural ecosystems and agricultural production. The present study investigates the historical changes and projected trends of EHT events using Coupled Model Intercomparison Project phase 6 (CMIP6) Global Climate Models (GCMs) and observation data in East Africa. The distribution mapping (DM) approach effectively reduces the biases and improves the CMIP6 GCMs to match the observation data. This study considers the intensity of hot days (TXx) and hot nights (TNx) and two frequency indices estimated from the 90th percentile of the hot days (TX90p) and hot nights (TN90p) to evaluate the EHT events. The Mann-Kendall (MK) test and Sens's slope were used to estimate the trend of the EHT indices and significance level. Moreover, a regression approach was utilized to examine the relationship between EHT indices and meteorological conditions during the historical period (1990–2011). The results demonstrate that the recent period (2001−2010) experienced more frequent hot days (∼10–15%) and nights (∼12–20%) compared to the first decade (∼0–7%) (1990–2000) in most of the East Africa region. In addition, most stations across East Africa show a significant positive trend for the frequency of hot nights. The ensemble of the CMIP6 model simulation showed that the EHT events were projected to continue rising during the end of the 21st century (2071–2095). The intensity of hot days and nights is projected to escalate between 1.5 and 3 °C and 1–4 °C under SSP2–4.5 and SSP5–8.5 scenarios. Although, a high upsurge was detected in the frequency of hot nights (∼23–30%) during far future (2071–2095) compared to mid-future (2041–2070) under SSP5–8.5 and SSP2–4.5 scenarios. In addition, a strong correlation (R2 = ±0.8) was found between meteorological conditions and EHT indices, indicating the influence of atmospheric circulation on extreme temperature over East Africa. Our study provides valuable information regarding developing new policies and early warning systems for water resource managers and policymakers. © 2023 Elsevier B.V. CMIP6; East Africa; Extreme Temperature; KNN imputation; Observation East Africa; Agriculture; Atmospheric temperature; Global warming; Coupled Model Intercomparison Project; Coupled model intercomparison project phase 6; East Africa; Extreme temperatures; High temperature events; Highest temperature; KNN imputation; Meteorological condition; Observation; Project phasis; atmospheric circulation; CMIP; extreme event; global warming; high temperature; observational method; Climate models";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
698;Contributions of Atmospheric Ridging and Low Soil Moisture to the Record-Breaking June 2023 Mexico-Texas Heatwave;June 2023 witnessed the hottest, largest, and longest-lasting heatwave across Mexico and Texas between 1940 and 2023. We apply constructed analogs with multiple linear regression models to quantify the contribution of different drivers to daily temperature anomalies during this heatwave. On the hottest day (20 June), circulation, soil moisture, and their interaction explained 3.82°C (90% CI: 2.72–4.91°C) of the 5.42°C observed anomaly with most of the residual attributed to the thermodynamic effects of long-term warming. Using CESM2-LENS2, we find that June 2023-like patterns are not projected to increase in frequency but will become 1.9°C hotter by the mid-21st century under SSP3-7.0. The hottest simulated day with these patterns could produce temperatures >50°C (122°F) across south Texas, representing a low-likelihood yet physically plausible worst-case scenario that could inform disaster preparedness and adaptation planning. © 2025. The Author(s).;"atmospheric circulation; attribution; constructed analogs; extreme heat; Mexico; Texas";"Mexico [North America]; Texas; United States; Miocene; %moisture; Atmospheric circulation; Attribution; Breakings; Constructed analog; Extreme heat; Heatwaves; Long lasting; Me-xico; Texas; atmospheric circulation; extreme event; heat wave; precipitation (climatology); soil moisture; temperature anomaly; Multiple linear regression";"Contributions of Atmospheric Ridging and Low Soil Moisture to the Record-Breaking June 2023 Mexico-Texas Heatwave June 2023 witnessed the hottest, largest, and longest-lasting heatwave across Mexico and Texas between 1940 and 2023. We apply constructed analogs with multiple linear regression models to quantify the contribution of different drivers to daily temperature anomalies during this heatwave. On the hottest day (20 June), circulation, soil moisture, and their interaction explained 3.82°C (90% CI: 2.72–4.91°C) of the 5.42°C observed anomaly with most of the residual attributed to the thermodynamic effects of long-term warming. Using CESM2-LENS2, we find that June 2023-like patterns are not projected to increase in frequency but will become 1.9°C hotter by the mid-21st century under SSP3-7.0. The hottest simulated day with these patterns could produce temperatures >50°C (122°F) across south Texas, representing a low-likelihood yet physically plausible worst-case scenario that could inform disaster preparedness and adaptation planning. © 2025. The Author(s). atmospheric circulation; attribution; constructed analogs; extreme heat; Mexico; Texas Mexico [North America]; Texas; United States; Miocene; %moisture; Atmospheric circulation; Attribution; Breakings; Constructed analog; Extreme heat; Heatwaves; Long lasting; Me-xico; Texas; atmospheric circulation; extreme event; heat wave; precipitation (climatology); soil moisture; temperature anomaly; Multiple linear regression";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
699;Early Warning Method for Abnormal Environmental Temperature and Humidity for Coastal Power Equipment Protection Missions;Addressing the issue of power equipment structural component failures induced by high temperature and humidity environments in coastal regions, an anomaly warning method for environmental temperature and humidity was proposed based on deep neural networks and trajectory clustering algorithms. The method aims to predict potential abnormal temperature and humidity conditions through environmental observation data, enabling early deployment of equipment protection measures. First, the method utilizes a bidirectional long short-term memory network (BiLSTM) to learn from historical temperature and humidity variation data, incorporating an attention mechanism to extract key features and provide high-precision forecasts of future trends in temperature and humidity. Subsequently, historical and predicted data are concatenated to form movement trajectories within a three-dimensional data space of temperature and humidity. Finally, the DBA clustering algorithm is applied to classify these trajectories into a known trajectory library, completing pre-inspection and identification of abnormal temperature and humidity conditions in the environment. Simulation experiments demonstrate that, in the test scenarios, the proposed network achieves an average prediction loss of 0.02% for environmental temperature and humidity data, representing an 83.78% improvement over traditional prediction networks. The method effectively distinguishes between normal and various abnormal data fluctuations, achieving the purpose of early warning. © 2024 IEEE.;"Abnormal warning; Deep learning; Power Equipment Protection; Trajectory clustering";"Environmental monitoring; Abnormal warning; Deep learning; Environmental humidities; Environmental temperature; Equipment protection; Power equipment; Power equipment protection; Temperature and humidities; Temperature conditions; Trajectory clustering; Deep neural networks";"Early Warning Method for Abnormal Environmental Temperature and Humidity for Coastal Power Equipment Protection Missions Addressing the issue of power equipment structural component failures induced by high temperature and humidity environments in coastal regions, an anomaly warning method for environmental temperature and humidity was proposed based on deep neural networks and trajectory clustering algorithms. The method aims to predict potential abnormal temperature and humidity conditions through environmental observation data, enabling early deployment of equipment protection measures. First, the method utilizes a bidirectional long short-term memory network (BiLSTM) to learn from historical temperature and humidity variation data, incorporating an attention mechanism to extract key features and provide high-precision forecasts of future trends in temperature and humidity. Subsequently, historical and predicted data are concatenated to form movement trajectories within a three-dimensional data space of temperature and humidity. Finally, the DBA clustering algorithm is applied to classify these trajectories into a known trajectory library, completing pre-inspection and identification of abnormal temperature and humidity conditions in the environment. Simulation experiments demonstrate that, in the test scenarios, the proposed network achieves an average prediction loss of 0.02% for environmental temperature and humidity data, representing an 83.78% improvement over traditional prediction networks. The method effectively distinguishes between normal and various abnormal data fluctuations, achieving the purpose of early warning. © 2024 IEEE. Abnormal warning; Deep learning; Power Equipment Protection; Trajectory clustering Environmental monitoring; Abnormal warning; Deep learning; Environmental humidities; Environmental temperature; Equipment protection; Power equipment; Power equipment protection; Temperature and humidities; Temperature conditions; Trajectory clustering; Deep neural networks";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
700;The associations of cold spells with human mortality in Shanghai, China;Current studies of climate change impacts on human health primarily focus on extreme heat. It is equally imperative to understand the substantial effects of extreme cold on human well-being. We defined 15 distinct cold spells based on various temperature percentiles and durations observed in Shanghai between 2016 and 2020. To estimate the association between cold spells and human mortality, we employed a quasi-Poisson regression with a distributed non-linear model (DLNM). We quantified the mortality burden attributed to cold spells. Our observations showed that the relative risks of cold spells on mortality varied depending on cold spell definitions and sub-population groups. People were more susceptible to cold spells that spanned at least two consecutive days with a daily mean temperature falling below the 5th percentile of temperature records. The prolonged lag effect of cold spells on mortality lasted for two weeks. Cold spells positively associated with total non-accidental deaths and cardiovascular disease. Sub-population groups, including females, the elderly, and individuals with low education, exhibited heightened vulnerability to cold spells compared to their respective counterparts. Our findings are expected to provide guidance for the development of an early warning system aimed at mitigating the adverse impacts of cold spells on human mortality. © 2024 Elsevier B.V.;"Attributable fraction; Cold spell; Distributed lag non-linear model; Human mortality; Relative risks";NULL;"The associations of cold spells with human mortality in Shanghai, China Current studies of climate change impacts on human health primarily focus on extreme heat. It is equally imperative to understand the substantial effects of extreme cold on human well-being. We defined 15 distinct cold spells based on various temperature percentiles and durations observed in Shanghai between 2016 and 2020. To estimate the association between cold spells and human mortality, we employed a quasi-Poisson regression with a distributed non-linear model (DLNM). We quantified the mortality burden attributed to cold spells. Our observations showed that the relative risks of cold spells on mortality varied depending on cold spell definitions and sub-population groups. People were more susceptible to cold spells that spanned at least two consecutive days with a daily mean temperature falling below the 5th percentile of temperature records. The prolonged lag effect of cold spells on mortality lasted for two weeks. Cold spells positively associated with total non-accidental deaths and cardiovascular disease. Sub-population groups, including females, the elderly, and individuals with low education, exhibited heightened vulnerability to cold spells compared to their respective counterparts. Our findings are expected to provide guidance for the development of an early warning system aimed at mitigating the adverse impacts of cold spells on human mortality. © 2024 Elsevier B.V. Attributable fraction; Cold spell; Distributed lag non-linear model; Human mortality; Relative risks NULL";1;Supervised Learning;Ranking,Learning to Rank,Supervised Learning by Classification,Supervised Learning by Regression,Structured Outputs,Cost-sensitive Learning;1.3;Meteorological;2;Preparation
